{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"book/0%20-%20Preface/","title":"Preface","text":"<p>All the world is a nonlinear system</p> <p>He linearised to the right</p> <p>He linearised to the left</p> <p>Till nothing was right</p> <p>And nothing was left</p> <p>Stephen A. Billings</p>"},{"location":"book/0%20-%20Preface/#nonlinear-system-identification-and-forecasting-theory-and-practice-with-sysidentpy","title":"Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy","text":"<p>Welcome to our companion book on System Identification! This book is a comprehensive approach to learning about dynamic models and forecasting. The main aim of this book is to describe a comprehensive set of algorithms for the identification, forecasting and analysis of nonlinear systems.</p> <p>Our book is specifically designed for those who are interested in learning system identification and forecasting.  We will guide you through the process step-by-step using Python and the SysIdentPy package. With SysIdentPy, you will be able to apply a range of techniques for modeling dynamic systems, making predictions, and exploring different design schemes for dynamic models, from polynomial to neural networks. This book is for graduates, postgraduates, researchers, and for all people from different research areas who have data and want to find models to understand their systems better.</p> <p>The research literature is filled with books and papers covering various aspects of nonlinear system identification, including NARMAX methods. In this book, our objective isn't to replicate all the numerous algorithm variations available. Instead, we want to show you how to model your data using those algorithms with SysIdentPy. We'll mention all the specific details and different versions of the algorithms in the book, so if you're more interested in the theoretical aspects, you can explore those ideas further. We aim to focus on the fundamental techniques, explaining them in straightforward language and showing how to use them in real-world situations. While there will be some math and technical details involved, the aim is to keep it as easy to understand as possible. In essence, this book aims to be a resource that readers from various fields can use to learn how to model dynamic nonlinear systems.</p> <p>The best part about our book is that it is open source material, meaning that it is freely available for anyone to use and contribute to. We hope this brings together people who share interest for system identification and forecasting techniques, from linear to nonlinear models.</p> <p>So, whether you're a student, researcher, data scientist or practitioner, we invite you to share your knowledge and contribute with us. Let\u2019s explore system identification and forecasting with SysIdentPy!</p> <p>To follow along with the Python examples in the book, you\u2019ll need to have some packages installed. We\u2019ll cover the main ones here and let you know if any additional packages are required as we proceed.</p> <pre><code>import sysidentpy\nimport pandas as pd\nimport numpy as np\nimport torch\nimport matplotlib\nimport scipy\n</code></pre>"},{"location":"book/0%20-%20Preface/#about-the-author","title":"About the Author","text":"<p>Wilson Rocha is the Head of Data Science at RD Sa\u00fade and the creator of the SysIdentPy library. He holds a degree in Electrical Engineering and a Master's in Systems Modeling and Control, both from Federal University of S\u00e3o Jo\u00e3o del-Rei (UFSJ), Brazil. Wilson began his journey in Machine Learning by developing soccer-playing robots and continues to advance his research in the fields of Multi-objective Nonlinear System Identification and Time Series Forecasting.</p> <p>Connect with Wilson Rocha through the following social networks:</p> <ul> <li>LinkedIn</li> <li>ResearchGate</li> <li>Discord</li> </ul>"},{"location":"book/0%20-%20Preface/#referencing-this-book","title":"Referencing This Book","text":"<p>If you find this book useful, please cite it as follows:</p> <pre><code>Lacerda Junior, W.R. (2024). *Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy*. Web version. https://sysidentpy.org\n</code></pre> <p>If you use SysIdentPy on your project, please drop me a line.</p> <p>If you use SysIdentPy on your scientific publication, we would appreciate citations to the following paper: - Lacerda et al., (2020). SysIdentPy: A Python package for System Identification using NARMAX models. Journal of Open Source Software, 5(54), 2384, https://doi.org/10.21105/joss.02384</p> <pre><code>@article{Lacerda2020,\n  doi = {10.21105/joss.02384},\n  url = {https://doi.org/10.21105/joss.02384},\n  year = {2020},\n  publisher = {The Open Journal},\n  volume = {5},\n  number = {54},\n  pages = {2384},\n  author = {Wilson Rocha Lacerda Junior and Luan Pascoal Costa da Andrade and Samuel Carlos Pessoa Oliveira and Samir Angelo Milani Martins},\n  title = {SysIdentPy: A Python package for System Identification using NARMAX models},\n  journal = {Journal of Open Source Software}\n}\n</code></pre>"},{"location":"book/0%20-%20Preface/#pdf-epub-and-mobi-version","title":"PDF, Epub and Mobi version","text":"<p>Download the pdf version of the book: pdf version</p> <p>Download the epub version of the book: epub version</p> <p>Download the mobi version of the book: mobi version</p>"},{"location":"book/0%20-%20Preface/#acknowledgments","title":"Acknowledgments","text":"<p>The System Identification class taught by Samir Martins  (in Portuguese) has been a great source of inspiration for this series. In this book, we will explore Dynamic Systems and learn how to master NARMAX models using Python and the SysIdentPy package. The Stephen A. Billings book, Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio - Temporal Domains, have been instrumental in showing us how powerful System Identification can be.</p> <p>In addition to these resources, we will also reference Luis Ant\u00f4nio Aguirre Introdu\u00e7\u00e3o \u00e0 Identifica\u00e7\u00e3o de Sistemas. T\u00e9cnicas Lineares e n\u00e3o Lineares Aplicadas a Sistemas. Teoria e Aplica\u00e7\u00e3o (in Portuguese), which has proven to be an invaluable tool in introducing complex dynamic modeling concepts in a straightforward way. As an open source material on System Identification and Forecasting, this book aims to provide a accessible yet rigorous approach to learning dynamic models and forecasting.</p>"},{"location":"book/0%20-%20Preface/#support-the-project","title":"Support the Project","text":"<p>The Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy is an extensive open-source resource dedicated to the science of System Identification. Our goal is to make this knowledge accessible to everyone, both financially and intellectually.</p> <p>If this book has been valuable to you, and you'd like to support our efforts, we welcome financial contributions through our Sponsor page.</p> <p>If you're not in a position to contribute financially, you can still support by helping us improve the book. We encourage you to report any typos, suggest edits, or provide feedback on sections that you found challenging. You can do this by visiting the book's repository and opening an issue. Additionally, if you enjoyed the content, please consider sharing it with others who might benefit from it, and give us a star on GitHub.</p> <p>Your support, in any form, helps us continue to enhance this project and maintain a high-quality resource for the community. Thank you for your contribution!</p>"},{"location":"book/0.1%20-%20Contents/","title":"Contents","text":"<p>Preface</p> <ol> <li>Introduction<ol> <li>Models</li> <li>System Identification</li> <li>Linear or Nonlinear System Identification<ol> <li>Linear Models</li> <li>Nonlinear Models</li> </ol> </li> <li>NARMAX Methods</li> <li>What is the Purpose of System Identification?</li> <li>Is System Identification Machine Learning?</li> <li>Nonlinear System Identification and Forecasting Applications: Case Studies</li> <li>Abbreviations</li> <li>Variables</li> <li>Book Organization</li> </ol> </li> <li>NARMAX Model Representation<ol> <li>Basis Function</li> <li>Linear Models<ol> <li>ARMAX</li> <li>ARX</li> <li>ARMA</li> <li>AR</li> <li>FIR</li> <li>Other Variants</li> </ol> </li> <li>Nonlinear Models<ol> <li>NARMAX</li> <li>NARMA</li> <li>NAR</li> <li>NFIR</li> <li>Mixed NARMAX Models</li> <li>Neural NARX Network</li> <li>General Model Set Representation</li> <li>MIMO Models</li> </ol> </li> </ol> </li> <li>Parameter Estimation<ol> <li>Least Squares</li> <li>Total Least Squares</li> <li>Recursive Least Squares</li> <li>Least Mean Squares</li> <li>Extended Least Squares Algorithms</li> </ol> </li> <li>Model Structure Selection<ol> <li>Introduction</li> <li>The Forward Regression Orthogonal Least Squares<ol> <li>Case Study</li> </ol> </li> <li>Information Criteria<ol> <li>Overview of the Information Criteria Methods<ol> <li>AIC</li> <li>AICc</li> <li>BIC</li> <li>LILC</li> <li>FPE</li> </ol> </li> </ol> </li> <li>Meta Model Structure Selection (MetaMSS)<ol> <li>Meta-heuristics</li> <li>Standard Particle Swarm Optimization (PSO)</li> <li>Standard Gravitational Search Algorithm (GSA)</li> <li>The Binary Hybrid Optimization Algorithm</li> <li>Meta-Model Structure Selection (MetaMSS): Building NARX for Regression</li> <li>Case Studies: Simulation Results</li> <li>MetaMSS vs FROLS</li> <li>Meta-MSS vs RJMCMC</li> <li>MetaMSS algorithm using SysIdentPy</li> </ol> </li> <li>Accelerated Orthogonal Least Squares</li> <li>Entropic Regression</li> </ol> </li> <li>Multiobjective Parameter Estimation<ol> <li>Introduction</li> <li>Multi-objective optimization problem</li> <li>Pareto Optimal Definition and Pareto Dominance</li> <li>Affine Information Least Squares Algorithm</li> <li>Case Study - Buck converter</li> </ol> </li> <li>Multiobjective Model Structure Selection<ol> <li>Introduction</li> <li>Multiobjective Error Reduction Ratio</li> <li>Multiobjective Meta Model Structure Selection</li> <li>Case Studies</li> <li>References</li> </ol> </li> <li>NARX Neural Network<ol> <li>Introduction</li> <li>NARX Neural Network</li> <li>NARX Neural Network vs. Recursive Neural Network</li> <li>Case Studies</li> <li>References</li> </ol> </li> <li>Severely Nonlinear Systems<ol> <li>Introduction</li> <li>Modeling Hysteresis With Polynomial NARX Model</li> <li>Continuous-time loading-unloading quasi-static signal</li> <li>Hysteresis loops in continuous time \\(\\mathcal{H}_t(\\omega)\\)</li> <li>Rate Independent Hysteresis  in polynomial NARX model</li> </ol> </li> <li>Validation<ol> <li>The <code>predict</code> Method in SysIdentPy</li> <li>Infinity-Step-Ahead Prediction</li> <li>One-step Ahead Prediction</li> <li>n-step Ahead Prediction</li> <li>Model Performance</li> <li>Metrics Available in SysIdentPy</li> <li>Case study</li> </ol> </li> <li>Case Studies: System Identification and Forecasting<ol> <li>M4 Dataset</li> <li>Coupled Eletric Device</li> <li>Wiener-Hammerstein</li> <li>Air Passenger Demand Forecasting</li> <li>System With Hysteresis - Modeling a Magneto-rheological Damper Device</li> <li>Silver box</li> <li>F-16 Ground Vibration Test Benchmark</li> <li>PV Forecasting</li> <li>Industrial Robot Identification Benchmark (coming soon)</li> <li>Two-Story Frame with Hysteretic Links (coming soon)</li> <li>Cortical Responses Evoked by Wrist Joint Manipulation (coming soon)</li> <li>Total quarterly beer production in Australia (coming soon)</li> <li>Australian Domestic Tourism Demand (coming soon)</li> <li>Electric Power Consumption (coming soon)</li> <li>Gas Rate CO2 (coming soon)</li> <li>Number of Patients Seen With Influenza-like Illness (coming soon)</li> <li>Monthly Sales of Heaters and Ice Cream (coming soon)</li> <li>Monthly Production of Milk (coming soon)</li> <li>Half-hourly Electricity Demand in England and Wales (coming soon)</li> <li>Daily Temperature in Melbourne (coming soon)</li> <li>Weekly U.S. Product Supplied of Finished Motor Gasoline (coming soon)</li> <li>Australian Total Wine Sales (coming soon)</li> <li>Quarterly Production of Woollen Yarn in Australia (coming soon)</li> <li>Hourly Nuclear Energy Generation (coming soon)</li> </ol> </li> </ol>"},{"location":"book/1%20-%20Introduction/","title":"Introduction","text":"<p>The concept of a mathematical model is fundamental in many fields of science. From engineering to sociology, models plays a central role to the study of complex systems as they allow to simulate what will happen in different scenarios and conditions, predict its output for a given input, analyse its properties and explore different design schemes. To accomplish these goals, however, it is crucial that the model is a proper representation of the system under study. The modeling of dynamic and steady-state behaviors is, therefore, fundamental to this type of analysis and depends on System Identification (SI) procedures.</p>"},{"location":"book/1%20-%20Introduction/#models","title":"Models","text":"<p>Mathematical modeling is a great way to understand and analyze different parts of our world. It gives us a clear framework to make sense of complex systems and how they behave. Whether it\u2019s for everyday tasks or big-picture issues like disease control, models are a key part of how we deal with various challenges.</p> <p>Typing efficiently on a conventional QWERTY keyboard layout is the result of a well-learned model of the QWERTY keyboard embedded in the individual cognitive processes. However, if you are faced with a different keyboard layout, such as the Dvorak or AZERTY, you will probably struggle to adapt to the new model. The system changed, so you will have to update you model.</p> <p></p> <p>QWERTY - Wikipedia - ANSI\u00a0QWERTY\u00a0keyboard layout\u00a0(US)</p> <p></p> <p>AZERTY layout used on a keyboard</p> <p>Mathematical modeling touches on many parts of our lives. Whether we're looking at economic trends, tracking how diseases spread, or figuring out consumer behavior, models are essential tools for gaining knowledge, making informed decisions, and take control over complex systems.</p> <p>In essence, mathematical models help us make sense of the world. They let us understand human behavior and the systems we deal with every day. By using these models, we can learn, adapt, and adjust our strategies to keep up with the changes around us.</p>"},{"location":"book/1%20-%20Introduction/#system-identification","title":"System Identification","text":"<p>System identification is a data-driven framework to model dynamical systems. Initially, scientists focused on linear system identification, but this has been changing over the past decades with more emphasis in nonlinear systems. Nonlinear system identification is widely considered to be one of the most important topics concerning the modeling of many different dynamical systems, from time-series to severally nonlinear dynamic behaviors.</p> <p>Extensive resources, including excellent textbooks covering linear system identification and time series forecasting are readily available. In this book, we revisit some known topics, but we also try to approach such subjects in a different and complementary way. We will explore the modeling of nonlinear dynamic systems  using NARMAX(Nonlinear AutoRegressive Moving Average model with eXogenous inputs) methods, which were introduced by [Stephen A. Billings] in 1981.</p>"},{"location":"book/1%20-%20Introduction/#linear-or-nonlinear-system-identification","title":"Linear or Nonlinear System Identification","text":""},{"location":"book/1%20-%20Introduction/#linear-models","title":"Linear Models","text":"<p>While most real world systems are nonlinear, you probably should give linear models a try first. Linear models usually serves as a strong baseline and can be good enough for your case, giving satisfactory performance. Astron and Murray and Glad and Ljung showed that many nonlinear systems can be well described by locally linear models. Besides, linear models are easy to fit, easy to interpret, and requires less computational resources than nonlinear models, allowing you to experiment fast and gather insights before thinking about gray box models or complex nonlinear models.</p> <p>Linear models can be very useful, even in the presence of strong nonlinearities, because it is much easier to deal with it. Moreover, the development of linear identification algorithms is still a very active and healthy research field, with many papers being released every year Sai Li, Linjun Zhang, T. Tony Cai &amp; Hongzhe Li, Maria Jaenada, Leandro Pardo, Xing Liu;\u00a0Lin Qiu, Youtong Fang;\u00a0Kui Wang;\u00a0Yongdong Li, Jose Rodr\u00edguez, Alessandro D\u2019Innocenzo and Francesco Smarra. Linear models work well most of the time and should be the first choice for many applications. However, when dealing with complex systems where linear assumptions don\u2019t hold, nonlinear models become essential.</p>"},{"location":"book/1%20-%20Introduction/#nonlinear-models","title":"Nonlinear Models","text":"<p>When linear models do not perform well enough, you should consider nonlinear models. It's important to notice, however, that changing from a linear to a nonlinear model is not always a simple task. For inexperienced users, it's common to build nonlinear models that performs worse than the linear ones. To work with nonlinear models, you must consider that characteristics such structural errors, noise, operation point, excitation signals and many others aspects of your system under study impact your modelling approach and strategy.</p> <p>As suggested by Johan Schoukens and Lennart Ljung in \"Nonlinear System Identi\ufb01cation - A User-Oriented Roadmap\", only start working with nonlinear models if there is enough evidence that linear models will not solve the problem.</p> <p>Nonlinear models are more flexible than linear models and can be built using many different mathematical representations, such as polynomial, generalized additive, neural networks, wavelet and many more (Billings, S. A.). Such flexibility, however, makes nonlinear system identification much more complex the linear ones, from the experiment design to the model selection. The user should consider that, besides the modeling complexity, moving to nonlinear models will require a revision in the road-map and computational resources defined when dealing with the linear models. In this respect, always ask yourself whether the potential benefits of nonlinear models are worth the effort.</p>"},{"location":"book/1%20-%20Introduction/#narmax-methods","title":"NARMAX Methods","text":"<p>NARMAX model is one of the most frequently employed nonlinear model representation and is widely used to represent a broad class of nonlinear systems. NARMAX methods were successfully applied in many scenarios, which include industrial processes, control systems, structural systems, economic and financial systems, biology, medicine, social systems, and much more. The NARMAX model representation and the class of systems that can be represented by it will be discussed later in the book.</p> <p>The main steps involved to build NARMAX models are (Billings, 2013):</p> <ol> <li>Model Representation: define the model mathematical representation.</li> <li>Model Structure Selection: define which terms are in the final model.</li> <li>Parameter Estimation: estimate the coefficients of each model term selected in step 1.</li> <li>Model validation: to make sure the model is unbiased and accurate;</li> <li>Model Prediction/Simulation: predict future outputs or simulate the behavior of the system given different inputs.</li> <li>Analysis: understanding the dynamical properties of the system under study</li> <li>Control: develop control design schemes based on the obtained model.</li> </ol> <p>Model Structure Selection (MSS) is the most important aspect of NARMAX methods and the most complex. Selecting the model terms is fundamental if the goal of the identification is to obtain models that can reproduce the dynamics of the original system and impacts every other aspect of the identification process. Problems related to overparameterization and numerical ill-conditioning are typical because of the limitations the identification algorithms in selecting the appropriate terms that should compose the final model (L. A. Aguirre e S. A. Billings, L. Piroddi e W. Spinelli).</p> <p>In SysIdentPy, you are allowed to interact directly with every item described in the 7 steps, except for the control one. SysIdentPy focuses on modeling, not on control design. You'll have to use some of the code bellow in every modeling task using SysIdentPy. You'll learn the details along the book, so don't worry if you are not familiar with those methods yet.</p> <pre><code>from sysidentpy.basis_function import Polynomial\nfrom sysidentpy.neural_network import NARXNN\nfrom sysidentpy.general_estimators import NARX\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.parameter_estimation import RecursiveLeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.simulation import SimulateNARMAX\n</code></pre>"},{"location":"book/1%20-%20Introduction/#what-is-the-purpose-of-system-identification","title":"What is the Purpose of System Identification?","text":"<p>Because of the Model Structure Selection problem, Billings, S. A. states that the goal of System Identification using NARMAX  methods is twofold: performance and parsimony.</p> <p>The first goal is often about approximation. Here, the main focus is to build a model that make predictions with with the lowest error possible. This approach is common in applications like weather forecasting, demand forecasting, predicting stock prices, speech recognition, target tracking, and pattern classification. In these cases, the specific form of the model isn't as critical. In other words, how the terms interact (in parametric models), the mathematical representation, the static behavior and so on are not that important; what matters most is finding a way to minimize prediction errors.</p> <p>But system identification isn't just about minimizing prediction errors. One of the main goals of System Identification is to build models that help the user to understand and interpret the system being modeled. Beyond just making accurate predictions, the goal is to develop models that truly capture the dynamic behavior of the system being studied, ideally in the simplest form possible. Science and engineering are all about understanding systems by breaking down complex behaviors into simpler ones that we can understand and control. For example, if the system's behavior can be described by a simple first-order dynamic model with a cubic nonlinear term in the input, system identification should help uncover that.</p>"},{"location":"book/1%20-%20Introduction/#is-system-identification-machine-learning","title":"Is System Identification Machine Learning?","text":"<p>First, let's take an overview of static and dynamic systems. Imagine you have an electric guitar connected to an effect processor that can apply various audio effects, such as reverb or distortion. The effect is controlled by a switch that toggles between \"on\" and \"off\". Let\u2019s consider this from the perspective of signals. The input signal represents the state of the effect switch: switch off (low level), switch on (high level). If we represent the guitar signal, we have a binary condition: effect off (original guitar sound), effect on (modified guitar sound). This is an example of a static system: the output (guitar sound) directly follows the input (state of the effect switch).</p> <p>When the effect switch is off, the output is just the clean, unaltered guitar signal. When the effect switch is on, the output is the guitar signal with the effect applied, such as amplification or distortion. In this system, the effect being on or off directly influences the guitar signal without any delay or additional processing.</p> <p>This example illustrates how a static system operates with binary control inputs, where the output directly reflects the input state, providing a straightforward mapping between the control signal and the system\u2019s response.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import signal\n\nt = np.linspace(0, 30, 500, endpoint=False)\nu = signal.square(0.2*np.pi * t)\nu[u &lt; 0] = 0\n# In a static system, the output y directly follows the input u\ny = u\n\n# Plot the input and output\nplt.figure(figsize=(15, 3))\nplt.plot(t, u, label='Input (State of the Switch)', color=\"grey\", linewidth=10, alpha=0.5)\nplt.plot(t, y, label='Output (Static System Response)', color='k', linewidth=0.5)\nplt.title('Static System Response to the Input')\nplt.xlabel('Time [s]')\nplt.ylabel('y')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>Static response representation. The input signal representing the state of the switch (switch off (low level), switch on (high level)), and the static response: original sound (low level), processed sound (high level).</p> <p>Now, let\u2019s consider a dynamic system: using an air conditioner to lower the room temperature. This example effectively illustrates the concepts of dynamic systems and how their output responds over time.</p> <p>Let\u2019s imagine this from the perspective of signals. The input signal represents the state of the air conditioner\u2019s control: turning the air conditioner on (high level) or turning it off (low level). When the air conditioner is turned on, it begins to cool the room. However, the room temperature does not drop instantaneously to the desired cooler level. It takes time for the air conditioner to affect the temperature, and the rate at which the temperature decreases can vary based on factors like the room size and insulation.</p> <p>Conversely, when the air conditioner is turned off, the room temperature does not immediately return to its original ambient temperature. Instead, it gradually warms up as the cooling effect diminishes.</p> <p></p> <p>Using an air conditioner to lower the room temperature as dynamic system representation.</p> <p>In this dynamic system, the output (room temperature) does not instantly follow the input (state of the air conditioner) because there is a time lag involved in both cooling and warming processes. The system has memory, meaning the current room temperature depends not only on the current state of the air conditioner but also on how long it has been running or off, and how much it has already cooled or allowed the room to warm up.</p> <p>This example highlights the nature of dynamic systems: the response to an input is gradual and affected by the system\u2019s internal dynamics. The air conditioner\u2019s effect on the room temperature exemplifies how dynamic systems have a time-dependent response, where the output changes over time and does not immediately match the input signal.</p> <p>For static systems, the output is a direct function of the input, represented by an algebraic equation:</p> \\[ y(t) = G \\cdot u(t) \\] <p>For dynamic systems, the output depends on the input and the rate of change of the input, represented by a differential equation. For example, the output \\(y(t)\\) can be modeled as:</p> \\[ y(t) = G \\cdot u(t) - \\tau \\cdot \\frac{dy(t)}{dt} \\] <p>Here, \\(G\\) is the gain, and \\(\\tau\\) is a constant that incorporates the system's memory. For discrete-time systems, we consider signals at specific and spaced intervals. The differential equation is discretized, and the derivative is approximated by a finite difference:</p> \\[ y[k] = \\alpha y[k-1] + \\beta u[k] \\] <p>where \\(\\alpha\\) and \\(\\beta\\) are constants that determine the system's response. The z-transform can be used to obtain the transfer function in the z-domain.</p> <p>In summary, static systems are modeled by algebraic equations, while dynamic systems are modeled by differential equations.</p> <p>As Luis Antonio Aguirre states in one of his classes on Youtube (in Portuguese), all physical systems are dynamic, but depending on the time scale, they can be modeled as static for simplification. For example, the transition between the effects on the guitar sound, if taken in seconds (as we did in the example), could be treated as static depending on your analysis. However, the pedal board have components like capacitors, which are dynamic electrical components, making it a dynamic system. The response, however, is so fast that we dealt with it like a static system. Therefore, representing a system as static is a modeling decision.</p> <p>Table 1 shows how this field can be categorized with respect to linear/nonlinear and static/dynamic systems.</p> System Characteristics Linear Model Nonlinear Model Static Linear Regression Machine Learning Dynamic Linear System Identification Nonlinear System Identification &gt; Table 1: Naming conventions in the System Identification field. Adapted from Oliver Nelles"},{"location":"book/1%20-%20Introduction/#nonlinear-system-identification-and-forecasting-applications-case-studies","title":"Nonlinear System Identification and Forecasting Applications: Case Studies","text":"<p>There\u2019s a lot of research out there on nonlinear system identification, including NARMAX methods. However, there are a relatively small number of books and papers showing how to apply these methods to real-life systems Instead in a way that\u2019s easy to understand. Our goal with this book is to change that. We want to make these methods practical and accessible. While we\u2019ll cover the necessary math and algorithms, we\u2019ll keep things as clear and simple as possible, making it easier for readers from all backgrounds to learn how to model dynamic nonlinear systems using SysIdentPy.</p> <p>Therefore, this book aims to fill a gap in the existing literature. In Chapter 10, we present real-world case studies to show how NARMAX methods can be applied to a variety of complex systems. Whether it\u2019s modeling a highly nonlinear system like the Bouc-Wen model, modeling a dynamic behavior in a full-scale F-16 aircraft, or working with the M4 dataset for benchmarking, we\u2019ll guide you through building NARMAX models using SysIdentPy.</p> <p>The case studies we've selected come from a wide range of fields, not just the typical timeseries or industrial examples you might expect from traditional system identification or timeseries books. Our aim is to showcase the versatility of NARMAX algorithms and SysIdentPy and illustrate the kind of in-depth analysis you can achieve with these tools.</p>"},{"location":"book/1%20-%20Introduction/#abbreviations","title":"Abbreviations","text":"Abbreviation Full Name AIC Akaike Information Criterion AICC Corrected Akaike Information Criterion AOLS Accelerated Orthogonal Least Squares ANN Artificial Neural Network AR AutoRegressive ARMAX AutoRegressive Moving Average with eXogenous Input ARARX AutoRegressive AutoRegressive with eXogenous Input ARX AutoRegressive with eXogenous Input BIC Bayesian Information Criterion ELS Extended Least Squares ER Entropic Regression ERR Error Reduction Ratio FIR Finite Impulse Response FPE Final Prediction Error FROLS Forward Regression Orthogonal Least Squares GLS Generalized Least Squares LMS Least Mean Square LS Least Squares LSTM Long Short-Term Memory MA Moving Average MetaMSS Meta Model Structure Selection MIMO Multiple Input Multiple Output MISO Multiple Input Single Output MLP Multilayer Perceptron MSE Mean Squared Error MSS Model Structure Selection NARMAX Nonlinear AutoRegressive Moving Average with eXogenous Input NARX Nonlinear AutoRegressive with eXogenous Input NFIR Nonlinear Finite Impulse Response NIIR Nonlinear Infinite Impulse Response NLS Nonlinear Least Squares NN Neural Network OBF Orthonormal Basis Function OE Output Error OLS Orthogonal Least Squares RBF Radial Basis Function RELS Recursive Extended Least Squares RLS Recursive Least Squares RMSE Root Mean Squared Error SI System Identification SISO Single Input Single Output SVD Singular Value Decomposition WLS Weighted Least Squares"},{"location":"book/1%20-%20Introduction/#variables","title":"Variables","text":"Variable Name Description \\(f(\\cdot)\\) function to be approximated \\(k\\) discrete time \\(m\\) dynamic order \\(x\\) system inputs \\(y\\) system output \\(\\hat{y}\\) model predicted output \\(\\lambda\\) regularization strength \\(\\sigma\\) standard deviation \\(\\theta\\) parameter vector \\(N\\) number of data points \\(\\Psi(\\cdot)\\) Information Matrix \\(n_{m^r}\\) Number of potential regressors for MIMO models \\(\\mathcal{F}\\) Arbitrary mathematical representation \\(\\Omega_{y^p x^m}\\) Term cluster of polynomial NARX \\(\\ell\\) nonlinearity degree of the model \\(\\hat{\\Theta}\\) Estimated Parameter Vector \\(\\hat{y}_k\\) model predicted output at discrete time \\(k\\) \\(\\mathbf{X}_k\\) Column vector of multiple system inputs at discrete-time \\(k\\) \\(\\mathbf{Y}_k\\) Column vector of multiple system outputs at discrete-time \\(k\\) \\(\\mathcal{H}_t(\\omega)\\) Hysteresis loop of the system in continuous-time \\(\\mathcal{H}\\) Bounding structure that delimits the system hysteresis loop \\(\\rho\\) Tolerance value \\(\\sum_{y^p x^m}\\) Cluster coefficients of polynomial NARX \\(e_k\\) error vector at discrete-time \\(k\\) \\(n_r\\) Number of potential regressors for SISO models \\(n_x\\) maximum lag of the input regressor \\(n_y\\) maximum lag of the output regressor \\(n\\) number of observations in a sample \\(x_k\\) system input at discrete-time \\(k\\) \\(y_k\\) system output at discrete-time \\(k\\)"},{"location":"book/1%20-%20Introduction/#book-organization","title":"Book Organization","text":"<p>This book focuses on making concepts easy to understand, emphasizing clear explanations and practical connections between different methods. We avoid excessive formalism and complex equations, opting instead to illustrate core ideas with plenty of hands-on examples. Written with a System Identification perspective, the book offers practical implementation details throughout the chapters.</p> <p>The goals of this book are to help you:</p> <ul> <li>Understand the advantages, drawbacks, and areas of application of different NARMAX models and algorithms.</li> <li>Choose the right approach for your specific problem.</li> <li>Adjust all hyperparameters properly.</li> <li>Interpret and comprehend the obtained results.</li> <li>valuate the reliability and limitations of your models.</li> </ul> <p>Many chapters includes real-world examples and data, guiding you on how to apply these methods using SysIdentPy in practice.</p>"},{"location":"book/10%20-%20Case%20Studies/","title":"10. Case Studies","text":""},{"location":"book/10%20-%20Case%20Studies/#m4-dataset","title":"M4 Dataset","text":"<p>The M4 dataset is a well known resource for time series forecasting, offering a wide range of data series used to test and improve forecasting methods. Created for the M4 competition organized by Spyros Makridakis, this dataset has driven many advancements in forecasting techniques.</p> <p>The M4 dataset includes 100,000 time series from various fields such as demographics, finance, industry, macroeconomics, and microeconomics, which were selected randomly from the ForeDeCk database. The series come in different frequencies (yearly, quarterly, monthly, weekly, daily, and hourly), making it a comprehensive collection for testing forecasting methods.</p> <p>In this case study, we will focus on the hourly subset of the M4 dataset. This subset consists of time series data recorded hourly, providing a detailed and high-frequency look at changes over time. Hourly data presents unique challenges due to its granularity and the potential for capturing short-term fluctuations and patterns.</p> <p>The M4 dataset provides a standard benchmark to compare different forecasting methods, allowing researchers and practitioners to evaluate their models consistently. With series from various domains and frequencies, the M4 dataset represents real-world forecasting challenges, making it valuable for developing robust forecasting techniques. The competition and the dataset itself have led to the creation of new algorithms and methods, significantly improving forecasting accuracy and reliability.</p> <p>We will present a end to end walkthrough using the M4 hourly dataset to demonstrate the capabilities of SysIdentPy. SysIdentPy offers a range of tools and techniques designed to effectively handle the complexities of time series data, but we will focus on fast and easy setup for this case. We will cover model selection and evaluation metrics specific to the hourly dataset.</p> <p>By the end of this case study, you will have a solid understanding of how to use SysIdentPy for forecasting with the M4 hourly dataset, preparing you to tackle similar forecasting challenges in real-world scenarios.</p>"},{"location":"book/10%20-%20Case%20Studies/#required-packages-and-versions","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\ndatasetsforecast==0.0.8\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\ns3fs==2024.6.1\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"book/10%20-%20Case%20Studies/#sysidentpy-configuration","title":"SysIdentPy configuration","text":"<p>In this section, we will demonstrate the application of SysIdentPy to the Silver box dataset.  The following code will guide you through the process of loading the dataset, configuring the SysIdentPy parameters, and building a model for mentioned system.</p> <pre><code>import warnings\nimport numpy as np\nimport pandas as pd\nfrom pandas.errors import SettingWithCopyWarning\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS, AOLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error, symmetric_mean_absolute_percentage_error\nfrom sysidentpy.utils.plotting import plot_results\n\nfrom datasetsforecast.m4 import M4, M4Evaluation\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nwarnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n\ntrain = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv')\ntest = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv').rename(columns={'y': 'y_test'})\n</code></pre> <p>The following plots provide a visualization of the training data for a small subset of the time series. The plot shows the raw data, giving you an insight into the patterns and behaviors inherent in each series.</p> <p>By observing the data, you can get a sense of the variety and complexity of the time series we are working with. The plots can reveal important characteristics such as trends, seasonal patterns, and potential anomalies within the time series. Understanding these elements is crucial for the development of accurate forecasting models.</p> <p>However, when dealing with a large number of different time series, it is common to start with broad assumptions rather than detailed individual analysis. In this context, we will adopt a similar approach. Instead of going into the specifics of each dataset, we will make some general assumptions and see how SysIdentPy handles them.</p> <p>This approach provides a practical starting point, demonstrating how SysIdentPy can manage different types of time series data without too much work. As you become more familiar with the tool, you can refine your models with more detailed insights. For now, let's focus on using SysIdentPy to create the forecasts based on these initial assumptions.</p> <p>Our first assumption is that there is a 24-hour seasonal pattern in the series. By examining the plots below, this seems reasonable. Therefore, we'll begin building our models with <code>ylag=24</code>.</p> <pre><code>ax = train[train[\"unique_id\"]==\"H10\"].reset_index(drop=True)[\"y\"].plot(figsize=(15, 2), title=\"H10\")\nxcoords = [a for a in range(24, 24*30, 24)]\n\nfor xc in xcoords:\n\u00a0 \u00a0 plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5)\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p>Lets check build a model for the <code>H20</code> group before we extrapolate the settings for every group. Because there are no input features, we will be using a <code>NAR</code> model type in SysIdentPy. To keep things simple and fast, we will start with Polynomial basis function with degree \\(1\\).</p> <pre><code>unique_id = \"H20\"\ny_id = train[train[\"unique_id\"]==unique_id][\"y\"].values.reshape(-1, 1)\ny_val = test[test[\"unique_id\"]==unique_id][\"y_test\"].values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    order_selection=True,\n    ylag=24,\n    estimator=LeastSquares(),\n    basis_function=basis_function,\n    model_type=\"NAR\",\n)\n\nmodel.fit(y=y_id)\ny_val = np.concatenate([y_id[-model.max_lag :], y_val])\ny_hat = model.predict(y=y_val, forecast_horizon=48)\nsmape = symmetric_mean_absolute_percentage_error(y_val[model.max_lag::], y_hat[model.max_lag::])\n\nplot_results(y=y_val[model.max_lag :], yhat=y_hat[model.max_lag :], n=30000, figsize=(15, 4), title=f\"Group: {unique_id} - SMAPE {round(smape, 4)}\")\n</code></pre> <p></p> <p>Probably, the result are not optimal and will not work for every group. However, let's check how this setting performs against the winner model \u00a0M4 time series competition: the Exponential Smoothing with Recurrent Neural Networks (ESRNN).</p> <pre><code>esrnn_url = 'https://github.com/Nixtla/m4-forecasts/raw/master/forecasts/submission-118.zip'\nesrnn_forecasts = M4Evaluation.load_benchmark('data', 'Hourly', esrnn_url)\nesrnn_evaluation = M4Evaluation.evaluate('data', 'Hourly', esrnn_forecasts)\n\nesrnn_evaluation\n</code></pre> SMAPE MASE OWA Hourly 9.328 0.893 0.440 &gt; Table 1. ESRNN SOTA results <p>The following code took only 49 seconds to run on my machine (AMD Ryzen 5 5600x processor, 32GB RAM at 3600MHz). Because of its efficiency, I didn't create a parallel version. By the end of this use case, you will see how SysIdentPy can be both fast and effective, delivering good results without too much optimization.</p> <pre><code>r = []\nds_test = list(range(701, 749))\nfor u_id, data in train.groupby(by=[\"unique_id\"], observed=True):\n\u00a0 \u00a0 y_id = data[\"y\"].values.reshape(-1, 1)\n\u00a0 \u00a0 basis_function = Polynomial(degree=1)\n\u00a0 \u00a0 model = FROLS(\n        ylag=24,\n        estimator=LeastSquares(),\n        basis_function=basis_function,\n        model_type=\"NAR\",\n        n_info_values=25,\n    )\n\u00a0 \u00a0 try:\n\u00a0 \u00a0 \u00a0 \u00a0 model.fit(y=y_id)\n\u00a0 \u00a0 \u00a0 \u00a0 y_val = y_id[-model.max_lag :].reshape(-1, 1)\n\u00a0 \u00a0 \u00a0 \u00a0 y_hat = model.predict(y=y_val, forecast_horizon=48)\n\u00a0 \u00a0 \u00a0 \u00a0 r.append(\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 u_id*len(y_hat[model.max_lag::]),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ds_test,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 y_hat[model.max_lag::].ravel()\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\n\u00a0 \u00a0 \u00a0 \u00a0 )\n\u00a0 \u00a0 except Exception:\n\u00a0 \u00a0 \u00a0 \u00a0 print(f\"Problem with {u_id}\")\n\nresults_1 = pd.DataFrame(r, columns=[\"unique_id\", \"ds\", \"NARMAX_1\"]).explode(['unique_id', 'ds', 'NARMAX_1'])\nresults_1[\"NARMAX_1\"] = results_1[\"NARMAX_1\"].astype(float)#.clip(lower=10)\npivot_df = results_1.pivot(index='unique_id', columns='ds', values='NARMAX_1')\nresults = pivot_df.to_numpy()\n\nM4Evaluation.evaluate('data', 'Hourly', results)\n</code></pre> SMAPE MASE OWA Hourly 16.034196 0.958083 0.636132 Table 2. First test with SysIdentPy <p>The initial results are reasonable, but they don't quite match the performance of <code>ESRNN</code>. These results are based solely on our first assumption. To better understand the performance, let\u2019s examine the groups with the worst results.</p> <p></p> <p>The following plot illustrates two such groups, <code>H147</code> and <code>H136</code>. Both exhibit a 24-hour seasonal pattern.</p> <p></p> <p></p> <p>However, a closer look reveals an additional insight: in addition to the daily pattern, these series also show a weekly pattern. Observe how the data looks like when we split the series into weekly segments.</p> <p></p> <pre><code>xcoords = list(range(0, 168*5, 168))\nfiltered_train = train[train[\"unique_id\"] == \"H147\"].reset_index(drop=True)\n\nfig, ax = plt.subplots(figsize=(10, 1.5 * len(xcoords[1:])))\nfor i, start in enumerate(xcoords[:-1]):\n\u00a0 \u00a0 end = xcoords[i + 1]\n\u00a0 \u00a0 ax = fig.add_subplot(len(xcoords[1:]), 1, i + 1)\n\u00a0 \u00a0 filtered_train[\"y\"].iloc[start:end].plot(ax=ax)\n\u00a0 \u00a0 ax.set_title(f'H147 -&gt; Slice {i+1}: Hour {start} to {end-1}')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Therefore, we will build models setting <code>ylag=168</code>.</p> <p>Note that this is a very high number for lags, so be careful if you want to try it with higher polynomial degrees because the time to run the models can increase significantly. I tried some configurations with polynomial degree equal to 2 and only took \\(6\\) minutes to run (even less, using <code>AOLS</code>), without making the code run in parallel. As you can see, SysIdentPy can be very fast and you can make it faster by applying parallelization.</p> <pre><code># this took 2min to run on my computer.\nr = []\nds_test = list(range(701, 749))\nfor u_id, data in train.groupby(by=[\"unique_id\"], observed=True):\n\u00a0 \u00a0 y_id = data[\"y\"].values.reshape(-1, 1)\n\u00a0 \u00a0 basis_function = Polynomial(degree=1)\n\u00a0 \u00a0 model = FROLS(\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ylag=168,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_type=\"NAR\",\n\u00a0 \u00a0 \u00a0 \u00a0 )\n\u00a0 \u00a0 try:\n\u00a0 \u00a0 \u00a0 \u00a0 model.fit(y=y_id)\n\u00a0 \u00a0 \u00a0 \u00a0 y_val = y_id[-model.max_lag :].reshape(-1, 1)\n\u00a0 \u00a0 \u00a0 \u00a0 y_hat = model.predict(y=y_val, forecast_horizon=48)\n\u00a0 \u00a0 \u00a0 \u00a0 r.append(\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 u_id*len(y_hat[model.max_lag::]),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ds_test,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 y_hat[model.max_lag::].ravel()\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\n\u00a0 \u00a0 \u00a0 \u00a0 )\n\u00a0 \u00a0 except Exception:\n\u00a0 \u00a0 \u00a0 \u00a0 print(f\"Problema no id {u_id}\")\n\nresults_1 = pd.DataFrame(r, columns=[\"unique_id\", \"ds\", \"NARMAX_1\"]).explode(['unique_id', 'ds', 'NARMAX_1'])\nresults_1[\"NARMAX_1\"] = results_1[\"NARMAX_1\"].astype(float)#.clip(lower=10)\npivot_df = results_1.pivot(index='unique_id', columns='ds', values='NARMAX_1')\nresults = pivot_df.to_numpy()\nM4Evaluation.evaluate('data', 'Hourly', results)\n</code></pre> SMAPE MASE OWA Hourly 10.475998 0.773749 0.446471 &gt; Table 3. Improved results using SysIdentPy <p>Now, the results are much closer to those of the <code>ESRNN</code> model! While the Symmetric Mean Absolute Percentage Error (<code>SMAPE</code>) is slightly worse, the Mean Absolute Scaled Error (<code>MASE</code>) is better when comparing against <code>ESRNN</code>, leading to a very similar Overall Weighted Average (<code>OWA</code>) metric. Remarkably, these results are achieved using only simple <code>AR</code> models. Next, let's see if the <code>AOLS</code> method can provide even better results.</p> <pre><code>r = []\nds_test = list(range(701, 749))\nfor u_id, data in train.groupby(by=[\"unique_id\"], observed=True):\n\u00a0 \u00a0 y_id = data[\"y\"].values.reshape(-1, 1)\n\u00a0 \u00a0 basis_function = Polynomial(degree=1)\n\u00a0 \u00a0 model = AOLS(\n        ylag=168,\n        basis_function=basis_function,\n        model_type=\"NAR\",\n        # due to high lag settings, k was increased to 6 as an initial guess\n        k=6,\n    )\n\u00a0 \u00a0 try:\n\u00a0 \u00a0 \u00a0 \u00a0 model.fit(y=y_id)\n\u00a0 \u00a0 \u00a0 \u00a0 y_val = y_id[-model.max_lag :].reshape(-1, 1)\n\u00a0 \u00a0 \u00a0 \u00a0 y_hat = model.predict(y=y_val, forecast_horizon=48)\n\u00a0 \u00a0 \u00a0 \u00a0 r.append(\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 u_id*len(y_hat[model.max_lag::]),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ds_test,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 y_hat[model.max_lag::].ravel()\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\n\u00a0 \u00a0 \u00a0 \u00a0 )\n\u00a0 \u00a0 except Exception:\n\u00a0 \u00a0 \u00a0 \u00a0 print(f\"Problema no id {u_id}\")\n\nresults_1 = pd.DataFrame(r, columns=[\"unique_id\", \"ds\", \"NARMAX_1\"]).explode(['unique_id', 'ds', 'NARMAX_1'])\nresults_1[\"NARMAX_1\"] = results_1[\"NARMAX_1\"].astype(float)#.clip(lower=10)\npivot_df = results_1.pivot(index='unique_id', columns='ds', values='NARMAX_1')\nresults = pivot_df.to_numpy()\nM4Evaluation.evaluate('data', 'Hourly', results)\n</code></pre> SMAPE MASE OWA Hourly 9.951141 0.809965 0.439755 &gt; Table 4. SysIdentPy results using AOLS algorithm <p>The Overall Weighted Average (<code>OWA</code>) is even better than that of the <code>ESRNN</code> model! Additionally, the <code>AOLS</code> method was incredibly efficient, taking only 6 seconds to run. This combination of high performance and rapid execution makes <code>AOLS</code> a compelling alternative for time series forecasting in cases with multiple series.</p> <p>Before we finish, let's verify how the performance of the <code>H147</code> model has improved with the <code>ylag=168</code> setting.</p> <p></p> <p>Based on the M4 benchmark paper, we could also clip the predictions lower than 10 to 10 and the results would be slightly better. But this is left to the user.</p> <p>We could achieve even better performance with some fine-tuning of the model configuration. However, I\u2019ll leave exploring these alternative adjustments as an exercise for the user. However, keep in mind that experimenting with different settings does not always guarantee improved results. A deeper theoretical knowledge can often lead you to better configurations and, hence, better results.</p>"},{"location":"book/10%20-%20Case%20Studies/#coupled-eletric-device","title":"Coupled Eletric Device","text":"<p>The CE8 coupled electric drives dataset presents a compelling use case for demonstrating the performance of SysIdentPy. This system involves two electric motors driving a pulley with a flexible belt, creating a dynamic environment ideal for testing system identification tools.</p> <p>The nonlinear benchmark website stands as a significant contribution to the system identification and machine learning community. The users are encouraged to explore all the papers referenced on the site.</p>"},{"location":"book/10%20-%20Case%20Studies/#system-overview","title":"System Overview","text":"<p>The CE8 system, illustrated in Figure 1, features: - Two Electric Motors: These motors independently control the tension and speed of the belt, providing symmetrical control around zero. This enables both clockwise and counterclockwise movements. - Pulley Mechanism: The pulley is supported by a spring, introducing a lightly damped dynamic mode that adds complexity to the system. - Speed Control Focus: The primary focus is on the speed control system. The pulley\u2019s angular speed is measured using a pulse counter, which is insensitive to the direction of the velocity.</p> <p></p> <p>Figure 1. CE8 system design.</p>"},{"location":"book/10%20-%20Case%20Studies/#sensor-and-filtering","title":"Sensor and Filtering","text":"<p>The measurement process involves: - Pulse Counter: This sensor measures the angular speed of the pulley without regard to the direction. - Analogue Low Pass Filtering: This reduces high-frequency noise, followed by anti-aliasing filtering to prepare the signal for digital processing. The dynamic effects are mainly influenced by the electric drive time constants and the spring, with the low pass filtering having a minimal impact on the output.</p>"},{"location":"book/10%20-%20Case%20Studies/#sota-results","title":"SOTA Results","text":"<p>SysIdentPy can be used to build robust models for identifying and modeling the complex dynamics of the CE8 system. The performance will be compared against a benchmark provided by Max D. Champneys, Gerben I. Beintema, Roland T\u00f3th, Maarten Schoukens, and Timothy J. Rogers -\u00a0Baselines for Nonlinear Benchmarks,\u00a0Workshop on Nonlinear System Identification Benchmarks, 2024.</p> <p></p> <p>The benchmark evaluate the average metric between the two experiments. That's why the SOTA method do not have the better metric for <code>test 1</code>, but it is still the best overall.  The goal of this case study is not only to showcase the robustness of SysIdentPy but also provides valuable insights into its practical applications in real-world dynamic systems.</p>"},{"location":"book/10%20-%20Case%20Studies/#required-packages-and-versions_1","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nnonlinear_benchmarks==0.1.2\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"book/10%20-%20Case%20Studies/#sysidentpy-configuration_1","title":"SysIdentPy configuration","text":"<p>In this section, we will demonstrate the application of SysIdentPy to the CE8 coupled electric drives dataset. This example showcases the robust performance of SysIdentPy in modeling and identifying complex dynamic systems. The following code will guide you through the process of loading the dataset, configuring the SysIdentPy parameters, and building a model for CE8 system.</p> <p>This practical example will help users understand how to effectively utilize SysIdentPy for their own system identification tasks, leveraging its advanced features to handle the complexities of real-world dynamic systems. Let's dive into the code and explore the capabilities of SysIdentPy.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_mean_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\nimport nonlinear_benchmarks\n\ntrain_val, test = nonlinear_benchmarks.CED(atleast_2d=True)\ndata_train_1, data_train_2 = train_val\ndata_test_1, data_test_2 \u00a0 = test\n</code></pre> <p>We used the <code>nonlinear_benchmarks</code> package to load the data. The user is referred to the package documentation to check the details of how to use it.</p> <p>The following plot detail the training and testing data of both experiments. Here we are trying to get two models, one for each experiment, that have a better performance than the mentioned baselines.</p> <pre><code>plt.plot(data_train_1.u)\nplt.plot(data_train_1.y)\nplt.title(\"Experiment 1: training data\")\nplt.show()\n\nplt.plot(data_test_1.u)\nplt.plot(data_test_1.y)\nplt.title(\"Experiment 1: testing data\")\nplt.show()\n\nplt.plot(data_train_2.u)\nplt.plot(data_train_2.y)\nplt.title(\"Experiment 2: training data\")\nplt.show()\n\nplt.plot(data_test_2.u)\nplt.plot(data_test_2.y)\nplt.title(\"Experiment 2: testing data\")\nplt.show()\n</code></pre> <p></p> <p></p> <p></p> <p></p>"},{"location":"book/10%20-%20Case%20Studies/#results","title":"Results","text":"<p>First, we will set the exactly same configuration to built models for both experiments. We can have better models by optimizing the configurations individually, but we will start simple.</p> <p>A basic configuration of FROLS using a polynomial basis function with degree equal 2 is defined. The information criteria will be the default one, the <code>aic</code>. The <code>xlag</code> and <code>ylag</code> are set to \\(7\\) in this first example.</p> <p>Model for experiment 1:</p> <pre><code>y_train = data_train_1.y\ny_test = data_test_1.y\nx_train = data_train_1.u\nx_test = data_test_1.u\n\nn = data_test_1.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=7,\n\u00a0 \u00a0 ylag=7,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 info_criteria=\"aic\",\n\u00a0 \u00a0 n_info_values=120\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=f\"Free Run simulation. Model 1 -&gt; RMSE: {round(rmse, 4)}\")\n</code></pre> <p></p> <p>Model for experiment 2: <pre><code>y_train = data_train_2.y\ny_test = data_test_2.y\nx_train = data_train_2.u\nx_test = data_test_2.u\n\nn = data_test_2.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=7,\n\u00a0 \u00a0 ylag=7,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 info_criteria=\"aic\",\n\u00a0 \u00a0 n_info_values=120\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=f\"Free Run simulation. Model 2 -&gt; RMSE: {round(rmse, 4)}\")\n</code></pre></p> <p></p> <p>The first configuration for experiment 1 is already better than the LTI ARX, LTI SS, GRU, LSTM, MLP NARX, MLP FIR, OLSTM, and the SOTA models shown in the benchmark table. Better than 8 out 11 models shown in the benchmark. For experiment 2, its better than LTI ARX, LTI SS, GRU, RNN, LSTM, OLSTM, and pNARX (7 out 11). It's a good start, but let's check if the performance improves if we set a higher lag for both <code>xlag</code> and <code>ylag</code>.</p> <p>The average metric is \\((0.1131 + 0.1059)/2 = 0.1095\\), which is very good, but worse than the SOTA (\\(0.0945\\)). We will now increase the lags for <code>x</code> and <code>y</code> to check if we get a better model. Before increasing the lags, the information criteria is shown:</p> <p><pre><code>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> </p> <p>It can be observed that after 22 regressors, adding new regressors do not improve the model performance (considering the configuration defined for that model). Because we want to try models with higher lags and higher nonlinearity degree, the stopping criteria will be changed to <code>err_tol</code> instead of information criteria. This will made the algorithm runs considerably faster.</p> <pre><code># experiment 1\ny_train = data_train_1.y\ny_test = data_test_1.y\nx_train = data_train_1.u\nx_test = data_test_1.u\n\nn = data_test_1.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=14,\n\u00a0 \u00a0 ylag=14,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 err_tol=0.9996,\n\u00a0 \u00a0 n_terms=22,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\nprint(model.final_model.shape, model.err.sum())\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=f\"Free Run simulation. Model 1 -&gt; RMSE: {round(rmse, 4)}\")\n</code></pre> <p></p> <pre><code># experiment 2\ny_train = data_train_2.y\ny_test = data_test_2.y\nx_train = data_train_2.u\nx_test = data_test_2.u\n\nn = data_test_2.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=14,\n\u00a0 \u00a0 ylag=14,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 info_criteria=\"aicc\",\n\u00a0 \u00a0 err_tol=0.9996,\n\u00a0 \u00a0 n_terms=22,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=f\"Free Run simulation. Model 2 -&gt; RMSE: {round(rmse, 4)}\")\n</code></pre> <p></p> <p>In the first experiment, the model showed a slight improvement, while the performance of the second experiment experienced a minor decline. Increasing the lag settings with these configurations did not result in significant changes. Therefore, let's set the polynomial degree to \\(3\\) and increase the number of terms to build the model to <code>n_terms=40</code> if the <code>err_tol</code> is not reached. It's important to note that these values are chosen empirically. We could also adjust the parameter estimation technique, the <code>err_tol</code>, the model structure selection algorithm, and the basis function, among other factors. Users are encouraged to employ hyperparameter tuning techniques to find the optimal combinations of hyperparameters.</p> <pre><code># experiment 1\ny_train = data_train_1.y\ny_test = data_test_1.y\nx_train = data_train_1.u\nx_test = data_test_1.u\n\nn = data_test_1.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=14,\n\u00a0 \u00a0 ylag=14,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 err_tol=0.9996,\n\u00a0 \u00a0 n_terms=40,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\nprint(model.final_model.shape, model.err.sum())\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=f\"Free Run simulation. Model 1 -&gt; RMSE: {round(rmse, 4)}\")\n</code></pre> <p></p> <pre><code># experiment 2\ny_train = data_train_2.y\ny_test = data_test_2.y\nx_train = data_train_2.u\nx_test = data_test_2.u\n\nn = data_test_2.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=14,\n\u00a0 \u00a0 ylag=14,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 info_criteria=\"aicc\",\n\u00a0 \u00a0 err_tol=0.9996,\n\u00a0 \u00a0 n_terms=40,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=f\"Free Run simulation. Model 2 -&gt; RMSE: {round(rmse, 4)}\")\n</code></pre> <p></p> <p>As shown in the plot, we have surpassed the state-of-the-art (SOTA) results with an average metric of \\((0.0969 + 0.0731)/2 = 0.0849\\). Additionally, the metric for the first experiment matches the best model in the benchmark, and the metric for the second experiment slightly exceeds the benchmark's best model. Using the same configuration for both models, we achieved the best overall results!</p>"},{"location":"book/10%20-%20Case%20Studies/#wiener-hammerstein","title":"Wiener-Hammerstein","text":"<p>The description content primarily derives from the benchmark website and associated paper. For a detailed description, readers are referred to the linked references.</p> <p>The nonlinear benchmark website stands as a significant contribution to the system identification and machine learning community. The users are encouraged to explore all the papers referenced on the site.</p> <p>This benchmark focuses on a Wiener-Hammerstein electronic circuit where process noise plays a significant role in distorting the output signal.</p> <p>The Wiener-Hammerstein structure is a well-known block-oriented system which contains a static nonlinearity sandwiched between two Linear Time-Invariant (LTI) blocks (Figure 2). This arrangement presents a challenging identification problem due to the presence of these LTI blocks.</p> <p></p> <p>Figure 2: the Wiener-Hammerstein system</p> <p>In Figure 2, the Wiener-Hammerstein system is illustrated with process noise \\(e_x(t)\\) entering before the static nonlinearity \\(f(x)\\), sandwiched between LTI blocks represented by \\(R(s)\\) and \\(S(s)\\) at the input and output, respectively. Additionally, small, negligible noise sources \\(e_u(t)\\) and \\(e_y(t)\\) affect the measurement channels. The measured input and output signals are denoted as \\(u_m(t)\\) and \\(y_m(t)\\).</p> <p>The first LTI block \\(R(s)\\) is effectively modeled as a third-order lowpass filter. The second LTI subsystem \\(S(s)\\) is configured as an inverse Chebyshev filter with a stop-band attenuation of \\(40 dB\\) and a cutoff frequency of \\(5 kHz\\). Notably, \\(S(s)\\) includes a transmission zero within the operational frequency range, complicating its inversion.</p> <p>The static nonlinearity \\(f(x)\\) is implemented using a diode-resistor network, resulting in saturation nonlinearity. Process noise \\(e_x(t)\\) is introduced as filtered white Gaussian noise, generated from a discrete-time third-order lowpass Butterworth filter followed by zero-order hold and analog low-pass reconstruction filtering with a cutoff of \\(20 kHz\\).</p> <p>Measurement noise sources \\(e_u(t)\\) and \\(e_y(t)\\) are minimal compared to \\(e_x(t)\\). The system's inputs and process noise are generated using an Arbitrary Waveform Generator (AWG), specifically the Agilent/HP E1445A, sampling at \\(78125 Hz\\), synchronized with an acquisition system (Agilent/HP E1430A) to ensure phase coherence and prevent leakage errors. Buffering between the acquisition cards and the system's inputs and outputs minimizes measurement equipment distortion.</p> <p>The benchmark provides two standard test signals through the benchmarking website: a random phase multisine and a sine-sweep signal. Both signals have an \\(rms\\) value of \\(0.71 Vrms\\) and cover frequencies from DC to \\(15 kHz\\) (excluding DC). The sine-sweep spans this frequency range at a rate of \\(4.29 MHz/min\\). These test sets serve as targets for evaluating the model's performance, emphasizing accurate representation under varied conditions.</p> <p>The Wiener-Hammerstein benchmark highlights three primary nonlinear system identification challenges:</p> <ol> <li>Process Noise: Significant in the system, influencing output fidelity.</li> <li>Static Nonlinearity: Indirectly accessible from measured data, posing identification challenges.</li> <li>Output Dynamics: Complex inversion due to transmission zero presence in \\(S(s)\\).</li> </ol> <p>The goal of this benchmark is to develop and validate robust models using separate estimation data, ensuring accurate characterization of the Wiener-Hammerstein system's behavior.</p>"},{"location":"book/10%20-%20Case%20Studies/#required-packages-and-versions_2","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nnonlinear_benchmarks==0.1.2\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"book/10%20-%20Case%20Studies/#sysidentpy-configuration_2","title":"SysIdentPy configuration","text":"<p>In this section, we will demonstrate the application of SysIdentPy to the Wiener-Hammerstein system dataset.  The following code will guide you through the process of loading the dataset, configuring the SysIdentPy parameters, and building a model for Wiener-Hammerstein system.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS, AOLS, MetaMSS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.parameter_estimation import LeastSquares, BoundedVariableLeastSquares, NonNegativeLeastSquares, LeastSquaresMinimalResidual\n\nfrom sysidentpy.metrics import root_mean_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\nimport nonlinear_benchmarks\n\ntrain_val, test = nonlinear_benchmarks.WienerHammerBenchMark(atleast_2d=True)\nx_train, y_train = train_val\nx_test, y_test = test\n</code></pre> <p>We used the <code>nonlinear_benchmarks</code> package to load the data. The user is referred to the package documentation to check the details of how to use it.</p> <p>The following plot detail the training and testing data of the experiment.</p> <pre><code>plot_n = 800\n\nplt.figure(figsize=(15, 4))\nplt.plot(x_train[:plot_n])\nplt.plot(y_train[:plot_n])\nplt.title(\"Experiment: training data\")\nplt.legend([\"x_train\", \"y_train\"])\nplt.show()\n\nplt.figure(figsize=(15, 4))\nplt.plot(x_test[:plot_n])\nplt.plot(y_test[:plot_n])\nplt.title(\"Experiment: testing data\")\nplt.legend([\"x_test\", \"y_test\"])\nplt.show()\n</code></pre> <p></p> <p></p> <p>The goal of this benchmark it to get a model that have a better performance than the SOTA model provided in the benchmarking paper.</p> <p></p> <p>State of the art results presented in the benchmarking paper. In this section we are only working with the Wiener-Hammerstein results, which are presented in the \\(W-H\\)  column.</p>"},{"location":"book/10%20-%20Case%20Studies/#results_1","title":"Results","text":"<p>We will start with a basic configuration of FROLS using a polynomial basis function with degree equal 2. The <code>xlag</code> and <code>ylag</code> are set to \\(7\\) in this first example. Because the dataset is considerably large, we will start with <code>n_info_values=50</code>. This means the FROLS algorithm will not include all regressors when calculating the information criteria used to determine the model order. While this approach might result in a sub-optimal model, it is a reasonable starting point for our first attempt.</p> <pre><code>n = test.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=7,\n\u00a0 \u00a0 ylag=7,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(unbiased=False),\n\u00a0 \u00a0 n_info_values=50,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nrmse_sota = rmse/y_test.std()\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=1000, title=f\"SysIdentPy -&gt; RMSE: {round(rmse, 4)}, NRMSE: {round(rmse_sota, 4)}\")\n</code></pre> <p></p> <p>The first configuration is already better than the SOTA models shown in the benchmark table! We started using <code>xlag=ylag=7</code> to have an idea of how well SysIdentPy would handle this dataset, but the results are pretty good already! However, the benchmarking paper indicates  that they used higher lags for their models. Let's check what happens if we set <code>xlag=ylag=10</code>.</p> <pre><code>x_train, y_train = train_val\nx_test, y_test = test\n\nn = test.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=10,\n\u00a0 \u00a0 ylag=10,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(unbiased=False),\n\u00a0 \u00a0 n_info_values=50,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nrmse_sota = rmse/y_test.std()\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=1000, title=f\"SysIdentPy -&gt; RMSE: {round(rmse, 4)}, NRMSE: {round(rmse_sota, 4)}\")\n</code></pre> <p></p> <p>The performance is even better now! For now, we are not worried about the model complexity (even in this case where we are comparing to a deep state neural network...). However, if we check the model order and the <code>AIC</code> plot, we see that the model have 50 regressors , but the <code>AIC</code> values do not change much after each added regression.</p> <pre><code>plt.plot(model.info_values)\n</code></pre> <p></p> <p>So, what happens if we set a model with half of the regressors?</p> <pre><code>x_train, y_train = train_val\nx_test, y_test = test\n\nn = test.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=10,\n\u00a0 \u00a0 ylag=10,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(unbiased=False),\n\u00a0 \u00a0 n_info_values=50,\n\u00a0 \u00a0 n_terms=25,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nrmse_sota = rmse/y_test.std()\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=1000, title=f\"SysIdentPy -&gt; RMSE: {round(rmse, 4)}, NRMSE: {round(rmse_sota, 4)}\")\n</code></pre> <p></p> <p>As shown in the figure above, the results still outperform the SOTA models presented in the benchmarking paper. The SOTA results from the paper could likely be improved as well. Users are encouraged to explore the deepsysid package, which can be used to build deep state neural networks.</p> <p>This basic configuration can serve as a starting point for users to develop even better models using SysIdentPy. Give it a try!</p>"},{"location":"book/10%20-%20Case%20Studies/#air-passenger-demand-forecasting-a-benchmarking","title":"Air Passenger Demand Forecasting - A Benchmarking","text":"<p>In this case study, we explore the capabilities of SysIdentPy by applying it to the Air Passenger dataset, a classic time series dataset widely used for evaluating time series forecasting methods. The primary goal of this analysis is to demonstrate that SysIdentPy can serve as a strong alternative for time series modeling, rather than to assert that one library is superior to another.</p>"},{"location":"book/10%20-%20Case%20Studies/#dataset-overview","title":"Dataset Overview","text":"<p>The Air Passenger dataset consists of monthly totals of international airline passengers from 1949 to 1960. This dataset is characterized by its strong seasonal patterns, trend components, and variability, making it an ideal benchmark for evaluating various time series forecasting methods. Specifically, the dataset includes:</p> <ul> <li>Total Monthly Passengers: The number of passengers (in thousands) for each month.</li> <li>Time Period: From January 1949 to December 1960, providing 144 data points.</li> </ul> <p>The dataset exhibits clear seasonal fluctuations and a trend, which poses a significant challenge for forecasting methods. It serves as a well-known benchmark for assessing the performance of different time series models due to its inherent complexity and well-documented behavior.</p>"},{"location":"book/10%20-%20Case%20Studies/#comparison-with-other-libraries","title":"Comparison with Other Libraries","text":"<p>We will compare the performance of SysIdentPy with other popular time series modeling libraries, focusing on the following tools:</p> <ul> <li>sktime: An extensive library for time series analysis in Python, offering various modeling techniques. For this case study, we will use:</li> <li><code>AutoARIMA</code>: Automatically selects the best ARIMA model based on the data.</li> <li><code>BATS</code> (Bayesian Structural Time Series): A model that captures complex seasonal patterns and trends.</li> <li><code>TBATS</code> (Trigonometric, Box-Cox, ARMA, Trend, and Seasonal): A model designed to handle multiple seasonal patterns.</li> <li><code>Exponential Smoothing</code>: A method that applies weighted averages to forecast future values.</li> <li><code>Prophet</code>: Developed by Facebook, it is particularly effective for capturing seasonality and holiday effects.</li> <li> <p><code>AutoETS</code> (Automatic Exponential Smoothing): Selects the best exponential smoothing model for the data.</p> </li> <li> <p>SysIdentPy: A library designed for system identification and time series modeling. We will focus on:</p> </li> <li><code>MetaMSS</code> (Meta-heuristic Model Structure Selection): Uses metaheuristic algorithms to select the best model structure.</li> <li><code>AOLS</code> (Accelerated Orthogonal Least Squares): A method for selecting relevant regressors in a model.</li> <li><code>FROLS</code> (Forward Regression with Orthogonal Least Squares, using polynomial base functions): A regression technique for model structure selection with polynomial terms.</li> <li><code>NARXNN</code> (Nonlinear Auto-Regressive model with Exogenous Inputs using Neural Networks): A flexible method for modeling nonlinear time series with external inputs.</li> </ul>"},{"location":"book/10%20-%20Case%20Studies/#objective","title":"Objective","text":"<p>The objective of this case study is to evaluate and compare the performance of these methods on the Air Passenger dataset. We aim to assess how well each library handles the complex seasonal and trend components of the data and to showcase SysIdentPy as a viable option for time series forecasting.</p>"},{"location":"book/10%20-%20Case%20Studies/#required-packages-and-versions_3","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npystan==2.19.1.1\nholidays==0.11.2\nfbprophet==0.7.1\nneuralprophet==0.2.7\npandas==1.3.2\nnumpy==1.23.3\nmatplotlib==3.8.4\npmdarima==1.8.3\nscikit-learn==0.24.2\nscipy==1.9.1\nsktime==0.8.0\nstatsmodels==0.12.2\ntbats==1.1.0\ntorch==1.12.1\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions. This practice isolates your project\u2019s dependencies and prevents version conflicts with other projects or system-wide packages. Additionally, be aware that some packages, such as <code>sktime</code> and <code>neuralprophet</code>, may install several dependencies automatically during their installation. Setting up a virtual environment helps manage these dependencies more effectively and keeps your project environment clean and reproducible.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul> <p>Let's begin by importing the necessary packages and setting up the environment for this analysis.</p> <pre><code>from warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy.signal.signaltools\n\ndef _centered(arr, newsize):\n\u00a0 \u00a0 # Return the center newsize portion of the array.\n\u00a0 \u00a0 # this is needed due a conflict error using the versions of the packages defined\n\u00a0 \u00a0 # for this example\n\u00a0 \u00a0 newsize = np.asarray(newsize)\n\u00a0 \u00a0 currsize = np.array(arr.shape)\n\u00a0 \u00a0 startind = (currsize - newsize) // 2\n\u00a0 \u00a0 endind = startind + newsize\n\u00a0 \u00a0 myslice = [slice(startind[k], endind[k]) for k in range(len(endind))]\n\u00a0 \u00a0 return arr[tuple(myslice)]\n\n\nscipy.signal.signaltools._centered = _centered\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.model_structure_selection import AOLS\nfrom sysidentpy.model_structure_selection import MetaMSS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\nfrom torch import nn\nfrom sysidentpy.neural_network import NARXNN\n\nfrom sktime.datasets import load_airline\nfrom sktime.forecasting.ets import AutoETS\nfrom sktime.forecasting.arima import ARIMA, AutoARIMA\nfrom sktime.forecasting.base import ForecastingHorizon\nfrom sktime.forecasting.exp_smoothing import ExponentialSmoothing\nfrom sktime.forecasting.fbprophet import Prophet\nfrom sktime.forecasting.tbats import TBATS\nfrom sktime.forecasting.bats import BATS\nfrom sktime.forecasting.model_selection import temporal_train_test_split\nfrom sktime.performance_metrics.forecasting import mean_squared_error\nfrom sktime.utils.plotting import plot_series\n\nfrom neuralprophet import NeuralProphet\nfrom neuralprophet import set_random_seed\n\nsimplefilter(\"ignore\", FutureWarning)\nnp.seterr(all=\"ignore\")\n%matplotlib inline\nloss = mean_squared_error\n</code></pre> <p>We use the <code>sktime</code> method to load the data. Besides, 23 samples is used as test data, following the definitions in the <code>sktime</code> examples.</p> <pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23) \u00a0# 23 samples for testing\nplot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"])\nfh = ForecastingHorizon(y_test.index, is_relative=False)\nprint(y_train.shape[0], y_test.shape[0])\n</code></pre> <p>The following image shows the data of the system to be modeled.</p> <p></p>"},{"location":"book/10%20-%20Case%20Studies/#results_2","title":"Results","text":"<p>Because we have several different models to test, the results are summarized in the following table. The user you will see that no hyperparameter tuning was made for SysIdentPy model. The idea here is to show how simple it can be to build good models in SysIdentPy.</p> No. Package Mean Squared Error 1 SysIdentPy (Neural Model) 316.54 2 SysIdentPy (MetaMSS) 450.99 3 SysIdentPy (AOLS) 476.64 4 NeuralProphet 501.24 5 SysIdentPy (FROLS) 805.95 6 Exponential Smoothing 910.52 7 Prophet 1186.00 8 AutoArima 1714.47 9 Manual Arima 2085.42 10 ETS 2590.05 11 BATS 7286.64 12 TBATS 7448.43"},{"location":"book/10%20-%20Case%20Studies/#sysidentpy-frols","title":"SysIdentPy: FROLS","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\nbasis_function = Polynomial(degree=1)\nsysidentpy = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 ylag=13, \u00a0# the lags for all models will be 13\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 model_type=\"NAR\",\n)\n\nsysidentpy.fit(y=y_train)\ny_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])\nyhat = sysidentpy.predict(y=y_test, forecast_horizon=23)\nfrols_loss = loss(\n\u00a0 \u00a0 pd.Series(y_test.flatten()[sysidentpy.max_lag :]),\n\u00a0 \u00a0 pd.Series(yhat.flatten()[sysidentpy.max_lag :]),\n)\nprint(frols_loss)\nplot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :])\n&gt;&gt;&gt; 805.95\n</code></pre>"},{"location":"book/10%20-%20Case%20Studies/#sysidentpy-aols","title":"SysIdentPy: AOLS","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\ndf_train, df_test = temporal_train_test_split(y, test_size=23)\ndf_train = df_train.reset_index()\ndf_train.columns = [\"ds\", \"y\"]\ndf_train[\"ds\"] = pd.to_datetime(df_train[\"ds\"].astype(str))\ndf_test = df_test.reset_index()\ndf_test.columns = [\"ds\", \"y\"]\ndf_test[\"ds\"] = pd.to_datetime(df_test[\"ds\"].astype(str))\n\nsysidentpy_AOLS = AOLS(\n\u00a0 \u00a0 ylag=13, k=2, L=1, model_type=\"NAR\", basis_function=basis_function\n)\n\nsysidentpy_AOLS.fit(y=y_train)\ny_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])\nyhat = sysidentpy_AOLS.predict(y=y_test, steps_ahead=None, forecast_horizon=23)\n\naols_loss = loss(\n\u00a0 \u00a0 pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),\n\u00a0 \u00a0 pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]),\n)\n\nprint(aols_loss)\nplot_results(y=y_test[sysidentpy_AOLS.max_lag :], yhat=yhat[sysidentpy_AOLS.max_lag :])\n&gt;&gt;&gt; 476.64\n</code></pre>"},{"location":"book/10%20-%20Case%20Studies/#sysidentpy-metamss","title":"SysIdentPy: MetaMSS","text":"<pre><code>set_random_seed(42)\ny = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\n\nsysidentpy_metamss = MetaMSS(\n\u00a0 \u00a0 basis_function=basis_function, ylag=13, model_type=\"NAR\", test_size=0.17\n)\n\nsysidentpy_metamss.fit(y=y_train)\n\ny_test = np.concatenate([y_train[-sysidentpy_metamss.max_lag :], y_test])\nyhat = sysidentpy_metamss.predict(y=y_test, steps_ahead=None, forecast_horizon=23)\n\nmetamss_loss = loss(\n\u00a0 \u00a0 pd.Series(y_test.flatten()[sysidentpy_metamss.max_lag :]),\n\u00a0 \u00a0 pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]),\n)\n\nprint(metamss_loss)\nplot_results(\n\u00a0 \u00a0 y=y_test[sysidentpy_metamss.max_lag :], yhat=yhat[sysidentpy_metamss.max_lag :]\n)\n\n&gt;&gt;&gt; 450.99\n</code></pre>"},{"location":"book/10%20-%20Case%20Studies/#sysidentpy-neural-narx","title":"SysIdentPy: Neural NARX","text":"<p>The network architecture is just the same as the one used in to show how to build a Neural NARX model in SysIdentPy docs.</p> <pre><code>import torch\ntorch.manual_seed(42)\n\ny = load_airline()\n# the split here will use 36 as test size just because the network will use the first values as initial conditions. It could be done like the others methods by concatenating the values\ny_train, y_test = temporal_train_test_split(y, test_size=36)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\nx_train = np.zeros_like(y_train)\nx_test = np.zeros_like(y_test)\n\nclass NARX(nn.Module):\n\u00a0 \u00a0 def __init__(self):\n\u00a0 \u00a0 \u00a0 \u00a0 super().__init__()\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin = nn.Linear(13, 20)\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin2 = nn.Linear(20, 20)\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin3 = nn.Linear(20, 20)\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin4 = nn.Linear(20, 1)\n\u00a0 \u00a0 \u00a0 \u00a0 self.relu = nn.ReLU()\n\n\n\u00a0 \u00a0 def forward(self, xb):\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin(xb)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.relu(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin2(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.relu(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin3(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.relu(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin4(z)\n\u00a0 \u00a0 \u00a0 \u00a0 return z\n\nnarx_net = NARXNN(\n\u00a0 \u00a0 net=NARX(),\n\u00a0 \u00a0 ylag=13,\n\u00a0 \u00a0 model_type=\"NAR\",\n\u00a0 \u00a0 basis_function=Polynomial(degree=1),\n\u00a0 \u00a0 epochs=900,\n\u00a0 \u00a0 verbose=False,\n\u00a0 \u00a0 learning_rate=2.5e-02,\n\u00a0 \u00a0 optim_params={}, \u00a0# optional parameters of the optimizer\n)\n\nnarx_net.fit(y=y_train)\nyhat = narx_net.predict(y=y_test, forecast_horizon=23)\n\nnarxnet_loss = loss(\n\u00a0 \u00a0 pd.Series(y_test.flatten()[narx_net.max_lag :]),\n\u00a0 \u00a0 pd.Series(yhat.flatten()[narx_net.max_lag :]),\n)\n\nprint(narxnet_loss)\nplot_results(y=y_test[narx_net.max_lag :], yhat=yhat[narx_net.max_lag :])\n</code></pre> <p></p>"},{"location":"book/10%20-%20Case%20Studies/#sktime-models","title":"sktime models","text":"<p>The following models are the ones available in the sktime package.</p> <pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23) \u00a0# 23 samples for testing\nplot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"])\nfh = ForecastingHorizon(y_test.index, is_relative=False)\n</code></pre>"},{"location":"book/10%20-%20Case%20Studies/#sktime-exponential-smoothing","title":"sktime: Exponential Smoothing","text":"<pre><code>es = ExponentialSmoothing(trend=\"add\", seasonal=\"multiplicative\", sp=12)\ny = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nes.fit(y_train)\ny_pred_es = es.predict(fh)\nplot_series(y_test, y_pred_es, labels=[\"y_test\", \"y_pred\"])\nes_loss = loss(y_test, y_pred_es)\nes_loss\n&gt;&gt;&gt; 910.46\n</code></pre>"},{"location":"book/10%20-%20Case%20Studies/#sktime-autoets","title":"sktime: AutoETS","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nets = AutoETS(auto=True, sp=12, n_jobs=-1)\nets.fit(y_train)\ny_pred_ets = ets.predict(fh)\nplot_series(y_test, y_pred_ets, labels=[\"y_test\", \"y_pred\"])\nets_loss = loss(y_test, y_pred_ets)\nets_loss\n&gt;&gt;&gt; 1739.11\n</code></pre>"},{"location":"book/10%20-%20Case%20Studies/#sktime-autoarima","title":"sktime: AutoArima","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\n\nauto_arima = AutoARIMA(sp=12, suppress_warnings=True)\nauto_arima.fit(y_train)\ny_pred_auto_arima = auto_arima.predict(fh)\nplot_series(y_test, y_pred_auto_arima, labels=[\"y_test\", \"y_pred\"])\nautoarima_loss = loss(y_test, y_pred_auto_arima)\nautoarima_loss\n&gt;&gt;&gt; 1714.47\n</code></pre>"},{"location":"book/10%20-%20Case%20Studies/#sktime-arima","title":"sktime: Arima","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nmanual_arima = ARIMA(\n\u00a0 \u00a0 order=(13, 1, 0), suppress_warnings=True\n) \u00a0# seasonal_order=(0, 1, 0, 12)\nmanual_arima.fit(y_train)\ny_pred_manual_arima = manual_arima.predict(fh)\nplot_series(y_test, y_pred_manual_arima, labels=[\"y_test\", \"y_pred\"])\nmanualarima_loss = loss(y_test, y_pred_manual_arima)\nmanualarima_loss\n&gt;&gt;&gt; 2085.42\n</code></pre>"},{"location":"book/10%20-%20Case%20Studies/#sktime-bats","title":"sktime: BATS","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nbats = BATS(sp=12, use_trend=True, use_box_cox=False)\nbats.fit(y_train)\ny_pred_bats = bats.predict(fh)\nplot_series(y_test, y_pred_bats, labels=[\"y_test\", \"y_pred\"])\nbats_loss = loss(y_test, y_pred_bats)\nbats_loss\n&gt;&gt;&gt; 7286.64\n</code></pre>"},{"location":"book/10%20-%20Case%20Studies/#sktime-tbats","title":"sktime: TBATS","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ntbats = TBATS(sp=12, use_trend=True, use_box_cox=False)\ntbats.fit(y_train)\ny_pred_tbats = tbats.predict(fh)\nplot_series(y_test, y_pred_tbats, labels=[\"y_test\", \"y_pred\"])\ntbats_loss = loss(y_test, y_pred_tbats)\ntbats_loss\n&gt;&gt;&gt; 7448.43\n</code></pre>"},{"location":"book/10%20-%20Case%20Studies/#sktime-prophet","title":"sktime: Prophet","text":"<pre><code>set_random_seed(42)\ny = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nz = y.copy()\nz = z.to_timestamp(freq=\"M\")\nz_train, z_test = temporal_train_test_split(z, test_size=23)\nprophet = Prophet(\n\u00a0 \u00a0 seasonality_mode=\"multiplicative\",\n\u00a0 \u00a0 n_changepoints=int(len(y_train) / 12),\n\u00a0 \u00a0 add_country_holidays={\"country_name\": \"Germany\"},\n\u00a0 \u00a0 yearly_seasonality=True,\n\u00a0 \u00a0 weekly_seasonality=False,\n\u00a0 \u00a0 daily_seasonality=False,\n)\nprophet.fit(z_train)\ny_pred_prophet = prophet.predict(fh.to_relative(cutoff=y_train.index[-1]))\ny_pred_prophet.index = y_test.index\nplot_series(y_test, y_pred_prophet, labels=[\"y_test\", \"y_pred\"])\nprophet_loss = loss(y_test, y_pred_prophet)\nprophet_loss\n&gt;&gt;&gt; 1186.00\n</code></pre>"},{"location":"book/10%20-%20Case%20Studies/#neural-prophet","title":"Neural Prophet","text":"<pre><code>set_random_seed(42)\ndf = pd.read_csv(r\".\\datasets\\air_passengers.csv\")\nm = NeuralProphet(seasonality_mode=\"multiplicative\")\ndf_train = df.iloc[:-23, :].copy()\ndf_test = df.iloc[-23:, :].copy()\n\nm = NeuralProphet(seasonality_mode=\"multiplicative\")\nmetrics = m.fit(df_train, freq=\"MS\")\nfuture = m.make_future_dataframe(\n\u00a0 \u00a0 df_train, periods=23, n_historic_predictions=len(df_train)\n)\nforecast = m.predict(future)\nplt.plot(forecast[\"yhat1\"].values[-23:])\nplt.plot(df_test[\"y\"].values)\nneuralprophet_loss = loss(forecast[\"yhat1\"].values[-23:], df_test[\"y\"].values)\nneuralprophet_loss\n&gt;&gt;&gt; 501.24\n</code></pre> <p>The final results can be summarized as follows, resulting in the table presented in the beginning of this case study:</p> <pre><code>results = {\n\u00a0 \u00a0 \"Exponential Smoothing\": es_loss,\n\u00a0 \u00a0 \"ETS\": ets_loss,\n\u00a0 \u00a0 \"AutoArima\": autoarima_loss,\n\u00a0 \u00a0 \"Manual Arima\": manualarima_loss,\n\u00a0 \u00a0 \"BATS\": bats_loss,\n\u00a0 \u00a0 \"TBATS\": tbats_loss,\n\u00a0 \u00a0 \"Prophet\": prophet_loss,\n\u00a0 \u00a0 \"SysIdentPy (Polynomial Model)\": frols_loss,\n\u00a0 \u00a0 \"SysIdentPy (Neural Model)\": narxnet_loss,\n\u00a0 \u00a0 \"SysIdentPy (AOLS)\": aols_loss,\n\u00a0 \u00a0 \"SysIdentPy (MetaMSS)\": metamss_loss,\n\u00a0 \u00a0 \"NeuralProphet\": neuralprophet_loss,\n}\nsorted(results.items(), key=lambda result: result[1])\n</code></pre>"},{"location":"book/10%20-%20Case%20Studies/#system-with-hysteresis-modeling-a-magneto-rheological-damper-device","title":"System With Hysteresis - Modeling a Magneto-rheological Damper Device","text":"<p>The memory effects between quasi-static input and output make the modeling of hysteretic systems very difficult. Physics-based models are often used to describe the hysteresis loops, but these models usually lack the simplicity and efficiency required in practical applications involving system characterization, identification, and control. As detailed in Martins, S. A. M. and Aguirre, L. A., NARX models have proven to be a feasible choice to describe the hysteresis loops. See Chapter 8 for a detailed background. However, even considering the sufficient conditions for rate independent hysteresis representation, classical structure selection algorithms fails to return a model with decent performance and the user needs to set a multi-valued function to ensure the occurrence of the bounding structure \\(\\mathcal{H}\\) (Martins, S. A. M. and Aguirre, L. A.).</p> <p>Even though some progress has been made, previous work has been limited to models with a single equilibrium point. The present case study aims to present new prospects in the model structure selection of hysteretic systems regarding the cases where the models have multiple inputs and it is not restricted concerning the number of equilibrium points. For that, the MetaMSS algorithm will be used to build a model for a magneto-rheological damper (MRD) considering the mentioned sufficient conditions.</p>"},{"location":"book/10%20-%20Case%20Studies/#a-brief-description-of-the-bouc-wen-model-of-magneto-rheological-damper-device","title":"A Brief description of the Bouc-Wen model of magneto-rheological damper device","text":"<p>The data used in this study-case is the Bouc-Wen model (Bouc, R, Wen, Y. X.) of an MRD whose schematic diagram is shown in the figure below.</p> <p></p> <p>The model for a magneto-rheological damper proposed by Spencer, B. F. and Sain, M. K..</p> <p>The general form of the Bouc-Wen model can be described as (Spencer, B. F. and Sain, M. K.):</p> \\[ \\begin{equation} \\dfrac{dz}{dt} = g\\left[x,z,sign\\left(\\dfrac{dx}{dt}\\right)\\right]\\dfrac{dx}{dt}, \\end{equation} \\] <p>where \\(z\\) is the hysteretic model output, \\(x\\) the input and \\(g[\\cdot]\\) a nonlinear function of \\(x\\), \\(z\\) and \\(sign (dx/dt)\\). Spencer, B. F. and Sain, M. K. proposed the following phenomenological model for the aforementioned device:</p> \\[ \\begin{align} f&amp;= c_1\\dot{\\rho}+k_1(x-x_0),\\nonumber\\\\ \\dot{\\rho}&amp;=\\dfrac{1}{c_0+c_1}[\\alpha z+c_0\\dot{x}+k_0(x-\\rho)],\\nonumber\\\\ \\dot{z}&amp;=-\\gamma|\\dot{x}-\\dot{\\rho}|z|z|^{n-1}-\\beta(\\dot{x}-\\dot{\\rho})|z|^n+A(\\dot{x}-\\dot{\\rho}),\\nonumber\\\\ \\alpha&amp;=\\alpha_a+\\alpha_bu_{bw},\\nonumber\\\\ c_1&amp;=c_{1a}+c_{1b}u_{bw},\\nonumber\\\\ c_0&amp;=c_{0a}+c_{0b}u_{bw},\\nonumber\\\\ \\dot{u}_{bw}&amp;=-\\eta(u_{bw}-E). \\end{align} \\] <p>where \\(f\\) is the damping force, \\(c_1\\) and \\(c_0\\) represent the viscous coefficients, \\(E\\) is the input voltage, \\(x\\) is the displacement and \\(\\dot{x}\\) is the velocity of the model. The parameters of the system (see table below) were taken from Leva, A. and Piroddi, L..</p> Parameter Value Parameter Value \\(c_{0_a}\\) \\(20.2 \\, N \\, s/cm\\) \\(\\alpha_{a}\\) \\(44.9 \\, N/cm\\) \\(c_{0_b}\\) \\(2.68 \\, N \\, s/cm \\, V\\) \\(\\alpha_{b}\\) \\(638 \\, N/cm\\) \\(c_{1_a}\\) \\(350 \\, N \\, s/cm\\) \\(\\gamma\\) \\(39.3 \\, cm^{-2}\\) \\(c_{1_b}\\) \\(70.7 \\, N \\, s/cm \\, V\\) \\(\\beta\\) \\(39.3 \\, cm^{-2}\\) \\(k_{0}\\) \\(15 \\, N/cm\\) \\(n\\) \\(2\\) \\(k_{1}\\) \\(5.37 \\, N/cm\\) \\(\\eta\\) \\(251 \\, s^{-1}\\) \\(x_{0}\\) \\(0 \\, cm\\) \\(A\\) \\(47.2\\) <p>For this particular study, both displacement and voltage inputs, \\(x\\) and \\(E\\), respectively, were generated by filtering a white Gaussian noise sequence using a Blackman-Harris FIR filter with \\(6\\)Hz cutoff frequency. The integration step-size was set to \\(h = 0.002\\), following the procedures described in Martins, S. A. M. and Aguirre, L. A.. These procedures are for identification purposes only since the inputs of a MRD could have several different characteristics.</p> <p>The data used in this example is provided by the Professor Samir Angelo Milani Martins.</p> <p>The challenges are:</p> <ul> <li>it possesses a nonlinearity featuring memory, i.e. a dynamic nonlinearity;</li> <li>the nonlinearity is governed by an internal variable z(t), which is not measurable;</li> <li>the nonlinear functional form in the Bouc Wen equation is nonlinear in the parameter;</li> <li>the nonlinear functional form in the Bouc Wen equation does not admit a finite Taylor series expansion because of the presence of absolute values</li> </ul>"},{"location":"book/10%20-%20Case%20Studies/#required-packages-and-versions_4","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nscikit-learn==1.4.2\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"book/10%20-%20Case%20Studies/#sysidentpy-configuration_3","title":"SysIdentPy Configuration","text":"<pre><code>import numpy as np\nfrom sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\ndf = pd.read_csv(\"boucwen_histeretic_system.csv\")\nscaler_x = MaxAbsScaler()\nscaler_y = MaxAbsScaler()\n\ninit = 400\nx_train = df[[\"E\", \"v\"]].iloc[init:df.shape[0]//2, :]\nx_train[\"sign_v\"] = np.sign(df[\"v\"])\nx_train = scaler_x.fit_transform(x_train)\n\nx_test = df[[\"E\", \"v\"]].iloc[df.shape[0]//2 + 1:df.shape[0] - init, :]\nx_test[\"sign_v\"] = np.sign(df[\"v\"])\nx_test = scaler_x.transform(x_test)\n\ny_train = df[[\"f\"]].iloc[init:df.shape[0]//2, :].values.reshape(-1, 1)\ny_train = scaler_y.fit_transform(y_train)\n\ny_test = df[[\"f\"]].iloc[df.shape[0]//2 + 1:df.shape[0] - init, :].values.reshape(-1, 1)\ny_test = scaler_y.transform(y_test)\n\n# Plotting the data\nplt.figure(figsize=(10, 8))\nplt.suptitle('Identification (training) data', fontsize=16)\n\nplt.subplot(221)\nplt.plot(y_train, 'k')\nplt.ylabel('Force - Output')\nplt.xlabel('Samples')\nplt.title('y')\nplt.grid()\nplt.axis([0, 1500, -1.5, 1.5])\n\nplt.subplot(222)\nplt.plot(x_train[:, 0], 'k')\nplt.ylabel('Control Voltage')\nplt.xlabel('Samples')\nplt.title('x_1')\nplt.grid()\nplt.axis([0, 1500, 0, 1])\n\nplt.subplot(223)\nplt.plot(x_train[:, 1], 'k')\nplt.ylabel('Velocity')\nplt.xlabel('Samples')\nplt.title('x_2')\nplt.grid()\nplt.axis([0, 1500, -1.5, 1.5])\n\nplt.subplot(224)\nplt.plot(x_train[:, 2], 'k')\nplt.ylabel('sign(Velocity)')\nplt.xlabel('Samples')\nplt.title('x_3')\nplt.grid()\nplt.axis([0, 1500, -1.5, 1.5])\n\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n</code></pre> <p>Let's check how is the histeretic behavior considering each input: <pre><code>plt.plot(x_train[:, 0], y_train)\nplt.xlabel(\"x1 - Voltage\")\nplt.ylabel(\"y - Force\")\n</code></pre></p> <p></p> <pre><code>plt.plot(x_train[:, 1], y_train)\nplt.xlabel(\"x2 - Velocity\")\nplt.ylabel(\"y - Force\")\n</code></pre> <p></p> <pre><code>plt.plot(x_train[:, 2], y_train)\nplt.xlabel(\"u3 - sign(Velocity)\")\nplt.ylabel(\"y - Force\")\n</code></pre> <p></p> <p>Now, we can just build a NARX model:</p> <pre><code>basis_function = Polynomial(degree=3)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=[[1], [1], [1]],\n\u00a0 \u00a0 ylag=1,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 info_criteria=\"aic\",\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=\"FROLS: sign(v) and MaxAbsScaler\")\n&gt;&gt;&gt; 0.0450\n</code></pre> <p></p> <p>If we remove the <code>sign(v)</code> input and try to build a NARX model using the same configuration, the model diverge, as can be seen in the following figure:</p> <pre><code>basis_function = Polynomial(degree=3)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=[[1], [1]],\n\u00a0 \u00a0 ylag=1,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 info_criteria=\"aic\",\n)\n\nmodel.fit(X=x_train[:, :2], y=y_train)\nyhat = model.predict(X=x_test[:, :2], y=y_test[:model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=\"FROLS: MaxAbsScaler, discarding sign(v)\")\n&gt;&gt;&gt; nan\n</code></pre> <p></p> <p>If we use the <code>MetaMSS</code> algorithm instead, the results are better.</p> <pre><code>from sysidentpy.model_structure_selection import MetaMSS\n\nbasis_function = Polynomial(degree=3)\nmodel = MetaMSS(\n\u00a0 \u00a0 xlag=[[1], [1]],\n\u00a0 \u00a0 ylag=1,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 random_state=42,\n)\n\nmodel.fit(X=x_train[:, :2], y=y_train)\nyhat = model.predict(X=x_test[:, :2], y=y_test[:model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=\"MetaMSS: MaxAbsScaler, discarding sign(v)\")\n&gt;&gt;&gt; 0.24\n</code></pre> <p></p> <p>However, when the output of the system reach its minimum value, the model oscillate</p> <pre><code>plot_results(y=y_test[1100 : 1200], yhat=yhat[1100 : 1200], n=10000, title=\"Unstable region\")\n</code></pre> <p></p> <p>If we add the <code>sign(v)</code> input again and use <code>MetaMSS</code>, the results are very close to the <code>FROLS</code> algorithm with all inputs</p> <pre><code>basis_function = Polynomial(degree=3)\nmodel = MetaMSS(\n\u00a0 \u00a0 xlag=[[1], [1], [1]],\n\u00a0 \u00a0 ylag=1,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 random_state=42,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=\"MetaMSS: sign(v) and MaxAbsScaler\")\n&gt;&gt;&gt; 0.0554\n</code></pre> <p></p> <p>This case will also highlight the significance of data scaling. Previously, we used the <code>MaxAbsScaler</code> method, which resulted in great models when using the <code>sign(v)</code> inputs, but also resulted in unstable models when removing that input feature. When scaling is applied using <code>MinMaxScaler</code>, however, the overall stability of the results improves, and the model does not diverge, even when the <code>sign(v)</code> input is removed, using the <code>FROLS</code> algorithm.</p> <p>The user can get the results bellow by just changing the data scaling method using</p> <pre><code>scaler_x = MinMaxScaler()\nscaler_y = MinMaxScaler()\n</code></pre> <p>and running the each model again. That is the only change to improve the results.</p> <p></p> <p>FROLS: with <code>sign(v)</code> and <code>MinMaxScaler</code>. RMSE: 0.1159</p> <p> FROLS: discarding <code>sign(v)</code> and using <code>MinMaxScaler</code>. RMSE: 0.1639</p> <p></p> <p>MetaMSS: discarding <code>sign(v)</code> and using <code>MinMaxScaler</code>. RMSE: 0.1762</p> <p></p> <p>MetaMSS: including <code>sign(v)</code> and using <code>MinMaxScaler</code>. RMSE: 0.0694</p> <p>In contrast, the MetaMSS method returned the best model overall, but not better than the best <code>FROLS</code> method using <code>MaxAbsScaler</code>.</p> <p>Here is the predicted histeretic loop: <pre><code>plt.plot(x_test[:, 1], yhat)\n</code></pre></p> <p></p>"},{"location":"book/10%20-%20Case%20Studies/#silver-box","title":"Silver box","text":"<p>The description content mainly derives (copy and paste) from the associated paper. For a detailed description, readers are referred to the linked reference.</p> <p>The Silverbox system can be seen as an electronic implementation of the Duffing oscillator. It is build as a 2<sup>nd</sup> order linear time-invariant system with a 3<sup>rd</sup> degree polynomial static nonlinearity around it in feedback. This type of dynamics are, for instance, often encountered in mechanical systems. Nonlinear Benchmark</p> <p>In this case study, we will create a NARX model for the Silver box benchmark. The Silver box represents a simplified version of mechanical oscillating processes, which are a critical category of nonlinear dynamic systems. Examples include vehicle suspensions, where shock absorbers and progressive springs play vital roles. The data generated by the Silver box provides a simplified representation of such combined components. The electrical circuit generating this data closely approximates, but does not perfectly match, the idealized models described below.</p> <p>As described in the original paper, the system was excited using a general waveform generator (HPE1445A). The input signal begins as a discrete-time signal \\(r(k)\\), which is converted to an analog signal \\(r_c(t)\\) using zero-order-hold reconstruction. The actual excitation signal \\(u_0(t)\\) is then obtained by passing \\(r_c(t)\\) through an analog low-pass filter \\(G(p)\\) to eliminate high-frequency content around multiples of the sampling frequency. Here, \\(p\\) denotes the differentiation operator. Thus, the input is given by:</p> \\[ u_0(t) = G(p) r_c(t). \\] <p>The input and output signals were measured using HP1430A data acquisition cards, with synchronized clocks for the acquisition and generator cards. The sampling frequency was:</p> \\[ f_s = \\frac{10^7}{2^{14}} = 610.35 \\, \\text{Hz}. \\] <p>The silver box uses analog electrical circuitry to generate data representing a nonlinear mechanical resonating system with a moving mass \\(m\\), viscous damping \\(d\\), and a nonlinear spring \\(k(y)\\). The electrical circuit is designed to relate the displacement \\(y(t)\\) (the output) to the force \\(u(t)\\) (the input) by the following differential equation:</p> \\[ m \\frac{d^2 y(t)}{dt^2} + d \\frac{d y(t)}{dt} + k(y(t)) y(t) = u(t). \\] <p>The nonlinear progressive spring is described by a static, position-dependent stiffness:</p> \\[ k(y(t)) = a + b y^2(t). \\] <p>The signal-to-noise ratio is sufficiently high to model the system without accounting for measurement noise. However, measurement noise can be included by replacing \\(y(t)\\) with the artificial variable \\(x(t)\\) in the equation above, and introducing disturbances \\(w(t)\\) and \\(e(t)\\) as follows:</p> \\[ \\begin{align} &amp; m \\frac{d^2 x(t)}{dt^2} + d \\frac{d x(t)}{dt} + k(x(t)) x(t) = u(t) + w(t), \\\\ &amp; k(x(t)) = a + b x^2(t), \\\\ &amp; y(t) = x(t) + e(t). \\end{align} \\]"},{"location":"book/10%20-%20Case%20Studies/#required-packages-and-versions_5","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nnonlinear_benchmarks==0.1.2\n</code></pre> <p>Then, install the packages using:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"book/10%20-%20Case%20Studies/#sysidentpy-configuration_4","title":"SysIdentPy configuration","text":"<p>In this section, we will demonstrate the application of SysIdentPy to the Silver box dataset.  The following code will guide you through the process of loading the dataset, configuring the SysIdentPy parameters, and building a model for mentioned system.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_mean_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\nimport nonlinear_benchmarks\n\ntrain_val, test = nonlinear_benchmarks.Silverbox(atleast_2d=True)\n</code></pre> <p>We used the <code>nonlinear_benchmarks</code> package to load the data. The user is referred to the package documentation to check the details of how to use it.</p> <p>The following plot detail the training and testing data of the experiment.</p> <pre><code>plt.plot(x_train)\nplt.plot(y_train, alpha=0.3)\nplt.title(\"Experiment 1: training data\")\nplt.show()\n\nplt.plot(x_test)\nplt.plot(y_test, alpha=0.3)\nplt.title(\"Experiment 1: testing data\")\nplt.show()\n\nplt.plot(test_arrow_full.u)\nplt.plot(test_arrow_full.y, alpha=0.3)\nplt.title(\"Experiment 2: training data\")\nplt.show()\n\nplt.plot(test_arrow_no_extrapolation.u)\nplt.plot(test_arrow_no_extrapolation.y, alpha=0.2)\nplt.title(\"Experiment 2: testing data\")\nplt.show()\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p>Important Note</p> <p>The goal of this benchmark is to develop a model that outperforms the state-of-the-art (SOTA) model presented in the benchmarking paper. However, the results in the paper differ from those provided in the  GitHub repository.</p> nx Set NRMS RMS (mV) 2 Train 0.10653 5.8103295 2 Validation 0.11411 6.1938068 2 Test 0.19151 10.2358533 2 Test (no extra) 0.12284 5.2789727 4 Train 0.03571 1.9478290 4 Validation 0.03922 2.1286373 4 Test 0.12712 6.7943448 4 Test (no extra) 0.05204 2.2365904 8 Train 0.03430 1.8707026 8 Validation 0.03732 2.0254112 8 Test 0.10826 5.7865255 8 Test (no extra) 0.04743 2.0382715 &gt; Table: results presented in the github. <p>It appears that the values shown in the paper actually represent the training time, not the error metrics. I will contact the authors to confirm this information. According to the Nonlinear Benchmark website, the information is as follows:</p> <p></p> <p>where the values in the \"Training time\" column matches the ones presented as error metrics in the paper.</p> <p>While we await confirmation of the correct values for this benchmark, we will demonstrate the performance of SysIdentPy. However, we will refrain from making any comparisons or attempting to improve the model at this stage.</p>"},{"location":"book/10%20-%20Case%20Studies/#results_3","title":"Results","text":"<p>We will start (as we did in every other case study) with a basic configuration of FROLS using a polynomial basis function with degree equal 2. The <code>xlag</code> and <code>ylag</code> are set to \\(7\\) in this first example. Because the dataset is considerably large, we will start with <code>n_info_values=40</code>. Because we dealing with a large training dataset, we will use the <code>err_tol</code> instead of information criteria to have a faster performance. We will also set <code>n_terms=40</code>, which means that the search will stop if the <code>err_tol</code> is reached or 40 regressors is tested in the <code>ERR</code> algorithm. While this approach might result in a sub-optimal model, it is a reasonable starting point for our first attempt. There are three different experiments: multisine, arrow (full), and arrow (no extrapolation).</p> <pre><code>x_train, y_train = train_val.u, train_val.y\ntest_multisine, test_arrow_full, test_arrow_no_extrapolation = test\nx_test, y_test = test_multisine.u, test_multisine.y\n\nn = test_multisine.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=7,\n\u00a0 \u00a0 ylag=7,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 err_tol=0.999,\n\u00a0 \u00a0 n_terms=40,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nnrmse = rmse/y_test.std()\nrmse_mv = 1000 * rmse\nprint(nrmse, rmse_mv)\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=30000, figsize=(15, 4), title=f\"Multisine. Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\")\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=300, figsize=(15, 4), title=f\"Multisine. Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\")\n\n&gt; 0.1423804033714937\n&gt; 7.727682109791501\n</code></pre> <p></p> <p></p> <pre><code>x_train, y_train = train_val.u, train_val.y\ntest_multisine, test_arrow_full, test_arrow_no_extrapolation = test\nx_test, y_test = test_arrow_full.u, test_arrow_full.y\n\nn = test_arrow_full.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=14,\n\u00a0 \u00a0 ylag=14,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 err_tol=0.9999,\n\u00a0 \u00a0 n_terms=80,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\n# we will not concatente the last values from train data to use as initial condition here because\n# this test data have a very different behavior.\n# However, if you want you can do that and you will see that the model will still perform\n# great after a few iterations\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nnrmse = rmse/y_test.std()\nrmse_mv = 1000 * rmse\n\nprint(nrmse, rmse_mv)\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=30000, figsize=(15, 4), title=f\"Arrow (full). Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\")\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=300, figsize=(15, 4), title=f\"Arrow (full). Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\")\n</code></pre> <p></p> <p></p> <pre><code>x_train, y_train = train_val.u, train_val.y\ntest_multisine, test_arrow_full, test_arrow_no_extrapolation = test\nx_test, y_test = test_arrow_no_extrapolation.u, test_arrow_no_extrapolation.y\n\nn = test_arrow_no_extrapolation.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=14,\n\u00a0 \u00a0 ylag=14,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 err_tol=0.9999,\n\u00a0 \u00a0 n_terms=40,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nnrmse = rmse/y_test.std()\nrmse_mv = 1000 * rmse\nprint(nrmse, rmse_mv)\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=30000, figsize=(15, 4), title=f\"Arrow (no extrapolation). Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\")\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=300, figsize=(15, 4), title=f\"Free Run simulation. Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\")\n</code></pre> <p></p> <p></p>"},{"location":"book/10%20-%20Case%20Studies/#f-16-ground-vibration-test-benchmark","title":"F-16 Ground Vibration Test Benchmark","text":"<p>The following examples are intended to demonstrate the application of SysIdentPy on a real-world dataset. Please note that these examples are not aimed at replicating the results presented in the cited manuscripts. The model parameters, such as <code>ylag</code> and <code>xlag</code>, as well as the size of the identification and validation data sets, differ from those used in the original studies. Additionally, adjustments related to sampling rates and other data preparation steps are not covered in this notebook.</p> <p>For a comprehensive reference regarding the F-16 Ground Vibration Test benchmark, please visit the nonlinear benchmark website.</p> <p>Note: This notebook serves as a preliminary demonstration of SysIdentPy's performance on the F-16 dataset. A more detailed analysis will be provided in a future publication. The nonlinear benchmark website offers valuable resources and references related to system identification and machine learning, and readers are encouraged to explore the papers and information available there.</p>"},{"location":"book/10%20-%20Case%20Studies/#benchmark-overview","title":"Benchmark Overview","text":"<p>The F-16 Ground Vibration Test benchmark is a notable experiment in the field of system identification and nonlinear dynamics. It involves a high-order system with clearance and friction nonlinearities at the mounting interfaces of payloads on a full-scale F-16 aircraft.</p> <p>Experiment Details: - Event: Siemens LMS Ground Vibration Testing Master Class - Date: September 2014 - Location: Saffraanberg military base, Sint-Truiden, Belgium</p> <p>During the test, two dummy payloads were mounted on the wing tips of the F-16 to simulate the mass and inertia of real devices typically equipped on the aircraft during flight. Accelerometers were installed on the aircraft structure to capture vibration data. A shaker was placed under the right wing to apply input signals. The key source of nonlinearity in the system was identified as the mounting interfaces of the payloads, particularly the right-wing-to-payload interface, which exhibited significant nonlinear distortions.</p> <p>Data and Resources: - Data Availability: The dataset, including detailed system descriptions, estimation and test data sets, and setup images, is available for download in both .csv and .mat file formats. - Reference: For in-depth information on the F-16 benchmark, refer to: J.P. No\u00ebl and M. Schoukens, \"F-16 aircraft benchmark based on ground vibration test data,\" 2017 Workshop on Nonlinear System Identification Benchmarks, pp. 19-23, Brussels, Belgium, April 24-26, 2017.</p> <p>The goal of this notebook is to illustrate how SysIdentPy can be applied to such complex datasets, showcasing its capabilities in modeling and analysis. For a thorough exploration of the benchmark and its methodologies, please consult the provided resources and references.</p>"},{"location":"book/10%20-%20Case%20Studies/#required-packages-and-versions_6","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"book/10%20-%20Case%20Studies/#sysidentpy-configuration_5","title":"SysIdentPy Configuration","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n\u00a0 \u00a0 compute_residues_autocorrelation,\n\u00a0 \u00a0 compute_cross_correlation,\n)\n</code></pre>"},{"location":"book/10%20-%20Case%20Studies/#procedure","title":"Procedure","text":"<p><pre><code>f_16 = pd.read_csv(r\"examples/datasets/f-16.txt\", header=None, names=[\"x1\", \"x2\", \"y\"])\nf_16.shape\nf_16[[\"x1\", \"x2\"]][0:500].plot(figsize=(12, 8))\n\n&gt;&gt;&gt; (32768, 3)\n</code></pre> </p> <pre><code>f_16[\"y\"][0:2000].plot(figsize=(12, 8))\n</code></pre> <p></p> <p>The following code is to split the dataset into training and test sets</p> <pre><code>x1_id, x1_val = f_16[\"x1\"][0:16384].values.reshape(-1, 1), f_16[\"x1\"][\n\u00a0 \u00a0 16384::\n].values.reshape(-1, 1)\nx2_id, x2_val = f_16[\"x2\"][0:16384].values.reshape(-1, 1), f_16[\"x2\"][\n\u00a0 \u00a0 16384::\n].values.reshape(-1, 1)\nx_id = np.concatenate([x1_id, x2_id], axis=1)\nx_val = np.concatenate([x1_val, x2_val], axis=1)\ny_id, y_val = f_16[\"y\"][0:16384].values.reshape(-1, 1), f_16[\"y\"][\n\u00a0 \u00a0 16384::\n].values.reshape(-1, 1)\n</code></pre> <p>We will set the lags for both inputs as</p> <pre><code>x1lag = list(range(1, 10))\nx2lag = list(range(1, 10))\n</code></pre> <p>and build a NARX model as follows</p> <pre><code>basis_function = Polynomial(degree=1)\nestimator = LeastSquares()\n\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=39,\n\u00a0 \u00a0 ylag=20,\n\u00a0 \u00a0 xlag=[x1lag, x2lag],\n\u00a0 \u00a0 info_criteria=\"bic\",\n\u00a0 \u00a0 estimator=estimator,\n\u00a0 \u00a0 basis_function=basis_function,\n)\n\nmodel.fit(X=x_id, y=y_id)\ny_hat = model.predict(X=x_val, y=y_val)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\n</code></pre> <p>The RRSE is \\(0.2910\\)</p> Regressors Parameters ERR y(k-1) 1.8387E+00 9.43378253E-01 y(k-2) -1.8938E+00 1.95167599E-02 y(k-3) 1.3337E+00 1.02432261E-02 y(k-6) -1.6038E+00 8.03485985E-03 y(k-9) 2.6776E-01 9.27874557E-04 x2(k-7) -2.2385E+01 3.76837313E-04 x1(k-1) 8.2709E+00 6.81508210E-04 x2(k-3) 1.0587E+02 1.57459800E-03 x1(k-8) -3.7975E+00 7.35086279E-04 x2(k-1) 8.5725E+01 4.85358786E-04 y(k-7) 1.3955E+00 2.77245281E-04 y(k-5) 1.3219E+00 8.64120037E-04 y(k-10) -2.9306E-01 8.51717688E-04 y(k-4) -9.5479E-01 7.23623116E-04 y(k-8) -7.1309E-01 4.44988077E-04 y(k-12) -3.0437E-01 1.49743148E-04 y(k-11) 4.8602E-01 3.34613282E-04 y(k-13) -8.2442E-02 1.43738964E-04 y(k-15) -1.6762E-01 1.25546584E-04 x1(k-2) -8.9698E+00 9.76699739E-05 y(k-17) 2.2036E-02 4.55983807E-05 y(k-14) 2.4900E-01 1.10314107E-04 y(k-19) -6.8239E-03 1.99734771E-05 x2(k-9) -9.6265E+01 2.98523208E-05 x2(k-8) 2.2620E+02 2.34402543E-04 x2(k-2) -2.3609E+02 1.04172323E-04 y(k-20) -5.4663E-02 5.37895336E-05 x2(k-6) -2.3651E+02 2.11392628E-05 x2(k-4) 1.7378E+02 2.18396315E-05 x1(k-7) 4.9862E+00 2.03811842E-05 <pre><code>plot_results(y=y_val, yhat=y_hat, n=1000)\nee = compute_residues_autocorrelation(y_val, y_hat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_val, y_hat, x_val[:, 0])\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"book/10%20-%20Case%20Studies/#pv-forecasting","title":"PV Forecasting","text":"<p>In this case study, we evaluate SysIdentPy's capabilities for forecasting solar irradiance data, which can serve as a proxy for solar photovoltaic (PV) production. The objective is to demonstrate that SysIdentPy provides a competitive alternative for time series modeling, rather than claiming superiority over other libraries.</p>"},{"location":"book/10%20-%20Case%20Studies/#dataset-overview_1","title":"Dataset Overview","text":"<p>The dataset used in this analysis consists of solar irradiance measurements, which are crucial for predicting solar PV production. Solar irradiance refers to the power of solar radiation received per unit area at the Earth's surface, typically measured in watts per square meter (W/m\u00b2). Accurate forecasting of solar irradiance is essential for optimizing energy production and managing grid stability in solar power systems.</p> <p>Dataset Details: - Source: The dataset can be accessed from the NeuralProphet GitHub repository. - Time Frame: The dataset covers a continuous period with frequent measurements. - Variables: Solar irradiance values over time, which will be used to model and forecast future irradiance levels.</p>"},{"location":"book/10%20-%20Case%20Studies/#comparison-with-other-libraries_1","title":"Comparison with Other Libraries","text":"<p>To assess the effectiveness of SysIdentPy, we will compare its performance with the NeuralProphet library. NeuralProphet is known for its flexibility and ability to capture complex seasonal patterns and trends, making it a suitable benchmark for this task.</p> <p>For the comparison, we will use the following methods:</p> <ul> <li>NeuralProphet:</li> <li> <p>The configuration for NeuralProphet models will be based on examples provided in the NeuralProphet documentation. This library employs advanced techniques for capturing temporal patterns and forecasting.</p> </li> <li> <p>SysIdentPy:</p> </li> <li>MetaMSS (Meta-heuristic Model Structure Selection): Utilizes metaheuristic algorithms to determine the optimal model structure.</li> <li>AOLS (Accelerated Orthogonal Least Squares): A method designed for selecting relevant regressors in a model.</li> <li>FROLS (Forward Regression with Orthogonal Least Squares, using polynomial base functions): A regression technique that incorporates polynomial terms to enhance model selection.</li> </ul>"},{"location":"book/10%20-%20Case%20Studies/#objective_1","title":"Objective","text":"<p>The goal of this case study is to compare the performance of SysIdentPy's forecasting methods with NeuralProphet. We will specifically focus on:</p> <ul> <li>1-Step Ahead Forecasting: Evaluating the models' ability to predict the next time step in the series based on historical data.</li> </ul> <p>We will train our models on 80% of the dataset and reserve the remaining 20% for validation purposes. This setup ensures that we test the models' performance on unseen data.</p>"},{"location":"book/10%20-%20Case%20Studies/#required-packages-and-versions_7","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npystan==2.19.1.1\nholidays==0.11.2\nfbprophet==0.7.1\nneuralprophet==0.2.7\npandas==1.3.2\nnumpy==1.23.3\nmatplotlib==3.8.4\npmdarima==1.8.3\nscikit-learn==0.24.2\nscipy==1.9.1\nsktime==0.8.0\nstatsmodels==0.12.2\ntbats==1.1.0\ntorch==1.12.1\n</code></pre> <p>Then, install the packages using:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions. This practice isolates your project\u2019s dependencies and prevents version conflicts with other projects or system-wide packages. Additionally, be aware that some packages, such as <code>sktime</code> and <code>neuralprophet</code>, may install several dependencies automatically during their installation. Setting up a virtual environment helps manage these dependencies more effectively and keeps your project environment clean and reproducible.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"book/10%20-%20Case%20Studies/#procedure_1","title":"Procedure","text":"<ol> <li>Data Preparation: Load and preprocess the solar irradiance dataset.</li> <li>Model Training: Apply the chosen methods from SysIdentPy and NeuralProphet to the training data.</li> <li>Evaluation: Assess the forecasting accuracy of each model on the validation set.</li> </ol> <p>By comparing these approaches, we aim to showcase SysIdentPy as a viable option for time series forecasting, highlighting its strengths and versatility in practical applications.</p> <p>Let\u2019s start by importing the necessary libraries and setting up the environment for this analysis.</p> <pre><code>from warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.model_structure_selection import AOLS\nfrom sysidentpy.model_structure_selection import MetaMSS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.neural_network import NARXNN\nfrom sysidentpy.metrics import mean_squared_error\nfrom neuralprophet import NeuralProphet\nfrom neuralprophet import set_random_seed\n\nsimplefilter(\"ignore\", FutureWarning)\nnp.seterr(all=\"ignore\")\n%matplotlib inline\n\nloss = mean_squared_error\ndata_location = r\".\\datasets\"\n</code></pre>"},{"location":"book/10%20-%20Case%20Studies/#neural-prophet_1","title":"Neural Prophet","text":"<pre><code>set_random_seed(42)\nfiles = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760))\ndf[\"y\"] = raw.iloc[:, 0].values\n\nm = NeuralProphet(\n\u00a0 \u00a0 n_lags=24,\n\u00a0 \u00a0 ar_sparsity=0.5,\n)\n\nmetrics = m.fit(df, freq=\"H\", valid_p=0.2)\ndf_train, df_val = m.split_df(df, valid_p=0.2)\nm.test(df_val)\n\nfuture = m.make_future_dataframe(df_val, n_historic_predictions=True)\nforecast = m.predict(future)\n\nprint(loss(forecast[\"y\"][24:-1], forecast[\"yhat1\"][24:-1]))\n\nplt.plot(forecast[\"y\"][-104:], \"ro-\")\nplt.plot(forecast[\"yhat1\"][-104:], \"k*-\")\n</code></pre> <p>The error is \\(MSE=4642.23\\) and will be used as baseline in this case. Lets check how SysIdentPy methods handle this data.</p>"},{"location":"book/10%20-%20Case%20Studies/#frols","title":"FROLS","text":"<pre><code>files = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760))\ndf[\"y\"] = raw.iloc[:, 0].values\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 ylag=24,\n\u00a0 \u00a0 xlag=24,\n\u00a0 \u00a0 info_criteria=\"bic\",\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 model_type=\"NARMAX\",\n\u00a0 \u00a0 estimator=LeastSquares(),\n)\n\nsysidentpy.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])\nyhat = sysidentpy.predict(X=x_test, y=y_test, steps_ahead=1)\nsysidentpy_loss = loss(\n\u00a0 \u00a0 pd.Series(y_test.flatten()[sysidentpy.max_lag :]),\n\u00a0 \u00a0 pd.Series(yhat.flatten()[sysidentpy.max_lag :]),\n)\n\nprint(sysidentpy_loss)\nplot_results(y=y_test[-104:], yhat=yhat[-104:])\n</code></pre> <p>The \\(MSE=3869.34\\) for this case.</p> <p></p>"},{"location":"book/10%20-%20Case%20Studies/#metamss","title":"MetaMSS","text":"<pre><code>set_random_seed(42)\nfiles = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760))\ndf[\"y\"] = raw.iloc[:, 0].values\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nestimator = LeastSquares()\n\nsysidentpy_metamss = MetaMSS(\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 xlag=24,\n\u00a0 \u00a0 ylag=24,\n\u00a0 \u00a0 estimator=estimator,\n\u00a0 \u00a0 maxiter=10,\n\u00a0 \u00a0 steps_ahead=1,\n\u00a0 \u00a0 n_agents=15,\n\u00a0 \u00a0 loss_func=\"metamss_loss\",\n\u00a0 \u00a0 model_type=\"NARMAX\",\n\u00a0 \u00a0 random_state=42,\n)\n\nsysidentpy_metamss.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy_metamss.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy_metamss.max_lag :], y_test])\nyhat = sysidentpy_metamss.predict(X=x_test, y=y_test, steps_ahead=1)\nmetamss_loss = loss(\n\u00a0 \u00a0 pd.Series(y_test.flatten()[sysidentpy_metamss.max_lag :]),\n\u00a0 \u00a0 pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]),\n)\n\nprint(metamss_loss)\nplot_results(y=y_test[-104:], yhat=yhat[-104:])\n</code></pre> <p>The MetaMSS algorithm was able to select a better model in this case, as can be observed in the error metric, \\(MSE=2157.77\\).</p> <p></p>"},{"location":"book/10%20-%20Case%20Studies/#aols","title":"AOLS","text":"<pre><code>set_random_seed(42)\nfiles = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760))\ndf[\"y\"] = raw.iloc[:, 0].values\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy_AOLS = AOLS(\n\u00a0 \u00a0 ylag=24, xlag=24, k=2, L=1, model_type=\"NARMAX\", basis_function=basis_function\n)\n\nsysidentpy_AOLS.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy_AOLS.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])\nyhat = sysidentpy_AOLS.predict(X=x_test, y=y_test, steps_ahead=1)\naols_loss = loss(\n\u00a0 \u00a0 pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),\n\u00a0 \u00a0 pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]),\n)\nprint(aols_loss)\nplot_results(y=y_test[-104:], yhat=yhat[-104:])\n</code></pre> <p>The error now is \\(MSE=2361.56\\).</p> <p></p>"},{"location":"book/2%20-%20NARMAX%20Model%20Representation/","title":"2. NARMAX Model Representation","text":"<p>There are several NARMAX model representations, including polynomial, Fourier, generalized additive, neural networks, and wavelet (Billings, S. A, Aguirra, L. A). This book focuses on the model representations available in SysIdentPy and we\u2019ll keep things updated as new methods are added to the package. If a particular representation is mentioned but is not available in SysIdentPy, it will be explicitly mentioned.</p> <p>To reproduce the codes presented in this section, make sure you have these packages installed:</p> <pre><code>sysidentpy, scikit-learn, scipy, pytorch, matplotlib\n</code></pre>"},{"location":"book/2%20-%20NARMAX%20Model%20Representation/#basis-function","title":"Basis Function","text":"<p>In System Identification, understanding the concept of basis functions is crucial for effectively modeling complex systems. Basis functions are predefined mathematical functions used to transform the input data into a new space, where the relationships within the data can be more easily modeled. By expressing the original data in terms of these basis functions, we can build nonlinear models in respect to it's structure while keeping it linear in the parameters, allowing the usage of straightforward parameter estimation methods.</p> <p>Basis functions commonly used in system identification:</p> <ol> <li> <p>Polynomial Basis Functions: These functions are powers of the input variables. They are useful for capturing simple nonlinear relationships.</p> </li> <li> <p>Fourier Basis Functions: These sinusoidal functions (sine and cosine) are ideal for representing periodic patterns within the data.</p> </li> <li> <p>Wavelet Basis Functions: These functions are localized in both time and frequency, making them suitable for analyzing data with varying frequency components. Not available in SysIdentPy yet.</p> </li> </ol> <p>In SysIdentPy you can define the basis function you want to use in your model by just import them:</p> <pre><code>from sysidentpy.basis_function import Polynomial, Fourier, Bernstein\n</code></pre> <p>To keep things simple for now, we will show simple examples of how basis function can be used in a modeling task. We will show a simple polynomial basis functions, a triangular basis function, a radial basis function and a rectangular basis function.</p> <p>SysIdentPy does not currently include Vandermonde or any of the other basis functions defined below. These functions are provided solely as examples to illustrate the significance of the basis functions. The examples are based on Fredrik Bagge Carlson's PhD thesis, which I highly recommended for anyone interested in Nonlinear System Identification.</p> <p>Although Vandermonde and Radial Basis Functions (RBF) are planned for inclusion as native basis functions in SysIdentPy version 1.0, users can already create and use their own custom basis functions with SysIdentPy. An example of how to do this is available on the SysIdentPy documentation page.</p>"},{"location":"book/2%20-%20NARMAX%20Model%20Representation/#example-vandermonde-matrix","title":"Example: Vandermonde Matrix","text":"<p>The polynomial basis functions used in this example is defined as:</p> \\[ \\phi_i(x) = x^i \\tag{2.1} \\] <p>where \\(i\\) is the degree of the polynomial and \\(x\\) is the input variable.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate simulated quadratic polynomial data\nnp.random.seed(0)\nx = np.linspace(-3, 3, 200)\ny = 0.2 * x**2 - 0.3 * x + 0.1 + np.random.normal(0, 0.1, size=x.shape)\n\n# Polynomial basis function\ndef poly_basis(x, degree):\n\u00a0 \u00a0 return np.vander(x, degree + 1, increasing=True)\n\n# Create polynomial features\ndegree = 2\nX_poly = poly_basis(x, degree)\n# Fit a linear regression model\nmodel = LinearRegression()\nmodel.fit(X_poly, y)\ny_pred = model.predict(X_poly)\n# Plot the original data (quadratic polynomial)\nplt.scatter(x, y, color='#ffc865', s=25)\n# Plot the polynomial approximation\nplt.plot(x, y_pred, color='#00008c', linewidth=5)\n# Plot the polynomial basis functions\nbasis_colors = [\"#00b262\", \"#20007e\", \"#b20000\"]\nfor i in range(degree + 1):\n\u00a0 \u00a0 plt.plot(x, poly_basis(x, degree)[:, i], linewidth=0.5, color=basis_colors[i % len(basis_colors)])\n\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(True)\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.gca().yaxis.set_ticks([])\nplt.show()\n</code></pre> <p></p> <p>Figure 1. Approximation using Vandermode Matrix. The yellow dots show the system data, the bold blue line represents the predicted values, and the other lines depict the basis functions.</p>"},{"location":"book/2%20-%20NARMAX%20Model%20Representation/#example-rectangular-basis-functions","title":"Example: Rectangular Basis Functions","text":"<p>The rectangular basis functions are defined as:</p> \\[ \\phi_{i}(x) = \\begin{cases} 1 &amp; \\text{if } c_i - \\frac{w}{2} \\leq x &lt; c_i + \\frac{w}{2} \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\tag{2.2} \\] <p>where \\(c_i\\) represents the center of the basis function, \\(w\\) is the width, and \\(x\\) is the input variable.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate simulated quadratic polynomial data\nnp.random.seed(0)\nx = np.linspace(-3, 3, 200)\ny = 0.2 * x**2 - 0.3 * x + 0.1 + np.random.normal(0, 0.1, size=x.shape)\n# Rectangular basis function\ndef rectangular_basis(x, centers, width):\n\u00a0 \u00a0 return np.column_stack([(np.abs(x - c) &lt; width).astype(float) for c in centers])\n\n# Create rectangular features\ncenters = np.linspace(-3, 3, 6)\nwidth = 3\nX_rect = rectangular_basis(x, centers, width)\n# Fit a linear regression model\nmodel = LinearRegression()\nmodel.fit(X_rect, y)\ny_pred = model.predict(X_rect)\n# Plot the original data (quadratic polynomial)\nplt.scatter(x, y, color='#ffc865', s=25)\n# Plot the rectangular approximation\nplt.plot(x, y_pred, color='#00008c', linewidth=5)\n# Plot the rectangular basis functions\nbasis_colors = [\"#00b262\", \"#20007e\", \"#b20000\"]\nfor i in range(len(centers)):\n\u00a0 \u00a0 plt.plot(x, rectangular_basis(x, centers, width)[:, i], linewidth=1, color=basis_colors[i % len(basis_colors)])\n\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(True)\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.gca().yaxis.set_ticks([])\nplt.show()\n</code></pre> <p></p> <p>Figure 2. Approximation using Rectangular Basis Function. The yellow dots show the system data, the bold blue line represents the predicted values, and the other lines depict the basis functions.</p>"},{"location":"book/2%20-%20NARMAX%20Model%20Representation/#example-triangular-basis-functions","title":"Example: Triangular Basis Functions","text":"<p>The triangular basis functions are defined as:</p> \\[ \\phi_{i}(x) = \\max \\left(0, 1 - \\frac{|x - c_i|}{w} \\right) \\tag{2.3} \\] <p>where \\(c_i\\) is the center of the basis function, \\(w\\) is the width, and \\(x\\) is the input variable.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate simulated quadratic polynomial data\nnp.random.seed(0)\nx = np.linspace(-3, 3, 200)\ny = 0.2 * x**2 - 0.3 * x + 0.1 + np.random.normal(0, 0.1, size=x.shape)\n# Triangular basis function\ndef triangular_basis(x, centers, width):\n\u00a0 \u00a0 return np.column_stack([np.maximum(0, 1 - np.abs((x - c) / width)) for c in centers])\n\n# Create triangular features\ncenters = np.linspace(-3, 3, 6)\nwidth = 1.5\nX_tri = triangular_basis(x, centers, width)\n# Fit a linear regression model\nmodel = LinearRegression()\nmodel.fit(X_tri, y)\ny_pred = model.predict(X_tri)\n# Plot the original data (quadratic polynomial)\nplt.scatter(x, y, color='#ffc865', s=25)\n# Plot the triangular approximation\nplt.plot(x, y_pred, color='#00008c', linewidth=5)\n# Plot the triangular basis functions\nbasis_colors = [\"#00b262\", \"#20007e\", \"#b20000\"]\nfor i in range(len(centers)):\n\u00a0 \u00a0 plt.plot(x, triangular_basis(x, centers, width)[:, i], linewidth=1, color=basis_colors[i % len(basis_colors)])\n\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(True)\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.gca().yaxis.set_ticks([])\nplt.show()\n</code></pre> <p></p> <p>Figure 3. Approximation using a Triangular Basis Function. The yellow dots show the system data, the bold blue line represents the predicted values, and the other lines depict the basis functions.</p>"},{"location":"book/2%20-%20NARMAX%20Model%20Representation/#example-radial-basis-function-rbf-gaussian","title":"Example: Radial Basis Function (RBF) - Gaussian","text":"<p>The Gaussian Radial Basis Function is defined as:</p> \\[ \\phi(x; c, \\sigma) = \\exp\\left(- \\frac{(x - c)^2}{2 \\sigma^2}\\right) \\tag{2.4} \\] <p>where: - \\(x\\) is the input variable. - \\(c\\) is the center of the RBF. - \\(\\sigma\\) is the spread (or scale) of the RBF.</p> <p>This function measures the distance between \\(x\\) and the center \\(c\\), and it decays exponentially based on the width \\(\\sigma\\). The smaller the \\(\\sigma\\), the more localized the basis function is around the center \\(c\\).</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate simulated quadratic polynomial data\nnp.random.seed(0)\nx = np.linspace(-3, 3, 200) \u00a0# More points for a smoother curve\ny = 0.2 * x**2 - 0.3 * x + 0.1 + np.random.normal(0, 0.1, size=x.shape) \u00a0# Quadratic polynomial with noise\n# RBF centers and sigma\ncenters = np.linspace(-3, 3, 6) \u00a0# More centers for better coverage\nsigma = 0.5 \u00a0# Spread of the RBF\n# RBF basis function\ndef rbf_basis(x, c, sigma):\n\u00a0 \u00a0 return np.exp(- (x - c) ** 2 / (2 * sigma ** 2))\n\n# Create RBF features\nX_rbf = np.column_stack([rbf_basis(x, c, sigma) for c in centers])\n# Fit a linear regression model\nmodel = LinearRegression()\nmodel.fit(X_rbf, y)\ny_pred = model.predict(X_rbf)\n# Plot the original data (quadratic polynomial)\nplt.scatter(x, y, color='#ffc865', s=25)\n# Basis function colors\nbasis_colors = [\"#00b262\", \"#20007e\", \"#b20000\"]\nn_colors = len(basis_colors)\n# Plot the basis functions\nfor i, c in enumerate(centers):\n\u00a0 \u00a0 color = basis_colors[i % n_colors]\n\u00a0 \u00a0 plt.plot(x, rbf_basis(x, c, sigma), linewidth=1, color=color, label=f'RBF Center {c:.2f}')\n\n# Plot the approximation\nplt.plot(x, y_pred, color='#00008c', linewidth=5)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(True)\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.gca().yaxis.set_ticks([])\nplt.show()\n</code></pre> <p></p> <p>Figure 4. Approximation using the Radial Basis Function. The yellow dots show the system data, the bold blue line represents the predicted values, and the other lines depict the basis functions.</p>"},{"location":"book/2%20-%20NARMAX%20Model%20Representation/#linear-models","title":"Linear Models","text":""},{"location":"book/2%20-%20NARMAX%20Model%20Representation/#armax","title":"ARMAX","text":"<p>You may have noticed the similarity between the acronym NARMAX with the well-known models ARX, ARMAX, etc., which are widely used for forecasting time series. And this resemblance is not by chance. The AutoRegressive models with Moving Average and Exogenous Input (ARMAX) and their variations AR, ARX, ARMA (to name just a few) are one of the most used mathematical representations for identifying linear systems. The ARMAX can be expressed as:</p> \\[ y_k= \\mathcal{\\phi}[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\tag{2.5} \\] <p>where \\(n_y\\in \\mathbb{N}\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\) , are the maximum lags for the system output, input and noise regressors (representing the moving average part), respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{\\phi}\\) is some linear function of the input and output regressors and \\(d\\) is a time delay typically set to \\(d=1\\).</p> <p>If \\(\\mathcal{F}\\) is a polynomial, we have a polynomial ARMAX model</p> \\[ y_k = \\sum_{0} + \\sum_{i=1}^{p}\\Theta_{y}^{i}y_{k-i} + \\sum_{j=1}^{q}\\Theta_{e}^{j}e_{k-j} + \\sum_{m=1}^{r}\\Theta_{x}^{m}x_{k-m} + e_k \\tag{2.6} \\] <p>where \\(\\sum\\nolimits_{0}\\), \\(\\Theta_{y}^{i}\\), \\(\\Theta_{e}^{j}\\), and \\(\\Theta_{x}^{m}\\) are constant parameters.</p> <p>The following example is a polynomial ARMAX model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}+0.1139x_{k-1} -0.1691x_{k-1} + 0.2245e_{k-1} \\end{align} \\tag{2.7} \\] <p>You can easily build a polynomial ARMAX model using SysIdentPy: <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=True)\n)\n</code></pre></p> <p>In the example above, we define the linear polynomial basis function by importing the Polynomial basis and setting the degree equal to 1 (this ensure that we do not have a nonlinear combination of the regressors). Don't worry about the <code>FROLS</code> and <code>LeastSquares</code> yet. We'll talk about them in chapters 3 and 4, respectively.</p> <p>For Figure 4, we conducted 10 separate simulations to analyse the effects of different noise process generation on the ARMAX system's behavior. Each simulation uses a unique sample of noise to observe how variations in this random component influence the overall system output. To illustrate this, we highlight one specific simulation while the others are displayed with less emphasis.</p> <p>It's important to notice that all simulations, whether highlighted or not, are governed by the same underlying model. The deterministic part of the model equation explains the behavior of all the signals shown. The noticeable differences among the signals arise solely from the distinct noise samples used in each simulation. Despite these variations, the core dynamics of the signal remain consistent and are described by the model's deterministic component.</p> <p>Most of the code presented in this chapter is intended to illustrate fundamental concepts rather than demonstrating how to use SysIdentPy specifically. Many examples are implemented using pure Python to help you better understand the underlying concepts, replicate the examples, and adapt them as needed. SysIdentPy itself will be introduced and be used in the examples in the following chapter.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\nrandom_samples = 50\nn = np.arange(random_samples)\ndef system_equation(y, u, nu):\n\u00a0 \u00a0 yk = 0.9*y[0] - 0.24*y[1] + 0.92*u[0] + 0.92*nu[0] + nu[1]\n\u00a0 \u00a0 return yk\n\n# Create a single figure and axis for all plots\nfig, ax = plt.subplots(figsize=(12, 6))\nu = np.random.normal(size=(random_samples,), scale=1)\nfor k in range(10):\n\u00a0 \u00a0 nu = np.random.normal(size=(random_samples,), scale=0.9)\n\u00a0 \u00a0 y = np.empty_like(nu)\n\u00a0 \u00a0 # Initial Conditions\n\u00a0 \u00a0 y0 = [0.5, -0.1]\n\u00a0 \u00a0 y[0:2] = y0\n\u00a0 \u00a0 for i in range(2, len(y)):\n\u00a0 \u00a0 \u00a0 \u00a0 y[i] = system_equation([y[i - 1], y[i - 2]], [u[i - 1]], [nu[i - 1], nu[i]])\n\n\u00a0 \u00a0 # Interpolate the data just to make the plot \"nicer\"\n\u00a0 \u00a0 interpolation_function = interp1d(n, y, kind='quadratic')\n\u00a0 \u00a0 n_fine = np.linspace(n.min(), n.max(), 10*len(n)) \u00a0# More points for a smoother curve\n\u00a0 \u00a0 y_interpolated = interpolation_function(n_fine)\n\u00a0 \u00a0 # Plotting the interpolated data\n\u00a0 \u00a0 if k == 0:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='k', alpha=1, linewidth=1.5)\n\u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='grey', linestyle=\":\", alpha=0.5, linewidth=1.5)\n\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.set_title(\"Simulation of an ARMAX model\")\nplt.show()\n</code></pre> <p></p> <p>Figure 4. Simulations to show the effects of different noise process generation on the ARMAX model's behavior.</p>"},{"location":"book/2%20-%20NARMAX%20Model%20Representation/#arx","title":"ARX","text":"<p>If we do not include noise terms \\(e_{k-n_e}\\)  in equation (1), we have ARX models.</p> \\[ y_k = \\sum_{0} + \\sum_{i=1}^{p}\\Theta_{y}^{i}y_{k-i} + \\sum_{m=1}^{r}\\Theta_{x}^{m}x_{k-m} + e_k \\tag{2.8} \\] <p>The following example is a polynomial ARX model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}+0.1139x_{k-1} -0.1691x_{k-1} \\end{align} \\tag{2.9} \\] <p>The only difference in SysIdentPy is setting the <code>unbiased=False</code></p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False)\n)\n</code></pre> <p>The following example shows 10 separate simulations to analyse the effects of different noise process generation on the ARX system's behavior.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\nrandom_samples = 50\nn = np.arange(random_samples)\ndef system_equation(y, u, nu):\n\u00a0 \u00a0 yk = 0.9*y[0] - 0.24*y[1] + 0.92*u[0] + nu[0]\n\u00a0 \u00a0 return yk\n\n# Create a single figure and axis for all plots\nfig, ax = plt.subplots(figsize=(12, 6))\nu = np.random.normal(size=(random_samples,), scale=1)\nfor k in range(10):\n\u00a0 \u00a0 nu = np.random.normal(size=(random_samples,), scale=0.9)\n\u00a0 \u00a0 y = np.empty_like(nu)\n\u00a0 \u00a0 # Initial Conditions\n\u00a0 \u00a0 y0 = [0.5, -0.1]\n\u00a0 \u00a0 y[0:2] = y0\n\u00a0 \u00a0 for i in range(2, len(y)):\n\u00a0 \u00a0 \u00a0 \u00a0 y[i] = system_equation([y[i - 1], y[i - 2]], [u[i - 1]], [nu[i]])\n\n\u00a0 \u00a0 # Interpolate the data just to make the plot easier to understand\n\u00a0 \u00a0 interpolation_function = interp1d(n, y, kind='quadratic')\n\u00a0 \u00a0 n_fine = np.linspace(n.min(), n.max(), 10*len(n)) \u00a0# More points for a smoother curve\n\u00a0 \u00a0 y_interpolated = interpolation_function(n_fine)\n\u00a0 \u00a0 # Plotting the interpolated data\n\u00a0 \u00a0 if k == 0:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='k', alpha=1, linewidth=1.5)\n\u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='grey', linestyle=\":\", alpha=0.5, linewidth=1.5)\n\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.set_title(\"Simulation of an ARX model\")\nplt.show()\n</code></pre> <p></p> <p>Figure 5. Simulations to show the effects of different noise process generation on the ARX model's behavior.</p>"},{"location":"book/2%20-%20NARMAX%20Model%20Representation/#arma","title":"ARMA","text":"<p>if we do not include input terms in equation (1), it turns to ARMA model</p> \\[ y_k = \\sum_{0} + \\sum_{i=1}^{p}\\Theta_{y}^{i}y_{k-i} + \\sum_{j=1}^{q}\\Theta_{e}^{j}e_{k-j} + e_k \\tag{2.10} \\] <p>The following example is a polynomial ARMA model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}+0.1139y_{k-3} -0.1691y_{k-4} + 0.2245e_{k-1} \\end{align} \\tag{2.11} \\] <p>Since the model representation do not have inputs, we have to set the model type to <code>NAR</code> and set <code>unbiased=True</code> again in <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=True),\n    model_type=\"NAR\"\n)\n</code></pre> <p>The figure bellow shows 10 separate simulations to analyse the effects of different noise process generation on the ARX system's behavior.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\nrandom_samples = 50\nn = np.arange(random_samples)\ndef system_equation(y, nu):\n\u00a0 \u00a0 yk = 0.5*y[0] - 0.4*y[1] + 0.8*nu[0] + nu[1]\n\u00a0 \u00a0 return yk\n\n# Create a single figure and axis for all plots\nfig, ax = plt.subplots(figsize=(12, 6))\nfor k in range(10):\n\u00a0 \u00a0 nu = np.random.normal(size=(random_samples,), scale=0.9)\n\u00a0 \u00a0 y = np.empty_like(nu)\n\u00a0 \u00a0 # Initial Conditions\n\u00a0 \u00a0 y0 = [0.5, -0.1]\n\u00a0 \u00a0 y[0:2] = y0\n\u00a0 \u00a0 for i in range(2, len(y)):\n\u00a0 \u00a0 \u00a0 \u00a0 y[i] = system_equation([y[i - 1], y[i - 2]], [nu[i - 1], nu[i]])\n\n\u00a0 \u00a0 # Interpolate the data just to make the plot easier to understand\n\u00a0 \u00a0 interpolation_function = interp1d(n, y, kind='quadratic')\n\u00a0 \u00a0 n_fine = np.linspace(n.min(), n.max(), 10*len(n)) \u00a0# More points for a smoother curve\n\u00a0 \u00a0 y_interpolated = interpolation_function(n_fine)\n\u00a0 \u00a0 # Plotting the interpolated data\n\u00a0 \u00a0 if k == 0:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='k', alpha=1, linewidth=1.5)\n\u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='grey', linestyle=\":\", alpha=0.5, linewidth=1.5)\n\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.set_title(\"Simulation of an ARMA model\")\nplt.show()\n</code></pre> <p></p> <p>Figure 6. Simulations to show the effects of different noise process generation on the ARMA model's behavior.</p>"},{"location":"book/2%20-%20NARMAX%20Model%20Representation/#ar","title":"AR","text":"<p>if we do not include input terms and noise terms in equation (1), it turns to AR model</p> \\[ y_k = \\sum_{0} + \\sum_{i=1}^{p}\\Theta_{y}^{i}y_{k-i} + e_k \\tag{2.12} \\] <p>The following example is a polynomial AR model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}+0.1139y_{k-3} -0.1691y_{k-4} \\end{align} \\tag{2.13} \\] <p>In this case, we have to set the model type to <code>NAR</code> and set <code>unbiased=False</code> in <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    model_type=\"NAR\"\n)\n</code></pre> <p>The figure bellow shows 10 separate simulations to analyse the effects of different noise process generation on the AR system's behavior.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\nrandom_samples = 50\nn = np.arange(random_samples)\ndef system_equation(y, nu):\n\u00a0 \u00a0 yk = 0.5*y[0] - 0.3*y[1] + nu[0]\n\u00a0 \u00a0 return yk\n\n# Create a single figure and axis for all plots\nfig, ax = plt.subplots(figsize=(12, 6))\nfor k in range(10):\n\u00a0 \u00a0 nu = np.random.normal(size=(random_samples,), scale=0.9)\n\u00a0 \u00a0 y = np.empty_like(nu)\n\u00a0 \u00a0 # Initial Conditions\n\u00a0 \u00a0 y0 = [0.5, -0.1]\n\u00a0 \u00a0 y[0:2] = y0\n\u00a0 \u00a0 for i in range(2, len(y)):\n\u00a0 \u00a0 \u00a0 \u00a0 y[i] = system_equation([y[i - 1], y[i - 2]], [nu[i]])\n\n\u00a0 \u00a0 # Interpolate the data just to make the plot easier to understand\n\u00a0 \u00a0 interpolation_function = interp1d(n, y, kind='quadratic')\n\u00a0 \u00a0 n_fine = np.linspace(n.min(), n.max(), 10*len(n)) \u00a0# More points for a smoother curve\n\u00a0 \u00a0 y_interpolated = interpolation_function(n_fine)\n\u00a0 \u00a0 # Plotting the interpolated data\n\u00a0 \u00a0 if k == 0:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='k', alpha=1, linewidth=1.5)\n\u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='grey', linestyle=\":\", alpha=0.5, linewidth=1.5)\n\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.set_title(\"Simulation of an AR model\")\nplt.show()\n</code></pre> <p></p> <p>Figure 7. Simulations to show the effects of different noise process generation on the AR model's behavior.</p>"},{"location":"book/2%20-%20NARMAX%20Model%20Representation/#fir","title":"FIR","text":"<p>if we only keep input terms in equation (1), it turns to NFIR model</p> \\[ y_k = \\sum_{m=1}^{r}\\Theta_{x}^{m}x_{k-m} + e_k \\tag{2.14} \\] <p>The following example is a polynomial FIR model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213x_{k-1}-0.5692x_{k-2}+0.1139x_{k-3} -0.1691x_{k-4} \\end{align} \\tag{2.15} \\] <p>In this case, we have to set the model type to <code>NFIR</code> and set <code>unbiased=False</code> in <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    model_type=\"NFIR\"\n)\n</code></pre> <p>The figure bellow shows 10 separate simulations to analyse the effects of different noise process generation on the FIR system's behavior.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\nrandom_samples = 50\nn = np.arange(random_samples)\ndef system_equation(u, nu):\n\u00a0 \u00a0 yk = 0.28*u[0] - 0.34*u[1] + nu[0]\n\u00a0 \u00a0 return yk\n\nu = np.random.normal(size=(random_samples,), scale=1)\n# Create a single figure and axis for all plots\nfig, ax = plt.subplots(figsize=(12, 6))\nfor k in range(10):\n\u00a0 \u00a0 nu = np.random.normal(size=(random_samples,), scale=0.9)\n\u00a0 \u00a0 y = np.empty_like(nu)\n\u00a0 \u00a0 # Initial Conditions\n\u00a0 \u00a0 y0 = [0.5, -0.1]\n\u00a0 \u00a0 y[0:2] = y0\n\u00a0 \u00a0 for i in range(2, len(y)):\n\u00a0 \u00a0 \u00a0 \u00a0 y[i] = system_equation([0.1*u[i - 1], u[i - 2]], [nu[i]])\n\n\u00a0 \u00a0 # Interpolate the data just to make the plot easier to understand\n\u00a0 \u00a0 interpolation_function = interp1d(n, y, kind='quadratic')\n\u00a0 \u00a0 n_fine = np.linspace(n.min(), n.max(), 10*len(n)) \u00a0# More points for a smoother curve\n\u00a0 \u00a0 y_interpolated = interpolation_function(n_fine)\n\u00a0 \u00a0 # Plotting the interpolated data\n\u00a0 \u00a0 if k == 0:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='k', alpha=1, linewidth=1.5)\n\u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='grey', linestyle=\":\", alpha=0.5, linewidth=1.5)\n\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.set_title(\"Simulation of an FIR model\")\nplt.show()\n</code></pre> <p></p> <p>Figure 8. Simulations to show the effects of different noise process generation on the FIR model's behavior.</p> <p>We didn't set the model_type for ARMAX and ARX because the default is <code>NARMAX</code>. SysIdentPy allows three different model types: <code>NARMAX</code>, <code>NAR</code>, and <code>NFIR</code>. Because ARMAX, ARX and others linear variants are subsets of NARMAX models, there is no need for specific <code>ARMAX</code> model type. The idea is to have model types for model with input and output regressors; models with only output regressors; and models with only input regressors.</p>"},{"location":"book/2%20-%20NARMAX%20Model%20Representation/#other-variants","title":"Other Variants","text":"<p>For the sake of simplicity, we defined Equation 2.5 and only approach the polynomial representations. However, you can extend the representations to other basis functions, like the Fourier. If you set \\(\\mathcal{F}\\) as the Fourier extension</p> \\[ \\mathcal{F}(x) = [\\cos(\\pi x), \\sin(\\pi x), \\cos(2\\pi x), \\sin(2\\pi x), \\ldots, \\cos(N\\pi x), \\sin(N\\pi x)] \\tag{2.16} \\] <p>In this case, the Fourier ARX representation will be:</p> \\[ \\begin{align} y_k = &amp;[ \\cos(\\pi y_{k-1}), \\sin(\\pi y_{k-1}), \\cos(2\\pi y_{k-1}), \\sin(2\\pi y_{k-1}), \\ldots, \\cos(N\\pi y_{k-1}), \\sin(N\\pi y_{k-1}),  \\\\ &amp;  \\cos(\\pi y_{k-n_y}), \\sin(\\pi y_{k-n_y}), \\cos(2\\pi y_{k-n_y}), \\sin(2\\pi y_{k-n_y}), \\ldots, \\cos(N\\pi y_{k-n_y}), \\sin(N\\pi y_{k-n_y}), \\\\ &amp; \\cos(\\pi x_{k-1}), \\sin(\\pi x_{k-1}), \\cos(2\\pi x_{k-1}), \\sin(2\\pi x_{k-1}), \\ldots, \\cos(N\\pi x_{k-1}), \\sin(N\\pi x_{k-1}), \\\\ &amp; \\cos(\\pi y_{k-n_y}), \\sin(\\pi y_{k-n_y}), \\cos(2\\pi y_{k-n_y}), \\sin(2\\pi y_{k-n_y}), \\ldots, \\cos(N\\pi y_{k-n_y}), \\sin(N\\pi y_{k-n_y})] \\\\ &amp; + e_k \\end{align} \\tag{2.17} \\] <p>To do that in SysIdentPy, just import the Fourier basis instead of the Polynomial</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Fourier\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Fourier(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    model_type=\"NARMAX\"\n)\n</code></pre>"},{"location":"book/2%20-%20NARMAX%20Model%20Representation/#nonlinear-models","title":"Nonlinear Models","text":""},{"location":"book/2%20-%20NARMAX%20Model%20Representation/#narmax","title":"NARMAX","text":"<p>The NARMAX model was proposed by  Stephen A. Billings and I.J. Leontaritis in 1981 (Billins, S. A.), and can be described as</p> \\[ \\begin{equation} y_k= \\mathcal{F}[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k, \\end{equation} \\tag{2.18} \\] <p>where \\(n_y\\in \\mathbb{N}^*\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\) , are the maximum lags for the system output and input respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) represents uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}\\) is some nonlinear function of the input and output regressors and \\(d\\) is a time delay typically set to \\(d=1\\).</p> <p>You can notice that the difference between Equation 2.5 and Equation 2.18 if the function representing the system. For NARMAX models, \\(\\mathcal{F}\\) can be any nonlinear function, while for Equation 2.5 only linear functions are allowed. Although there are many possible approximations of \\(\\mathcal{F}(\\cdot)\\) (e.g., Neural Networks, Fuzzy, Wavelet, Radial Basis Function), the power-form Polynomial NARMAX model is the most commonly used (Billings, S. A.; Khandelwal, D. and Schoukens, M. and Toth, R.):</p> \\[ \\begin{align}   y_k = \\sum_{i=1}^{p}\\Theta_i \\times \\prod_{j=0}^{n_x}x_{k-j}^{b_i, j}\\prod_{l=1}^{n_e}e_{k-l}^{d_i, l}\\prod_{m=1}^{n_y}y_{k-m}^{a_i, m} \\end{align} \\tag{2.19} \\] <p>where \\(p\\) is the number of regressors, \\(\\Theta_i\\) are the model parameters, and \\(a_i, m\\), \\(b_i, j\\) and \\(d_i, l \\in \\mathbb{N}\\) are the exponents of the output, input and noise terms, respectively.</p> <p>The Equation 2.20 describes a polynomial NARMAX model where the nonlinearity degree is equal to \\(2\\), identified from experimental data of a DC motor/generator with no prior knowledge of the model form, taken from Lacerda Junior, W. R, Almeida, V. M., Martins, S. A. M.:</p> \\[ \\begin{align}   y_k =&amp; 1.7813y_{k-1}-0.7962y_{k-2}+0.0339x_{k-1} -0.1597x_{k-1} y_{k-1} +0.0338x_{k-2} + \\\\   &amp; + 0.1297x_{k-1}y_{k-2} - 0.1396x_{k-2}y_{k-1}+ 0.1086x_{k-2}y_{k-2}+0.0085y_{k-2}^2 + 0.0247e_{k-1}e_{k-2} \\end{align} \\tag{2.20} \\] <p>The \\(\\Theta\\) values are the coefficients of each term of the polynomial equation.</p> <p>Polynomial basis functions are one of the most used representations of NARMAX models due to several interesting attributes, such as (Billings, S. A.):</p> <ul> <li>All polynomial functions are smooth in \\(\\mathbb{R}\\).</li> <li>The Weierstrass approximation theorem states that any continuous real-valued function defined on a closed and bounded space \\([a,b]\\) can be uniformly approximated using a polynomial on that interval.</li> <li>They can describe several nonlinear dynamical systems, including industrial processes, control systems, structural systems, economic and financial systems, biology, medicine, and social systems (some examples are detailed in Lacerda Junior, W. R. and Martins, S. A. M. and Nepomuceno, E. G. and Lacerda, Marcio J. ; Fung, E. H. K. and Wong, Y. K. and Ho, H. F. and Mignolet, M. P.; Kukreja, S. L. and Galiana, H. L. and Kearney, R. E.; Billings, S. A; Aguirre, L. A.; and many others).</li> <li>Several algorithms have been developed for structure selection and parameter estimation of polynomial NARMAX models, and it remains an active area of research.</li> <li>Polynomial NARMAX models are versatile and can be used both for prediction and inference. The structure of polynomial NARMAX models are easy to interpret and can be related to the underlying system, which is much harder to achieve with neural networks or wavelet functions, for instance.</li> </ul> <p>You can easily build a polynomial NARMAX model using SysIdentPy. Note that the difference for ARMAX, in this case, is the degree of the polynomial function.</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=True)\n)\n</code></pre> <p>One could think that is a simple change, but in nonlinear scenarios the course of dimensionality becomes a real problem. The number of candidate regressors, \\(n_r\\), of polynomial NARX can be defined as (Korenberg, M. L. and Billings, S. A. and Liu, Y. P. and McIlroy, P. J.):</p> \\[ \\begin{equation}     n_r = M+1, \\end{equation} \\tag{2.21} \\] <p>where</p> \\[ \\begin{align}     M = &amp; \\sum_{i=1}^{\\ell}n_i \\\\     n_i = &amp; \\frac{n_{i-1}(n_y+n_x+i-1)}{i}, n_{0} = 1. \\end{align} \\tag{2.22} \\] <p>As we mentioned in the Introduction of the book, NARMAX methods aims to build the simplest models possible. The idea is to be reproduce a wide range of behaviors using a small subset of terms from the vast search space formed by candidate regressors.</p> <p>Lets use SysIdentPy to see how the search space grows in the linear versus the nonlinear scenario. The <code>regressor_code</code> method available in <code>narmax_tools</code> can be used the check how many regressors exists in the search space given the number of inputs, the delays of <code>y</code> and <code>x</code> regressors and the basis function. We will use <code>xlag=ylag=10</code> and the polynomial basis function. The user can simulate different scenarios by setting different parameters.</p> <pre><code>from sysidentpy.utils.narmax_tools import regressor_code\nfrom sysidentpy.basis_function._basis_function import Polynomial]\nimport numpy as np\n</code></pre> <p>For the linear case with 1 input we have 21 regressors: <pre><code>x_train = np.random.rand(10, 1)  # simulating a case with 1 input\nbasis_function = Polynomial(degree=1)\nregressors = regressor_code(\n\u00a0 \u00a0 X=x_train,\n\u00a0 \u00a0 xlag=10,\n\u00a0 \u00a0 ylag=10,\n\u00a0 \u00a0 model_type=\"NARMAX\",\n\u00a0 \u00a0 model_representation=\"Polynomial\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\nn_regressors = regressors.shape[0] \u00a0# the number of features of the NARX net\nn_regressors\n&gt;&gt;&gt; 21\n</code></pre></p> <p>For the linear case with 2 inputs, the number of regressors jumps to 111:</p> <pre><code>x_train = np.random.rand(10, 2)  # simulating a case with 2 inputs\nbasis_function = Polynomial(degree=1)\nxlag = [list(range(1, 11))] * x_train.shape[1]\nregressors = regressor_code(\n\u00a0 \u00a0 X=x_train,\n\u00a0 \u00a0 xlag=xlag,\n\u00a0 \u00a0 ylag=10,\n\u00a0 \u00a0 model_type=\"NARMAX\",\n\u00a0 \u00a0 model_representation=\"Polynomial\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\nn_regressors = regressors.shape[0] \u00a0# the number of features of the NARX net\nn_regressors\n&gt;&gt;&gt; 111\n</code></pre> <p>If we consider a nonlinear case with 1 input by just changing the degree to 2, we have 231 regressors.</p> <pre><code>x_train = np.random.rand(10, 1)  # simulating a case with 1 input\nbasis_function = Polynomial(degree=2)\nregressors = regressor_code(\n\u00a0 \u00a0 X=x_train,\n\u00a0 \u00a0 xlag=10,\n\u00a0 \u00a0 ylag=10,\n\u00a0 \u00a0 model_type=\"NARMAX\",\n\u00a0 \u00a0 model_representation=\"Polynomial\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\nn_regressors = regressors.shape[0] \u00a0# the number of features of the NARX net\nn_regressors\n&gt;&gt;&gt; 231\n</code></pre> <p>If we set the degree to 3, the number of terms increases significantly to 1771 regressors.</p> <pre><code>x_train = np.random.rand(10, 1)  # simulating a case with 1 input\nbasis_function = Polynomial(degree=2)\nregressors = regressor_code(\n\u00a0 \u00a0 X=x_train,\n\u00a0 \u00a0 xlag=10,\n\u00a0 \u00a0 ylag=10,\n\u00a0 \u00a0 model_type=\"NARMAX\",\n\u00a0 \u00a0 model_representation=\"Polynomial\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\nn_regressors = regressors.shape[0] \u00a0# the number of features of the NARX net\nn_regressors\n&gt;&gt;&gt; 1771\n</code></pre> <p>If you have 2 inputs in the nonlinear scenario with <code>degree=2</code>, the number of regressors is 496:</p> <pre><code>x_train = np.random.rand(10, 2)  # simulating a case with 2 input\nbasis_function = Polynomial(degree=2)\nxlag = [list(range(1, 11))] * x_train.shape[1]\nregressors = regressor_code(\n\u00a0 \u00a0 X=x_train,\n\u00a0 \u00a0 xlag=xlag,\n\u00a0 \u00a0 ylag=10,\n\u00a0 \u00a0 model_type=\"NARMAX\",\n\u00a0 \u00a0 model_representation=\"Polynomial\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\nn_regressors = regressors.shape[0] \u00a0# the number of features of the NARX net\nn_regressors\n&gt;&gt;&gt; 496\n</code></pre> <p>If you have 2 inputs in the nonlinear scenario with <code>degree=3</code>, the number jumps to 5456 regressors:</p> <pre><code>x_train = np.random.rand(10, 2)  # simulating a case with 2 input\nbasis_function = Polynomial(degree=3)\nxlag = [list(range(1, 11))] * x_train.shape[1]\nregressors = regressor_code(\n\u00a0 \u00a0 X=x_train,\n\u00a0 \u00a0 xlag=xlag,\n\u00a0 \u00a0 ylag=10,\n\u00a0 \u00a0 model_type=\"NARMAX\",\n\u00a0 \u00a0 model_representation=\"Polynomial\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\nn_regressors = regressors.shape[0] \u00a0# the number of features of the NARX net\nn_regressors\n&gt;&gt;&gt; 5456\n</code></pre> <p>As you can notice, the number of regressors increases significantly as the degree of the polynomial and the number of inputs increases. That makes the model structure selection much more complex! In the linear case with 10 inputs we have <code>2^31=2.15e+09</code> possible model combinations. When <code>degree=2</code> with 2 inputs we have <code>2^496=2.05e+149</code> possible combinations! Try to get the number of possible model combinations when <code>degree=3</code> with 2 inputs. Moreover, try that with more inputs and higher nonlinear degree and see how the course of dimensionality is a big problem.</p> <p>As you can see, getting a simple model in such a large search space is complex model structure selection task. To select the most significant terms from a huge dictionary of possible terms is not an easy task. And it is hard not only because the complex combinatoric problem and the uncertainty concerning the model order. Identifying the most significant terms in a nonlinear scenario is very difficult because depends on the type of the nonlinearity (sparse singularity or near-singular behavior, memory or dumping effects and many others), dynamical response (spatial-temporal systems, time-dependent), the steady-state response,  frequency of the data, the noise and many more.</p> <p>Because of the model structure selection algorithms developed for NARMAX models, even linear models like ARMAX can have different performance when obtained using SysIdentPy when compared to other libraries, like Statsmodels. We have a case study showing exactly that in Chapter 10.</p>"},{"location":"book/2%20-%20NARMAX%20Model%20Representation/#narx","title":"NARX","text":"<p>If we do not include noise terms \\(e_{k-n_e}\\)  in Equation (2.19), we have NARX models.</p> \\[ \\begin{align}   y_k = \\sum_{i=1}^{p}\\Theta_i \\times \\prod_{j=0}^{n_x}x_{k-j}^{b_i, j}\\prod_{m=1}^{n_y}y_{k-m}^{a_i, m} \\end{align} \\tag{2.23} \\] <p>The Equation 2.24 describes a simple polynomial NARX model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}^2+0.1139y_{k-1}x_{k-1} \\end{align} \\tag{2.24} \\] <p>The only difference in SysIdentPy is setting the <code>unbiased=False</code></p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False)\n)\n</code></pre> <p>The user can use the codes provided for linear models to analyse the nonlinear models with different noise realizations.</p>"},{"location":"book/2%20-%20NARMAX%20Model%20Representation/#narma","title":"NARMA","text":"<p>if we do not include input terms in Equation 2.19, it turns to NARMA model</p> \\[ \\begin{align}   y_k = \\sum_{i=1}^{p}\\Theta_i \\times\\prod_{l=1}^{n_e}e_{k-l}^{d_i, l}\\prod_{m=1}^{n_y}y_{k-m}^{a_i, m} \\end{align} \\tag{2.25} \\] <p>The following example is a polynomial NARMA model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}^3+0.1139y_{k-3}y_{k-4} + 0.2245e_{k-1} \\end{align} \\tag{2.26} \\] <p>Since the model representation do not have inputs, we have to set the model type to <code>NAR</code> and set <code>unbiased=True</code> again in <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=True),\n    model_type=\"NAR\"\n)\n</code></pre>"},{"location":"book/2%20-%20NARMAX%20Model%20Representation/#nar","title":"NAR","text":"<p>if we do not include input terms and noise terms in Equation 2.19, it turns to AR model</p> \\[ \\begin{align}   y_k = \\sum_{i=1}^{p}\\Theta_i \\times\\prod_{m=1}^{n_y}y_{k-m}^{a_i, m} \\end{align} \\tag{2.27} \\] <p>The following example is a polynomial NAR model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}^2+0.1139y_{k-3}^3 -0.1691y_{k-4}y_{k-5} \\end{align} \\tag{2.28} \\] <p>In this case, we have to set the model type to <code>NAR</code> and set <code>unbiased=False</code> in <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    model_type=\"NAR\"\n)\n</code></pre>"},{"location":"book/2%20-%20NARMAX%20Model%20Representation/#nfir","title":"NFIR","text":"<p>If we only keep input terms in Equation 2.19, it becomes a NFIR model</p> \\[ \\begin{align}   y_k = \\sum_{i=1}^{p}\\Theta_i \\times \\prod_{j=0}^{n_x}x_{k-j}^{b_i, j} \\end{align} \\tag{2.29} \\] <p>The following example is a polynomial NFIR model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213x_{k-1}-0.5692x_{k-2}^2+0.1139x_{k-3}x_{k-4} -0.1691x_{k-4}^3 \\end{align} \\tag{2.30} \\] <p>In this case, we have to set the model type to <code>NFIR</code> and set <code>unbiased=False</code> in <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    model_type=\"NFIR\"\n)\n</code></pre>"},{"location":"book/2%20-%20NARMAX%20Model%20Representation/#mixed-narmax-models","title":"Mixed NARMAX Models","text":"<p>In some applications, using a single basis functions cannot provide a satisfactory description for the relationship between the input (or independent) variables and the output (or response) variable. In order to improve the performance of the model, it has been proposed to use a linear combination of a set of nonlinear functions to replace the linear counterparts.</p> <p>You can achieve that in SysIdentPy using ensembles in basis functions. You can build a Fourier model where terms have interactions. You can also build a model with mixed basis functions, using terms expanded by polynomial basis and Fourier basis or any other basis function available is the package.</p> <p>You can only mix a basis function with the polynomial basis for now in SysIdentPy. You can mix Fourier with Polynomial, but you can't mix Fourier with Bernstein.</p> <p>To mix Fourier or Bernstein basis with Polynomial, the user just have to set <code>ensamble=True</code> in the basis function definition</p> <pre><code>from sysidentpy.basis_function import Fourier\n\nbasis_function = Fourier(degree=2, ensamble=True)\n</code></pre>"},{"location":"book/2%20-%20NARMAX%20Model%20Representation/#neural-narx-network","title":"Neural NARX Network","text":"<p>Neural networks are models composed of interconnected layers of nodes (neurons) designed for tasks like classification and regression. Each neuron is a basic unit within these networks. Mathematically, a neuron is represented by a function \\(f\\) that takes an input vector \\(\\mathbf{x} = [x_1, x_2, \\ldots, x_n]\\) and generates an output \\(y\\). This function usually involves a weighted sum of the inputs, an optional bias term \\(b\\), and an activation function \\(\\phi\\):</p> \\[ y = \\phi \\left( \\sum_{i=1}^{n} w_i x_i + b \\right) \\tag{2.31} \\] <p>where \\(\\mathbf{w} = [w_1, w_2, \\ldots, w_n]\\) are the weights associated with the inputs. The activation function \\(\\phi\\) introduces nonlinearity into the model, allowing the network to learn complex patterns. Common activation functions include:</p> <ul> <li> <p>Sigmoid: \\(\\phi(z) = \\frac{1}{1 + e^{-z}}\\)   Produces outputs between 0 and 1, making it useful for binary classification.</p> </li> <li> <p>Hyperbolic Tangent (tanh): \\(\\phi(z) = \\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\\)   Outputs values between -1 and 1, often used to center data around zero.</p> </li> <li> <p>Rectified Linear Unit (ReLU): \\(\\phi(z) = \\max(0, z)\\)   Outputs zero for negative values and the input value itself for positive values, helping to mitigate the vanishing gradient problem.</p> </li> <li> <p>Leaky ReLU: \\(\\phi(z) = \\max(0.01z, z)\\)   A variant of ReLU that allows a small, non-zero gradient when the input is negative, addressing the problem of dying neurons.</p> </li> <li> <p>Softmax: \\(\\phi(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\\)   Converts logits into probabilities for multi-class classification, ensuring that the outputs sum to 1.</p> </li> </ul> <p>Each activation function has its own advantages and is chosen based on the specific needs of the neural network and the task at hand.</p> <p>As mentioned, neural network is composed of multiple layers, each consisting of several neurons. In this respect, the layers can be categorized into:</p> <ul> <li>Input Layer: The layer that receives the input data.</li> <li>Hidden Layers: Intermediate layers that process the inputs through weighted connections and activation functions.</li> <li>Output Layer: The final layer that produces the output of the network.</li> </ul> <p>The network itself therefore has a very simple architecture. The terminology used in neural networks is also slightly different from the standard notation that is universal in system identification and statistics. So, instead of talking about model parameters, the term network weights is used, and instead of estimation, the term learning is used. This terminology was no doubt introduced to make it appear that something completely new was being discussed, whereas some of the problems addressed are quite traditional - Stephen A. Billings</p> <p>Notice that the network itself is simply a collection of nonlinear activation units \\(\\phi(\\cdot)\\)  that are simple static functions. There are no dynamics within the network. This is fine for applications such as pattern recognition, but to use the network in system identification lagged inputs and outputs are necessary and these have to be supplied as inputs either explicitly or through a recurrent procedure. In this respect, if we set \\(\\mathcal{F}\\) as a neural function, we can adapt it to create a neural NARX model by transforming the neural architecture into a NARX architecture. The neural NARX, however, is not linear in the parameters like the NARMAX models based on basis functions. So, algorithms like Orthogonal Least Squares are not adequate to estimate the weights of the model.</p> <p>SysIdentPy\u00a0support a Series-Parallel (open-loop) Feedforward Network training process, which make the training process easier. We convert the NARX network from Series-Parallel to the Parallel (closed-loop) configuration for prediction.</p> <p>Series-Parallel allows us to use <code>pytorch</code> directly for training, so SysIdentPy uses <code>pytorch</code> in the backend for neural NARX along with auxiliary methods available only in SysIdentPy.</p> <p>A simple neural NARX model can be represented as a Multi-Layer Perceptron neural network with autoregressive component along with delayed inputs.</p> <p></p> <p>Figure 9. Parallel and series-parallel neural network architectures for modeling the dynamic system \\(\\mathbf{y}[k]=\\mathbf{F}(\\mathbf{y}[k-1], \\mathbf{y}[k-2], \\mathbf{u}[k-1], \\mathbf{u}[k-2])\\). The delay operator \\(q^{-1}\\) is such that \\(\\mathbf{y}[k-1]=q^{-1} \\mathbf{y}[k]\\). Reference: Antonio H. Ribeiro and Luis A. Aguirre</p> <p>Neural NARX is not the same model as Recurrent Neural Networks (RNN). The user is referred to the following paper for more details A Note on the Equivalence of NARX and RNN</p> <p>To build a Neural NARX network in SysIdentPy, the user must use <code>pytorch</code>. We use <code>pytorch</code> to make the definition of the network architecture flexible. However, this require that the user have a better understanding of how a neural networks. See the script bellow of how to build a simple Neural NARX model in SysIdentPy</p> <pre><code>from torch import nn\nimport torch\n\nfrom sysidentpy.neural_network import NARXNN\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.narmax_tools import regressor_code\n\n# simulated data\nx_train, x_valid, y_train, y_valid = get_siso_data(\n\u00a0 \u00a0 n=1000, colored_noise=False, sigma=0.01, train_percentage=80\n)\n</code></pre> <p>The user can use <code>cuda</code> following the same approach when build a neural network in pytorch</p> <pre><code>torch.cuda.is_available()\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n</code></pre> <p>The user can create a NARXNN object and choose the maximum lag of both input and output for building the regressor matrix to serve as input of the network. In addition, you can choose the loss function, the optimizer, the optional parameters of the optimizer, the number of epochs.</p> <p>Because we built this feature on top of Pytorch, you can choose any of the loss function of the torch.nn.functional. Click here for a list of the loss functions you can use. You just need to pass the name of the loss function you want.</p> <p>Similarly, you can choose any of the optimizer of the torch.optim. Click here for a list of optimizer available.</p> <pre><code>basis_function = Polynomial(degree=1)\nnarx_net = NARXNN(\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 model_type=\"NARMAX\",\n\u00a0 \u00a0 loss_func=\"mse_loss\",\n\u00a0 \u00a0 optimizer=\"Adam\",\n\u00a0 \u00a0 epochs=2000,\n\u00a0 \u00a0 verbose=False,\n\u00a0 \u00a0 device=device,\n\u00a0 \u00a0 optim_params={\n\u00a0 \u00a0 \u00a0 \u00a0 \"betas\": (0.9, 0.999),\n\u00a0 \u00a0 \u00a0 \u00a0 \"eps\": 1e-05,\n\u00a0 \u00a0 }, \u00a0# optional parameters of the optimizer\n)\n</code></pre> <p>Because the NARXNN model were defined using \\(ylag=2\\), \\(xlag=2\\) and a polynomial basis function with \\(degree=1\\), we have a regressor matrix with 4 features. We need the size of the regressor matrix to build the layers of our network. Our input data(<code>x_train</code>) have only one feature, but since we are creating an NARX network, a regressor matrix is built behind the scenes with new features based on the <code>xlag</code> and <code>ylag</code>.</p> <p>If you need help finding how many regressors are created behind the scenes you can use the <code>narmax_tools</code> function <code>regressor_code</code> and take the size of the regressor code generated:</p> <pre><code>basis_function = Polynomial(degree=1)\nregressors = regressor_code(\n\u00a0 \u00a0 X=x_train, # t\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 model_type=\"NARMAX\",\n\u00a0 \u00a0 model_representation=\"neural_network\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\n\nn_features = regressors.shape[0] \u00a0# the number of features of the NARX net\nn_features\n&gt;&gt;&gt; 4\n</code></pre> <p>The configuration of your network follows exactly the same pattern of a network defined in Pytorch. The following representing our NARX neural network.</p> <pre><code>class NARX(nn.Module):\n\u00a0 \u00a0 def __init__(self):\n\u00a0 \u00a0 \u00a0 \u00a0 super().__init__()\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin = nn.Linear(n_features, 30)\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin2 = nn.Linear(30, 30)\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin3 = nn.Linear(30, 1)\n\u00a0 \u00a0 \u00a0 \u00a0 self.tanh = nn.Tanh()\n\n\n\u00a0 \u00a0 def forward(self, xb):\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin(xb)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.tanh(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin2(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.tanh(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin3(z)\n\u00a0 \u00a0 \u00a0 \u00a0 return z\n</code></pre> <p>The user have to pass the defined network to our NARXNN estimator and set <code>cuda</code> if available (or needed):</p> <pre><code>narx_net.net = NARX()\n\nif device == \"cuda\":\n\u00a0 \u00a0 narx_net.net.to(torch.device(\"cuda\"))\n</code></pre> <p>Because we have a fit (for training) and predict function for Polynomial NARMAX, we create the same pattern for the NARX net. So, you only have to fit and predict using the following:</p> <pre><code>narx_net.fit(X=x_train, y=y_train, X_test=x_valid, y_test=y_valid)\nyhat = narx_net.predict(X=x_valid, y=y_valid)\n</code></pre> <p>If the net configuration is built before calling the NARXNN, just pass the model to the NARXNN as follows:</p> <pre><code>class NARX(nn.Module):\n\u00a0 \u00a0 def __init__(self):\n\u00a0 \u00a0 \u00a0 \u00a0 super().__init__()\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin = nn.Linear(n_features, 30)\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin2 = nn.Linear(30, 30)\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin3 = nn.Linear(30, 1)\n\u00a0 \u00a0 \u00a0 \u00a0 self.tanh = nn.Tanh()\n\n\n\u00a0 \u00a0 def forward(self, xb):\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin(xb)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.tanh(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin2(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.tanh(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin3(z)\n\u00a0 \u00a0 \u00a0 \u00a0 return z\n\n\nnarx_net2 = NARXNN(\n\u00a0 \u00a0 net=NARX(),\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 model_type=\"NARMAX\",\n\u00a0 \u00a0 loss_func=\"mse_loss\",\n\u00a0 \u00a0 optimizer=\"Adam\",\n\u00a0 \u00a0 epochs=2000,\n\u00a0 \u00a0 verbose=False,\n\u00a0 \u00a0 optim_params={\n\u00a0 \u00a0 \u00a0 \u00a0 \"betas\": (0.9, 0.999),\n\u00a0 \u00a0 \u00a0 \u00a0 \"eps\": 1e-05,\n\u00a0 \u00a0 }, \u00a0# optional parameters of the optimizer\n)\n\nnarx_net2.fit(X=x_train, y=y_train)\nyhat = narx_net2.predict(X=x_valid, y=y_valid)\n</code></pre>"},{"location":"book/2%20-%20NARMAX%20Model%20Representation/#general-model-set-representation","title":"General Model Set Representation","text":"<p>Based on the ideia of transforming a static neural network in a neural NARX model, we can extend the method for basically any model class. SysIdentPy do not aim to implement every model class that exists in literature. However, we created a functionality that allows the usage of any other machine learning package that follows a <code>fit</code> and <code>predict</code> API inside SysIdentPy to convert such models to NARX versions of them.</p> <p>Lets take XGboost (eXtreme Gradient Boosting) Algorithm as an example. XGBoost is a well kown model class used for regression tasks. XGboost, however, are not a common choice when you are dealing with a dynamical system identification task because they are originally made for modeling static systems. You can easily transform XGboost into a NARX model using SysIdentPy.</p> <p>Scikit-learn, for example, is another great example. You can transform any Scikit-learn model into NARX models using SysIdentPy. We will see such applications in detail at Chapter 11, but you can see how easy it in the script bellow</p> <pre><code>from sysidentpy.general_estimators import NARX\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sklearn.linear_model import BayesianRidge\nimport xgboost as xgb\n\nbasis_function = Fourier(degree=1)\n# define the scikit estimator\nscikit_estimator = BayesianRidge()\n# transform scikit_estimator into NARX model\ngb_narx = NARX(\n    base_estimator=scikit_estimator,\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\ngb_narx.fit(X=x_train, y=y_train)\nyhat = gb_narx.predict(X=x_valid, y=y_valid)\n\n# XGboost examples\nxgb_estimator = xgb.XGBRegressor()\nxgb_narx = NARX(\n    base_estimator=xgb_estimator,\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\nxgb_narx.fit(X=x_train, y=y_train)\nyhat = xgb_narx.predict(X=x_valid, y=y_valid)\n</code></pre> <p>You can use any other model by just changing the model class and passing it to the <code>base_estimator</code> in <code>NARX</code> functionality.</p>"},{"location":"book/2%20-%20NARMAX%20Model%20Representation/#mimo-models","title":"MIMO Models","text":"<p>To keep things simple, only SISO models were represented in previous sections. However,  the NARMAX  models can effortlessly be extended to MIMO case (Billings, S. A. and Chen, S. and Korenberg, M. J.):</p> \\[ \\begin{align}      y_{{_i}k}=&amp; F_{{_i}}^\\ell \\bigl[y_{{_1}k-1},  \\dotsc, y_{{_1}k-n^i_{y{_1}}},\\dotsc, y_{{_s}k-1},  \\dotsc, y_{{_s}k-n^i_{y{_s}}}, x_{{_1}k-d}, \\\\      &amp; x_{{_1}k-d-1}, \\dotsc, x_{{_1}k-d-n^i_{x{_1}}}, \\dotsc, x_{{_r}k-d}, x_{{_r}k-d-1}, \\dotsc, x_{{_r}k-d-n^i_{x{_r}}}\\bigr] + \\xi_{{_i}k}, \\end{align} \\tag{2.32} \\] <p>where for \\(i = 1, \\dotsc, s\\), each linear in the parameter sub-model can change regarding different maximum lags. More generally, considering</p> \\[ \\begin{align}     Y_k = \\begin{bmatrix}     y_{{_1}k} \\\\     y_{{_2}k} \\\\     \\vdots \\\\     y_{{_s}k}     \\end{bmatrix},     X_k = \\begin{bmatrix}     x_{{_1}k} \\\\     x_{{_2}k} \\\\     \\vdots \\\\     x_{{_r}k}     \\end{bmatrix},     \\Xi_k = \\begin{bmatrix}     \\xi_{{_1}k} \\\\     \\xi_{{_2}k} \\\\     \\vdots \\\\     \\xi_{{_r}k}     \\end{bmatrix}, \\end{align} \\tag{2.33} \\] <p>the MIMO model can be denoted as</p> \\[ \\begin{equation}              Y_k= F^\\ell[Y_{k-1},  \\dotsc, Y_{k-n_y},X_{k-d}, X_{k-d-1}, \\dotsc, X_{k-d-n_x}] + \\Xi_k, \\end{equation} \\tag{2.34} \\] <p>where \\(Xk ~= \\{x_{{_1}k}, x_{{_2}k}, \\dotsc, x_{{_r}k}\\}\\in \\mathbb{R}^{n^i_{x{_r}}}\\) and \\(Yk~= \\{y_{{_1}k}, y_{{_2}k}, \\dotsc, y_{{_s}k}\\}\\in \\mathbb{R}^{n^i_{y{_s}}}\\). The number of possibles terms of MIMO NARX model given the \\(i\\)-th polynomial degree, \\(\\ell_i\\), is:</p> \\[ \\begin{equation}     n_{{_{m}}r} = \\sum_{j = 0}^{\\ell_i}n_{ij}, \\end{equation} \\tag{2.35} \\] <p>where</p> \\[ \\begin{align}     n_{ij} = \\frac{ n_{ij-1} \\biggl[ \\sum\\limits_{k=1}^{s} n^i_{y_k} + \\sum\\limits_{k=1}^{r} n^i_{x_k} + j - 1 \\biggr]}{j}, \\qquad n_{i0}=1, j=1, \\dotsc, \\ell_i. \\end{align} \\tag{2.36} \\] <p>If \\(s=1\\), we have a MISO model that can be represented by a single polynomial function. Additionally, a MIMO model can be decomposed into MISO models, as presented in the following figure:</p> <p></p> <p>Figure 10. A MIMO model split into individual MISO models.</p> <p>SysIdentPy do not support MIMO models yet, only MISO models. You can, however, decompose a MIMO system as presented in Figure 9 and use SysIdentPy to create models for each subsystem.</p>"},{"location":"book/3%20-%20Parameter%20Estimation/","title":"3. Parameter Estimation","text":""},{"location":"book/3%20-%20Parameter%20Estimation/#least-squares","title":"Least Squares","text":"<p>Consider the NARX model described in a generic form as</p> \\[ \\begin{equation}     y_k = \\psi^\\top_{k-1}\\hat{\\Theta} + \\xi_k, \\end{equation} \\tag{3.1} \\] <p>where \\(\\psi^\\top_{k-1} \\in \\mathbb{R}^{n_r \\times n}\\) is the information matrix, also known as the regressors matrix. The information matrix is the input and output transformation based in a basis function and \\(\\hat{\\Theta}~\\in \\mathbb{R}^{n_{\\Theta}}\\) the vector of estimated parameters. The model above can also be represented in a matrix form as:</p> \\[ \\begin{equation}     y = \\Psi\\hat{\\Theta} + \\Xi, \\end{equation} \\tag{3.2} \\] <p>where</p> \\[ \\begin{align}     Y = \\begin{bmatrix}     y_1 \\\\     y_2 \\\\     \\vdots \\\\     y_n     \\end{bmatrix},     \\Psi = \\begin{bmatrix}     \\psi_{{_1}} \\\\     \\psi_{{_2}} \\\\     \\vdots \\\\     \\psi_{{_{n_{\\Theta}}}}     \\end{bmatrix}^\\top=     \\begin{bmatrix}     \\psi_{{_1}1} &amp; \\psi_{{_2}1} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}1} \\\\     \\psi_{{_1}2} &amp; \\psi_{{_2}2} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}2} \\\\     \\vdots &amp; \\vdots &amp;       &amp; \\vdots \\\\     \\psi_{{_1}n} &amp; \\psi_{{_2}n} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}n} \\\\     \\end{bmatrix},     \\hat{\\Theta} = \\begin{bmatrix}     \\hat{\\Theta}_1 \\\\     \\hat{\\Theta}_2 \\\\     \\vdots \\\\     \\hat{\\Theta}_{n_\\Theta}     \\end{bmatrix},     \\Xi = \\begin{bmatrix}     \\xi_1 \\\\     \\xi_2 \\\\     \\vdots \\\\     \\xi_n     \\end{bmatrix}. \\end{align} \\tag{3.3} \\] <p>We will consider the polynomial basis function to keep the examples straightforward, but the methods here will work for any other basis function.</p> <p>The parametric NARX model is linear in the parameters \\(\\Theta\\), so we can use well known algorithms, like the linear Least Squares algorithm developed by Gauss in \\(1795\\), to estimate the model parameters. The idea is to find the parameter vector that minimizes the \\(l2\\)-norm, also known as the residual sum of squares, described as</p> \\[ \\begin{equation}     J_{\\hat{\\Theta}} = \\Xi^\\top \\Xi = (y - \\Psi\\hat{\\Theta})^\\top(y - \\Psi\\hat{\\Theta}) = \\lVert y - \\Psi\\hat{\\Theta} \\rVert^2. \\end{equation} \\tag{3.4} \\] <p>In Equation 3.4 , \\(\\Psi\\hat{\\Theta}\\) is the one-step ahead prediction of \\(y_k\\), expressed as</p> \\[ \\begin{equation}     \\hat{y}_{1_k} = g(y_{k-1}, u_{k-1}\\lvert ~\\Theta), \\end{equation} \\tag{3.5} \\] <p>where \\(g\\) is some unknown polynomial function. If the gradient of \\(J_{\\Theta}\\) with respect to \\(\\Theta\\) is equal to zero, then we have the normal equation and the Least Squares estimate is expressed as</p> \\[ \\begin{equation}     \\hat{\\Theta}  = (\\Psi^\\top\\Psi)^{-1}\\Psi^\\top y, \\end{equation} \\tag{3.6} \\] <p>where \\((\\Psi^\\top\\Psi)^{-1}\\Psi^\\top\\) is called the pseudo-inverse of the matrix \\(\\Psi\\), denoted \\(\\Psi^+ \\in \\mathbb{R}^{n \\times n_r}\\).</p> <p>In order to have a bias-free estimator, the following are the basic assumptions needed for the least-squares method: - A1 - There is no correlation between the error vector, \\(\\Xi\\), and the matrix of regressors, \\(\\Psi\\). Mathematically: - \\(\\mathrm{E}\\{[(\\Psi^\\top\\Psi)^{-1}\\Psi^\\top] \\Xi\\} = \\mathrm{E}[(\\Psi^\\top\\Psi)^{-1}\\Psi^\\top] \\mathrm{E}[\\Xi]; \\tag{3.7}\\) - A2 - The error vector \\(\\Xi\\) is a zero mean white noise sequence: - \\(\\mathrm{E}[\\Xi] = 0; \\tag{3.8}\\) - A3 - The covariance matrix of the error vector is - \\(\\mathrm{Cov}[\\hat{\\Theta}] = \\mathrm{E}[(\\Theta - \\hat{\\Theta})(\\Theta - \\hat{\\Theta})^\\top] = \\sigma^2(\\Psi^\\top\\Psi); \\tag{3.9}\\) - A4 - The matrix of regressors, \\(\\Psi\\), is full rank.</p> <p>The aforementioned assumptions are needed to guarantee that the Least Squares algorithm produce a unbiased final model.</p>"},{"location":"book/3%20-%20Parameter%20Estimation/#example","title":"Example","text":"<p>Lets see a practical example. Consider the model</p> \\[ y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-2} + e_{k} \\tag{3.10} \\] <p>We can generate the input <code>X</code>  and output <code>y</code> using SysIdentPy. Before getting in the details, let run a simple model using SysIdentPy. Because we know a priori that the system we are trying to have is not linear (the simulated system have an interaction term \\(0.1y_{k-1}x_{k-1}\\)) and the order is 2 (the maximum lag of the input and output), we will set the hyperparameters accordingly. Note that this a simulated scenario and you'll not have such information a priori in a real identification task. But don't worry, the idea, for now, is just show how things works and we will develop some real models along the book.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.display_results import results\n\nx_train, x_test, y_train, y_test = get_siso_data(\n\u00a0 \u00a0 n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    n_info_values=3,\n    ylag=1,\n    xlag=2,\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\n# print the identified model\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nRegressors   Parameters             ERR\n0        x1(k-2)  9.0001E-01  9.56885108E-01\n1         y(k-1)  2.0000E-01  3.96313039E-02\n2  x1(k-1)y(k-1)  1.0001E-01  3.48355000E-03\n</code></pre> <p>As you can see, the final model have the same 3 regressors of the simulated system and the parameters are very close the ones used to simulate the system. This show us that the Least Squares performed well for this data.</p> <p>In this example, however, we are applying a Model Structure Selection algorithm (FROLS), which we will see in chapter 6. That's why the final model have only 3 regressors. The parameter estimation algorithm do not choose which terms to include in the model, so if we have a expanded basis function with 6 regressors, it will estimate the parameter for each one of the regressors.</p> <p>To check how this work, we can use SysIdentPy without Model Structure Selection by generating the information matrix and applying the parameter estimation algorithm directly.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.narmax_base import InformationMatrix\n\n\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\nregressor_matrix = InformationMatrix(xlag=2, ylag=2).build_input_output_matrix(\n    X=x_train, y=y_train\n)\nmax_lag = 2\n# apply the basis function\npsi = Polynomial(degree=2).fit(regressor_matrix, max_lag=max_lag)\ntheta = LeastSquares().optimize(psi, y_train[max_lag:, :])\ntheta\n\narray([\n   [-1.08377785e-05],\n   [ 2.00002486e-01],\n   [ 1.73422294e-05],\n   [-3.50957931e-06],\n   [ 8.99927332e-01],\n   [ 2.04427279e-05],\n   [-1.47542408e-04],\n   [ 1.00062340e-01],\n   [ 4.53379771e-05],\n   [ 8.90006341e-05],\n   [ 1.15234873e-04],\n   [ 1.57770755e-04],\n   [ 1.58414037e-04],\n   [-3.09236444e-05],\n   [-1.60377753e-04]]\n   )\n</code></pre> <p>In this case, we have 15 model parameters. If we take a look in the basis function expansion where the degree of the polynomial is equal to 2 and the lags for <code>y</code> and <code>x</code> are set to 2, we have</p> <pre><code>from sysidentpy.utils.narmax_tools import regressor_code\nbasis_function = Polynomial(degree=2)\nregressors = regressor_code(\n\u00a0 \u00a0 X=x_train,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 model_type=\"NARMAX\",\n\u00a0 \u00a0 model_representation=\"Polynomial\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\nregressors\n\narray([[ 0, 0],\n   [1001, 0],\n   [1002, 0],\n   [2001, 0],\n   [2002, 0],\n   [1001, 1001],\n   [1002, 1001],\n   [2001, 1001],\n   [2002, 1001],\n   [1002, 1002],\n   [2001, 1002],\n   [2002, 1002],\n   [2001, 2001],\n   [2002, 2001],\n   [2002, 2002]]\n   )\n</code></pre> <p>The regressors is how SysIdentPy encode the polynomial basis function following this  codification pattern:</p> <ul> <li>\\(0\\) is the constant term,\\n\",</li> <li>\\([1001] = y_{k-1}\\)</li> <li>\\([100n] = y_{k-n}\\)</li> <li>\\([200n] = x1_{k-n}\\)</li> <li>\\([300n] = x2_{k-n}\\)</li> <li>\\([1011, 1001] = y_{k-11} \\\\times y_{k-1}\\)</li> <li>\\([100n, 100m] = y_{k-n} \\times y_{k-m}\\)</li> <li>\\([12001, 1003, 1001] = x11_{k-1} \\times y_{k-3} \\times y_{k-1}\\),</li> <li>and so on</li> </ul> <p>So, if you take a look at the parameters, we can see that the Least Squares algorithm estimation for the terms that belongs to the simulated system are very close to the real values.</p> <pre><code>[1001, 0] -&gt; [ 2.00002486e-01]\n[2002, 0] -&gt; [ 8.99927332e-01]\n[2001, 1001] -&gt; [ 1.00062340e-01]\n</code></pre> <p>Moreover, the parameters estimated for the other regressors are considerably lower values than the ones estimated for the correct terms, indicating that the other might not be relevant to the model.</p> <p>You can start thinking that we only need to define a basis function and apply some parameter estimation technique to build NARMAX models. However, as mentioned before, the main goal of the NARMAX methods is to build the best model possible while keeping it simple. And that's true for the case where we applied the FROLS algorithm. Besides, when dealing with system identification we want to recover the dynamics of the system under study, so adding more terms than necessary can lead to unexpected behaviors, poor performance and unstable models. Remember, this is only a toy example, so in real cases the model structure selection is fundamental.</p> <p>You can implement Least Squares method as simple as</p> <pre><code>import numpy as np\n\ndef simple_least_squares(psi, y):\n\u00a0 \u00a0 return np.linalg.pinv(psi.T @ psi) @ psi.T @ y\n\n# use the psi and y data created in previous examples or\n# create them again here to run the example.\ntheta = simple_least_squares(psi, y_train[max_lag:, :])\n\ntheta\n\narray(\n    [\n       [-1.08377785e-05],\n       [ 2.00002486e-01],\n       [ 1.73422294e-05],\n       [-3.50957931e-06],\n       [ 8.99927332e-01],\n       [ 2.04427279e-05],\n       [-1.47542408e-04],\n       [ 1.00062340e-01],\n       [ 4.53379771e-05],\n       [ 8.90006341e-05],\n       [ 1.15234873e-04],\n       [ 1.57770755e-04],\n       [ 1.58414037e-04],\n       [-3.09236444e-05],\n       [-1.60377753e-04]\n    ]\n)\n</code></pre> <p>As you can see, the estimated parameters are very close. However, be careful when using such approach in under-, well-, or over-determined systems. We recommend to use the numpy or scipy <code>lstsq</code> methods.</p>"},{"location":"book/3%20-%20Parameter%20Estimation/#total-least-squares","title":"Total Least Squares","text":"<p>This section is based on the Overview of total least squares methods paper .</p> <p>The Total Least Squares (TLS) algorithm, is a statistical method used to find the best-fitting linear relationship between variables when both the input and output signals present white noise pertubation. Unlike ordinary least squares (OLS), which assumes that only the dependent variable is subject to error, TLS considers errors in all measured variables, providing a more robust solution in many practical applications. The algorithm was proposed by Golub and Van Loan.</p> <p>In TLS, we assume errors in both \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\), denoted as \\(\\Delta \\mathbf{X}\\) and \\(\\Delta \\mathbf{Y}\\), respectively. The true model becomes:</p> \\[ \\mathbf{Y} + \\Delta \\mathbf{Y} = (\\mathbf{X} + \\Delta \\mathbf{X}) \\mathbf{B} \\tag{3.11} \\] <p>Rearranging, we get:</p> \\[ \\Delta \\mathbf{Y} = \\Delta \\mathbf{X} \\mathbf{B} \\tag{3.12} \\]"},{"location":"book/3%20-%20Parameter%20Estimation/#objective-function","title":"Objective Function","text":"<p>The TLS solution minimizes the Frobenius norm of the total perturbations in \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\):</p> \\[ \\min_{\\Delta \\mathbf{X}, \\Delta \\mathbf{Y}} \\|[\\Delta \\mathbf{X}, \\Delta \\mathbf{Y}]\\|_F \\tag{3.13} \\] <p>subject to:</p> \\[ (\\mathbf{X} + \\Delta \\mathbf{X}) \\mathbf{B} = \\mathbf{Y} + \\Delta \\mathbf{Y} \\tag{3.14} \\] <p>where \\(\\| \\cdot \\|_F\\) denotes the Frobenius norm.</p>"},{"location":"book/3%20-%20Parameter%20Estimation/#classical-solution","title":"Classical Solution","text":"<p>The classical approach to solve the TLS problem is by using Singular Value Decomposition (SVD). The augmented matrix \\([\\mathbf{X}, \\mathbf{Y}]\\) is decomposed as:</p> \\[ [\\mathbf{X}, \\mathbf{Y}] = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T \\tag{3.15} \\] <p>where \\(\\mathbf{U}\\) is an \\(n \\times n\\) orthogonal matrix, \\(\\Sigma=\\operatorname{diag}\\left(\\sigma_1, \\ldots, \\sigma_{n+d}\\right)\\) is a diagonal matrix of singular values; and \\(\\mathbf{V}\\) is an orthogonal matrix defined as</p> \\[ V:=\\left[\\begin{array}{cc} V_{11} &amp; V_{12} \\\\ V_{21} &amp; V_{22} \\end{array}\\right] \\quad \\begin{aligned} \\end{aligned} \\quad \\text { and } \\quad \\Sigma:=\\left[\\begin{array}{cc} \\Sigma_1 &amp; 0 \\\\ 0 &amp; \\Sigma_2 \\end{array}\\right] \\begin{gathered} \\end{gathered} . \\tag{3.16} \\] <p>A total least squares solution exists if and only if \\(V_{22}\\) is non-singular. In addition, it is unique if and only if \\(\\sigma_n \\neq \\sigma_{n+1}\\). In the case when the total least squares solution exists and is unique, it is given by</p> \\[ \\widehat{X}_{\\mathrm{tls}}=-V_{12} V_{22}^{-1} \\tag{3.17} \\] <p>and the corresponding total least squares correction matrix is</p> \\[ \\Delta C_{\\mathrm{tls}}:=\\left[\\begin{array}{ll} \\Delta A_{\\mathrm{tls}} &amp; \\Delta B_{\\mathrm{tls}} \\end{array}\\right]=-U \\operatorname{diag}\\left(0, \\Sigma_2\\right) V^{\\top} . \\tag{3.18} \\] <p>This is implemented in SysIdentPy as follows:</p> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Estimate the model parameters using Total Least Squares method.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    \"\"\"\n    self._check_linear_dependence_rows(psi)\n    full = np.hstack((psi, y))\n    n = psi.shape[1]\n    _, _, v = np.linalg.svd(full, full_matrices=True)\n    theta = -v.T[:n, n:] / v.T[n:, n:]\n    return theta.reshape(-1, 1)\n</code></pre> <p>To use it in the modeling task, just import it like we did in the Least Squares example.</p> <p>From now on the examples will not include the Model Structure Selection step. The goal here is to focus on the parameter estimation methods. However, we already provided an example including MSS in the Least Squares section, so you will not have any problem to test that with other parameter estimation algorithms.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import TotalLeastSquares\nfrom sysidentpy.narmax_base import InformationMatrix\n\n\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\nregressor_matrix = InformationMatrix(xlag=2, ylag=2).build_input_output_matrix(\n    X=x_train, y=y_train\n)\nmax_lag = 2\n# apply the basis function\npsi = Polynomial(degree=2).fit(regressor_matrix, max_lag=max_lag)\ntheta = TotalLeastSquares().optimize(psi, y_train[max_lag:, :])\ntheta\n\narray([\n   [-4.29547026e-05],\n   [ 2.00085880e-01],\n   [-5.20210280e-05],\n   [ 4.03019271e-05],\n   [ 8.99982943e-01],\n   [-1.49492072e-05],\n   [ 9.72399351e-05],\n   [ 1.00072619e-01],\n   [ 7.25253323e-05],\n   [-1.37338363e-04],\n   [ 1.37115512e-04],\n   [ 2.42269843e-04],\n   [ 1.20466767e-04],\n   [ 6.32937185e-05],\n   [ 1.36101591e-04]])\n</code></pre>"},{"location":"book/3%20-%20Parameter%20Estimation/#recursive-least-squares","title":"Recursive Least Squares","text":"<p>Consider the regression model</p> \\[ y_k = \\mathbf{\\Psi}_k^T \\theta_k + \\epsilon_k \\tag{3.19}\\] <p>where: - \\(y_k\\) is the observed output at time $ k $. - \\(\\mathbf{\\Psi}_k\\) is the information matrix at time \\(k\\). - \\(\\theta_k\\) is the parameter vector to be estimated at time \\(k\\). - \\(\\epsilon_k\\) is the noise at time \\(k\\).</p> <p>The Recursive Least Squares (RLS) algorithm updates the parameter estimate \\(\\theta_k\\) recursively as new data points \\((\\mathbf{x}_k, y_k)\\) become available, minimizing a weighted linear least squares cost function relating to the information matrix in a sequencial manner. RLS is particularly useful in real-time applications where the data arrives sequentially and the model needs continuous updating or for modeling time varying systems (if the forgetting factor is included).</p> <p>Because its a recursive estimation, it is useful to relate \\(\\hat{\\Theta}_k\\) to \\(\\hat{\\Theta}_{k-1}\\). In other words, the new \\(\\hat{\\Theta}_k\\) depends on the last estimated value (k). Moreover, to estimate \\(\\hat{\\Theta}_k\\), we need to incorporate the current information present in \\(y_k\\).</p> <p>Aguirre BOOK defines the Recursive Least Squares estimator with forgetting factor \\(\\lambda\\) as</p> \\[ \\left\\{\\begin{array}{c} K_k= Q_k\\psi_k = \\frac{P_{k-1} \\psi_k}{\\psi_k^{\\mathrm{T}} P_{k-1} \\psi_k+\\lambda} \\\\ \\hat{\\theta}_k=\\hat{\\theta}_{k-1}+K_k\\left[y(k)-\\psi_k^{\\mathrm{T}} \\hat{\\theta}_{k-1}\\right] \\\\ P_k=\\frac{1}{\\lambda}\\left(P_{k-1}-\\frac{P_{k-1} \\psi_k \\psi_k^{\\mathrm{T}} P_{k-1}}{\\psi_k^{\\mathrm{T}} P_{k-1} \\psi_k+\\lambda}\\right) \\end{array}\\right. \\tag{3.20} \\] <p>where \\(K_k\\) is the gain vector calculation (also known as Kalman gain), \\(P_k\\) is the covariance matrix update, and \\(y_k - \\mathbf{\\Psi}_k^T \\theta_{k-1}\\) is the a priori estimation error. The forgetting factor \\(\\lambda\\) (\\(0 &lt; \\lambda \\leq 1\\)) is usually defined between \\(0.94\\) and \\(0.99\\). If you set \\(\\lambda = 1\\) you will be using the traditional recursive algorithm. The equation above consider that the regressor vector \\(\\psi(k-1)\\) has been rewritten as \\(\\psi_k\\), since this vector is updated at iteration \\(k\\) and contains information up to time instant \\(k-1\\). We can  Initialize the parameter estimate \\(\\theta_0\\) as</p> \\[ \\theta_0 = \\mathbf{0} \\tag{3.21}\\] <p>and Initialize the inverse of the covariance matrix \\(\\mathbf{P}_0\\) with a large value:</p> \\[ \\mathbf{P}_0 = \\frac{\\mathbf{I}}{\\delta} \\tag{3.22}\\] <p>where \\(\\delta\\) is a small positive constant, and \\(\\mathbf{I}\\) is the identity matrix.</p> <p>The forgetting factor \\(\\lambda\\) controls how quickly the algorithm forgets past data: - \\(\\lambda = 1\\) means no forgetting, and all past data are equally weighted. - \\(\\lambda &lt; 1\\) means that when new data is available, all weights are multiplied by \\(\\lambda\\), which can be interpreted as the ratio between consecutive weights for the same data.</p> <p>You can access the source code to check how SysIdentPy implements the RLS algorithm. The following example present how you can use it in SysIdentPy.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import RecursiveLeastSquares\nfrom sysidentpy.narmax_base import InformationMatrix\nimport matplotlib.pyplot as plt\n\nx_train, x_test, y_train, y_test = get_siso_data(\n\u00a0 \u00a0 n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nregressor_matrix = InformationMatrix(xlag=2, ylag=2).build_input_output_matrix(\n\u00a0 \u00a0 X=x_train, y=y_train\n)\nmax_lag = 2\npsi = Polynomial(degree=2).fit(regressor_matrix, max_lag=max_lag)\nestimator = RecursiveLeastSquares(lam=0.99)\ntheta = estimator.optimize(psi, y_train[max_lag:, :])\ntheta\n\narray(\n  [\n  [-1.42674741e-04],\n  [ 2.00108231e-01],\n  [-7.15658788e-05],\n  [-2.63118243e-05],\n  [ 9.00023979e-01],\n  [ 3.38729072e-04],\n  [ 8.22835678e-07],\n  [ 9.95974120e-02],\n  [ 8.52966331e-05],\n  [-1.47007370e-04],\n  [-4.65885373e-04],\n  [-2.57612698e-04],\n  [-2.61078457e-04],\n  [ 4.78599408e-04],\n  [ 3.50698606e-04]\n  ]\n  )\n</code></pre> <p>You can plot the evolution of the estimated parameters over time by accessing the <code>theta_evolution</code> values <pre><code># plotting only the first 50 values\nplt.plot(estimator.theta_evolution.T[:50, :])\nplt.xlabel(\"iterations\")\nplt.ylabel(\"theta\")\n</code></pre> </p> <p>Figure 1. Evolution of the estimated parameters over time using the RLS algorithm.</p>"},{"location":"book/3%20-%20Parameter%20Estimation/#least-mean-squares","title":"Least Mean Squares","text":"<p>The Least Mean Squares (LMS) adaptive filter is a popular stochastic gradient algorithm developed by Widrow and Hoff in 1960. The LMS adaptive filter aims to adaptively change its filter coefficients to achieve the best possible filtering of a signal. This is done by minimizing the error between the desired signal \\(d(n)\\) and the filter output \\(y(n)\\). We can derive the LMS algorithm from the RLS formulation.</p> <p>In RLS, the \\(\\lambda\\) is related to the minimization of the sum of weighted squares of the innovation</p> \\[ J_k = \\sum^k_{j=1}\\lambda^{k-j}e^2_j. \\tag{3.23} \\] <p>The \\(Q_k\\) in Equation 3.20, defined as</p> \\[ Q_k = \\frac{P_{k-1}}{\\psi_k^{\\mathrm{T}} P_{k-1} \\psi_k+\\lambda} \\\\ \\tag{3.24} \\] <p>is derived from the general form the Kalman Filter (KF) algorithm.</p> \\[ Q_k = \\frac{P_{k-1}}{\\psi_k^{\\mathrm{T}} P_{k-1} \\psi_k+v_0} \\\\ \\tag{3.25} \\] <p>where \\(v_0\\) is the variance of the noise in the definition of the KF, in which the cost function is defined as the sum of squares of the innovation (noise). You can check the details in the Billings, S. A. book</p> <p>If we change \\(Q_k\\) in Equation 3.25 to scaled identity matrix</p> \\[ Q_k = \\frac{\\mu}{\\Vert \\psi_k \\Vert^2}I \\tag{3.26} \\] <p>where \\(\\mu \\in \\mathbb{R}^+\\), the \\(Q_k\\) and \\(\\hat{\\theta}_k\\) in Equation 3.20 becomes</p> \\[ \\hat{\\theta}_k=\\hat{\\theta}_{k-1}+\\frac{\\mu\\left[y(k)-\\psi_k^{\\mathrm{T}} \\hat{\\theta}_{k-1}\\right]}{\\Vert \\psi_k \\Vert^2}\\psi_k \\tag{3.27} \\] <p>where \\(\\psi_k^{\\mathrm{T}} \\hat{\\theta}_{k-1} = \\hat{y}_k\\), which is known as the LMS algorithm.</p>"},{"location":"book/3%20-%20Parameter%20Estimation/#convergence-and-step-size","title":"Convergence and Step-Size","text":"<p>The step-size parameter \\(\\mu\\) plays a crucial role in the performance of the LMS algorithm. If \\(\\mu\\) is too large, the algorithm may become unstable and fail to converge. If \\(\\mu\\) is too small, the algorithm will converge slowly. The choice of \\(\\mu\\) is typically:</p> \\[ 0 &lt; \\mu &lt; \\frac{2}{\\lambda_{\\max}} \\tag{3.28} \\] <p>where \\(\\lambda_{\\max}\\) is the largest eigenvalue of the input signal\u2019s autocorrelation matrix.</p> <p>In SysIdentPy, you can use several variants of the LMS algorithm:</p> <ol> <li>LeastMeanSquareMixedNorm</li> <li>LeastMeanSquares</li> <li>LeastMeanSquaresFourth</li> <li>LeastMeanSquaresLeaky</li> <li>LeastMeanSquaresNormalizedLeaky</li> <li>LeastMeanSquaresNormalizedSignRegressor</li> <li>LeastMeanSquaresNormalizedSignSign</li> <li>LeastMeanSquaresSignError</li> <li>LeastMeanSquaresSignSign</li> <li>AffineLeastMeanSquares</li> <li>NormalizedLeastMeanSquares</li> <li>NormalizedLeastMeanSquaresSignError</li> <li>LeastMeanSquaresSignRegressor</li> </ol> <p>To use any one on the methods above, you just need to import it and set the <code>estimator</code> using the option you want:</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastMeanSquares\nfrom sysidentpy.narmax_base import InformationMatrix\nimport matplotlib.pyplot as plt\n\nx_train, x_test, y_train, y_test = get_siso_data(\n\u00a0 \u00a0 n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nregressor_matrix = InformationMatrix(xlag=2, ylag=2).build_input_output_matrix(\n\u00a0 \u00a0 X=x_train, y=y_train\n)\nmax_lag = 2\npsi = Polynomial(degree=2).fit(regressor_matrix, max_lag=max_lag)\nestimator = LeastMeanSquares(mu=0.1)\ntheta = estimator.optimize(psi, y_train[max_lag:, :])\ntheta\n\narray([[ 7.53481862e-05],\n      [ 2.00336327e-01],\n      [ 4.03075867e-04],\n      [ 5.16401623e-05],\n      [ 8.99539417e-01],\n      [-6.38551780e-04],\n      [-5.68962465e-05],\n      [ 1.00643282e-01],\n      [ 5.48097106e-04],\n      [ 5.15983109e-04],\n      [ 5.46693676e-04],\n      [-3.18759644e-04],\n      [-4.90420481e-04],\n      [ 4.09662242e-04],\n      [ 5.39826714e-04]])\n</code></pre>"},{"location":"book/3%20-%20Parameter%20Estimation/#extended-least-squares-algorithm","title":"Extended Least Squares Algorithm","text":"<p>Let's show an example of the effect of a biased parameter estimation. To make things simple,The data is generated by simulating the following model:</p> \\[ y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-2} + e_{k} \\] <p>In this case, we know the values of the true parameters, so it will be easier to understand how they are affected by a biased estimation. The data is generated using a method from SysIdentPy. If colored_noise is set to True in the method, a colored noise is added to the data:</p> \\[e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\] <p>where \\(x\\) is a uniformly distributed random variable and \\(\\nu\\) is a gaussian distributed variable with \\(\\mu=0\\) and \\(\\sigma\\) is defined by the user.</p> <p>We will generate a data with 1000 samples with white noise and selecting 90% of the data to train the model.</p> <pre><code>x_train, x_valid, y_train, y_valid = get_siso_data(\n\u00a0 \u00a0 n=1000, colored_noise=True, sigma=0.2, train_percentage=90\n)\n</code></pre> <p>First we will train a model without the Extended Least Squares Algorithm for comparison purpose.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares(unbiased=False)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=False,\n\u00a0 \u00a0 n_terms=3,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"aic\",\n\u00a0 \u00a0 estimator=estimator,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 err_tol=None,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\n\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\n</code></pre> Regressors Parameters ERR x1(k-2) 9.0442E-01 7.55518391E-01 y(k-1) 2.7405E-01 7.57565084E-02 x1(k-1)y(k-1) 9.8757E-02 3.12896171E-03 <p>Clearly we have something wrong with the obtained model. The estimated parameters differs from the true one defined in the equation that generated the data. As we can observe above, the model structure is exact the same the one that generate the data. You can se that the ERR ordered the terms in the correct way. And this is an important note regarding the ERR algorithm: it is very robust to colored noise!!</p> <p>That is a great feature! However, although the structure is correct, the model parameters are not correct! Here we have a biased estimation! For instance, the real parameter for \\(y_{k-1}\\) is \\(0.2\\), not \\(0.274\\).</p> <p>In this case, we are actually modeling using a NARX model, not a NARMAX. The MA part exists to allow a unbiased estimation of the parameters. To achieve a unbiased estimation of the parameters we have the Extend Least Squares algorithm.</p> <p>Before applying the Extended Least Squares Algorithm we will run several NARX models to check how different the estimated parameters are from the real ones.</p> <pre><code>parameters = np.zeros([3, 50])\nfor i in range(50):\n\u00a0 \u00a0 x_train, x_valid, y_train, y_valid = get_siso_data(\n\u00a0 \u00a0 \u00a0 \u00a0 n=3000, colored_noise=True, train_percentage=90\n\u00a0 \u00a0 )\n\u00a0 \u00a0 model.fit(X=x_train, y=y_train)\n\u00a0 \u00a0 parameters[:, i] = model.theta.flatten()\n\n# Set the theme for seaborn (optional)\nsns.set_theme()\nplt.figure(figsize=(14, 4))\n# Plot KDE for each parameter\nsns.kdeplot(parameters.T[:, 0], label='Parameter 1')\nsns.kdeplot(parameters.T[:, 1], label='Parameter 2')\nsns.kdeplot(parameters.T[:, 2], label='Parameter 3')\n# Plot vertical lines where the real values must lie\nplt.axvline(x=0.1, color='k', linestyle='--', label='Real Value 0.1')\nplt.axvline(x=0.2, color='k', linestyle='--', label='Real Value 0.2')\nplt.axvline(x=0.9, color='k', linestyle='--', label='Real Value 0.9')\nplt.xlabel('Parameter Value')\nplt.ylabel('Density')\nplt.title('Kernel Density Estimate of Parameters')\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Figure 2.: Kernel Density Estimates (KDEs) of the estimated parameters obtained from 50 NARX models realizations, each fitted to data with colored noise. The vertical dashed lines indicate the true parameter values used to generate the data. While the model structure is correctly identified, the estimated parameters are biased due to the omission of the Moving Average (MA) component, highlighting the need for the Extended Least Squares algorithm to achieve unbiased parameter estimation</p> <p>As shown in figure above, we have a problem to estimate the parameter for \\(y_{k-1}\\). Now we will use the Extended Least Squares Algorithm. In SysIdentPy, just set <code>unbiased=True</code> in the parameter estimation definition and the ELS algorithm will be applied.</p> <pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares(unbiased=True)\nparameters = np.zeros([3, 50])\nfor i in range(50):\n\u00a0 \u00a0 x_train, x_valid, y_train, y_valid = get_siso_data(\n\u00a0 \u00a0 \u00a0 \u00a0 n=3000, colored_noise=True, train_percentage=90\n\u00a0 \u00a0 )\n\u00a0 \u00a0 model = FROLS(\n\u00a0 \u00a0 \u00a0 \u00a0 order_selection=False,\n\u00a0 \u00a0 \u00a0 \u00a0 n_terms=3,\n\u00a0 \u00a0 \u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 \u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 \u00a0 \u00a0 elag=2,\n\u00a0 \u00a0 \u00a0 \u00a0 info_criteria=\"aic\",\n\u00a0 \u00a0 \u00a0 \u00a0 estimator=estimator,\n\u00a0 \u00a0 \u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 )\n\n\u00a0 \u00a0 model.fit(X=x_train, y=y_train)\n\u00a0 \u00a0 parameters[:, i] = model.theta.flatten()\n\nplt.figure(figsize=(14, 4))\n# Plot KDE for each parameter\nsns.kdeplot(parameters.T[:, 0], label='Parameter 1')\nsns.kdeplot(parameters.T[:, 1], label='Parameter 2')\nsns.kdeplot(parameters.T[:, 2], label='Parameter 3')\n# Plot vertical lines where the real values must lie\nplt.axvline(x=0.1, color='k', linestyle='--', label='Real Value 0.1')\nplt.axvline(x=0.2, color='k', linestyle='--', label='Real Value 0.2')\nplt.axvline(x=0.9, color='k', linestyle='--', label='Real Value 0.9')\nplt.xlabel('Parameter Value')\nplt.ylabel('Density')\nplt.title('Kernel Density Estimate of Parameters')\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Figure 3. Kernel Density Estimates (KDEs) of the estimated parameters obtained from 50 NARX models using the Extended Least Squares (ELS) algorithm with unbiased estimation. The vertical dashed lines indicate the true parameter values used to generate the data.</p> <p>Unlike the previous biased estimation, these KDEs in Figure 3 show that the estimated parameters are now closely aligned with the true values, demonstrating the effectiveness of the ELS algorithm in achieving unbiased parameter estimation, even in the presence of colored noise.</p> <p>The Extended Least Squares algorithm is iterative by nature. In SysIdentPy, the default number of iterations is set to 30 (<code>uiter=30</code>). However, the literature suggests that the algorithm typically converges quickly, often within 10 to 20 iterations. Therefore, you may want to test different numbers of iterations to find the optimal balance between convergence speed and computational efficiency.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/","title":"4. Model Structure Selection","text":""},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#introduction","title":"Introduction","text":"<p>This section is taken mainly from my master thesis, which was based on Billings, S. A.</p> <p>Selecting the model structure is crucial to develop models that can correctly reproduce the system behavior. If some prior information about the system are known, e.g., the dynamic order and degree of nonlinearity, determining the terms and then estimate the parameters is trivial. In real life scenarios, however, in most of the times there is no information about what terms should be included in the model and the correct regressors has to be selected in the identification framework. If the MSS is not performed with the necessary concerns, the scientific law that describes the system may will not be revealed and resulting in misleading interpretations about the system. To illustrate this scenario, consider the following example.</p> <p>Let \\(\\mathcal{D}\\) denote an arbitrary dataset</p> \\[ \\begin{equation}     \\mathcal{D} = \\{(x_k, y_k), k = 1, 2, \\dotsc, n\\}, \\end{equation} \\tag{1} \\] <p>where \\(x_k \\in \\mathbb{R}^{n_x}\\) and \\(y_k\\in \\mathbb{R}^{n_y}\\) are the input and output of an unknown system and \\(n\\) is the number of samples in the dataset. The following are two polynomial NARX models built to describe that system:</p> \\[ \\begin{align}     y_{ak} &amp;= 0.7077y_{ak-1} + 0.1642u_{k-1} + 0.1280u_{k-2} \\end{align} \\tag{2} \\] \\[ \\begin{align}     y_{bk} &amp;= 0.7103y_{bk-1} + 0.1458u_{k-1} + 0.1631u_{k-2} \\\\            &amp;\\quad - 1467y_{bk-1}^3 + 0.0710y_{bk-2}^3 + 0.0554y_{bk-3}^2u_{k-3}. \\end{align} \\tag{3} \\] <p>Figure 1 shows the predicted values of each model and the real data. As can be observed, the nonlinear model 2 seems to fit the data better than the linear model 1. The original system under consideration is an RLC circuit, consisting of a resistor (R), inductor (L), and capacitor (C) connected in series with a voltage source. It is well known that the behavior of such an RLC series circuit can be accurately described by a linear second-order differential equation that relates the current \\(I(t)\\) and the applied voltage \\(V(t)\\):</p> \\[ L\\frac{d^2I(t)}{dt^2} + R\\frac{dI(t)}{dt} + \\frac{1}{C}I(t) = \\frac{dV(t)}{dt} \\tag{4} \\] <p>Given this linear relationship, an adequate model for the RLC circuit should reflect this second-order linearity. While Model 2, which includes nonlinear terms, may provide a closer fit to the data, it is clearly over-parameterized. Such over-parameterization can introduce spurious nonlinear effects, often referred to as \"ghost\" nonlinearities, which do not correspond to the actual dynamics of the system. Therefore, these models need to be interpreted with caution, as the use of an overly complex model could obscure the true linear nature of the system and lead to incorrect conclusions about its behavior.</p> <p></p> <p>Figure 1.Results for two polynomial NARX models fitted to data from an unknown system. Model 1 (left) is a linear model, while Model 2 (right) includes nonlinear terms. The figure illustrates that Model 2 provides a closer fit to the data compared to Model 1. However, since the original system is a linear RLC circuit known to have a second-order linear behavior, the improved fit of Model 2 may be misleading due to over-parameterization. This highlights the importance of considering the physical characteristics of the system when interpreting model results to avoid misinterpretation of artificial nonlinearities. Reference: Meta Model Structure Selection: An Algorithm For Building Polynomial NARX Models For Regression And Classification</p> <p>Correctly identifying the structure of a model is crucial for accurately analyzing the system's dynamics. A well-chosen model structure ensures that the model reflects the true behavior of the system, allowing for consistent and meaningful analysis. In this respect, several algorithms have been developed to select the appropriate terms for constructing a polynomial NARX model. The primary goal of model structure selection (MSS) algorithms is to reveal the system's characteristics by producing the simplest model that adequately describes the data. While some systems may indeed require more complex models, it is essential to strike a balance between simplicity and accuracy. As Einstein aptly put it:</p> <p>A model should be as simple as possible, but not simpler.</p> <p>This principle emphasizes the importance of avoiding unnecessary complexity while ensuring that the model still captures the essential dynamics of the system.</p> <p>We see at chapter 2 that regressors selection, however, is not a simple task. If the nonlinear degree, the order of the model and the number inputs increases, the number of candidate models becomes too large for brute force approach. Considering the MIMO case, this problem is far worse than the SISO one if many inputs and outputs are required. The number of all different models can be calculated as</p> \\[ \\begin{align}     n_m =     \\begin{cases}     2^{n_r} &amp; \\text{for SISO models}, \\\\     2^{n_{{_{m}}r}} &amp; \\text{for MIMO models},     \\end{cases} \\end{align} \\tag{5} \\] <p>where \\(n_r\\) and  and \\(n_{{_{m}}r}\\) are the values computed using the equations presented in Chapter 2.</p> <p>A classical solution to regressors selection problem is the Forward Regression Orthogonal Least Squares (FROLS) algorithm associated with Error Reduction Ratio (ERR) algorithm. This technique is based on the Prediction Error Minimization framework and, one at time, select the most relevant regressor by using a step-wise regression. The FROLS method adapt the set of regressors in the search space into a set of orthogonal vectors, which ERR evaluates the individual contribution to the desired output variance.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#the-forward-regression-orthogonal-least-squares-algorithm","title":"The Forward Regression Orthogonal Least Squares Algorithm","text":"<p>Consider the general NARMAX model defined in Equation 2.23 described in a generic form as</p> \\[ \\begin{equation}     y_k = \\psi^\\top_{k-1}\\hat{\\Theta} + \\xi_k, \\end{equation} \\tag{6} \\] <p>where \\(\\psi^\\top_{k-1} \\in \\mathbb{R}^{n_r \\times n}\\) is a vector of some combinations of the regressors and \\(\\hat{\\Theta} \\in \\mathbb{R}^{n_{\\Theta}}\\) the vector of estimated parameters. In a more compact form, the NARMAX model can be represented in a matrix form as:</p> \\[ \\begin{equation}     y = \\Psi\\hat{\\Theta} + \\Xi, \\end{equation} \\tag{7} \\] <p>where</p> \\[ \\begin{align}     Y = \\begin{bmatrix}     y_1 \\\\     y_2 \\\\     \\vdots \\\\     y_n     \\end{bmatrix},     \\Psi = \\begin{bmatrix}     \\psi_{{_1}} \\\\     \\psi_{{_2}} \\\\     \\vdots \\\\     \\psi_{{_{n_{\\Theta}}}}     \\end{bmatrix}^\\top=     \\begin{bmatrix}     \\psi_{{_1}1} &amp; \\psi_{{_2}1} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}1} \\\\     \\psi_{{_1}2} &amp; \\psi_{{_2}2} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}2} \\\\     \\vdots &amp; \\vdots &amp;       &amp; \\vdots \\\\     \\psi_{{_1}n} &amp; \\psi_{{_2}n} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}n} \\\\     \\end{bmatrix},     \\hat{\\Theta} = \\begin{bmatrix}     \\hat{\\Theta}_1 \\\\     \\hat{\\Theta}_2 \\\\     \\vdots \\\\     \\hat{\\Theta}_{n_\\Theta}     \\end{bmatrix},     \\Xi = \\begin{bmatrix}     \\xi_1 \\\\     \\xi_2 \\\\     \\vdots \\\\     \\xi_n     \\end{bmatrix}. \\end{align} \\tag{8} \\] <p>The parameters in equation above could be estimated as a result of a Least Squares-based algorithm, but this would require to optimize all parameters at the same time on account of the fact of interaction between regressors due to non-orthogonality characteristic. Consequently, the computational demand becomes impractical for high number of regressors. In this respect, the FROLS transforms the non-orthogonal model presented in the equation above into a orthogonal one.</p> <p>The regressor matrix \\(\\Psi\\) can be orthogonally decomposed as</p> \\[ \\begin{equation}     \\Psi = QA, \\end{equation} \\tag{9} \\] <p>where \\(A \\in \\mathbb{R}^{n_{\\Theta}\\times n_{\\Theta}}\\) is an unit upper triangular matrix according to</p> \\[ \\begin{align} A =     \\begin{bmatrix}     1       &amp; a_{12} &amp; a_{13} &amp; \\dotsc &amp; a_{1n_{\\Theta}} \\\\     0       &amp;   1    &amp; a_{23} &amp; \\dotsc &amp; a_{2n_{\\Theta}} \\\\     0       &amp;   0    &amp;   1    &amp; \\dotsc &amp;     \\vdots       \\\\     \\vdots  &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; a_{n_{\\Theta}-1n_{\\Theta}} \\\\     0       &amp;  0     &amp;  0     &amp;  0     &amp; 1     \\end{bmatrix}, \\end{align} \\tag{10} \\] <p>and \\(Q \\in \\mathbb{R}^{n\\times n_{\\Theta}}\\) is a matrix with orthogonal columns \\(q_i\\), described as</p> \\[ \\begin{equation}     Q =         \\begin{bmatrix}         q_{{_1}} &amp; q_{{_2}} &amp; q_{{_3}} &amp; \\dotsc &amp; q_{{_{n_{\\Theta}}}}         \\end{bmatrix}, \\end{equation} \\tag{11} \\] <p>such that \\(Q^\\top Q = \\Lambda\\) and \\(\\Lambda\\) is diagonal with entry \\(d_i\\) and can be expressed as:</p> \\[ \\begin{align}     d_i = q_i^\\top q_i = \\sum^{k=1}_{n}q_{{_i}k}q_{{_i}k}, \\qquad 1\\leq i \\leq n_{\\Theta}. \\end{align} \\] <p>Because the space spanned by the orthogonal basis \\(Q\\) (Equation 11) is the same as that spanned by the basis set \\(\\Psi\\) (Equation 8) (i.e, contains every linear combination of elements of such subspace), we can define the Equation 7 as</p> \\[ \\begin{equation}     Y = \\underbrace{(\\Psi A^{-1})}_{Q}\\underbrace{(A\\Theta)}_{g}+ \\Xi = Qg+\\Xi, \\end{equation} \\tag{12} \\] <p>where \\(g\\in \\mathbb{R}^{n_\\Theta}\\) is an auxiliary parameter vector. The solution of the model described in Equation 12 is given by</p> \\[ \\begin{equation}     g = \\left(Q^\\top Q\\right)^{-1}Q^\\top Y = \\Lambda^{-1}Q^\\top Y \\end{equation} \\tag{13} \\] <p>or</p> \\[ \\begin{equation}     g_{{_i}} = \\frac{q_{{_i}}^\\top Y}{q_{{_i}}^\\top q_{{_i}}}. \\end{equation} \\tag{14} \\] <p>Since the parameter \\(\\Theta\\) and \\(g\\) satisfies the triangular system \\(A\\Theta = g\\), any orthogonalization method like Householder, Gram-Schmidt, modified Gram-Schmidt or Givens transformations can be used to solve the equation and estimate the original parameters. Assuming that \\(E[\\Psi^\\top \\Xi] = 0\\), the output variance can be derived by multiplying Equation 12 with itself and dividing by \\(n\\), resulting in</p> \\[ \\begin{equation}     \\frac{1}{n}Y^\\top Y = \\underbrace{\\frac{1}{n}\\sum^{i = 1}_{n_{\\Theta}}g_{{_i}}^2q^\\top_{{_i}}q_{{_i}}}_{\\text{{output explained by the regressors}}} + \\underbrace{\\frac{1}{n}\\Xi^\\top \\Xi}_{\\text{{unexplained variance}}}. \\end{equation} \\tag{15} \\] <p>Thus, the ERR due to the inclusion of the regressor \\(q_{{_i}}\\) is expressed as:</p> \\[ [\\text{ERR}]_i = \\frac{g_{i}^2 \\cdot q_{i}^\\top q_{i}}{Y^\\top Y}, \\qquad \\text{for } i=1,2,\\dotsc, n_\\Theta. \\] <p>There are many ways to terminate the algorithm. An approach often used is stop the algorithm if the model output variance drops below some predetermined limit \\(\\varepsilon\\):</p> \\[ \\begin{equation}     1 - \\sum_{i = 1}^{n_{\\Theta}}\\text{ERR}_i \\leq \\varepsilon, \\end{equation} \\tag{17} \\]"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#keep-it-simple","title":"Keep it simple","text":"<p>For the sake of simplicity, let's present the FROLS along with simple examples to make the intuition clear. First, let define the ERR calculation and then explain the idea of the FRLOS in simple terms.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#orthogonal-case","title":"Orthogonal case","text":"<p>Consider the case where we have a set of inputs defined as \\(x_1, x_2, \\ldots, x_n\\) and an output called \\(y\\). These inputs are orthogonal vectors.</p> <p>Lets suppose that we want to create a model to approximate \\(y\\)  using \\(x_1, x_2, \\ldots, x_n\\), as follows:</p> \\[ y=\\hat{\\theta}_1 x_1+\\hat{\\theta}_2 x_2+\\ldots+\\hat{\\theta}_n x_n+e \\tag{18} \\] <p>where \\(\\hat{\\theta}_1, \\hat{\\theta}_2, \\ldots, \\hat{\\theta}_n\\) are parameters and \\(e\\) is white noise and independent of \\(x\\) and \\(y\\) (remember the  \\(E[\\Psi^\\top \\Xi] = 0\\), in previous section). In this case, we can rewrite the equation above as</p> \\[ y = \\hat{\\theta} x \\tag{19} \\] <p>so</p> \\[ \\left\\langle x, y\\right\\rangle = \\left\\langle \\hat{\\theta} x, x\\right\\rangle = \\hat{\\theta} \\left\\langle x, x\\right\\rangle \\tag{20} \\] <p>Which implies that</p> \\[ \\hat{\\theta} = \\frac{\\left\\langle x, y\\right\\rangle}{\\left\\langle x, x\\right\\rangle} \\tag{21} \\] <p>Therefore we can show that</p> \\[ \\begin{align} &amp; \\left\\langle x_1, y\\right\\rangle=\\hat{\\theta}_1\\left\\langle x_1, x_1\\right\\rangle \\Rightarrow \\hat{\\theta}_1=\\frac{\\left\\langle x_1, y\\right\\rangle}{\\left\\langle x_1, x_1\\right\\rangle}=\\frac{x_1^T y}{x_1^T x_1} \\\\ &amp; \\left\\langle x_2, y\\right\\rangle=\\hat{\\theta}_2\\left\\langle x_2, x_2\\right\\rangle \\Rightarrow \\hat{\\theta}_2=\\frac{\\left\\langle x_2, y\\right\\rangle}{\\left\\langle x_2, x_2\\right\\rangle}=\\frac{x_2^T y}{x_2^T x_2}, \\ldots \\\\ &amp; \\left\\langle x_n, y\\right\\rangle=\\hat{\\theta}_n\\left\\langle x_n, x_n\\right\\rangle \\Rightarrow \\hat{\\theta}_n=\\frac{\\left\\langle x_n, y\\right\\rangle}{\\left\\langle x_n, x_n\\right\\rangle}=\\frac{x_n^T y}{x_n^T x_n}, \\end{align} \\tag{22} \\] <p>Following the same idea, we can also show that</p> \\[ \\langle y, y\\rangle=\\hat{\\theta}_1^2\\left\\langle x_1, x_1\\right\\rangle+\\hat{\\theta}_2^2\\left\\langle x_2, x_2\\right\\rangle+\\ldots+\\hat{\\theta}_n^2\\left\\langle x_n, x_n\\right\\rangle+\\langle e, e\\rangle \\tag{23} \\] <p>which can be described as</p> \\[ y^T y=\\hat{\\theta}_1^2 x_1^T x_1+\\hat{\\theta}_2^2 x_2^T x_2+\\ldots+\\hat{\\theta}_n^2 x_n^T x_n+e^T e \\tag{24} \\] <p>or</p> \\[ \\|y\\|^2=\\hat{\\theta}_1^2\\left\\|x_1\\right\\|^2+\\hat{\\theta}_2^2\\left\\|x_2\\right\\|^2+\\ldots+\\hat{\\theta}_n^2\\left\\|x_n\\right\\|^2+\\|e\\|^2 \\tag{25} \\] <p>So, dividing both sides of the equation by \\(y\\) and rearranging the equation, we have</p> \\[ \\frac{\\|e\\|^2}{\\|y\\|^2}=1-\\hat{\\theta}_1^2 \\frac{\\left\\|x_1\\right\\|^2}{\\|y\\|^2}-\\hat{\\theta}_2^2 \\frac{\\left\\|x_2\\right\\|^2}{\\|y\\|^2}-\\ldots-\\hat{\\theta}_n^2 \\frac{\\left\\|x_n\\right\\|^2}{\\|y\\|^2} \\tag{26} \\] <p>Because \\(\\hat{\\theta}_k=\\frac{x_k^T y}{x_k^T x_k}=\\frac{x_k^T y}{\\left\\|x_k\\right\\|^2}, k=1,2, . ., n\\), we have</p> \\[ \\begin{align} \\frac{\\|e\\|^2}{\\|y\\|^2} &amp; =1-\\left(\\frac{x_1^T y}{\\left\\|x_1\\right\\|^2}\\right)^2 \\frac{\\left\\|x_1\\right\\|^2}{\\|y\\|^2}-\\left(\\frac{x_2^T y}{\\left\\|x_2\\right\\|^2}\\right)^2 \\frac{\\left\\|x_2\\right\\|^2}{\\|y\\|^2}-\\ldots-\\left(\\frac{x_n^T y}{\\left\\|x_n\\right\\|^2}\\right)^2 \\frac{\\left\\|x_n\\right\\|^2}{\\|y\\|^2} \\\\ &amp; =1-\\frac{\\left(x_1^T y\\right)^2}{\\left\\|x_1\\right\\|\\left\\|^2\\right\\| y \\|^2}-\\frac{\\left(x_2^T y\\right)^2}{\\left\\|x_2\\right\\|^2\\|y\\|^2}-\\cdots-\\frac{\\left(x_n^T y\\right)^2}{\\left\\|x_n\\right\\|^2\\|y\\|^2} \\\\ &amp; =1-E R R_1 \\quad-E R R_2-\\cdots-E R R_n \\end{align} \\tag{27} \\] <p>where \\(\\operatorname{ERR}_k(k=1,2 \\ldots, n)\\) is the Error Reduction Ratio defined in previous section.</p> <p>Check the example bellow using the fundamental basis</p> <pre><code>import numpy as np\n\ny = np.array([3, 7, 8])\n# Orthogonal Basis\nx1 = np.array([1, 0, 0])\nx2 = np.array([0, 1, 0])\nx3 = np.array([0, 0, 1])\n\ntheta1 = (x1.T@y)/(x1.T@x1)\ntheta2 = (x2.T@y)/(x2.T@x2)\ntheta3 = (x3.T@y)/(x3.T@x3)\n\nsquared_y = y.T @ y\nerr1 = (x1.T@y)**2/((x1.T@x1) * squared_y)\nerr2 = (x2.T@y)**2/((x2.T@x2) * squared_y)\nerr3 = (x3.T@y)**2/((x3.T@x3) * squared_y)\n\nprint(f\"x1 represents {round(err1*100, 2)}% of the variation in y, \\n x2 represents {round(err2*100, 2)}% of the variation in y, \\n x3 represents {round(err3*100, 2)}% of the variation in y\")\n\nx1 represents 7.38% of the variation in y,\nx2 represents 40.16% of the variation in y,\nx3 represents 52.46% of the variation in y\n</code></pre> <p>Lets see what happens in a non-orthogonal scenario.</p> <p><pre><code>y = np.array([3, 7, 8])\nx1 = np.array([1, 2, 2])\nx2 = np.array([-1, 0, 2])\nx3 = np.array([0, 0, 1])\n\ntheta1 = (x1.T@y)/(x1.T@x1)\ntheta2 = (x2.T@y)/(x2.T@x2)\ntheta3 = (x3.T@y)/(x3.T@x3)\n\nsquared_y = y.T @ y\nerr1 = (x1.T@y)**2/((x1.T@x1) * squared_y)\nerr2 = (x2.T@y)/((x2.T@x2) * squared_y)\nerr3 = (x3.T@y)**2/((x3.T@x3) * squared_y)\n\nprint(f\"x1 represents {round(err1*100, 2)}% of the variation in y, \\n x2 represents {round(err2*100, 2)}% of the variation in y, \\n x3 represents {round(err3*100, 2)}% of the variation in y\")\n\n&gt;&gt;&gt; x1 represents 99.18% of the variation in y,\n&gt;&gt;&gt; x2 represents 2.13% of the variation in y,\n&gt;&gt;&gt; x3 represents 52.46% of the variation in y\n</code></pre> In this case, \\(x1\\) have the highest \\(err\\) value, so we have choose it to be the first orthogonal vector.</p> <pre><code>q1 = x1.copy()\n\nv1 = x2 - (q1.T@x2)/(q1.T@q1)*q1\nerrv1 = (v1.T@y)**2/((v1.T@v1) * squared_y)\n\nv2 = x3 - (q1.T@x3)/(q1.T@q1)*q1\nerrv2 = (v2.T@y)**2/((v2.T@v2) * squared_y)\n\nprint(f\"v1 represents {round(errv1*100, 2)}% of the variation in y, \\n v2 represents {round(errv2*100, 2)}% of the variation in y\")\n\n&gt;&gt;&gt; v1 represents 0.82% of the variation in y,\n&gt;&gt;&gt; v2 represents 0.66% of the variation in y\n</code></pre> <p>So, in this case, when we sum the err values of the first two orthogonal vectors, \\(x1\\) and \\(v1\\), we get \\(err_3 + errv1 = 100\\%\\). Then there is no need to keep the iterations looking for more terms. The model with this two terms already explain all the variance in the data.</p> <p>That's the idea of the FROLS algorithm. We calculate the ERR, choose the vector with the highest ERR to be the first orthogonal vector, orthogonalize every vector but the one we choose in the first step, calculate the ERR for each one of them, choose the vector with the highest ERR value and keep doing that until we reach some criteria.</p> <p>In SysIdentPy, we have 2 hyperparameters called <code>n_terms</code> and <code>err_tol</code>. Both of them can be used to stop the iterations. The first one will iterate until <code>n_terms</code> are chosen. The second one iterate until the \\(\\sum ERR_i &gt; err_{tol}\\) . If you set both, the algorithm stop when any of the conditions is true.</p> <pre><code>model = FROLS(\n\u00a0 \u00a0 \u00a0 \u00a0 n_terms=50,\n\u00a0 \u00a0 \u00a0 \u00a0 ylag=7,\n\u00a0 \u00a0 \u00a0 \u00a0 xlag=7,\n\u00a0 \u00a0 \u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 \u00a0 \u00a0 err_tol=0.98\n\u00a0 \u00a0 )\n</code></pre> <p>SysIdentPy apply the Golub -Householder method for the orthogonal decomposition. A more detailed discussion about Householder and orthogonalization procedures in general can be found in Chen, S. and Billings, S. A. and Luo, W.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#case-study","title":"Case Study","text":"<p>An example using real data will be described using SysIdentPy. In this example, we will build models linear and nonlinear models to describe the behavior of a DC motor operating as generator. Details of the experiment used to generate this data can be found in the paper (in Portuguese) IDENTIFICA\u00c7\u00c3O DE UM MOTOR/GERADOR CC POR MEIO DE MODELOS POLINOMIAIS AUTORREGRESSIVOS E REDES NEURAIS ARTIFICIAIS</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\n\ndf1 = pd.read_csv(\"examples/datasets/x_cc.csv\")\ndf2 = pd.read_csv(\"examples/datasets/y_cc.csv\")\n\n# checking the ouput\ndf2[5000:80000].plot(figsize=(10, 4))\n</code></pre> <p></p> <p>Figure 2. Output of the electromechanical system.</p> <p>In this example, we will decimate the data using \\(d = 500\\). The rationale behind decimation here is that the data is oversampled due to the experimental setup. A future section will provide a detailed explanation of how to handle oversampled data in the context of system identification. For now, consider this approach as the most appropriate solution.</p> <pre><code>x_train, x_valid = np.split(df1.iloc[::500].values, 2)\ny_train, y_valid = np.split(df2.iloc[::500].values, 2)\n</code></pre> <p>In this case, we will build a NARX model. In SysIdentPy, this means setting <code>unbiased=False</code> in the <code>LeastSquares</code> definition. We'll use a <code>Polynomial</code> basis function and set the maximum lag for both input and output to 2. This configuration results in 15 terms in the information matrix, so we'll set <code>n_terms=15</code>. This specification is necessary because, in this example, <code>order_selection</code> is set to <code>False</code>. We will discuss <code>order_selection</code> in more detail in the Information Criteria section later on.</p> <p><code>order_selection</code> is <code>True</code> by default in SysIdentPy. When <code>order_selection=False</code> the user must pass a values to <code>n_terms</code> because it is an optional argument and its default value is <code>None</code>. If we set <code>n_terms=5</code>, for exemple, the FROLS will stop after choosing the first 5 regressors. We do not want that in this case because we want the FROLS stop only when <code>e_tol</code> is reached.</p> <pre><code>basis_function = Polynomial(degree=2)\n\nmodel = FROLS(\n    order_selection=False,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 estimator=LeastSquares(unbiased=False),\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 e_tol=0.9999\n\u00a0 \u00a0 n_terms=15\n)\n</code></pre> <p>SysIdentPy aims to simplify the use of algorithms like <code>FROLS</code> for the user. Building, training, or fitting a model is made straightforward through a simple interface called <code>fit</code>. By using this method, the entire process is handled internally, requiring no further interaction from the user.</p> <pre><code>model.fit(X=x_train, y=y_train)\n</code></pre> <p>SysIdentPy also offers a method to retrieve detailed information about the fitted model. Users can check the terms included in the model, the estimated parameters, the Error Reduction Ratio (ERR) values, and more.</p> <p>We're using <code>pandas</code> here only to make the output more readable, but it's optional.</p> <pre><code>r = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\n</code></pre> Regressors Parameters ERR y(k-1) 1.0998E+00 9.86000384E-01 x1(k-1)^2 1.0165E+02 7.94805130E-03 y(k-2)^2 -1.9786E-05 2.50905908E-03 x1(k-1)y(k-1) -1.2138E-01 1.43301039E-03 y(k-2) -3.2621E-01 1.02781443E-03 x1(k-1)y(k-2) 5.3596E-02 5.35200312E-04 x1(k-2) 3.4655E+02 2.79648078E-04 x1(k-2)y(k-1) -5.1647E-02 1.12211942E-04 x1(k-2)x1(k-1) -8.2162E+00 4.54743448E-05 y(k-2)y(k-1) 4.0961E-05 3.25346101E-05 &gt;Table 1 <p>The table above shows that 10 regressors (out of the 15 available) were needed to reach the defined <code>e_tol</code>, with the sum of the ERR for the selected regressors being \\(0.99992\\).</p> <p>Next, let's evaluate the model's performance using the test data. Similar to the <code>fit</code> method, SysIdentPy provides a <code>predict</code> method. To obtain the predicted values and plot the results, simply follow these steps:</p> <pre><code>yhat = model.predict(X=x_valid, y=y_valid)\n# plot only the first 100 samples (n=100)\nplot_results(y=y_valid, yhat=yhat, n=100)\n</code></pre> <p></p> <p>Figure 3. Free run simulation (or infinity-steps ahead prediction) of the fitted model.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#information-criteria","title":"Information Criteria","text":"<p>We said that there are many ways to terminate the algorithm and select the model terms, but only ERR criteria was defined in previous section. Different ways to terminate the algorithm is by using some information criteria, e.g, Akaike Information Criteria (AIC). For Least Squares based regression analysis, the AIC indicates the number of regressors by minimizing the objective function (Akaike, H.):</p> \\[ \\begin{equation}     J_{\\text{AIC}} = \\underbrace{n\\log\\left(Var[\\xi_k]\\right)}_{\\text{first component}}+\\underbrace{2n_{\\Theta}}_{\\text{{second component}}}. \\end{equation} \\tag{28} \\] <p>It is important to note that the equation above illustrates a trade-off between model fit and model complexity. Specifically, this trade-off involves balancing the model's ability to accurately fit the data (the first component) against its complexity, which is related to the number of parameters included (the second component). As additional terms are included in the model, the Akaike Information Criterion (AIC) value initially decreases, reaching a minimum that represents an optimal balance between model complexity and predictive accuracy. However, if the number of parameters becomes excessive, the penalty for complexity outweighs the benefit of a better fit, causing the AIC value to increase. The AIC and many others variants have been extensively used for linear and nonlinear system identification. Check Wei, H. and Zhu, D. and Billings, S. A. and Balikhin, M. A., Martins, S. A. M. and Nepomuceno, E. G. and Barroso, M. F. S., Hafiz, F. and Swain, A. and Mendes, E. M. A. M. and Patel, N., Gu, Y. and Wei, H. and Balikhin, M. M. and references therein.</p> <p>Despite their effectiveness in many linear model selection scenarios, information criteria such as AIC can struggle to select an appropriate number of parameters when dealing with systems exhibiting significant nonlinear behavior. Additionally, these criteria may lead to suboptimal models if the search space does not encompass all the necessary terms required to accurately represent the true model. Consequently, in highly nonlinear systems or when critical model components are missing, information criteria might not provide reliable guidance, resulting in models that exhibit poor performance.</p> <p>Besides AIC, SysIdentPy provides other four different information criteria: Bayesian Information Criteria (BIC), Final Prediction Error (FPE), Low of Iterated Logarithm Criteria (LILC), and Corrected Akaike Information Criteria (AICc), which can be described respectively as</p> \\[ \\begin{align} \\operatorname{FPE}\\left(n_\\theta\\right) &amp; =N \\ln \\left[\\sigma_{\\text {erro }}^2\\left(n_\\theta\\right)\\right]+N \\ln \\left[\\frac{N+n_\\theta}{N-n_\\theta}\\right] \\\\ B I C\\left(n_\\theta\\right) &amp; =N \\ln \\left[\\sigma_{\\text {erro }}^2\\left(n_\\theta\\right)\\right]+n_\\theta \\ln N \\\\ A I C c &amp;=A I C+2 n_p * \\frac{n_p+1}{N-n_p-1} \\\\ LILC &amp;= 2n_{\\theta}\\ln(\\ln(N)) + N \\ln(\\left[\\sigma_{\\text {erro }}^2\\left(n_\\theta\\right)\\right]) \\end{align} \\tag{29} \\] <p>To use any information criteria in SysIdentPy, set <code>order_selection=True</code> (as said before, the default value is already <code>True</code>). Besides <code>order_selection</code>, you can define how many regressors you want to evaluate before stopping the algorithm by using the <code>n_info_values</code> hyperparameter. The default value is \\(15\\), but the user should increase it based on how many regressors exists given the <code>ylag</code>, <code>xlag</code> and the degree of the basis function.</p> <p>Using information Criteria can take a long time depending on how many regressors you are evaluating and the number of samples. To calculate the criteria, the ERR algorithm is executed <code>n</code> times where <code>n</code> is the number defined in <code>n_info_values</code>. Make sure to understand how it works to define whether you have to use it or not.</p> <p>Running the same example, but now using the BIC information criteria to select the order of the model, we have</p> <pre><code>model = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=15,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"bic\",\n\u00a0 \u00a0 estimator=LeastSquares(unbiased=False),\n\u00a0 \u00a0 basis_function=basis_function\n)\nmodel.fit(X=x_train, y=y_train)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\n</code></pre> Regressors Parameters ERR y(k-1) 1.3666E+00 9.86000384E-01 x1(k-1)^2 1.0500E+02 7.94805130E-03 y(k-2)^2 -5.8577E-05 2.50905908E-03 x1(k-1)y(k-1) -1.2427E-01 1.43301039E-03 y(k-2) -5.1414E-01 1.02781443E-03 x1(k-1)y(k-2) 5.3001E-02 5.35200312E-04 x1(k-2) 3.1144E+02 2.79648078E-04 x1(k-2)y(k-1) -4.8013E-02 1.12211942E-04 x1(k-2)x1(k-1) -8.0561E+00 4.54743448E-05 x1(k-2)y(k-2) 4.1381E-03 3.25346101E-05 1 -5.6653E+01 7.54107553E-06 y(k-2)y(k-1) 1.5679E-04 3.52002717E-06 y(k-1)^2 -9.0164E-05 6.17373260E-06 &gt;Table 2 <p>In this case, instead of 8 regressors, the final model have 13 terms.</p> <p>Currently, the number of regressors is determined by identifying the index of the last value where the difference between the current and previous value is less than 0. To inspect these values, you can use the following approach:</p> <pre><code>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <p></p> <p>Figure 4. The plot shows the Information Criterion values (BIC) as a function of the number of terms included in the model. The model selection process, using the BIC criterion, iteratively adds regressors until the BIC reaches a minimum, indicating the optimal balance between model complexity and fit. The point where the BIC value stops decreasing marks the optimal number of terms, resulting in a final model with 13 terms.</p> <p>The model prediction in this case is shown in Figure 5</p> <pre><code>yhat = model.predict(X=x_valid, y=y_valid)\n# plot only the first 100 samples (n=100)\nplot_results(y=y_valid, yhat=yhat, n=100)\n</code></pre> <p></p> <p>Figure 5. Free run simulation (or infinity-steps ahead prediction) of the fitted model using BIC.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#overview-of-the-information-criteria-methods","title":"Overview of the Information Criteria Methods","text":"<p>In this section, simulated data are used to provide users with a clearer understanding of the information criteria available in SysIdentPy.</p> <p>Here, we're working with a known model structure, which allows us to focus on how different information criteria perform. When dealing with real data, the correct number of terms in the model is unknown, making these methods invaluable for guiding model selection.</p> <p>If you review the metrics below, you'll notice excellent performance across all models. However, it's crucial to remember that System Identification is about finding the optimal model structure. Model Structure Selection is at the heart of NARMAX methods!</p> <p>The data is generated by simulating the following model:</p> \\[ y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_k \\tag{30} \\] <p>If <code>colored_noise</code> is set to <code>True</code>, the noise term is defined as:</p> \\[ e_k = 0.8\\nu_{k-1} + \\nu_k \\tag{31} \\] <p>where \\(x\\) is a uniformly distributed random variable and \\(\\nu\\) is a Gaussian-distributed variable with \\(\\mu = 0\\) and \\(\\sigma = 0.1\\).</p> <p>In the next example, we will generate data with 100 samples, using white noise, and select 70% of the data to train the model.</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\n\n\nx_train, x_valid, y_train, y_valid = get_siso_data(\n\u00a0 \u00a0 n=100, colored_noise=False, sigma=0.1, train_percentage=70\n)\n</code></pre> <p>The idea is to show the impact of the information criteria to select the number of terms to compose the final model. You will se why it is an auxiliary tool and let the algorithm select the number of terms based on the minimum value is not always a good idea when dealing with data highly corrupted by noise (even white noise).</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#aic","title":"AIC","text":"<pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"aic\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <p>The regressors, the free run simulation and the AIC values are detailed bellow.</p> Regressors Parameters ERR x1(k-2) 9.4236E-01 9.26094341E-01 y(k-1) 2.4933E-01 3.35898283E-02 x1(k-1)y(k-1) 1.3001E-01 2.35736200E-03 x1(k-1) 8.4024E-02 4.11741791E-03 x1(k-1)^2 7.0807E-02 2.54231877E-03 x1(k-2)^2 -9.1138E-02 1.39658893E-03 y(k-1)^2 1.1698E-01 1.70257419E-03 x1(k-2)y(k-2) 8.3745E-02 1.11056684E-03 y(k-2)^2 -4.1946E-02 1.01686239E-03 x1(k-2)x1(k-1) 5.9034E-02 7.47435512E-04 &gt;Table 3 <p></p> <p>Figure 5. Free run simulation (or infinity-steps ahead prediction) of the fitted model using AIC.</p> <p></p> <p>Figure 6. The plot shows the Information Criterion values (AIC) as a function of the number of terms included in the model. The model selection process, using the AIC criterion, iteratively adds regressors until the AIC reaches a minimum, indicating the optimal balance between model complexity and fit. The point where the AICc value stops decreasing marks the optimal number of terms, resulting in a final model with 10 terms.</p> <p>For this case, we have a model with 10 terms. We know that the correct number is 3 because of the simulated system we are using as example.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#aicc","title":"AICc","text":"<p>The only change we have to do to use AICc instead of AIC is changing the information criteria hyperparameter: <code>information_criteria=\"aicc\"</code></p> <pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=15,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"aicc\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> Regressors Parameters ERR x1(k-2) 9.2282E-01 9.26094341E-01 y(k-1) 2.4294E-01 3.35898283E-02 x1(k-1)y(k-1) 1.2753E-01 2.35736200E-03 x1(k-1) 6.9597E-02 4.11741791E-03 x1(k-1)^2 7.0578E-02 2.54231877E-03 x1(k-2)^2 -1.0523E-01 1.39658893E-03 y(k-1)^2 1.0949E-01 1.70257419E-03 x1(k-2)y(k-2) 7.1821E-02 1.11056684E-03 y(k-2)^2 -3.9756E-02 1.01686239E-03 &gt;Table 4 <p></p> <p>Figure 7. Free run simulation (or infinity-steps ahead prediction) of the fitted model using AICc.</p> <p></p> <p>Figure 8. The plot shows the Information Criterion values (AICc) as a function of the number of terms included in the model. The model selection process, using the AIC criterion, iteratively adds regressors until the AICc reaches a minimum, indicating the optimal balance between model complexity and fit. The point where the AICc value stops decreasing marks the optimal number of terms, resulting in a final model with 9 terms.</p> <p>This time we have a model with 9 regressors.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#bic","title":"BIC","text":"<pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=15,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"bic\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> Regressors Parameters ERR x1(k-2) 9.1726E-01 9.26094341E-01 y(k-1) 1.8670E-01 3.35898283E-02 &gt;Table 5 <p>Figure 9. Free run simulation (or infinity-steps ahead prediction) of the fitted model using BIC.</p> <p></p> <p>Figure 10. The plot shows the Information Criterion values (BIC) as a function of the number of terms included in the model. The model selection process, using the BIC criterion, iteratively adds regressors until the BIC reaches a minimum, indicating the optimal balance between model complexity and fit. The point where the BIC value stops decreasing marks the optimal number of terms, resulting in a final model with 2 terms.</p> <p>BIC returned a model with only 2 regressors!</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#lilc","title":"LILC","text":"<pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=15,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"lilc\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> Regressors Parameters ERR x1(k-2) 9.1160E-01 9.26094341E-01 y(k-1) 2.3178E-01 3.35898283E-02 x1(k-1)y(k-1) 1.2080E-01 2.35736200E-03 x1(k-1) 6.3113E-02 4.11741791E-03 x1(k-1)^2 5.4088E-02 2.54231877E-03 x1(k-2)^2 -9.0683E-02 1.39658893E-03 y(k-1)^2 8.2157E-02 1.70257419E-03 &gt;Table 6 <p>Figure 11. Free run simulation (or infinity-steps ahead prediction) of the fitted model using LILC.</p> <p></p> <p>Figure 12. The plot shows the Information Criterion values (LILC) as a function of the number of terms included in the model. The model selection process, using the LILC criterion, iteratively adds regressors until the LILC reaches a minimum, indicating the optimal balance between model complexity and fit. The point where the LILC value stops decreasing marks the optimal number of terms, resulting in a final model with 7 terms.</p> <p>LILC returned a model with 7 regressors.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#fpe","title":"FPE","text":"<pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=15,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"fpe\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> Regressors Parameters ERR x1(k-2) 9.4236E-01 9.26094341E-01 y(k-1) 2.4933E-01 3.35898283E-02 x1(k-1)y(k-1) 1.3001E-01 2.35736200E-03 x1(k-1) 8.4024E-02 4.11741791E-03 x1(k-1)^2 7.0807E-02 2.54231877E-03 x1(k-2)^2 -9.1138E-02 1.39658893E-03 y(k-1)^2 1.1698E-01 1.70257419E-03 x1(k-2)y(k-2) 8.3745E-02 1.11056684E-03 y(k-2)^2 -4.1946E-02 1.01686239E-03 x1(k-2)x1(k-1) 5.9034E-02 7.47435512E-04 &gt;Table 7 <p>Figure 13. Free run simulation (or infinity-steps ahead prediction) of the fitted model using FPE.</p> <p></p> <p>Figure 14. The plot shows the Information Criterion values (FPE) as a function of the number of terms included in the model. The model selection process, using the FPE criterion, iteratively adds regressors until the FPE reaches a minimum, indicating the optimal balance between model complexity and fit. The point where the FPE value stops decreasing marks the optimal number of terms, resulting in a final model with 10 terms.</p> <p>FPE returned a model with 10 regressors.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#meta-model-structure-selection-metamss","title":"Meta Model Structure Selection (MetaMSS)","text":"<p>This section largely reflects content from a paper I published on ArXiv titled \"Meta-Model Structure Selection: Building Polynomial NARX Models for Regression and Classification.\" This paper was initially written for journal publication based on the results of my master's thesis. However, as I transitioned into a Data Scientist role and considering the lengthy journal submission process and academic delays, I decided not to pursue journal publication at this time. Thus, the paper remains available only on ArXiv.</p> <p>The work extends a previous paper I presented at a Brazilian conference (in Portuguese), where part of the results were initially shared.</p> <p>This section introduces a meta-heuristic approach for selecting the structure of polynomial NARX models in regression tasks. The proposed method considers both the complexity of the model and the contribution of each term to construct parsimonious models through a novel cost function formulation. The robustness of this new algorithm is evaluated using various simulated and experimental systems with different nonlinear characteristics. The results demonstrate that the algorithm effectively identifies the correct model when the true structure is known and produces parsimonious models for experimental data, even in cases where traditional and contemporary methods often fail. The new approach is compared against classical methods such as FROLS and recent randomized techniques.</p> <p>We mentioned that selecting the appropriate model terms is crucial for accurately capturing the dynamics of the original system. Challenges such as overparameterization and numerical ill-conditioning often arise due to the limitations of existing identification algorithms in selecting the right terms for the final model. Check Aguirre, L. A. and Billings, S. A., Piroddi, L. and Spinelli, W.. We also mentioned that one of the most traditionally algorithms for structure selection of polynomial NARMAX is the ERR algorithm. Numerous variants of FROLS algorithm has been developed to improve the model selection performance such as Billings, S. A. and Chen, S. and Korenberg, M. J., Farina, M. and Piroddi, L., Guo, Y. and Guo, L. Z. and Billings, S. A. and Wei, H., Mao, K. Z. and Billings, S. A.. The drawbacks of the FROLS have been extensively reviewed in the literature, e.g., in Billings, S. A. and Aguirre, L. A., Palumbo, P. and Piroddi, L.,  Falsone, A. and Piroddi, L. and Prandini, M.. Most of these weak points are related to (i) the Prediction Error Minimization (PEM) framework; (ii) the inadequacy of the ERR index in measuring the absolute importance of regressors; (iii) the use of information criteria such as AIC, FPE and the BIC, to select the model order. Regarding the information criteria, although these techniques work well for linear models, in a nonlinear context no simple relation between model size and accuracy can be established Falsone, A. and Piroddi, L. and Prandini, M. , Chen, S. and Hong, X. and Harris, C. J..</p> <p>Due to the limitations of Ordinary Least Squares (OLS)-based algorithms, recent research has presented solutions that diverged from the classical FROLS approach. New methods have reformulated the Model Structure Selection (MSS) process within a probabilistic framework and employed random sampling techniques Falsone, A. and Piroddi, L. and Prandini, M., Tempo, R. and Calafiore, G. and Dabbene, F., Baldacchino, T. and Anderson, S. R. and Kadirkamanathan, V., Rodriguez-Vazquez, K. and Fonseca, C. M. and Fleming, P. J., Severino, A. G. V. and Araujo, F. M. U. de. Despite their advancements, these meta-heuristic and probabilistic approaches exhibit certain shortcomings. In particular, these methods often rely on information criteria such as AIC, FPE, and BIC to define the cost function for optimization, which frequently leads to over-parameterized models.</p> <p>Consider \\(\\mathcal{F}\\) as a class of bounded functions \\(\\phi: \\mathbf{R} \\mapsto \\mathbf{R}\\). If the properties of \\(\\phi(x)\\) satisfy</p> \\[ \\begin{align}     &amp;\\lim\\limits_{x \\to \\infty} \\phi(x) = \\alpha \\nonumber \\\\     &amp;\\lim\\limits_{x \\to -\\infty} \\phi(x) = \\beta \\quad \\text{with } \\alpha &gt; \\beta,  \\nonumber \\end{align} \\tag{32} \\] <p>the function is called sigmoidal.</p> <p>In this particular case and following definition Equation 32 with \\(alpha = 0\\) and \\(\\beta = 1\\), we write a \"S\" shaped curve as</p> \\[ \\begin{equation}     \\varsigma(x) = \\frac{1}{1+e^{-a(x-c)}}. \\end{equation} \\tag{33} \\] <p>In that case, we can specify \\(a\\), the rate of change. If \\(a\\) is close to zero, the sigmoid function will be gradual. If \\(a\\) is large, the sigmoid function will have an abrupt or sharp transition. If \\(a\\) is negative, the sigmoid will go from \\(1\\) to zero. The parameter \\(c\\) corresponds to the x value where \\(y = 0.5\\).</p> <p>The Sigmoid Linear Unit Function (SiLU) is defined by the sigmoid function multiplied by its input</p> \\[ \\begin{equation}     \\text{silu}(x) = x \\varsigma(x), \\end{equation} \\tag{34} \\] <p>which can be viewed as an steeper sigmoid function with overshoot.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#meta-heuristics","title":"Meta-heuristics","text":"<p>Over the past two decades, nature-inspired optimization algorithms have gained prominence due to their flexibility, simplicity, versatility, and ability to avoid local optima in real-world applications.</p> <p>Meta-heuristic algorithms are characterized by two fundamental features: exploitation and exploration Blum, C. and Roli, A.. Exploitation focuses on utilizing local information to refine the search around the current best solution, improving the quality of nearby solutions. Conversely, exploration aims to search a broader area of the solution space to discover potentially superior solutions and prevent the algorithm from getting trapped in local optima.</p> <p>Despite the lack of a universal consensus on the definitions of exploration and exploitation in evolutionary computing, as highlighted by Eiben, Agoston E and Schippers, Cornelis A, it is generally agreed that these concepts function as opposing forces that are challenging to balance. To address this challenge, hybrid metaheuristics combine multiple algorithms to leverage both exploitation and exploration, resulting in more robust optimization methods.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#the-binary-hybrid-particle-swarm-optimization-and-gravitational-search-algorithm-bpsogsa-algorithm","title":"The Binary hybrid Particle Swarm Optimization and Gravitational Search Algorithm (BPSOGSA) algorithm","text":"<p>Achieving a balance between exploration and exploitation is a significant challenge in most meta-heuristic algorithms. For this method, we enhance performance and flexibility in the search process by employing a hybrid approach that combines Binary Particle Swarm Optimization (BPSO) with Gravitational Search Algorithm (GSA), as proposed by Mirjalili, S. and Hashim, S. Z. M.. This hybrid method incorporates a low-level co-evolutionary heterogeneous technique originally introduced by Talbi, E. G..</p> <p>The BPSOGSA approach leverages the strengths of both algorithms: the Particle Swarm Optimization (PSO) component is known to be good in exploring the entire search space to identify the global optimum, while the Gravitational Search Algorithm (GSA) component effectively refines the search by focusing on local solutions within a binary space. This combination aims to provide a more comprehensive and effective optimization strategy, ensuring a better balance between exploration and exploitation.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#standard-particle-swarm-optimization-pso","title":"Standard Particle Swarm Optimization (PSO)","text":"<p>In Particle Swarm Optimization (PSO) Kennedy, J. and Eberhart, R. C., Kennedy, J., each particle represents a candidate solution and is characterized by two components: its position in the search space, denoted as \\(\\vec{x}_{\\,np,d} \\in \\mathbb{R}^{np \\times d}\\), and its velocity, \\(\\vec{v}_{\\,np,d} \\in \\mathbb{R}^{np \\times d}\\). Here, \\(np = 1, 2, \\ldots, n_a\\) where \\(n_a\\) is the size of the swarm, and \\(d\\) is the dimensionality of the problem. The initial population is represented as follows:</p> \\[ \\vec{x}_{\\,np,d} = \\begin{bmatrix} x_{1,1} &amp; x_{1,2} &amp; \\cdots &amp; x_{1,d} \\\\ x_{2,1} &amp; x_{2,2} &amp; \\cdots &amp; x_{2,d} \\\\ \\vdots  &amp; \\vdots  &amp; \\ddots &amp; \\vdots \\\\ x_{n_a,1} &amp; x_{n_a,2} &amp; \\cdots &amp; x_{n_a,d} \\end{bmatrix} \\tag{35} \\] <p>At each iteration \\(t\\), the position and velocity of a particle are updated using the following equations:</p> \\[ v_{np,d}^{t+1} = \\zeta v_{np,d}^{t} + c_1 \\kappa_1 (pbest_{np}^{t} - x_{np,d}^{t}) + c_2 \\kappa_2 (gbest_{np}^{t} - x_{np,d}^{t}), \\tag{36} \\] <p>where \\(\\kappa_j \\in \\mathbb{R}\\) for \\(j = [1,2]\\) are continuous random variables in the interval \\([0,1]\\), \\(\\zeta \\in \\mathbb{R}\\) is the inertia factor that controls the influence of the previous velocity on the current one and represents a trade-off between exploration and exploitation, \\(c_1\\) is the cognitive factor associated with the personal best position \\(pbest\\), and \\(c_2\\) is the social factor associated with the global best position \\(gbest\\). The velocity \\(\\vec{v}_{\\,np,d}\\) is typically constrained within the range \\([v_{min}, v_{max}]\\) to prevent particles from moving outside the search space. The updated position is then computed as:</p> \\[ x_{np,d}^{t+1} = x_{np,d}^{t} + v_{np,d}^{t+1}. \\tag{37} \\]"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#standard-gravitational-search-algorithm-gsa","title":"Standard Gravitational Search Algorithm (GSA)","text":"<p>In the Gravitational Search Algorithm (GSA) Rashedi, Esmat and Nezamabadi-Pour, Hossein and Saryazdi, Saeid, agents are represented by masses, where the magnitude of each mass is proportional to the fitness value of the agent. These masses interact through gravitational forces, attracting each other towards locations closer to the global optimum. Heavier masses (agents with better fitness) move more slowly, while lighter masses (agents with poorer fitness) move more rapidly. Each mass in GSA has four properties: position, inertial mass, active gravitational mass, and passive gravitational mass. The position of a mass represents a candidate solution to the problem, and its gravitational and inertial masses are derived from the fitness function.</p> <p>Consider a population of agents as described by the following equations. At a specific time \\(t\\), the velocity and position of each agent are updated as follows:</p> \\[ \\begin{align}     v_{i,d}^{t+1} &amp;= \\kappa_i \\times v_{i,d}^t + a_{i,d}^t, \\\\     x_{i,d}^{t+1} &amp;= x_{i,d}^t + v_{i,d}^{t+1}. \\end{align} \\tag{38} \\] <p>Here, \\(\\kappa_i\\) introduces stochastic characteristics to the search process. The acceleration \\(a_{i,d}^t\\) is computed according to the law of motion Rashedi, Esmat and Nezamabadi-Pour, Hossein and Saryazdi, Saeid:</p> \\[ \\begin{equation}     a_{i,d}^t = \\frac{F_{i,d}^t}{M_{ii}^{t}}, \\end{equation} \\tag{39} \\] <p>where \\(M_{ii}^{t}\\) is the inertial mass of agent \\(i\\) and \\(F_{i,d}^t\\) represents the gravitational force acting on agent \\(i\\) in the \\(d\\)-dimensional space. The detailed process for calculating and updating \\(F_{i,d}\\) and \\(M_{ii}\\) can be found in Rashedi, Esmat and Nezamabadi-Pour, Hossein and Saryazdi, Saeid.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#the-binary-hybrid-optimization-algorithm","title":"The Binary Hybrid Optimization Algorithm","text":"<p>The combination of algorithms follows the approach described in Mirjalili, S. and Hashim, S. Z. M.:</p> \\[ \\begin{align}     v_{i}^{t+1} = \\zeta \\times v_i^t + \\mathrm{c}'_{1} \\times \\kappa \\times a_i^t + \\mathrm{c}'_2 \\times \\kappa \\times (gbest - x_i^t), \\end{align} \\tag{40} \\] <p>where \\(\\mathrm{c}'_j \\in \\mathbb{R}\\) are acceleration coefficients. This formulation accelerates the exploitation phase by incorporating the best mass location found so far. However, this method may negatively impact the exploration phase. To address this issue, Mirjalili, S. and Mirjalili, S. M. and Lewis, A. proposed adaptive values for \\(\\mathrm{c}'_j\\), as described in Mirjalili, S. and Wang, Gai-Ge and Coelho, L. dos S.:</p> \\[ \\begin{align}     \\mathrm{c}_1' &amp;= -2 \\times \\frac{t^3}{\\max(t)^3} + 2, \\\\     \\mathrm{c}_2' &amp;= 2 \\times \\frac{t^3}{\\max(t)^3} + 2. \\end{align} \\tag{41} \\] <p>In each iteration, the positions of particles are updated according to the following rules, with continuous space mapped to discrete solutions using a transfer function (Mirjalili, S. and Lewis, A.):</p> \\[ \\begin{equation}     S(v_{ik}) = \\left|\\frac{2}{\\pi}\\arctan\\left(\\frac{\\pi}{2}v_{ik}\\right)\\right|. \\end{equation} \\tag{42} \\] <p>With a uniformly distributed random number \\(\\kappa \\in (0,1)\\), the positions of the agents in the binary space are updated as follows:</p> \\[ \\begin{equation}     x_{np,d}^{t+1} =     \\begin{cases}         (x_{np,d}^{t})^{-1}, &amp; \\text{if } \\kappa &lt; S(v_{ik}^{t+1}), \\\\         x_{np,d}^{t}, &amp; \\text{if } \\kappa \\geq S(v_{ik}^{t+1}).     \\end{cases} \\end{equation} \\tag{43} \\]"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#meta-model-structure-selection-metamss-building-narx-for-regression","title":"Meta-Model Structure Selection (MetaMSS): Building NARX for Regression","text":"<p>In this section, we explore the meta-heuristic approach for selecting the structure of NARX models using BPSOGSA proposed in my master's thesis. This method searches for the optimal model structure within a decision space defined by a predefined dictionary of regressors. The objective function for this optimization problem is based on the root mean squared error (RMSE) of the free-run simulation output, augmented by a penalty factor that accounts for the model's complexity and the contribution of each regressor to the final model.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#encoding-scheme","title":"Encoding Scheme","text":"<p>The process of using BPSOGSA for model structure selection involves defining the dimensions of the test function. Specifically, \\(n_y\\), \\(n_x\\), and \\(\\ell\\) are set to cover all possible regressors, and a general matrix of regressors, \\(\\Psi\\), is constructed. The number of columns in \\(\\Psi\\) is denoted as \\(noV\\), and the number of agents, \\(N\\), is specified. A binary \\(noV \\times N\\) matrix, referred to as \\(\\mathcal{X}\\), is then randomly generated to represent the position of each agent in the search space. Each column of \\(\\mathcal{X}\\) represents a potential solution, which is essentially a candidate model structure to be evaluated in each iteration. In this matrix, a value of 1 indicates that the corresponding column of \\(\\Psi\\) is included in the reduced matrix of regressors, while a value of 0 indicates exclusion.</p> <p>For example, consider a case where all possible regressors are defined with \\(\\ell = 1\\) and \\(n_y = n_u = 2\\). The matrix \\(\\Psi\\) is:</p> \\[ \\begin{align} [ \\text{constant} \\quad y(k-1) \\quad y(k-2) \\quad u(k-1) \\quad u(k-2) ] \\end{align} \\tag{44} \\] <p>With 5 possible regressors, \\(noV = 5\\). Assuming \\(N = 5\\), \\(\\mathcal{X}\\) might be represented as:</p> \\[ \\begin{equation}     \\mathcal{X} =     \\begin{bmatrix}         0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\         1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\         0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\         0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\\\         1 &amp; 0 &amp; 1 &amp; 1 &amp; 0     \\end{bmatrix} \\end{equation} \\tag{45} \\] <p>Each column of \\(\\mathcal{X}\\) is transposed to generate a candidate solution. For example, the first column of \\(\\mathcal{X}\\) yields:</p> \\[ \\begin{equation*}     \\mathcal{X} =     \\begin{bmatrix}         \\text{constant} &amp; y(k-1) &amp; y(k-2) &amp; u(k-1) &amp; u(k-2) \\\\         1 &amp; 1 &amp; 1 &amp; 0 &amp; 1     \\end{bmatrix} \\end{equation*} \\tag{46} \\] <p>In this scenario, the first model to be evaluated is \\(\\alpha y(k-1) + \\beta u(k-2)\\), where \\(\\alpha\\) and \\(\\beta\\) are parameters estimated using some parameter estimation method. The process is repeated for each subsequent column of \\(\\mathcal{X}\\).</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#formulation-of-the-objective-function","title":"Formulation of the objective function","text":"<p>For each candidate model structure randomly defined, the linear-in-the-parameters system can be solved directly using the Least Squares algorithm or any other method available in SysIdentPy. The variance of the estimated parameters can be calculated as:</p> \\[ \\hat{\\sigma}^2 = \\hat{\\sigma}_e^2 V_{jj}, \\tag{47} \\] <p>where \\(\\hat{\\sigma}_e^2\\) is the estimated noise variance, given by:</p> \\[ \\hat{\\sigma}_e^2 = \\frac{1}{N-m} \\sum_{k=1}^{N} (y_k - \\psi_{k-1}^\\top \\hat{\\Theta}), \\tag{48} \\] <p>and \\(V_{jj}\\) is the \\(j\\)th diagonal element of \\((\\Psi^\\top \\Psi)^{-1}\\).</p> <p>The estimated standard error of the \\(j\\)th regression coefficient \\(\\hat{\\Theta}_j\\) is the positive square root of the diagonal elements of \\(\\hat{\\sigma}^2\\):</p> \\[ \\mathrm{se}(\\hat{\\Theta}_j) = \\sqrt{\\hat{\\sigma}^2_{jj}}. \\tag{49} \\] <p>To assess the statistical relevance of each regressor, a penalty test considers the standard error of the regression coefficients. In this case, the \\(t\\)-test is used to perform a hypothesis test on the coefficients, evaluating the significance of individual regressors in the multiple linear regression model. The hypothesis statements are:</p> \\[ \\begin{align*}    H_0 &amp;: \\Theta_j = 0, \\\\    H_a &amp;: \\Theta_j \\neq 0. \\end{align*} \\tag{50} \\] <p>The \\(t\\)-statistic is computed as:</p> \\[ T_0 = \\frac{\\hat{\\Theta}_j}{\\mathrm{se}(\\hat{\\Theta}_j)}, \\tag{51} \\] <p>which measures how many standard deviations \\(\\hat{\\Theta}_j\\) is from zero. More precisely, if:</p> \\[ -t_{\\alpha/2, N-m} &lt; T_0 &lt; t_{\\alpha/2, N-m}, \\tag{52} \\] <p>where \\(t_{\\alpha/2, N-m}\\) is the \\(t\\) value obtained considering \\(\\alpha\\) as the significance level and \\(N-m\\) as the degrees of freedom, then if \\(T_0\\) falls outside this acceptance region, the null hypothesis \\(H_0: \\Theta_j = 0\\) is rejected. This implies that \\(\\Theta_j\\) is significant at the \\(\\alpha\\) level. Otherwise, if \\(T_0\\) lies within the acceptance region, \\(\\Theta_j\\) is not significantly different from zero, and the null hypothesis cannot be rejected.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#penalty-value-based-on-the-derivative-of-the-sigmoid-linear-unit-function","title":"Penalty value based on the Derivative of the Sigmoid Linear Unit function","text":"<p>We propose a penalty value based on the derivative of the sigmoid function, defined as:</p> \\[ \\dot{\\varsigma}(x(\\varrho)) = \\varsigma(x) [1 + (a(x - c))(1 - \\varsigma(x))]. \\tag{53} \\] <p>In this formulation, the parameters are defined as follows: \\(x\\) has the dimension of \\(noV\\); \\(c = noV / 2\\); and \\(a\\) is set as the ratio of the number of regressors in the current test model to \\(c\\). This approach results in a distinct curve for each model, with the slope of the sigmoid curve becoming steeper as the number of regressors increases. The penalty value, \\(\\varrho\\), corresponds to the \\(y\\) value of the sigmoid curve for the given number of regressors in \\(x\\). Since the derivative of the sigmoid function can return negative values, we normalize \\(\\varsigma\\) as:</p> \\[ \\varrho = \\varsigma - \\mathrm{min}(\\varsigma), \\tag{54} \\] <p>ensuring that \\(\\varrho \\in \\mathbb{R}^{+}\\).</p> <p>However, two different models with the same number of regressors can yield significantly different results due to the varying importance of each regressor. To address this, we use the \\(t\\)-student test to assess the statistical relevance of each regressor and incorporate this information into the penalty function. Specifically, we calculate \\(n_{\\Theta, H_{0}}\\), the number of regressors that are not significant for the model. The penalty value is then adjusted based on the model size:</p> \\[ \\mathrm{model\\_size} = n_{\\Theta} + n_{\\Theta, H_{0}}. \\tag{55} \\] <p>The objective function, which integrates the relative root mean squared error of the model with \\(\\varrho\\), is defined as:</p> \\[ \\mathcal{F} = \\frac{\\sqrt{\\sum_{k=1}^{n} (y_k - \\hat{y}_k)^2}}{\\sqrt{\\sum_{k=1}^{n} (y_k - \\bar{y})^2}} \\times \\varrho. \\tag{56} \\] <p>This approach ensures that even if models have the same number of regressors, those with redundant regressors are penalized more heavily.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#case-studies-simulation-results","title":"Case Studies: Simulation Results","text":"<p>In this section, six simulation examples are considered to illustrate the effectiveness of the MetaMSS algorithm. An analysis of the algorithm performance has been carried out considering different tuning parameters. The selected systems are generally used as a benchmark for model structures algorithms and were taken from Wei, H. and Billings, S. A., Falsone, A. and Piroddi, L. and Prandini, M., Baldacchino, T. and Anderson, S. R. and Kadirkamanathan, V., Piroddi, L. and Spinelli, W., Guo, Y. and Guo, L. Z. and Billings, S. A. and Wei, H., Bonin, M. and Seghezza, V. and Piroddi, L., Aguirre, L. A. and Barbosa, B. H. G. and Braga, A. P.. Finally, a comparative analysis with respect to the Randomized Model Structure Selection (RaMSS), the FROLS, and the Reversible-jump Markov chain Monte Carlo (RJMCMC)  algorithms has been accomplished to check out the goodness of the proposed method.</p> <p>The simulation models are described as:</p> \\[ \\begin{align}     &amp; S_1: \\quad y_k = -1.7y_{k-1} - 0.8y_{k-2} + x_{k-1} + 0.81x_{k-2} + e_k, \\\\     &amp; \\qquad \\quad \\text{with } x_k \\sim \\mathcal{U}(-2, 2) \\text{ and } e_k \\sim \\mathcal{N}(0, 0.01^2); \\nonumber \\\\     &amp; S_2: \\quad y_k = 0.8y_{k-1} + 0.4x_{k-1} + 0.4x_{k-1}^2 + 0.4x_{k-1}^3 + e_k, \\\\     &amp; \\qquad \\quad \\text{with } x_k \\sim \\mathcal{N}(0, 0.3^2) \\text{ and } e_k \\sim \\mathcal{N}(0, 0.01^2). \\nonumber \\\\     &amp; S_3: \\quad y_k = 0.2y_{k-1}^3 + 0.7y_{k-1}x_{k-1} + 0.6x_{k-2}^2 \\nonumber \\\\     &amp;- 0.7y_{k-2}x_{k-2}^2 -0.5y_{k-2}+ e_k, \\\\     &amp; \\qquad \\quad \\text{with } x_k \\sim \\mathcal{U}(-1, 1) \\text{ and } e_k \\sim \\mathcal{N}(0, 0.01^2). \\nonumber \\\\     &amp; S_4: \\quad y_k = 0.7y_{k-1}x_{k-1} - 0.5y_{k-2} + 0.6x_{k-2}^2 \\nonumber \\\\     &amp;- 0.7y_{k-2}x_{k-2}^2 + e_k, \\\\     &amp; \\qquad \\quad \\text{with } x_k \\sim \\mathcal{U}(-1, 1) \\text{ and } e_k \\sim \\mathcal{N}(0, 0.04^2). \\nonumber \\\\     &amp; S_5: \\quad y_k = 0.7y_{k-1}x_{k-1} - 0.5y_{k-2} + 0.6x_{k-2}^2 \\nonumber \\\\     &amp;- 0.7y_{k-2}x_{k-2}^2 + 0.2e_{k-1} \\nonumber \\\\     &amp; \\qquad \\quad - 0.3x_{k-1}e_{k-2} + e_k,\\\\     &amp; \\qquad \\quad \\text{with } x_k \\sim \\mathcal{U}(-1, 1) \\text{ and } e_k \\sim \\mathcal{N}(0, 0.02^2); \\nonumber \\\\     &amp; S_6: \\quad y_k = 0.75y_{k-2} + 0.25x_{k-2} - 0.2y_{k-2}x_{k-2} + e_k \\nonumber \\\\     &amp; \\qquad \\quad \\text{with } x_k \\sim \\mathcal{N}(0, 0.25^2) \\text{ and } e_k \\sim \\mathcal{N}(0, 0.02^2); \\nonumber \\end{align} \\tag{57} \\] <p>where \\(\\mathcal{U}(a, b)\\) are samples evenly distributed over~\\([a, b]\\), and \\(\\mathcal{N}(\\eta, \\sigma^2)\\) are samples with a Gaussian distribution with mean \\(\\eta\\) and standard deviation \\(\\sigma\\). All realizations of the systems are composed of a total of \\(500\\) input-output data samples. Also, the same random seed is used to reproducibility purpose.</p> <p>All tests shown in this section are based on the original implementation and are took from the results of my master thesis. At the time, the algorithm was performed in Matlab \\(2018\\)a environment, on a Dell Inspiron \\(5448\\) Core i\\(5-5200\\)U CPU \\(2.20\\)GHz with \\(12\\)GB of RAM. However, it is not a hard task to adapt them to SysIdentPy.</p> <p>Following the aforementioned studies, the maximum lags for the input and output are chosen to be, respectively, \\(n_u=n_y=4\\) and the nonlinear degree is \\(\\ell = 3\\). The parameters related to the BPSOGSA are detailed on Table 8.</p> Parameters \\(n_u\\) \\(n_y\\) \\(\\ell\\) p-value max_iter n_agents \\(\\alpha\\) \\(G_0\\) Values \\(4\\) \\(4\\) \\(3\\) \\(0.05\\) \\(30\\) \\(10\\) \\(23\\) \\(100\\) &gt;Table 8. Parameters used in MetaMSS <p>\\(300\\) runs of the Meta-MSS algorithm have been executed for each model, aiming to compare some statistics about the algorithm performance. The elapsed time, the time required to obtain the final model, and correctness, the percentage of exact model selections, are analyzed.</p> <p>The results in Table 9 are obtained with the parameters configured accordingly to Table 8.</p> \\(S_1\\) \\(S_2\\) \\(S_3\\) \\(S_4\\) \\(S_5\\) \\(S_6\\) Correct model 100\\% 100\\% 100\\% 100\\% 100\\% 100\\% Elapsed time (mean) 5.16s 3.90s 3.40s 2.37s 1.40s 3.80s &gt;Table 9. Overall performance of the MetaMSS <p>Table 9 shows that all the model terms are correctly selected using the Meta-MSS. It is worth to notice that even the model \\(S_5\\), which have an autoregressive noise, was correctly selected using the proposed algorithm. This result resides in the evaluation of all regressors individually, and the ones considered redundant are removed from the model.</p> <p>Figure 15 presents the convergence of each execution of Meta-MSS. It is noticeable that the majority of executions converges to the correct model structures with \\(10\\) or fewer iterations. The reason for this relies on the maximum number of iterations and the number of search agents. The first one is related to the acceleration coefficient, which boosts the exploration phase of the algorithm, while the latter increases the number of candidate models to be evaluated. Intuitively, one can see that both parameters influence the elapsed time and, more importantly, the model structure selected to compose the final model. Consequently, an inappropriate choice of one of them may results in sub/over-parameterized models, since the algorithm can converge to a local optimum. The next subsection presents an analysis of the max_iter and n_agents influence in the algorithm performance.</p> <p></p> <p>Figure 15. Convergence of Meta-MSS for different model structures. The figure illustrates the convergence behavior of the Meta-MSS algorithm across multiple executions. Each curve represents the convergence trajectory for a specific model structure from \\(S_1\\) to \\(S_6\\) over a maximum of 30 iterations.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#influence-of-the-max_iter-and-n_agents-parameters","title":"Influence of the \\(max\\_iter\\) and \\(n\\_agents\\) parameters","text":"<p>The simulation models are used to show the performance of the Meta-MSS considering different tuning for <code>max_iter</code> and <code>n_agents</code> parameters. First, we set and uphold the <code>max_iter=30</code> while the <code>n_agents</code> are changed. Then, we set and uphold the <code>n_agents</code> while the <code>max_iter</code> is modified. The results detailed in this section have been obtained by setting the remaining parameters according to Table 8.</p> \\(S_1\\) \\(S_2\\) \\(S_3\\) \\(S_4\\) \\(S_5\\) \\(S_6\\) max_iter = 30, n_agents = 1 Correct model \\(65\\%\\) \\(55.66\\%\\) \\(14\\%\\) \\(14\\%\\) \\(7.3\\%\\) \\(20.66\\%\\) Elapsed time (mean) \\(0.26\\)s \\(0.19\\)s \\(0.15\\)s \\(0.11\\)s \\(0.13\\)s \\(0.13\\)s max_iter = 30, n_agents = 5 Correct model \\(100\\%\\) \\(100\\%\\) \\(99\\%\\) \\(98\\%\\) \\(91.66\\%\\) \\(98.33\\%\\) Elapsed time (mean) \\(2.08\\)s \\(1.51\\)s \\(1.41\\)s \\(0.99\\)s \\(0.59\\)s \\(1.13\\)s max_iter = 30, n_agents = 20 Correct model \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) Elapsed time (mean) \\(12.88\\)s \\(9.10\\)s \\(8.77\\)s \\(5.70\\)s \\(3.37\\)s \\(9.50\\)s max_iter = 5, n_agents = 10 Correct model \\(96.33\\%\\) \\(99\\%\\) \\(86\\%\\) \\(93.66\\%\\) \\(93\\%\\) \\(97.33\\%\\) Elapsed time (mean) \\(0.92\\)s \\(0.73\\)s \\(0.72\\)s \\(0.52\\)s \\(0.29\\)s \\(0.64\\)s max_iter = 15, n_agents = 10 Correct model \\(100\\%\\) \\(100\\%\\) \\(99\\%\\) \\(99\\%\\) \\(100\\%\\) \\(100\\%\\) Elapsed time (mean) \\(2.80\\)s \\(2.33\\)s \\(2.25\\)s \\(1.60\\)s \\(0.90\\)s \\(2.30\\)s max_iter = 50, n_agents = 10 Correct model \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) Elapsed time (mean) \\(7.38\\)s \\(5.44\\)s \\(4.56\\)s \\(3.01\\)s \\(2.10\\)s \\(4.52\\)s &gt;Table 10. <p>The aggregated results in Table 10 confirms the expected performance regarding the elapsed time and percentage of correct models. Indeed, both metrics increases significantly as the number of agents and the maximum number of iteration increases. The number of agents is very relevant because it yields a broader exploration of the search space. All system are affected by the increase in the number of agents and the maximum number of iterations.</p> <p>Regarding all tested systems, it is straightforward to notice that the more extensive exploration dramatically impacts on the exactitude of the selection procedure. If only a few agents are assigned, the performance of Meta-MSS algorithm deteriorates significantly, especially for systems \\(S_3, S_4\\) and \\(S_5\\). The maximum number of iteration empowers agents to explore, globally and locally, the space around the candidate models tested so far. In this sense, as the number of iterations increases, more the agents can explore the search space and examine different regressors.</p> <p>If these parameters are improperly chosen, the algorithm might fail to select the best model structure. In this respect, the results presented here concerns only the selected systems. The larger the search space, the larger the number of agents and iterations should be. Although the computational effort increases with larger values for n_agents and max_iteration, the algorithm remains very efficient regarding the elapsed time for all tuning configurations that ensured the selection of the exact model structures.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#selection-of-over-and-sub-parameterized-models","title":"Selection of over and sub-parameterized models","text":"<p>Regardless of the successful selection of all models structures by the Meta-Structure Selection Algorithm, one can ask how the models differs from the true ones in the cases presented in Table 10 where the algorithm failed to ensure \\(100\\%\\) of correctness. Figure 16 depicts the distribution of terms number selected in each case. It is evident that the number of over-parameterized models selected is higher than the sub-parameterized in overall. Regarding the cases where the number of search agents are low, due to low exploration and exploitation capacity, the algorithm converged early and resulted in models with a high number of spurious regressors. In respect to \\(S_2\\) and \\(S_5\\), for example, with n_agents\\(=1\\), the algorithm ends up selecting models with more than \\(20\\) terms. One can say this was a extreme scenario for comparison purpose. However, a suitable choice for the parameters is intrinsically related to the dimension of the search space. Referring to cases where n_agents\\(\\geq 5\\), the number of spurious terms decreased significantly where the algorithm failed to select the true models.</p> <p>Furthermore, it is interesting to point out the importance of tuning the parameters properly because since the exploration and exploitation phase of the algorithm are strongly dependent on them. A premature convergence of the algorithm may result in models with the factual number of terms, but with wrong ones. This happened with all cases with <code>n_agents=1</code>. For example, the algorithm generates models with correct number of terms in \\(33.33\\%\\) of the cases analyzed regarding \\(S_3\\). However, Table 10 shows that only \\(14\\%\\) are, in fact, equivalent to the true model.</p> <p></p> <p>Figure 16. The distribution of terms number selected for each simulated models concerning the variation of the <code>max_iter</code> and <code>n_agents</code>.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#selection-of-over-and-sub-parameterized-models_1","title":"Selection of over and sub-parameterized models","text":"<p>Regardless of the successful selection of all models structures by the MetaMSS, one can ask how the models differ from the true ones in the cases presented in Table 10 where the algorithm failed to ensure \\(100\\%\\) of correctness. Figure 16 depicts the distribution of terms number selected in each case. It is evident that the number of over-parameterized models selected is higher than the sub-parameterized in overall. Regarding the cases where the number of search agents is low, due to low exploration and exploitation capacity, the algorithm converged early and resulted in models with a high number of spurious regressors. In respect to \\(S_2\\) and \\(S_5\\), for example, with <code>n_agents=1</code>, the algorithm ends up selecting models with more than \\(20\\) terms. One can say this was an extreme scenario for comparison purpose. However, a suitable choice for the parameters is intrinsically related to the dimension of the search space. By referring to cases where <code>n_agents</code>\\(\\geq 5\\), the number of spurious terms decreased significantly where the algorithm failed to select the true models.</p> <p>Furthermore, it is interesting to point out the importance of tuning the parameters properly because since the exploration and exploitation phase of the algorithm is strongly dependent on them. A premature convergence of the algorithm may result in models with the actual number of terms, but with wrong ones. This issue happened with all cases with <code>n_agents=1</code>. For example, the algorithm generates models with the correct number of terms in \\(33.33\\%\\) of the cases analyzed regarding \\(S_3\\). However, Table 10 shows that only \\(14\\%\\) are, in fact, equivalent to the true model.</p> <p>The systems \\(S_1\\), \\(S_2\\), \\(S_3\\), \\(S_4\\) and \\(S_6\\) has been used as benchmark by Bianchi, F., Falsone, A., Prandini, M. and Piroddi, L., so we can compare directly our results with those reported by the author in his thesis. All techniques used \\(n_y=n_u=4\\) and \\(\\ell = 3\\). The RaMSS and the RaMSS with Conditional Linear Family (C-RaMSS) used the following configuration for the tuning parameters: \\(K=1\\), \\(\\alpha = 0.997\\), \\(NP = 200\\) and \\(v=0.1\\). The Meta-Structure Selection Algorithm was tuned according to Table 8.</p> \\(S_1\\) \\(S_2\\) \\(S_3\\) \\(S_4\\) \\(S_6\\) Meta-MSS Correct model \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) Elapsed time (mean) \\(5.16\\)s \\(3.90\\)s \\(3.40\\)s \\(2.37\\)s \\(3.80\\)s RaMSS- \\(NP=100\\) Correct model \\(90.33\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(66\\%\\) Elapsed time (mean) \\(3.27\\)s \\(1.24\\)s \\(2.59\\)s \\(1.67\\)s \\(6.66\\)s RaMSS- \\(NP=200\\) Correct model \\(78.33\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(82\\%\\) Elapsed time (mean) \\(6.25\\)s \\(2.07\\)s \\(4.42\\)s \\(2.77\\)s \\(9.16\\)s C-RaMSS Correct model \\(93.33\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) Elapsed time (mean) \\(18\\)s \\(10.50\\)s \\(16.96\\)s \\(10.56\\)s \\(48.52\\)s &gt; Table 11. Comparative analysis between MetaMSS, RaMSS, and C-RaMSS <p>In terms of correctness, the MetaMSS outperforms (or at least equals) the RaMSS and C-RaMSS for all analyzed systems as shown in Table 11. Regarding \\(S_6\\), the correctness rate increased by \\(18\\%\\) when compared with RaMSS and the elapsed time required for C-RaMSS obtain \\(100\\%\\) of correctness is \\(1276.84\\%\\) higher than the MetaMSS. Furthermore, the MetaMSS is notably more computationally efficient than C-RaMSS and similar to RaMSS.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#metamss-vs-frols","title":"MetaMSS vs FROLS","text":"<p>The FROLS algorithm was applied to all tested systems, with the results summarized in Table 12. The algorithm successfully selected the correct model terms for \\(S_2\\) and \\(S_6\\). However, it failed to identify two out of four regressors for \\(S_1\\). For \\(S_3\\), FROLS included \\(y_{k-1}\\) instead of the correct term \\(y_{k-1}^3\\). Similarly, \\(S_4\\) incorrectly included \\(y_{k-4}\\) rather than the required term \\(y_{k-2}\\). Additionally, for \\(S_5\\), the algorithm produced an incorrect model structure by including the spurious term \\(y_{k-4}\\).</p> Meta-MSS Regressor Correct FROLS Regressor Correct \\(S_1\\) \\(y_{k-1}\\) yes \\(y_{k-1}\\) yes \\(y_{k-2}\\) yes \\(y_{k-4}\\) no \\(x_{k-1}\\) yes \\(x_{k-1}\\) yes \\(x_{k-2}\\) yes \\(x_{k-4}\\) no \\(S_2\\) \\(y_{k-1}\\) yes \\(y_{k-1}\\) yes \\(x_{k-1}\\) yes \\(x_{k-1}\\) yes \\(x_{k-1}^2\\) yes \\(x_{k-1}^2\\) yes \\(x_{k-1}^3\\) yes \\(x_{k-1}^3\\) yes \\(S_3\\) \\(y_{k-1}^3\\) yes \\(y_{k-1}\\) no \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-1}x_{k-1}\\) yes \\(x_{k-2}^2\\) yes \\(x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(y_{k-2}\\) yes \\(y_{k-2}\\) yes \\(S_4\\) \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-2}\\) yes \\(y_{k-4}\\) no \\(x_{k-2}^2\\) yes \\(x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(S_5\\) \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-2}\\) yes \\(y_{k-4}\\) no \\(x_{k-2}^2\\) yes \\(x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(S_6\\) \\(y_{k-2}\\) yes \\(y_{k-2}\\) yes \\(x_{k-1}\\) yes \\(x_{k-1}\\) yes \\(y_{k-2}x_{k-2}\\) yes \\(y_{k-2}x_{k-1}\\) yes &gt; Table 12. Comparative analysis between MetaMSS and FROLS"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#meta-mss-vs-rjmcmc","title":"Meta-MSS vs RJMCMC","text":"<p>The \\(S_4\\) model is taken from Baldacchino, Anderson, and Kadirkamanathan's work (Computational System Identification for Bayesian NARMAX Modelling). In their study, the maximum lags for the input and output are \\(n_y = n_u = 4\\), and the nonlinear degree is \\(\\ell = 3\\). The authors ran the RJMCMC algorithm 10 times on the same input-output data. The RJMCMC method successfully identified the true model structure 7 out of 10 times. In contrast, the MetaMSS algorithm consistently identified the true model structure in all runs. These results are summarized in Table 13.</p> <p>Additionally, the RJMCMC method has notable drawbacks that are addressed by the MetaMSS algorithm. Specifically, RJMCMC is computationally intensive, requiring \\(30,000\\) iterations to achieve results. Furthermore, it relies on various probability distributions to simplify the parameter estimation process, which can complicate the computations. In contrast, MetaMSS offers a more efficient and straightforward approach, avoiding these issues.</p> Meta-MSS Model Correct RJMCMC Model 1 (\\(7\\times\\)) RJMCMC Model 2 RJMCMC Model 3 RJMCMC Model 4 Correct \\(S_4\\) \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-1}x_{k-1}\\) \\(y_{k-1}x_{k-1}\\) \\(y_{k-1}x_{k-1}\\) \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-2}\\) yes \\(y_{k-2}\\) \\(y_{k-2}\\) \\(y_{k-2}\\) \\(y_{k-2}\\) yes \\(x_{k-2}^2\\) yes \\(x_{k-2}^2\\) \\(x_{k-2}^2\\) \\(x_{k-2}^2\\) \\(x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) \\(y_{k-2}x_{k-2}^2\\) \\(y_{k-2}x_{k-2}^2\\) \\(y_{k-2}x_{k-2}^2\\) yes - - - \\(y_{k-3}x_{k-3}\\) \\(x_{k-4}^2\\) \\(x_{k-1}x_{k-3}^2\\) no &gt; Table 13. Comparative analysis between MetaMSS and RJMCMC."},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#metamss-algorithm-using-sysidentpy","title":"MetaMSS algorithm using SysIdentPy","text":"<p>Consider the same data used in the Overview of the Information Criteria Methods.</p> <pre><code>from sysidentpy.model_structure_selection import MetaMSS\n\n\nbasis_function = Polynomial(degree=2)\nmodel = MetaMSS(\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 random_state=42,\n\u00a0 \u00a0 basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <p>The MetaMSS algorithm does not rely on information criteria methods such as ERR for model structure selection, which is why it does not involve those hyperparameters. This is also true for the AOLS and ER algorithms. For more details on how to use these methods and their associated hyperparameters, please refer to the documentation.</p> <p>When it comes to parameter estimation, SysIdentPy allows the use of any available method, regardless of the model structure selection algorithm. Users can select from a range of parameter estimation methods to apply to their chosen model structure. This flexibility enables users to explore various modeling approaches and customize their system identification process. While the examples provided use the default parameter estimation method, users are encouraged to experiment with different options to find the best fit for their needs.</p> <p>The results of the MetaMSS are</p> Regressors Parameters ERR y(k-1) 1.8004E-01 0.00000000E+00 x1(k-2) 8.9747E-01 0.00000000E+00 <p></p> <p>Figure 17. Free Run Simulation for the model fitted using MetaMSS.</p> <p>The <code>results</code> method brings ERR as 0 for every regressor because, as mentioned, ERR algorithm is not executed in this case.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#accelerated-orthogonal-least-squares-aols-and-entropic-regression-er","title":"Accelerated Orthogonal Least Squares (AOLS) and Entropic Regression (ER)","text":"<p>In addition to FROLS and MetaMSS, SysIdentPy includes two other methods for model structure selection: Accelerated Orthogonal Least Squares (AOLS) and Entropic Regression (ER). While I won't delve into the details of these methods in this section as I have with FROLS and MetaMSS, I will provide an overview and references for further reading:</p> <ul> <li>Accelerated Orthogonal Least Squares (AOLS): For an in-depth exploration of AOLS, refer to the original paper here.</li> <li>Entropic Regression (ER): Detailed information about ER can be found in the original paper here.</li> </ul> <p>For now, I will demonstrate how to use these methods within SysIdentPy.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#accelerated-orthogonal-least-squares","title":"Accelerated Orthogonal Least Squares","text":"<pre><code>from sysidentpy.model_structure_selection import AOLS\n\nbasis_function = Polynomial(degree=2)\nmodel = AOLS(\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> Regressors Parameters ERR x1(k-2) 9.1542E-01 0.00000000E+00 <p>Figure 18. Free Run Simulation for the model fitted using AOLS algorithm.</p>"},{"location":"book/4%20-%20Model%20Structure%20Selection%20%28MSS%29/#entropic-regression","title":"Entropic Regression","text":"<pre><code>from sysidentpy.model_structure_selection import ER\n\nbasis_function = Polynomial(degree=2)\nmodel = ER(\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> Regressors Parameters ERR 1 -2.4554E-02 0.00000000E+00 x1(k-2) 9.0273E-01 0.00000000E+00 <p>Figure 19. Free Run Simulation for the model fitted using Entropic Regression algorithm.</p>"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/","title":"5. Multiobjective Parameter Estimation","text":"<p>Multiobjective parameter estimation represents a fundamental paradigm shift in the way we approach the parameter tuning problem for NARMAX models. Instead of seeking a single set of parameter values that optimally fits the model to the data, multiobjective approaches aim to identify a set of parameter solutions, known as the Pareto front, that provide a trade-off between competing objectives. These objectives often encompass a spectrum of model performance criteria, such as goodness-of-fit, model complexity, and robustness.</p> <p>What does that mean? It means that when we are modeling a dynamical system we are, most of the time, building models that are only good to represent the dynamical behavior of the system under study. Well, that is valid most of the time because we are building dynamical models, so if it doesn't perform well in static scenarios, it won't be a problem. However, that's not always the case and we might build a model that is good in both dynamical and static behavior. In such cases, the methods for purely dynamical systems are not adequate and multiobjective algorithms can help us in such task.</p> <p>The main idea in multiobjective parameter estimation is the inclusion of the affine information. The affine information is an auxiliary information that can be defined a priori such as the static gain and the static function of the system. Formally, the affine information can be defined as follows:</p> <p>Let the parameter vector \\(\\Theta \\in \\mathbb{R}^{n_{\\Theta}}\\), a vector \\(\\mathrm{v}\\in \\mathbb{R}^p\\) and a matrix \\(\\mathrm{G}\\in \\mathbb{R}^{n_{\\Theta}\\times p}\\) where \\(\\mathrm{v}\\) and \\(\\mathrm{G}\\) are assumed to be accessible. Suppose \\(\\mathrm{G}\\Theta\\) be an estimate of \\(\\mathrm{v}\\). Hence \\(\\mathrm{v} = \\mathrm{G}\\Theta + \\xi\\). Then, \\([\\mathrm{v}, \\mathrm{G}]\\) is a pair of affine information of the system.</p>"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#multi-objective-optimization-problem","title":"Multi-objective optimization problem","text":"<p>Lets define what is a multiobjective problem. Given \\(m\\) objective functions</p> \\[ \\begin{equation}     \\mathrm{J}(\\hat{\\Theta}) = [J_1(\\hat{\\Theta}), J_2(\\hat{\\Theta}), \\cdots, J_m(\\hat{\\Theta})]^\\top, \\end{equation} \\tag{5.1} \\] <p>where \\(\\mathrm{J}(\\cdot):\\mathbb{R}^n \\mapsto \\mathbb{R}^m\\), a multi-objective optimization problem can be generally stated as (A. Baykasoglu, S. Owen, e N. Gindy)</p> \\[ \\begin{equation}     \\begin{aligned}         &amp; \\underset{\\Theta}{\\text{minimize}} &amp; &amp; \\mathrm{J}(\\Theta) \\\\ &amp; \\text{subject to} &amp; &amp; \\Theta \\in \\mathrm{S} = \\left\\{\\Theta \\mid \\Theta \\in \\mathrm{A}^n, g_i(\\Theta) \\leq a_i, h_j(\\Theta) = b_j \\right\\}, \\\\ &amp; &amp; &amp; i = 1, \\ldots, m, \\quad j = 1, \\ldots, n     \\end{aligned} \\end{equation} \\tag{5.2} \\] <p>where \\(\\Theta\\) is an \\(n\\)-dimensional vector of the decision variables, \\(\\mathrm{S}\\) is the set of feasible solutions bounded by \\(m\\) inequality constraints (\\(g_i\\)) and \\(n\\) equality constraints (\\(h_j\\)), and \\(a_i\\) and \\(b_j\\) are constants. For continuous variables \\(A = \\mathbb{R}\\) while \\(A\\) contains the set of permissible values for discrete variables.</p> <p>Usually problems with \\(1 &lt; m &lt; 4\\) are called multiobjective optimization problems. When there are more objectives (\\(m\\geq 4\\)), it is referred as many-objective optimization problems, a emergence class of multi-objective problems for solving complex modern real-world tasks. More details can be found in Fleming, P. J. and Purshouse, R. C. and Lygoe, R. J., Li, B. and Li, J. and Tang, K. and Yao, X..</p>"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#pareto-optimal-definition-and-pareto-dominance","title":"Pareto Optimal Definition and Pareto Dominance","text":"<p>Consider \\([y^{(1)}, y^{(2)}] \\in \\mathbb{R}^m\\) two vectors in the objective space. If and only if \\(\\forall \\in \\{1, \\ldots, m \\}: y_i^{(1)}\\leq y_i^{(2)}\\) and \\(\\exists j \\in \\{1, \\ldots, m \\}: y_j^{(1)} &lt; y_j^{(2)}\\) one can said \\(y^{(1)} \\prec y^{(2)}\\) (P. L. Yu).</p> <p>The concept of Pareto optimality is generally used to describe the trade-off among the minimization of different objectives. Following the pareto definition: the pareto optimal is any parameter vector representing an efficient solution  where no objective function can be improved without  making at least one objective function worse off will be referred to as a Pareto-model.</p> <p>In the system identification field, that means to find a model where you can't get a better dynamic performance without making the static performance worse.</p> <p>A hypothetical Pareto set is shown in Figure 1.</p> <p></p> <p>Figure 1. The figure illustrates the concept of Pareto optimality, where each point in the objective space represents a solution. The Pareto front is depicted as a curve, demonstrating the trade-off between two conflicting objectives. Points on the front cannot be improved in one objective without worsening the other, highlighting the balance in optimal solutions.</p> <p>In this case the model structure is assumed to be known and therefore there is a one-to-one correspondence between each parameter vector on the Pareto optimal solution and a model (Nepomuceno, E. G. and Takahashi, R. H. C. and Aguirre, L. A.). One can build a Pareto set by applying the Weighted Sum Method, where a set of objectives are scalarized into a single objective by adding each objective multiplied by a user supplied weight. Consider</p> \\[ \\begin{equation}     \\mathrm{W} = \\Bigg\\{ w|w \\in \\mathbb{R}^m, w_j\\geq 0 \\quad \\textrm{and} \\quad \\sum^{m}_{j=1}w_j=1 \\Bigg\\} \\end{equation} \\tag{5.3} \\] <p>as non-negative weights. Then, the convex optimization problem can be stated as</p> \\[ \\begin{equation} \\begin{aligned} \\Theta^* &amp;= \\underset{\\Theta}{\\text{argmin}} \\, \\langle w, \\mathrm{J}(\\Theta) \\rangle \\end{aligned} \\end{equation} \\tag{5.4} \\] <p>where \\(w\\) is a combination of weights to the different objectives functions. Therefore the Pareto-set is associated to the set of realizations of \\(w \\in \\mathrm{W}\\). An efficient single-step computational strategy was presented by (Nepomuceno, E. G. and Takahashi, R. H. C. and Aguirre, L. A.) for solving Equation 5.4 by means of a Least Squares formulation, which is presented in the following section.</p>"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#affine-information-least-squares-algorithm","title":"Affine Information Least Squares Algorithm","text":"<p>Consider the \\(m\\) affine information pairs \\([\\mathrm{v}_i \\in \\mathbb{R}^{pi}, \\mathrm{G}_i \\mathbb{R}^{pi\\times n}]\\) with \\(i = 1, \\ldots, m\\). Assume there is exist a full column rank \\(\\mathrm{G}_i\\) and let \\(M\\) be a model of the form</p> \\[ y = \\Psi\\Theta + \\epsilon. \\tag{5.5} \\] <p>Then the \\(m\\) affine information pairs can be considered in the parameter estimation by solving</p> \\[ \\begin{equation} \\begin{aligned} \\Theta^* &amp;= \\underset{\\Theta}{\\text{argmin}} \\sum_{i=1}^{m} w_i (\\mathrm{v}_i - \\mathrm{G}_i \\Theta)^\\top (\\mathrm{v}_i - \\mathrm{G}_i \\Theta) \\end{aligned} \\end{equation} \\tag{5.6} \\] <p>with \\(w = [w_i, \\ldots, w_m]^\\top \\in \\mathrm{W}\\). The solution of equation above is given by</p> \\[ \\begin{equation}     \\Theta^* = \\left[\\sum^{m}_{i=1}w_i\\mathrm{G}_i^\\top\\mathrm{G}_i\\right]^{-1}  \\left[\\sum^{m}_{i=1}w_i\\mathrm{G}_i^\\top\\mathrm{v}_i\\right]. \\end{equation} \\tag{5.7} \\] <p>If there exists only one information, the problem reduces to the mono-objective Least Squares solution.</p> <p>To make things straightforward, lets check a detailed case study.</p>"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#case-study-buck-converter","title":"Case Study - Buck converter","text":"<p>A buck converter is a type of DC/DC converter that decreases the voltage (while increasing the current) from its input (power supply) to its output (load). It is similar to a boost converter (elevator) and is a type of switched-mode power supply (SMPS) that typically contains at least two semiconductors (a diode and a transistor, although modern buck converters replace the diode with a second transistor used for synchronous rectification) and at least one energy storage element, a capacitor, inductor or both combined.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.multiobjective_parameter_estimation import AILS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.narmax_tools import set_weights\n</code></pre>"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#dynamic-behavior","title":"Dynamic Behavior","text":"<pre><code>df_train = pd.read_csv(r\"datasets/buck_id.csv\")\ndf_valid = pd.read_csv(r\"datasets/buck_valid.csv\")\n\n# Plotting the measured output (identification and validation data)\nplt.figure(1)\nplt.title(\"Output\")\nplt.plot(df_train.sampling_time, df_train.y, label=\"Identification\", linewidth=1.5)\nplt.plot(df_valid.sampling_time, df_valid.y, label=\"Validation\", linewidth=1.5)\nplt.xlabel(\"Samples\")\nplt.ylabel(\"Voltage\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code># Plotting the measured input (identification and validation data)\nplt.figure(2)\nplt.title(\"Input\")\nplt.plot(df_train.sampling_time, df_train.input, label=\"Identification\", linewidth=1.5)\nplt.plot(df_valid.sampling_time, df_valid.input, label=\"Validation\", linewidth=1.5)\nplt.ylim(2.1, 2.6)\nplt.ylabel(\"u\")\nplt.xlabel(\"Samples\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#buck-converter-static-function","title":"Buck Converter Static Function","text":"<p>The duty cycle, represented by the symbol \\(D\\), is defined as the ratio of the time the system is on (\\(T_{on}\\)\u200b) to the total operation cycle time (\\(T\\)). Mathematically, this can be expressed as \\(D=\\frac{T_{on}}{T}\\). The complement of the duty cycle, represented by \\(D'\\), is defined as the ratio of the time the system is off (\\(T_{off}\\)) to the total operation cycle time (\\(T\\)) and can be expressed as \\(D'=\\frac{T_{off}}{T}\\).</p> <p>The load voltage (\\(V_o\\)) is related to the source voltage (\\(V_d\\)) by the equation \\(V_o\u200b=D\u22c5V_d\u200b=(1\u2212D\u2019)\u22c5V_d\\). \u00a0For this particular converter, it is known that \\(D\u2032=\\frac{\\bar{u}-1}{3}\u200b\\),\u200b which means that the static function of this system can be derived from theory to be:</p> \\[ V_o = \\frac{4V_d}{3} - \\frac{V_d}{3}\\cdot \\bar{u} \\] <p>If we assume that the source voltage \\(V_d\\)\u200b is equal to 24 V, then we can rewrite the above expression as follows:</p> \\[ V_o = (4 - \\bar{u})\\cdot 8 \\] <pre><code># Static data\nVd = 24\nUo = np.linspace(0, 4, 50)\nYo = (4 - Uo) * Vd / 3\nUo = Uo.reshape(-1, 1)\nYo = Yo.reshape(-1, 1)\nplt.figure(3)\nplt.title(\"Buck Converter Static Curve\")\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{y}$\")\nplt.plot(Uo, Yo, linewidth=1.5, linestyle=\"-\", marker=\"o\")\nplt.show()\n</code></pre> <p></p>"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#buck-converter-static-gain","title":"Buck converter Static Gain","text":"<p>The gain of a Buck converter is a measure of how its output voltage changes in response to changes in its input voltage. Mathematically, the gain can be calculated as the derivative of the converter\u2019s static function, which describes the relationship between its input and output voltages.</p> <p>In this case, the static function of the Buck converter is given by the equation:</p> \\[ V_o = (4 - \\bar{u})\\cdot 8 \\] <p>Taking the derivative of this equation with respect to \\(\\hat{u}\\), we find that the gain of the Buck converter is equal to \u22128. In other words, for every unit increase in the input voltage \\(\\hat{u}\\), the output voltage Vo\u200b will decrease by 8 units, so</p> \\[ gain=V_o'=-8 \\] <pre><code># Defining the gain\ngain = -8 * np.ones(len(Uo)).reshape(-1, 1)\nplt.figure(3)\nplt.title(\"Buck Converter Static Gain\")\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{gain}$\")\nplt.plot(Uo, gain, linewidth=1.5, label=\"gain\", linestyle=\"-\", marker=\"o\")\nplt.legend()\nplt.show()\n</code></pre> <p></p>"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#building-a-dynamic-model-using-the-mono-objective-approach","title":"Building a dynamic model using the mono-objective approach","text":"<pre><code>x_train = df_train.input.values.reshape(-1, 1)\ny_train = df_train.y.values.reshape(-1, 1)\nx_valid = df_valid.input.values.reshape(-1, 1)\ny_valid = df_valid.y.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=8,\n\u00a0 \u00a0 extended_least_squares=False,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"aic\",\n\u00a0 \u00a0 estimator=\"least_squares\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\n</code></pre>"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#affine-information-least-squares-algorithm-ails","title":"Affine Information Least Squares Algorithm (AILS)","text":"<p>AILS is a multiobjective parameter estimation algorithm, based on a set of affine information pairs. The multiobjective approach proposed in the mentioned paper and implemented in SysIdentPy leads to a convex multiobjective optimization problem, which can be solved by AILS. AILS is a LeastSquares-type non-iterative scheme for finding the Pareto-set solutions for the multiobjective problem.</p> <p>So, with the model structure defined (we will be using the one built using the dynamic data above), one can estimate the parameters using the multiobjective approach.</p> <p>The information about static function and static gain, besides the usual dynamic input/output data, can be used to build the pair of affine information to estimate the parameters of the model. We can model the cost function as:</p> \\[ \\gamma(\\hat\\theta) = w_1\\cdot J_{LS}(\\hat{\\theta})+w_2\\cdot J_{SF}(\\hat{\\theta})+w_3\\cdot J_{SG}(\\hat{\\theta}) \\]"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#multiobjective-parameter-estimation-considering-3-different-objectives-the-prediction-error-the-static-function-and-the-static-gain","title":"Multiobjective parameter estimation considering 3 different objectives: the prediction error, the static function and the static gain","text":"<pre><code># you can use any set of model structure you want in your use case, but in this notebook we will use the one obtained above the compare with other work\nmo_estimator = AILS(final_model=model.final_model)\n# setting the log-spaced weights of each objective function\nw = set_weights(static_function=True, static_gain=True)\n# you can also use something like\n# w = np.array(\n# \u00a0 \u00a0 [\n# \u00a0 \u00a0 \u00a0 \u00a0 [0.98, 0.7, 0.5, 0.35, 0.25, 0.01, 0.15, 0.01],\n# \u00a0 \u00a0 \u00a0 \u00a0 [0.01, 0.1, 0.3, 0.15, 0.25, 0.98, 0.35, 0.01],\n# \u00a0 \u00a0 \u00a0 \u00a0 [0.01, 0.2, 0.2, 0.50, 0.50, 0.01, 0.50, 0.98],\n# \u00a0 \u00a0 ]\n# )\n\n# to set the weights. Each row correspond to each objective\n</code></pre> <p>AILS has an <code>estimate</code> method that returns the cost functions (J), the Euclidean norm of the cost functions (E), the estimated parameters referring to each weight (theta), the regressor matrix of the gain and static_function affine information HR and QR, respectively.</p> <pre><code>J, E, theta, HR, QR, position = mo_estimator.estimate(\n\u00a0 \u00a0 X=x_train, y=y_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\n\u00a0 \u00a0 \"w1\": w[0, :],\n\u00a0 \u00a0 \"w2\": w[2, :],\n\u00a0 \u00a0 \"w3\": w[1, :],\n\u00a0 \u00a0 \"J_ls\": J[0, :],\n\u00a0 \u00a0 \"J_sg\": J[1, :],\n\u00a0 \u00a0 \"J_sf\": J[2, :],\n\u00a0 \u00a0 \"||J||:\": E,\n}\npd.DataFrame(result)\n</code></pre> w1 w2 w3 J_ls J_sg J_sf \\(\\lVert J \\rVert\\) 0.006842 0.003078 0.990080 0.999970 1.095020e-05 0.000013 0.245244 0.007573 0.002347 0.990080 0.999938 2.294665e-05 0.000016 0.245236 0.008382 0.001538 0.990080 0.999885 6.504913e-05 0.000018 0.245223 0.009277 0.000642 0.990080 0.999717 4.505541e-04 0.000021 0.245182 0.006842 0.098663 0.894495 1.000000 7.393246e-08 0.000015 0.245251 ... ... ... ... ... ... ... 0.659632 0.333527 0.006842 0.995896 3.965699e-04 1.000000 0.244489 0.730119 0.263039 0.006842 0.995632 5.602981e-04 0.972842 0.244412 0.808139 0.185020 0.006842 0.995364 8.321071e-04 0.868299 0.244300 0.894495 0.098663 0.006842 0.995100 1.364999e-03 0.660486 0.244160 0.990080 0.003078 0.006842 0.992584 9.825987e-02 0.305492 0.261455 <p>Now we can set theta related to any weight results</p> <pre><code>model.theta = theta[-1, :].reshape(\n\u00a0 \u00a0 -1, 1\n) \u00a0# setting the theta estimated for the last combination of the weights\n\n# the model structure is exactly the same, but the order of the regressors is changed in estimate method. Thats why you have to change the model.final_model\n\nmodel.final_model = mo_estimator.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=3,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</code></pre> Regressors Parameters ERR 1 2.2930E+00 9.999E-01 y(k-1) 2.3307E-01 2.042E-05 y(k-2) 6.3209E-01 1.108E-06 x1(k-1) -5.9333E-01 4.688E-06 y(k-1)^2 2.7673E-01 3.922E-07 y(k-2)y(k-1) -5.3228E-01 8.389E-07 x1(k-1)y(k-1) 1.6667E-02 5.690E-07 y(k-2)^2 2.5766E-01 3.827E-06"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#the-dynamic-results-for-that-chosen-theta-is","title":"The dynamic results for that chosen theta is","text":"<pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre>"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#the-static-gain-result-is","title":"The static gain result is","text":"<pre><code>plt.figure(4)\nplt.title(\"Gain\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 gain,\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"o\",\n\u00a0 \u00a0 label=\"Buck converter static gain\",\n)\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 HR.dot(model.theta),\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#the-static-function-result-is","title":"The static function result is","text":"<pre><code>plt.figure(5)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 QR.dot(model.theta),\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX \u200b\u200bstatic representation\",\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#getting-the-best-weight-combination-based-on-the-norm-of-the-cost-function","title":"Getting the best weight combination based on the norm of the cost function","text":"<p>The variable <code>position</code> returned in <code>estimate</code> method give the position of the best weight combination. The model structure is exactly the same, but the order of the regressors is changed in estimate method. Thats why you have to change the model.final_model. The dynamic, static gain, and the static function  results for that chosen theta is shown below.</p> <pre><code>model.theta = theta[position, :].reshape(\n\u00a0 \u00a0 -1, 1\n) \u00a0# setting the theta estimated for the best combination of the weights\n\n# changing the model.final_model\n\nmodel.final_model = mo_estimator.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=3,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\n# dynamic results\nplot_results(y=y_valid, yhat=yhat, n=1000)\n\n# static gain\nplt.figure(4)\nplt.title(\"Gain\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 gain,\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"o\",\n\u00a0 \u00a0 label=\"Buck converter static gain\",\n)\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 HR.dot(model.theta),\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n\n# static function\nplt.figure(5)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 QR.dot(model.theta),\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX \u200b\u200bstatic representation\",\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n)\n\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre> Regressors Parameters ERR 1 1.5405E+00 9.999E-01 y(k-1) 2.9687E-01 2.042E-05 y(k-2) 6.4693E-01 1.108E-06 x1(k-1) -4.1302E-01 4.688E-06 y(k-1)^2 2.7671E-01 3.922E-07 y(k-2)y(k-1) -5.3474E-01 8.389E-07 x1(k-1)y(k-1) 4.0624E-03 5.690E-07 y(k-2)^2 2.5832E-01 3.827E-06 <p></p> <p></p> <p></p> <p>You can also plot the pareto-set solutions</p> <pre><code>plt.figure(6)\nax = plt.axes(projection=\"3d\")\nax.plot3D(J[0, :], J[1, :], J[2, :], \"o\", linewidth=0.1)\nax.set_title(\"Pareto-set solutions\", fontsize=15)\nax.set_xlabel(\"$J_{ls}$\", fontsize=10)\nax.set_ylabel(\"$J_{sg}$\", fontsize=10)\nax.set_zlabel(\"$J_{sf}$\", fontsize=10)\nplt.show()\n</code></pre> <p></p>"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#detailing-ails","title":"Detailing AILS","text":"<p>The polynomial NARX model built using the mono-objective approach has the following structure:</p> \\[ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 u(k-1) y(k-1) + \\theta_4 + \\theta_5 y(k-1)^2 + \\theta_6 u(k-1) + \\theta_7 y(k-2)y(k-1) + \\theta_8 y(k-2)^2 \\] <p>The, the goal when using the static function and static gain information in the multiobjective scenario is to estimate the vector \\(\\hat{\\theta}\\) based on:</p> \\[ \\theta = [w_1\\Psi^T\\Psi + w_2(HR)^T(HR) + w_3(QR)(QR)^T]^{-1} [w_1\\Psi^T y + w_2(HR)^T\\overline{g}+w_3(QR)^T\\overline{y}] \\] <p>The \\(\\Psi\\) matrix is built using the usual mono-objective dynamic modeling approach in SysIdentPy. However, it is still necessary to find the Q, H and R matrices. AILS have the methods to compute all of those matrices. Basically, to do that, \\(q_i^T\\) is first estimated:</p> \\[ q_i^T = \\begin{bmatrix} 1 &amp; \\overline{y_i} &amp; \\overline{u_1} &amp; \\overline{y_i}^2 &amp; \\cdots &amp; \\overline{y_i}^l &amp; \u00a0F_{yu} &amp; \\overline{u_i}^2 &amp; \\cdots &amp; \\overline{u_i}^l \\end{bmatrix} \\] <p>where \\(F_{yu}\\) stands for all non-linear monomials in the model that are related to \\(y(k)\\) and \\(u(k)\\), \\(l\\) is the largest non-linearity in the model for input and output terms. For a model with a degree of nonlinearity equal to 2, we can obtain:</p> \\[ q_i^T = \\begin{bmatrix} 1 &amp; \\overline{y_i} &amp; \\overline{u_i} &amp; \\overline{y_i}^2 &amp; \\overline{u_i}\\:\\overline{y_i} &amp; \\overline{u_i}^2 \\end{bmatrix} \\] <p>It is possible to encode the \\(q_i^T\\) matrix so that it follows the model encoding defined in SysIdentPy. To do this, 0 is considered as a constant, \\(y_i\\) equal to 1 and \\(u_i\\) equal to 2. The number of columns indicates the degree of nonlinearity of the system and the number of rows reflects the number of terms:</p> \\[ q_i = \\begin{bmatrix} 0 &amp; 0\\\\ 1 &amp; 0\\\\ 2 &amp; 0\\\\ 1 &amp; 1\\\\ 2 &amp; 1\\\\ 2 &amp; 2\\\\ \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ \\overline{y_i}\\\\ \\overline{u_i}\\\\ \\overline{y_i}^2\\\\ \\overline{u_i}\\:\\overline{y_i}\\\\ \\overline{u_i}^2\\\\ \\end{bmatrix} \\] <p>Finally, the result can be easily obtained using the \u2018regressor_space\u2019 method of SysIdentPy</p> <pre><code>from sysidentpy.narmax_base import RegressorDictionary\n\nobject_qit = RegressorDictionary(xlag=1, ylag=1)\nR_example = object_qit.regressor_space(n_inputs=1) // 1000\nprint(f\"R = {R_example}\")\n</code></pre> \\[ R = \\begin{bmatrix} 0 &amp; 0 \\\\ 1 &amp; 0 \\\\ 2 &amp; 0 \\\\ 1 &amp; 1 \\\\ 2 &amp; 1 \\\\ 2 &amp; 2 \\end{bmatrix} \\] <p>such that:</p> \\[ \\overline{y_i} = q_i^T R\\theta \\] <p>and:</p> \\[ \\overline{g_i} = H R\\theta \\] <p>where \\(R\\) is the linear mapping of the static regressors represented by \\(q_i^T\\). In addition, the \\(H\\) matrix holds affine information regarding \\(\\overline{g_i}\\), which is equal to \\(\\overline{g_i} = \\frac{d\\overline{y}}{d\\overline{u}}{\\big |}_{(\\overline{u_i}\\:\\overline{y_i})}\\).</p> <p>From now on, we will begin to apply the parameter estimation in a multiobjective manner. This will be done with the NARX polynomial model of the BUCK converter in mind. In this context, \\(q_i^T\\) will be generic and will assume a specific format for the problem at hand. For this task, the \\(R_qit\\) method will be used, whose objective is to return the \\(q_i^T\\) related to the model and the matrix of the linear mapping \\(R\\):</p> <pre><code>R, qit = mo_estimator.build_linear_mapping()\nprint(\"R matrix:\")\nprint(R)\nprint(\"qit matrix:\")\nprint(qit)\n</code></pre> \\[ R = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{bmatrix} \\] <p>and</p> \\[ qit = \\begin{bmatrix} 0 &amp; 0 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 2 &amp; 0 \\\\ 1 &amp; 1 \\\\ \\end{bmatrix} \\] <p>So</p> \\[ q_i = \\begin{bmatrix} 0 &amp; 0 \\\\ 1 &amp; 0 \\\\ 2 &amp; 0 \\\\ 1 &amp; 1 \\\\ 2 &amp; 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ \\overline{y} \\\\ \\overline{u} \\\\ \\overline{y^2} \\\\ \\overline{u} \\cdot \\overline{y} \\\\ \\end{bmatrix} \\] <p>You can notice that the method produces outputs consistent with what is expected:</p> \\[ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 u(k-1) y(k-1) + \\theta_4 + \\theta_5 y(k-1)^2 + \\theta_6 u(k-1) + \\theta_7 y(k-2)y(k-1) + \\theta_8 y(k-2)^2 \\] <p>and:</p> \\[ R = \\begin{bmatrix} term/\\theta &amp; \\theta_1 &amp; \\theta_2 &amp; \\theta_3 &amp; \\theta_4 &amp; \\theta_5 &amp; \\theta_6 &amp; \\theta_7 &amp; \\theta_8\\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\overline{y} &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\overline{u} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\\\ \\overline{y^2} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1\\\\ \\overline{y}\\:\\overline{u} &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\end{bmatrix} \\]"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#validation","title":"Validation","text":"<p>The following model structure will be used to validate the approach:</p> \\[ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 + \\theta_4 u(k-1) + \\theta_5 u(k-1)^2 + \\theta_6 u(k-2)u(k-1)+\\theta_7 u(k-2) + \\theta_8 u(k-2)^2 \\] \\[ \\therefore \\] \\[ final\\_model = \\begin{bmatrix} 1001 &amp; 0\\\\ 1002 &amp; 0\\\\ 0 &amp; 0\\\\ 2001 &amp; 0\\\\ 2001 &amp; 2001\\\\ 2002 &amp; 2001\\\\ 2002 &amp; 0\\\\ 2002 &amp; 2002 \\end{bmatrix} \\] <p>defining in code:</p> <pre><code>final_model = np.array(\n\u00a0 \u00a0 [\n\u00a0 \u00a0 \u00a0 \u00a0 [1001, 0],\n\u00a0 \u00a0 \u00a0 \u00a0 [1002, 0],\n\u00a0 \u00a0 \u00a0 \u00a0 [0, 0],\n\u00a0 \u00a0 \u00a0 \u00a0 [2001, 0],\n\u00a0 \u00a0 \u00a0 \u00a0 [2001, 2001],\n\u00a0 \u00a0 \u00a0 \u00a0 [2002, 2001],\n\u00a0 \u00a0 \u00a0 \u00a0 [2002, 0],\n\u00a0 \u00a0 \u00a0 \u00a0 [2002, 2002],\n\u00a0 \u00a0 ]\n)\nfinal_model\n</code></pre> 1001 0 1002 0 0 0 2001 0 2001 2001 2002 2001 2002 0 2002 2002 <pre><code>mult2 = AILS(final_model=final_model)\n\ndef psi(X, Y):\n\u00a0 \u00a0 PSI = np.zeros((len(X), 8))\n\u00a0 \u00a0 for k in range(2, len(Y)):\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 0] = Y[k - 1]\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 1] = Y[k - 2]\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 2] = 1\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 3] = X[k - 1]\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 4] = X[k - 1] ** 2\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 5] = X[k - 2] * X[k - 1]\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 6] = X[k - 2]\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 7] = X[k - 2] ** 2\n\u00a0 \u00a0 return np.delete(PSI, [0, 1], axis=0)\n</code></pre> <p>The value of theta with the lowest mean squared error obtained with the same code implemented in Scilab was:</p> \\[ W_{LS} = 0.3612343 \\] <p>and:</p> \\[ W_{SG} = 0.3548699 \\] <p>and:</p> \\[ W_{SF} = 0.3548699 \\] <pre><code>PSI = psi(x_train, y_train)\nw = np.array([[0.3612343], [0.2838959], [0.3548699]])\nJ, E, theta, HR, QR, position = mult2.estimate(\n\u00a0 \u00a0 y=y_train, X=x_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\n\u00a0 \u00a0 \"w1\": w[0, :],\n\u00a0 \u00a0 \"w2\": w[2, :],\n\u00a0 \u00a0 \"w3\": w[1, :],\n\u00a0 \u00a0 \"J_ls\": J[0, :],\n\u00a0 \u00a0 \"J_sg\": J[1, :],\n\u00a0 \u00a0 \"J_sf\": J[2, :],\n\u00a0 \u00a0 \"||J||:\": E,\n}\n\npd.DataFrame(result)\n</code></pre> w1 w2 w3 J_ls J_sg J_sf \\(\\lVert J \\rVert\\) 0.361234 0.35487 0.283896 1.0 1.0 1.0 1.0 The order of the weights is different because the way we implemented in Python, but the results are very close as expected."},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#dynamic-results","title":"Dynamic results","text":"<pre><code>model.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = mult2.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\n\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=3,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</code></pre> Regressors Parameters ERR 1 1.4287E+00 9.999E-01 y(k-1) 5.5147E-01 2.042E-05 y(k-2) 4.0449E-01 1.108E-06 x1(k-1) -1.2605E+01 4.688E-06 x1(k-2) 1.2257E+01 3.922E-07 x1(k-1)^2 8.3274E+00 8.389E-07 x1(k-2)x1(k-1) -1.1416E+01 5.690E-07 x1(k-2)^2 3.0846E+00 3.827E-06 <pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre>"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#static-gain","title":"Static gain","text":"<pre><code>plt.figure(7)\nplt.title(\"Gain\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 gain,\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"o\",\n\u00a0 \u00a0 label=\"Buck converter static gain\",\n)\n\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 HR.dot(model.theta),\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#static-function","title":"Static function","text":"<pre><code>plt.figure(8)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 QR.dot(model.theta),\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX \u200b\u200bstatic representation\",\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n)\n\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#pareto-set-solutions","title":"Pareto-set solutions","text":"<pre><code>plt.figure(9)\nax = plt.axes(projection=\"3d\")\nax.plot3D(J[0, :], J[1, :], J[2, :], \"o\", linewidth=0.1)\nax.set_title(\"Optimum pareto-curve\", fontsize=15)\nax.set_xlabel(\"$J_{ls}$\", fontsize=10)\nax.set_ylabel(\"$J_{sg}$\", fontsize=10)\nax.set_zlabel(\"$J_{sf}$\", fontsize=10)\nplt.show()\n</code></pre> <p>The following table show the results reported in \u2018IniciacaoCientifica2007\u2019 and the ones obtained with SysIdentPy implementation</p> Theta SysIdentPy IniciacaoCientifica2007 \\(\\theta_1\\) 0.5514725 0.549144 \\(\\theta_2\\) 0.40449005 0.408028 \\(\\theta_3\\) 1.42867821 1.45097 \\(\\theta_4\\) -12.60548863 -12.55788 \\(\\theta_5\\) 8.32740057 8.1516315 \\(\\theta_6\\) -11.41574116 -11.09728 \\(\\theta_7\\) 12.25729955 12.215782 \\(\\theta_8\\) 3.08461195 2.9319577 <p>where:</p> \\[ E_{Scilab} = \u00a0 \u00a017.426613 \\] <p>and:</p> \\[ E_{Python} = 17.474865 \\] <p>Note: as mentioned before, the order of the regressors in the model change, but it is the same structure. The tables shows the respective regressor parameter concerning <code>SysIdentPy</code> and <code>IniciacaoCientifica2007</code>, \u00a0but the order \\(\\Theta_1\\), \\(\\Theta_2\\) and so on are not the same of the ones in <code>model.final_model</code></p> <pre><code>R, qit = mult2.build_linear_mapping()\nprint(\"R matrix:\")\nprint(R)\nprint(\"qit matrix:\")\nprint(qit)\n</code></pre> \\[ R = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\\\ \\end{bmatrix} \\] <p>and</p> \\[ qit = \\begin{bmatrix} 0 &amp; 0 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 2 \\\\ \\end{bmatrix} \\] <p>model's structure that will be utilized (\u2018IniciacaoCientifica2007\u2019):</p> \\[ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 + \\theta_4 u(k-1) + \\theta_5 u(k-1)^2 + \\theta_6 u(k-2)u(k-1)+\\theta_7 u(k-2) + \\theta_8 u(k-2)^2 \\] \\[ q_i = \\begin{bmatrix} 0 &amp; 0 \\\\ 1 &amp; 0 \\\\ 2 &amp; 0 \\\\ 2 &amp; 2 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ \\overline{y} \\\\ \\overline{u} \\\\ \\overline{u^2} \\end{bmatrix} \\]"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#biobjective-optimization","title":"Biobjective optimization","text":""},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#an-use-case-applied-to-buck-converter-cc-cc-using-as-objectives-the-static-curve-information-and-the-prediction-error-dynamic","title":"An use case applied to Buck converter CC-CC using as objectives the static curve information and the prediction error (dynamic)","text":"<pre><code>bi_objective = AILS(\n\u00a0 \u00a0 static_function=True, static_gain=False, final_model=final_model, normalize=True\n)\n</code></pre> <p>the value of theta with the lowest mean squared error obtained through the routine in Scilab was:</p> \\[ W_{LS} = 0.9931126 \\] <p>and:</p> \\[ W_{SF} = 0.0068874 \\] <pre><code>w = np.zeros((2, 2000))\nw[0, :] = np.logspace(-0.01, -6, num=2000, base=2.71)\nw[1, :] = np.ones(2000) - w[0, :]\nJ, E, theta, HR, QR, position = bi_objective.estimate(\n\u00a0 \u00a0 y=y_train, X=x_train, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\n\nresult = {\"w1\": w[0, :], \"w2\": w[1, :], \"J_ls\": J[0, :], \"J_sg\": J[1, :], \"||J||:\": E}\n\npd.DataFrame(result)\n</code></pre> w1 w2 J_ls J_sg \\(\\lVert J \\rVert\\) 0.990080 0.009920 0.990863 1.000000 0.990939 0.987127 0.012873 0.990865 0.987032 0.990939 0.984182 0.015818 0.990867 0.974307 0.990939 0.981247 0.018753 0.990870 0.961803 0.990940 0.978320 0.021680 0.990873 0.949509 0.990941 ... ... ... ... ... 0.002555 0.997445 0.999993 0.000072 0.999993 0.002547 0.997453 0.999994 0.000072 0.999994 0.002540 0.997460 0.999996 0.000071 0.999996 0.002532 0.997468 0.999998 0.000071 0.999998 0.002525 0.997475 1.000000 0.000070 1.000000 <pre><code>model.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = bi_objective.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=3,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nr\n</code></pre> Regressors Parameters ERR 0 1 1.3873E+00 9.999E-01 1 y(k-1) 5.4941E-01 2.042E-05 2 y(k-2) 4.0804E-01 1.108E-06 3 x1(k-1) -1.2515E+01 4.688E-06 4 x1(k-2) 1.2227E+01 3.922E-07 5 x1(k-1)^2 8.1171E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1047E+01 5.690E-07 7 x1(k-2)^2 2.9043E+00 3.827E-06 <pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <p></p> <pre><code>plt.figure(10)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 QR.dot(model.theta),\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX \u200b\u200bstatic representation\",\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n)\n\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code>plt.figure(11)\nplt.title(\"Costs Functions\")\nplt.plot(J[1, :], J[0, :], \"o\")\nplt.xlabel(\"Static Curve Information\")\nplt.ylabel(\"Prediction Error\")\nplt.show()\n</code></pre> <p></p> <p>where the best estimated \\(\\Theta\\) is</p> Theta SysIdentPy IniciacaoCientifica2007 \\(\\theta_1\\) 0.54940883 0.5494135 \\(\\theta_2\\) 0.40803995 0.4080312 \\(\\theta_3\\) 1.38725684 3.3857601 \\(\\theta_4\\) -12.51466378 -12.513688 \\(\\theta_5\\) 8.11712897 8.116575 \\(\\theta_6\\) -11.04664789 -11.04592 \\(\\theta_7\\) 12.22693907 12.227184 \\(\\theta_8\\) 2.90425844 2.9038468 <p>where:</p> \\[ E_{Scilab} = 17.408934 \\] <p>and:</p> \\[ E_{Python} = 17.408947 \\]"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#multiobjective-parameter-estimation","title":"Multiobjective parameter estimation","text":""},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#use-case-considering-2-different-objectives-the-prediction-error-and-the-static-gain","title":"Use case considering 2 different objectives: the prediction error and the static gain","text":"<pre><code>bi_objective_gain = AILS(\n\u00a0 \u00a0 static_function=False, static_gain=True, final_model=final_model, normalize=False\n)\n</code></pre> <p>the value of theta with the lowest mean squared error obtained through the routine in Scilab was:</p> \\[ W_{LS} = 0.9931126 \\] <p>and:</p> \\[ W_{SF} = 0.0068874 \\] <pre><code>w = np.zeros((2, 2000))\nw[0, :] = np.logspace(0, -6, num=2000, base=2.71)\nw[1, :] = np.ones(2000) - w[0, :]\nJ, E, theta, HR, QR, position = bi_objective_gain.estimate(\n\u00a0 \u00a0 X=x_train, y=y_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\n\nresult = {\"w1\": w[0, :], \"w2\": w[1, :], \"J_ls\": J[0, :], \"J_sg\": J[1, :], \"||J||:\": E}\n\npd.DataFrame(result)\n</code></pre> w1 w2 J_ls J_sg \\(\\lVert J \\rVert\\) 1.000000 0.000000 17.407256 3.579461e+01 39.802849 0.997012 0.002988 17.407528 2.109260e-01 17.408806 0.994033 0.005967 17.407540 2.082067e-01 17.408785 0.991063 0.008937 17.407559 2.056636e-01 17.408774 0.988102 0.011898 17.407585 2.031788e-01 17.408771 ... ... ... ... ... 0.002555 0.997445 17.511596 3.340081e-07 17.511596 0.002547 0.997453 17.511596 3.320125e-07 17.511596 0.002540 0.997460 17.511597 3.300289e-07 17.511597 0.002532 0.997468 17.511598 3.280571e-07 17.511598 0.002525 0.997475 17.511599 3.260972e-07 17.511599 <pre><code># Writing the results\nmodel.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = bi_objective_gain.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=3,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nr\n</code></pre> Regressors Parameters ERR 0 1 1.4853E+00 9.999E-01 1 y(k-1) 5.4940E-01 2.042E-05 2 y(k-2) 4.0806E-01 1.108E-06 3 x1(k-1) -1.2581E+01 4.688E-06 4 x1(k-2) 1.2210E+01 3.922E-07 5 x1(k-1)^2 8.1686E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1122E+01 5.690E-07 7 x1(k-2)^2 2.9455E+00 3.827E-06 <pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <p></p> <pre><code>plt.figure(12)\nplt.title(\"Gain\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 gain,\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"o\",\n\u00a0 \u00a0 label=\"Buck converter static gain\",\n)\n\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 HR.dot(model.theta),\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <pre><code>plt.figure(11)\nplt.title(\"Costs Functions\")\nplt.plot(J[1, :], J[0, :], \"o\")\nplt.xlabel(\"Gain Information\")\nplt.ylabel(\"Prediction Error\")\nplt.show()\n</code></pre> <p></p> <p>being the selected \\(\\theta\\):</p> Theta SysIdentPy IniciacaoCientifica2007 \\(\\theta_1\\) 0.54939785 0.54937289 \\(\\theta_2\\) 0.40805603 0.40810168 \\(\\theta_3\\) 1.48525190 1.48663719 \\(\\theta_4\\) -12.58066084 -12.58127183 \\(\\theta_5\\) 8.16862622 8.16780294 \\(\\theta_6\\) -11.12171897 -11.11998621 \\(\\theta_7\\) 12.20954849 12.20927355 \\(\\theta_8\\) 2.94548501 2.9446532 <p>where:</p> \\[ E_{Scilab} = \u00a017.408997 \\] <p>and:</p> \\[ E_{Python} = 17.408781 \\]"},{"location":"book/5%20-%20Multiobjective%20Parameter%20Estimation/#additional-information","title":"Additional Information","text":"<p>You can also access the matrix Q and H using the following methods</p> <p>Matrix Q:</p> <pre><code>bi_objective_gain.build_static_function_information(Uo, Yo)[1]\n</code></pre> <p>Matrix H+R:</p> <pre><code>bi_objective_gain.build_static_gain_information(Uo, Yo, gain)[1]\n</code></pre>"},{"location":"book/6%20-%20Multiobjective%20Model%20Structure%20Selection/","title":"6. Multiobjective Model Structure Selection","text":"<p>Coming soon</p>"},{"location":"book/7%20-%20NARX%20Neural%20Network/","title":"7. NARX Neural Network","text":"<p>Coming soon</p>"},{"location":"book/8%20-%20Severely%20Nonlinear%20System%20Identification/","title":"8. Severely Nonlinear System Identification","text":"<p>We have categorized systems into two different classes for now: linear systems and nonlinear systems. As mentioned, linear systems has been extensively studied with several different well-established methods available, while nonlinear systems is a very active field with several problems that are still open for research. Besides linear and nonlinear systems, there are the ones called Severely Nonlinear Systems. Severely Nonlinear Systems are the ones that exhibit highly complex and exotic dynamic behaviors like sub-harmonics, chaotic behavior and hysteresis. For now, we will focus on system with hysteresis.</p>"},{"location":"book/8%20-%20Severely%20Nonlinear%20System%20Identification/#modeling-hysteresis-with-polynomial-narx-model","title":"Modeling Hysteresis With Polynomial NARX Model","text":"<p>Hysteresis nonlinearity is a severely nonlinear behavior commonly found in electromagnetic devices, sensors, semiconductors, intelligent materials, and many more, which have memory effects between quasi-static input and output (Visintin, A., Ahmad, I.). A hysteretic system is one that exhibits a path-dependent behavior, meaning its response depends not only on its current state but also on its history.  In a hysteretic system, when you apply an input, the system's response (like displacement or stress) doesn't follow the same path to the starting point when you remove the input. Instead, it forms a loop-like pattern called a hysteresis loop. This is because the system have the ability to preserve a deformation caused by an input, characterizing a memory effect.</p> <p>The identification of hysteretic systems using polynomial NARX models is typically an intriguing task because the traditional Model Structure Selection algorithms do not work properly (Martins, S. A. M. and Aguirre, L. A., Leva, A. and Piroddi, L.). Martins, S. A. M. and Aguirre, L. A. presented the sufficient conditions to describe hysteresis using polynomial models by providing the concept of bounding structure \\(\\mathcal{H}\\). Polynomial NARX models with a single equilibrium can be used in a full characterization of the hysteresis behavior adopting the bounding structure concept.</p> <p>The following are some of the essential concepts and formal definitions for understanding how NARX model can be used to describe systems with hysteresis.</p>"},{"location":"book/8%20-%20Severely%20Nonlinear%20System%20Identification/#continuous-time-loading-unloading-quasi-static-signal","title":"Continuous-time loading-unloading quasi-static signal","text":"<p>One important characteristic to model hysteretic systems is the input signal. A loading-unloading quasi-static signal is a periodic continuous time signal \\(x_t\\) with period \\(T = (t_f - t_i)\\) and frequency \\(\\omega = 2\\pi f\\) where \\(x_t\\) increases monotonically from \\(x_{min}\\) to \\(x_{max}\\), considering \\(t_i \\leq t \\leq t_m\\) (loading) and decreases monotonically from \\(x_{max}\\) to \\(x_{min}\\), considering \\(t_m \\leq t \\leq t_f\\) (unloading). If the loading-unloading signal changes with \\(\\omega \\rightarrow 0\\), the signal is also called a quasi-static signal. Visually, this is much more simple to understand. The following image shows a continuous-time loading-unloading quasi-static signal.</p> <p></p> <p>Figure 1. Continuous-time loading-unloading quasi-static signal, demonstrating the periodic increase and decrease of the input signal.</p> <p>In this respect, Martins, S. A. M. and Aguirre, L. A. also presented the idea of transforming the inputs of the system using multi-valued functions.</p> <p>Multi-valued functions - Let \\(\\phi (\\Delta x_{k}): \\mathbb{R} \\rightarrow \\mathbb{R}\\). If~\\(\\Delta x_{k}=x_k-x_{k-1}\\), \\(\\phi (\\Delta x_{k})\\) is a multi-valued function if:</p> \\[ \\begin{equation}     \\phi (\\Delta x_{k})=     \\begin{cases}         \\phi_1, &amp; if \\ \\Delta x_{k} &gt; \\epsilon; \\\\         \\phi_2, &amp; if \\ \\Delta x_{k} &lt; \\epsilon; \\\\         \\phi_3, &amp; if \\ \\Delta x_{k} = \\epsilon; \\\\     \\end{cases} \\end{equation} \\tag{1} \\] <p>where \\(\\epsilon \\in \\mathbb{R}\\), \\(\\phi_1 \\neq \\phi_2 \\neq \\phi_3\\). For some inputs  \\(\\Delta x_{k}\\neq \\epsilon, \\ \\forall{k} \\in \\mathbb{N}\\) , and the last value in equation above is not used.</p> <p>A frequently used multi-valued function is the sign\\((\\cdot): \\mathbb{R} \\rightarrow \\mathbb{R}\\):</p> \\[  \\begin{equation}  sign(x)=     \\begin{cases}         1, &amp; if \\ x &gt; 0; \\\\         -1, &amp; if \\ x &lt; 0; \\\\         0, &amp; if \\ x = 0. \\\\     \\end{cases} \\end{equation} \\tag{2} \\]"},{"location":"book/8%20-%20Severely%20Nonlinear%20System%20Identification/#hysteresis-loops-in-continuous-time-mathcalh_tomega","title":"Hysteresis loops in continuous time \\(\\mathcal{H}_t(\\omega)\\)","text":"<p>Let \\(x_t\\) be a continuous-time loading-unloading quasi-static signal applied to a continuous-time system and \\(y_t\\) is the system output. \\(\\mathcal{H}_t(\\omega)\\) denotes a closed loop in the \\(x_t - y_t\\) plane, which shape depend on \\(\\omega\\). If the system presents hysteretic nonlinearity, \\(\\mathcal{H}_t(\\omega)\\) is denoted as:</p> \\[ \\begin{equation} \\mathcal{H}_t(\\omega) =     \\begin{cases}         \\mathcal{H}_t(\\omega)^{+}, \\ for \\ t_i \\ \\leq \\ t \\ \\leq \\ t_m, \\\\         \\mathcal{H}_t(\\omega)^{-}; \\ for \\ t_m \\ \\leq \\ t \\ \\leq \\ t_f, \\\\     \\end{cases} \\end{equation} \\tag{3} \\] <p>where \\(\\mathcal{H}_t(\\omega)^{+} \\neq \\mathcal{H}_t(\\omega)^{-}\\), \\(\\forall t \\neq t_m\\). \\(t_i \\leq t \\leq t_m\\) and~\\(t_m \\leq t \\leq t_f\\) correspond to the regime when \\(x_t\\) is loading and unloading, respectively. \\(\\mathcal{H}_t(\\omega)^{+}\\) corresponds to the part of the loop formed in the \\(x_t - y_t\\) plane, while \\(t_i \\leq t \\leq t_m\\) (when \\(x_t\\) is loading) whereas \\(\\mathcal{H}_t(\\omega)^{-}\\) is the part of the loop formed in the~\\(x_t - y_t\\) plane for~\\(t_m \\leq t \\leq t_f\\) (when \\(x_t\\) is unloading), as shown in the Figure 2:</p> <p></p> <p>Figure 2. Example of a hysteresis curve.</p> <p>Rate Independent Hysteresis (RIH) (Visintin, A.)- The hysteresis behavior is called to be rate independent if the path \\(ABCD\\), which depends on pair \\(x(t), y(t)\\), is invariant with respect to any increasing diffeomorphism~\\(\\varphi : [0,T] \\rightarrow [0,T]\\), i.e.:</p> \\[ \\begin{align}         F(u \\ o \\ \\varphi, y^{0}) = F(u,y^0)\\ o \\ \\varphi &amp; \\ em \\ [0,T]. \\end{align} \\tag{4} \\] <p>This means that at any instant \\(t\\), \\(y(t)\\) depends only on \\(u:[0,T] \\rightarrow \\mathbb{R}\\) and on the order in which values have been attained before \\(t\\). In other words, the memory effect is not affected by the frequency of the input.</p>"},{"location":"book/8%20-%20Severely%20Nonlinear%20System%20Identification/#rate-independent-hysteresis-in-polynomial-narx-model","title":"Rate Independent Hysteresis  in polynomial NARX model","text":"<p>Martins, S. A. M. and Aguirre, L. A. presented the sufficient conditions for NARX model to represent hysteresis. One of the developed concepts in the Bounding Structure \\(\\mathcal{H}\\).</p> <p>Bounding Structure \\(\\mathcal{H}\\) (Martins, S. A. M. and Aguirre, L. A.) - Let \\(\\mathcal{H}_t(\\omega)\\) be the system hysteresis. \\(\\mathcal{H}= \\lim_{\\omega \\to 0} \\mathcal{H}_t(\\omega)\\) is defined as the bounding structure that delimits \\(\\mathcal{H}_t(\\omega)\\).</p> <p>Now, consider a polynomial NARX excited by a loading-unloading quasi-static signal. If the model has one real and stable equilibrium point whose location depends on input and loading/unloading regime, the polynomial will exhibit a Rate Independent Hysteresis loop \\(\\mathcal{H}_t(\\omega)\\) in the \\(x-y\\) plane.</p> <p>Here is an example. Let \\(y_k  =  0.8y_{k-1} + 0.4\\phi_{k-1} + 0.2x_{k-1}\\), where \\(\\phi_{k} = \\rm{sign}(\\Delta(x_{k}))\\) and \\(x_{k} = sin(\\omega k)\\) and \\(\\omega\\) is the frequency of the input signal \\(x\\). The equilibria of this model is given by:</p> \\[ \\begin{equation}     \\overline{y}(\\overline{\\phi},\\overline{x})=     \\begin{cases}         \\frac{0.6+0.2\\overline{x}}{1-0.8} \\ = 3 \\ + \\ \\overline{x} \\ , &amp; for \\ loading; \\\\         \\frac{-0.6+0.2\\overline{x}}{1-0.8} \\ = -3 \\ + \\ \\overline{x} \\ , &amp; for \\ unloading; \\\\     \\end{cases} \\end{equation} \\tag{5} \\] <p>where \\(\\overline {x}\\) is a loading-unloading quasi-static input signal. Since the equilibrium points are asymptotically stable, the output converges to \\(\\mathcal{H}_k (w)\\) in the \\(x-y\\) plane. Note that for a constant input value \\(x ~ = ~ 1 ~ = ~ \\overline{x}\\), the equilibrium lies in \\(\\overline{y} ~ = ~ 3\\) for loading regime and \\(\\overline {y} ~ = ~ -1\\) for unloading regime. Analogously, for \\(\\overline {x} ~ = ~ -1\\), the equilibrium lies in \\(\\overline {y} ~ = ~ 1\\) for loading regime and \\(\\overline {y} ~ = ~ -3\\) for unloading regime, as shown in the figure below:</p> <p></p> <p>Figure 3. Example of a bounding structure \\(\\mathcal{H}\\). The black dots are on \\(\\mathcal{H}_{k}(\\omega)\\) for model \\(y_k  =  0.8y_{k-1} + 0.4\\phi_{k-1} + 0.2x_{k-1}\\). The bounding structure \\(\\mathcal{H}\\), in red, confines \\(\\mathcal{H}_{k}(\\omega)\\).}</p> <p>As can be observed in the Figure 3, in we guarantee the sufficient conditions proposed by Martins, S. A. M. and Aguirre, L. A., a NARX model can reproduce a histeretic behavior. Chapter 10 presents a case study of a system with hysteresis.</p> <p>The following code can be used to reproduce the behavior shown in Figure 3. Change <code>w</code> from \\(1\\) to \\(0.1\\) to see how the bounded structure \\(\\mathcal{H}\\) converge to the equilibria of the system.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nw = 1\nt = np.arange(0, 60.1, 0.1)\ny = np.zeros(len(t))\nx = np.sin(w * t)\n\n# Initialize y and fi\nfi = np.zeros(len(t))\n# Iterate over the time array to calculate y\nfor k in range(1, len(t)):\n\u00a0 \u00a0 fi[k] = np.sign(x[k] - x[k-1])\n\u00a0 \u00a0 y[k] = 0.8 * y[k-1] + 0.2 * x[k-1] + 0.4 * fi[k-1]\n\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Example')\nplt.show()\n</code></pre> <p></p> <p>Figure 4.  Reproduction of a bounding structure \\(\\mathcal{H}\\) using python.</p>"},{"location":"book/9%20-%20Validation/","title":"9. Validation","text":""},{"location":"book/9%20-%20Validation/#the-predict-method-in-sysidentpy","title":"The <code>predict</code> Method in SysIdentPy","text":"<p>Before getting into the validation process in System Identification, it's essential to understand how the <code>predict</code> method works in SysIdentPy.</p>"},{"location":"book/9%20-%20Validation/#using-the-predict-method","title":"Using the <code>predict</code> Method","text":"<p>A typical usage of the <code>predict</code> method in SysIdentPy looks like this:</p> <pre><code>yhat = model.predict(X=x_test, y=y_test)\n</code></pre> <p>SysIdentPy users often have two common questions about this method:</p> <ol> <li>Why do we need to pass the test data, <code>y_test</code>, as an argument in the <code>predict</code> method?</li> <li>Why are the initial predicted values identical to the values in the test data?</li> </ol> <p>To address these questions, let\u2019s first explain the concepts of infinity-step-ahead prediction, n-step-ahead prediction, and one-step-ahead prediction in dynamic systems.</p>"},{"location":"book/9%20-%20Validation/#infinity-step-ahead-prediction","title":"Infinity-Step-Ahead Prediction","text":"<p>Infinity-step-ahead prediction, also known as free run simulation, refers to making predictions using previously predicted values, \\(\\hat{y}_{k-n_y}\\), in the prediction loop.</p> <p>For example, consider the following test input and output data:</p> \\[ x_{test} = [1, 2, 3, 4, 5, 6, 7] \\] \\[ y_{test} = [8, 9, 10, 11, 12, 13, 14] \\] <p>Suppose we want to validate a model \\(m\\) defined by:</p> \\[ m \\rightarrow y_k = 1*y_{k-1} + 2*x_{k-1} \\] <p>To predict the first value, we need access to both \\(y_{k-1}\\) and \\(x_{k-1}\\). This requirement explains why you need to pass <code>y_test</code> as an argument in the <code>predict</code> method. It also answers the second question: SysIdentPy requires the user to provide the initial conditions explicitly. The <code>y_test</code> data passed in the <code>predict</code> method is not used entirely; only the initial values needed for the model\u2019s lag structure are used.</p> <p>In this example, the model's maximum lag is 1, so we need only 1 initial condition. The predicted values, <code>yhat</code>, are then calculated as follows:</p> <pre><code>y_initial = yhat(0) = 8\nyhat(1) = 1*8 + 2*1 = 10\nyhat(2) = 1*10 + 2*2 = 14\nyhat(3) = 1*14 + 2*3 = 20\nyhat(4) = 1*20 + 2*4 = 28\n</code></pre> <p>As shown, the first value of <code>yhat</code> matches the first value of <code>y_test</code> because it serves as the initial condition. Another key point is that the prediction loop uses the previously predicted values, not the actual <code>y_test</code> values, which is why it's called infinity-step-ahead or free run simulation.</p> <p>In system identification, we often aim for models that perform well in infinity-step-ahead predictions. Since the prediction error propagates over time, a model that shows good performance in free run simulation is considered a robust model.</p> <p>In SysIdentPy, users only need to pass the initial conditions when performing an infinity-step-ahead prediction. If you pass only the initial conditions, the results will be the same! Therefore</p> <pre><code>yhat = model.predict(X=x_test, y=y_test)\n</code></pre> <p>is actually the same as</p> <pre><code>yhat = model.predict(X=x_test, y=y_test[:model.max_lag].reshape(-1, 1))\n</code></pre> <p><code>model.max_lag</code> can be accessed after we fit the model using the code below.</p> <pre><code>model = FROLS(\n    order_selection=False,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 estimator=LeastSquares(unbiased=False),\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 e_tol=0.9999\n\u00a0 \u00a0 n_terms=15\n)\nmodel.fit(X=x, y=y)\nmodel.max_lag\n</code></pre> <p>Its important to mention that, in current version of SysIdentPy, the maximum lag considered is actually the maximum lag between <code>xlag</code> and <code>ylag</code> definition. This is important because you can pass <code>ylag = xlag = 10</code> and the final model, after the model structure selection, select terms where the maximum lag is 3. You have to pass 10 initial conditions, but internally the calculations are done using the correct regressors. This is necessary due the way the regressors are created after that the model is fitted. Therefore is recommend to use the <code>model.max_lag</code> to be sure.</p>"},{"location":"book/9%20-%20Validation/#1-step-ahead-prediction","title":"1-step ahead prediction","text":"<p>The difference between 1 step-ahead prediction and infinity-steps ahead prediction is that the model take the previous real <code>y_test</code> test values in the loop instead of the predicted <code>yhat</code> values. And that is a huge and important difference. Lets do prediction using 1-step ahead method:</p> <pre><code>y_initial = yhat(0) = 8\nyhat(1) = 1*8 + 2*1 = 10\nyhat(2) = 1*9 + 2*2 = 13\nyhat(3) = 1*10 + 2*3 = 16\nyhat(4) = 1*11 + 2*4 = 19\nand so on\n</code></pre> <p>The model uses real values in the loop and only predict the next value. The prediction error, in this case, is always corrected because we are not propagating the error using the predicted values in the loop.</p> <p>SysIdentPy's <code>predict</code> method allow the user to perform a 1-step ahead prediction by setting <code>steps_ahead=1</code></p> <pre><code>yhat = model.predict(X=x_test, y=y_test, steps_ahead=1)\n</code></pre> <p>In this case, as you can imagine, we need to pass the all the <code>y_test</code> data because the method have to access the real values at each iteration. If you pass only the initial conditions, <code>yhat</code> will have only the initial conditions plus 1 more sample, that is the 1-step ahead prediction. To predict another point, you would need to pass the new initial conditions again and so on. SysIdentPy already to everything for you, so just pass all the data you want to validate using the 1-step ahead method.</p>"},{"location":"book/9%20-%20Validation/#n-steps-ahead-prediction","title":"n-steps ahead prediction","text":"<p>The n-steps ahead prediction is almost the same as the 1-step ahead, but here you can define the number of steps ahead you want to test your model. If you set <code>steps_ahead=5</code>, for example, it means that the first 5 values will be predicted using <code>yhat</code> in the loop, but then the process is restarted by feeding the real values in <code>y_test</code> in the next iteration, then performing other 5 predictions using the <code>yhat</code> and so on. Lets check the example considering <code>steps_ahead=2</code>:</p> <pre><code>y_initial = yhat(0) = 8\nyhat(1) = 1*8 + 2*1 = 10\nyhat(2) = 1*10 + 2*2 = 14\nyhat(3) = 1*10 + 2*3 = 16\nyhat(4) = 1*16 + 2*4 = 24\nand so on\n</code></pre>"},{"location":"book/9%20-%20Validation/#model-performance","title":"Model Performance","text":"<p>Model validation is one of the most crucial part in system identification. As we mentioned before, in system identification we are trying the model the dynamic of the process without for task like control design. In such cases, we can not only rely on regression metrics, but also ensuring that residuals are unpredictable across various combinations of past inputs and outputs (Billings, S. A. and Voon, W. S. F.). One often used statistical tests is the normalized RMSE, called RRSE, which can be expressed by</p> \\[ \\begin{equation}         \\textrm{RRSE}= \\frac{\\sqrt{\\sum\\limits_{k=1}^{n}(y_k-\\hat{y}_k)^2}}{\\sqrt{\\sum\\limits_{k=1}^{n}(y_k-\\bar{y})^2}}, \\end{equation} \\tag{1} \\] <p>where \\(\\hat{y}_k \\in \\mathbb{R}\\) the model predicted output and \\(\\bar{y} \\in \\mathbb{R}\\) the mean of the measured output \\(y_k\\). The RRSE gives some indication regarding the quality of the model, but concluding about the best model by evaluating only this quantity may leads to an incorrect interpretation, as shown in following example.</p> <p>Consider the models  $$ y_{{a}k} = 0.7077y{{a}k-1} + 0.1642u $$} + 0.1280u_{k-2</p> <p>and \\(y_{{_b}k}=0.7103y_{{_b}k-1} + 0.1458u_{k-1} + 0.1631u_{k-2} -1467y^3_{{_b}k-1} + 0.0710y^3_{{_b}k-2} +0.0554y^2_{{_b}k-3}u_{k-3}\\) defined in the Meta Model Structure Selection: An Algorithm For Building Polynomial NARX Models For Regression And Classification. The former results in a \\(RRSE = 0.1202\\) while the latter gives \\(RRSE~=0.0857\\). Although the model \\(y_{{_b}k}\\) fits the data better, it is only a biased representation to one piece of data and not a good description of the entire system.</p> <p>The RRSE (or any other metric) shows that validations test might be performed carefully. Another traditional practice is split the data set in two parts. In this respect, one can test the models obtained from the estimation part of the data using an specific data for validation. However, the one-step-ahead performance of NARX models generally results in misleading interpretations because even strongly biased models may fit the data well. Therefore, a free run simulation approach usually allows a better interpretation if the model is adequate or not (Billings, S. A.).</p> <p>Statistical tests for SISO models based on the correlation functions were proposed in Billings, S. A. and Voon, W. S. F., Model validity tests for non-linear signal processing applications. The tests are:</p> \\[ \\begin{align}     \\phi_{_{\\xi \\xi}\\tau} &amp;= E\\{\\xi_k \\xi_{k-\\tau}\\} = \\delta_{\\tau}, \\\\     \\phi_{_{\\xi x}\\tau} &amp;= E\\{\\xi_k x_{k-\\tau}\\} = 0 \\forall \\tau, \\\\     \\phi_{_{\\xi \\xi x}\\tau} &amp;= E\\{\\xi_k \\xi_{k-\\tau} x_{k-\\tau}\\} = 0 \\forall \\tau, \\\\     \\phi_{_{x^2 \\xi}\\tau} &amp;= E\\{(u^2_k - E\\{x^2_k\\})\\xi_{k-\\tau}\\} = 0 \\forall \\tau, \\\\     \\phi_{_{x^2 \\xi^2}\\tau} &amp;= E\\{(u^2_k - E\\{x^2_k\\})\\xi^2_{k-\\tau}\\} = 0 \\forall \\tau, \\\\     \\phi_{_{(y\\xi) x^2}\\tau} &amp;= E\\{(y_k\\xi_k - E\\{y_k\\xi_k\\})(x^2_{k-\\tau} - E\\{x^2_k\\})\\} = 0 \\forall \\tau, \\end{align} \\tag{2} \\] <p>where \\(\\delta\\) is the Dirac delta function and the cross-correlation function \\(\\phi\\) is denoted by (Billings, S. A. and Voon, W. S. F.):</p> \\[ \\begin{equation} \\phi_{{_{ab}}\\tau} = \\frac{\\frac{1}{n}\\sum\\limits_{k=1}^{n-\\tau}(a_k - \\hat{a})(b_{k+\\tau}-\\hat{b})}{\\sqrt{\\frac{1}{n}\\sum\\limits_{k=1}^{n}(a_k-\\hat{a})^2} \\sqrt{\\frac{1}{n}\\sum\\limits_{k=1}^{n}(b_k-\\hat{b})^2}} = \\frac{\\sum\\limits_{k=1}^{n-\\tau}(a_k - \\hat{a})(b_{k+\\tau}-\\hat{b})}{\\sqrt{\\sum\\limits_{k=1}^{n}(a_k-\\hat{a})^2} \\sqrt{\\sum\\limits_{k=1}^{n}(b_k-\\hat{b})^2}}, \\end{equation} \\tag{3} \\] <p>where \\(a\\) and \\(b\\) are two signal sequences. If the tests are true, then the model residues can be considered as white noise.</p>"},{"location":"book/9%20-%20Validation/#metrics-available-in-sysidentpy","title":"Metrics Available in SysIdentPy","text":"<p>SysIdentPy provides the following regression metrics out of the box:</p> <ul> <li>forecast_error</li> <li>mean_forecast_error</li> <li>mean_squared_error</li> <li>root_mean_squared_error</li> <li>normalized_root_mean_squared_error</li> <li>root_relative_squared_error</li> <li>mean_absolute_error</li> <li>mean_squared_log_error</li> <li>median_absolute_error</li> <li>explained_variance_score</li> <li>r2_score</li> <li>symmetric_mean_absolute_percentage_error</li> </ul> <p>To use them, the user only need to import the desired metric using, for example</p> <pre><code>from sysidentpy.metrics import root_relative_squared_error\n</code></pre> <p>SysIdentPy also provides methods for calculate and analyse the residues correlation</p> <pre><code>from sysidentpy.utils.plotting import plot_residues_correlation\nfrom sysidentpy.residues.residues_correlation import (\n\u00a0 \u00a0 compute_residues_autocorrelation,\n\u00a0 \u00a0 compute_cross_correlation,\n)\n</code></pre> <p>Lets check the metrics of the eletro mechanical system modeled in Chapter 4.</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n\u00a0 \u00a0 compute_residues_autocorrelation,\n\u00a0 \u00a0 compute_cross_correlation,\n)\nfrom sysidentpy.metrics import root_relative_squared_error\n\ndf1 = pd.read_csv(\"examples/datasets/x_cc.csv\")\ndf2 = pd.read_csv(\"examples/datasets/y_cc.csv\")\n\nx_train, x_valid = np.split(df1.iloc[::500].values, 2)\ny_train, y_valid = np.split(df2.iloc[::500].values, 2)\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=15,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"bic\",\n\u00a0 \u00a0 estimator=LeastSquares(unbiased=False),\n\u00a0 \u00a0 basis_function=basis_function\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n# plot only the first 100 samples (n=100)\nplot_results(y=y_valid, yhat=yhat, n=100)\n\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <p></p> <p></p> <p></p> <p>The RRSE is 0.0800, which is a very good metric. However, we can see that the residues have somo high auto-correlations anda with the input. This mean that our model maybe is not good enough as it could be.</p> <p>Lets check what happens if we increase <code>xlag</code>, <code>ylag</code> and change the parameter estimation algorithm from Least Squares to the Recursive Least Squares</p> <pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=50,\n\u00a0 \u00a0 ylag=5,\n\u00a0 \u00a0 xlag=5,\n\u00a0 \u00a0 info_criteria=\"bic\",\n\u00a0 \u00a0 estimator=RecursiveLeastSquares(unbiased=False),\n\u00a0 \u00a0 basis_function=basis_function\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n# plot only the first 100 samples (n=100)\nplot_results(y=y_valid, yhat=yhat, n=100)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <p>Now the RRSE is 0.0568 and we have a better residual correlation!</p> <p></p> <p></p> <p></p> <p>In the end of day, the best model will be the model that satisfy the user needs. However, its important to understand how to analyse the models so you can have an idea if you can get some improvements without too much work.</p> <p>For the sake of curiosity, lets check how the model perform if we run a 1-step ahead prediction. We don't need to fit the model again, just make another prediction using the 1-step option.</p> <pre><code>yhat = model.predict(X=x_valid, y=y_valid, steps_ahead=1)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n# plot only the first 100 samples (n=100)\nplot_results(y=y_valid, yhat=yhat, n=100)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <p>The same model, but evaluating the 1-step ahead prediction, now return a RRSE\\(= 0.02044\\) and the residues are even better. But remember, that is expected, as explained in the previous section.</p> <p></p> <p></p> <p></p>"},{"location":"changelog/changelog/","title":"Changes in SysIdentPy","text":""},{"location":"changelog/changelog/#v040","title":"v0.4.0","text":""},{"location":"changelog/changelog/#contributors","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes","title":"CHANGES","text":"<p>This update introduces several major features and changes, including some breaking changes. There is a guide to help you update your code to the new version. Depending on your model definition, you might not need to change anything. I decided to go directly to version v0.4.0 instead of providing incremental updates (0.3.5, 0.3.6, etc.) because the breaking changes are easy to fix and the new features are highly beneficial. This release provides crucial groundwork for the future development of SysIdentPy, making easier to add new features and improve the code, setting the stage for a robust and feature-complete 1.0.0 release in the feature.</p> <ul> <li>New Features:</li> <li>MAJOR: NonNegative Least Squares algorithm for parameter estimation.</li> <li>MAJOR: Bounded Variables Least Squares algorithm for parameter estimation.</li> <li>MAJOR: Least Squares Minimal Residual algorithm for parameter estimation.</li> <li>MAJOR: Error Reduction Ratio algorithm enhancement for FROLS model structure selection. Users can now set an <code>err_tol</code> value to stop the algorithm when the sum of the ERR values reaches this threshold, offering a faster alternative to Information Criteria algorithms. A new example is available in the documentation.</li> <li>MAJOR: New Bernstein basis function available, allowing users to choose between Polynomial, Fourier, and Bernstein.</li> <li> <p>MAJOR: v0.1 of the companion book \"Nonlinear System Identification: Theory and Practice With SysIdentPy.\" This open-source book serves as robust documentation for the SysIdentPy package and a friendly introduction to Nonlinear System Identification and Timeseries Forecasting. There are case studies in the book that were not included in the documentation at the time of the update release. The book will always feature more in-depth studies and will be updated regularly with additional case studies.</p> </li> <li> <p>Documentation:</p> </li> <li>All examples updated to reflect changes in v0.4.0.</li> <li>Added guide on defining a custom parameter estimation method and integrating it with SysIdentPy.</li> <li>Documentation moved to the <code>gh-pages</code> branch.</li> <li>Defined a GitHub Action to automatically build the docs when changes are pushed to the main branch.</li> <li> <p>Removal of unused code in general</p> </li> <li> <p>Datasets:</p> </li> <li> <p>Datasets are now available in a separate repository.</p> </li> <li> <p>API Changes:</p> </li> <li>BREAKING CHANGE: Parameter estimation method must now be imported and passed to the model definition, replacing the previous string method. For example, use <code>from sysidentpy.parameter_estimation import LeastSquares</code> instead of <code>\"least_squares\"</code>. This change enhances code flexibility, organization, readability, and facilitates easier integration of custom methods. A specific doc page is available to guide migration from v0.3.4 to v0.4.0.</li> <li>BREAKING CHANGE: The <code>fit</code> method in MetaMSS now requires only <code>X</code> and <code>y</code> values, omitting the need to pass <code>fit(X=, y=, X_test=, y_test=)</code>.</li> <li>Added support for Python 3.12.</li> <li>Introduced <code>test_size</code> hyperparameter to set the proportion of training data used in the fitting process.</li> <li>Extensive code refactoring, including type hint improvements, docstring enhancements, removal of unused code, and other behind-the-scenes changes to support new features.</li> </ul>"},{"location":"changelog/changelog/#v034","title":"v0.3.4","text":""},{"location":"changelog/changelog/#contributors_1","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> <li>dj-gauthier</li> <li>mtsousa</li> </ul>"},{"location":"changelog/changelog/#changes_1","title":"CHANGES","text":"<ul> <li>New Features:</li> <li> <p>MAJOR: Ridge Regression Parameter Estimation:</p> <ul> <li>Introducing Ridge algorithm for model parameter estimation (Issue #104). Set <code>estimator=\"ridge_regression\"</code> and control regularization with the <code>alpha</code> parameter. Special thanks to @dj-gauthier and @mtsousa for their contribution. Users are encouraged to visit https://www.researchgate.net/publication/380429918_Controlling_chaos_using_edge_computing_hardware to explore how @dj-gauthier used SysIdentPy in his research.</li> </ul> </li> <li> <p>API Changes:</p> </li> <li>Improved <code>plotting.py</code> code with type hints and new options for plotting results.</li> <li>Refactored methods to resolve future warnings from numpy.</li> <li>Code refactoring following PEP8 guidelines.</li> <li> <p>Set \"default\" as the default style for plotting to avoid errors in new versions of matplotlib.</p> </li> <li> <p>Datasets:</p> </li> <li> <p>Added <code>buck_id.csv</code> and <code>buck_valid.csv</code> datasets to the SysIdentPy repository.</p> </li> <li> <p>Documentation:</p> </li> <li>Add NFIR example (Issue #103). The notebook show how to build models without past output regressors (using only input regressors).</li> <li>Enhanced usage example for MetaMSS.</li> <li>Continued adding type hints to methods.</li> <li>Improved docstrings throughout the codebase.</li> <li>Minor additions and grammar fixes in documentation.</li> <li> <p>@dj-gauthier provided valuable suggestions for enhancing the documentation, which are currently undergoing refinement and will soon be accessible.</p> </li> <li> <p>Development Tools:</p> </li> <li>Added pre-commit hooks to the repository.</li> <li>Enhanced <code>pyproject.toml</code> to assist contributors in setting up their own environment.</li> </ul>"},{"location":"changelog/changelog/#v033","title":"v0.3.3","text":""},{"location":"changelog/changelog/#contributors_2","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> <li>GabrielBuenoLeandro</li> <li>samirmartins</li> </ul>"},{"location":"changelog/changelog/#changes_2","title":"CHANGES","text":"<ul> <li> <p>The update v0.3.3  has been released with additional features, API changes and fixes.</p> </li> <li> <p>MAJOR: Multiobjective Framework: Affine Information Least Squares Algorithm (AILS)</p> <ul> <li>Now you can use AILS to estimate parameters of NARMAX models (and variants) using a multiobjective approach.</li> <li>AILS can be accessed using <code>from sysidentpy.multiobjective_parameter_estimation import AILS</code></li> <li>See the docs for a more in depth explanation of how to use AILS.</li> <li>This feature is related to Issue #101. This work is the result of an undergraduate research conducted by Gabriel Bueno Leandro under the supervision of Samir Milani Martins and Wilson Rocha Lacerda Junior.</li> <li>Several new methods were implemented to get the new feature and you can check all of it in sysidentpy -&gt; multiobjective_parameter_estimation.</li> </ul> </li> <li> <p>API Change: <code>regressor_code</code> variable was renamed as <code>enconding</code> to avoid using the same name as the method in <code>narmax_tool</code> <code>regressor_code</code> method.</p> </li> <li> <p>DATASET: Added buck_id.csv and buck_valid.csv dataset to SysIdentPy repository.</p> </li> <li> <p>DOC: Add a Multiobjetive Parameter Optimization Notebook showing how to use the new AILS method</p> </li> <li> <p>DOC: Minor additions and grammar fixes.</p> </li> </ul>"},{"location":"changelog/changelog/#v032","title":"v0.3.2","text":""},{"location":"changelog/changelog/#contributors_3","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_3","title":"CHANGES","text":"<ul> <li> <p>The update v0.3.2  has been released with API changes and fixes.</p> </li> <li> <p>Major:</p> <ul> <li>Added Akaike Information Criteria corrected in FROLS. Now the user can use aicc as the information criteria to select the model order when using FROLS algorithm.</li> </ul> </li> <li> <p>FIX: Issue #114. Replace yhat with y in root relative squared error. Thanks @miroder</p> </li> <li> <p>TESTS: Minor changes in tests by removing unnecessary data load.</p> </li> <li> <p>Remove unused code and comments.</p> </li> <li> <p>Docs: Minor changes in notebooks. Added AICc method in the information criteria example.</p> </li> </ul>"},{"location":"changelog/changelog/#v031","title":"v0.3.1","text":""},{"location":"changelog/changelog/#contributors_4","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_4","title":"CHANGES","text":"<ul> <li> <p>The update v0.3.1  has been released with API changes and fixes.</p> </li> <li> <p>API Change:</p> <ul> <li>MetaMSS was returning the max lag of the final model instead of the maximum lag related to the xlag and ylag. This is not wrong (its related to the issue #55), but this change will be made for all methods at the same time. In this respect, I'm reverted this to return the maximum lag of the xlag and ylag.</li> </ul> </li> <li> <p>API Change: Added build_matrix method in BaseMSS. This change improved overall code readability by rewriting if/elif/else clauses in every model structure selection algorithm.</p> </li> <li> <p>API Change: Added bic, aic, fpe, and lilc methods in FROLS. Now the method is selected by using a predefined dictionary with the available options. This change improved overall code readability by rewriting if/elif/else clauses in the FROLS algorithm.</p> </li> <li> <p>TESTS: Added tests for Neural NARX class. The issue with pytorch was fixed and now we have the tests for every model class.</p> </li> <li> <p>Remove unused code and comments.</p> </li> </ul>"},{"location":"changelog/changelog/#v030","title":"v0.3.0","text":""},{"location":"changelog/changelog/#contributors_5","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> <li>gamcorn</li> <li>Gabo-Tor</li> </ul>"},{"location":"changelog/changelog/#changes_5","title":"CHANGES","text":"<ul> <li> <p>The update v0.3.0  has been released with additional features, API changes and fixes.</p> </li> <li> <p>MAJOR: Estimators support in AOLS</p> <ul> <li>Now you can use any SysIdentPy estimator in AOLS model structure selection.</li> </ul> </li> <li> <p>API Change:</p> <ul> <li>Refactored base class for model structure selection. A refactored base class for model structure selection has been introduced in SysIdentPy. This update aims to enhance the system identification process by preparing the package for new features that are currently in development, like multiobjective parameter estimation, new basis functions and more.</li> </ul> <p>Several methods within the base class have undergone significant restructuring to improve their functionality and optimize their performance. This reorganization will facilitate the incorporation of advanced model selection techniques in the future, which will enable users to obtain dynamic models with robust dynamic and static performance. - Avoid unnecessary inheritance in every MSS method and improve the readability with better structured classes. - Rewritten methods to avoid code duplication. - Improve overall code readability by rewriting if/elif/else clauses.</p> </li> <li> <p>Breaking Change: <code>X_train</code> and <code>y_train</code> were replaced respectively by <code>X</code> and <code>y</code> in <code>fit</code> method in MetaMSS model structure selection algorithm.  <code>X_test</code> and <code>y_test</code> were replaced by <code>X</code> and <code>y</code> in <code>predict</code> method in MetaMSS.</p> </li> <li> <p>API Change: Added BaseBasisFunction class, an abstract base class for implementing basis functions.</p> </li> <li> <p>Enhancement: Added support for python 3.11.</p> </li> <li> <p>Future Deprecation Warning: The user will have to define the estimator and pass it to every model structure selection algorithm instead of using a string to define the Estimator. Currently the estimator is defined like \"estimator='least_squares'\". In version 0.4.0 the definition will be like \"estimator=LeastSquares()\"</p> </li> <li> <p>FIX: Issue #96. Fix issue with numpy 1.24.* version. Thanks for the contribution @gamcorn.</p> </li> <li> <p>FIX: Issue #91. Fix r2_score metric issue with 2 dimensional arrays.</p> </li> <li> <p>FIX: Issue #90.</p> </li> <li> <p>FIX: Issue #88 .Fix one step ahead prediction error in SimulateNARMAX class (thanks for pointing out, Lalith).</p> </li> <li> <p>FIX: Fix error in selecting the correct regressors in AOLS.</p> </li> <li> <p>Fix: Fix n step ahead prediction method not returning all values of the defined steps-ahead value when passing only the initial condition.</p> </li> <li> <p>FIX: Fix Visible Deprecation Warning raised in get_max_lag method.</p> </li> <li> <p>FIX: Fix deprecation warning in Extended Least Squares Example</p> </li> <li> <p>DATASET: Added air passengers dataset to SysIdentPy repository.</p> </li> <li> <p>DATASET: Added San Francisco Hospital Load dataset to SysIdentPy repository.</p> </li> <li> <p>DATASET: Added San Francisco PV GHI dataset to SysIdentPy repository.</p> </li> <li> <p>DOC: Improved documentation in Setting Specif Lags page. Now we bring an example of how to set specific lags for MISO models.</p> </li> <li> <p>DOC: Minor additions and grammar fixes.</p> </li> <li> <p>DOC: Improve image visualization using mkdocs-glightbox.</p> </li> <li> <p>Update dev packages versions</p> </li> </ul>"},{"location":"changelog/changelog/#v021","title":"v0.2.1","text":""},{"location":"changelog/changelog/#contributors_6","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_6","title":"CHANGES","text":"<ul> <li> <p>The update v0.2.1  has been released with additional feature, minor API changes and fixes.</p> </li> <li> <p>MAJOR: Neural NARX now support CUDA</p> <ul> <li>Now the user can build Neural NARX models with CUDA support. Just add <code>device='cuda'</code> to use the GPU benefits.</li> <li>Updated docs to show how to use the new feature.</li> </ul> </li> <li> <p>MAJOR: New documentation website</p> <ul> <li>The documentation is now entirely based on Markdown (no rst anymore).</li> <li>We use MkDocs and Material for MkDocs theme now.</li> <li>Dark theme option.</li> <li>The Contribute page have more details to help those who wants to contribute with SysIdentPy.</li> <li>New sections (e.g., Blog, Sponsors, etc.)</li> <li>Many improvements under the hood.</li> </ul> </li> <li> <p>MAJOR: Github Sponsor</p> <ul> <li>Now you can support SysIdentPy by becoming a Sponsor! Details: https://github.com/sponsors/wilsonrljr</li> </ul> </li> <li> <p>Tests:</p> <ul> <li>Now there are test for almost every function.</li> <li>Neural NARX tests are raising numpy issues. It'll be fixed til next update.</li> </ul> </li> <li> <p>FIX: NFIR models in General Estimators</p> <ul> <li>Fix support for NFIR models using sklearn estimators.</li> </ul> </li> <li> <p>The setup is now handled by the pyproject.toml file.  </p> </li> <li> <p>Remove unused code.</p> </li> <li> <p>Fix docstring variables.</p> </li> <li> <p>Fix code format issues.</p> </li> <li> <p>Fix minor grammatical and spelling mistakes.</p> </li> <li> <p>Fix issues related to html on Jupyter notebooks examples on documentation.</p> </li> <li> <p>Updated Readme.</p> </li> </ul>"},{"location":"changelog/changelog/#v020","title":"v0.2.0","text":""},{"location":"changelog/changelog/#contributors_7","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_7","title":"CHANGES","text":"<ul> <li> <p>The update v0.2.0  has been released with additional feature, minor API changes and fixes.</p> </li> <li> <p>MAJOR: Many new features for General Estimators</p> <ul> <li>Now the user can build General NARX models with Fourier basis function.</li> <li>The user can choose which basis they want by importing it from sysidentpy.basis_function. Check the notebooks with examples of how to use it.</li> <li>Now it is possible to build General NAR models. The user just need to pass model_type=\"NAR\" to build NAR models.</li> <li>Now it is possible to build General NFIR models. The user just need to pass model_type=\"NFIR\" to build NAR models.</li> <li>Now it is possible to run n-steps ahead prediction using General Estimators. Until now only infinity-steps ahead were allowed. Now the users can set any steps they want.</li> <li>Polynomial and Fourier are supported for now. New basis functions will be added in next releases.</li> <li>No need to pass the number of inputs anymore.</li> <li>Improved docstring.</li> <li>Fixed minor grammatical and spelling mistakes.</li> <li>many under the hood changes.</li> </ul> </li> <li> <p>MAJOR: Many new features for NARX Neural Network</p> <ul> <li>Now the user can build Neural NARX models with Fourier basis function.</li> <li>The user can choose which basis they want by importing it from sysidentpy.basis_function. Check the notebooks with examples of how to use it.</li> <li>Now it is possible to build Neural NAR models. The user just need to pass model_type=\"NAR\" to build NAR models.</li> <li>Now it is possible to build Neural NFIR models. The user just need to pass model_type=\"NFIR\" to build NAR models.</li> <li>Now it is possible to run n-steps ahead prediction using Neural NARX. Until now only infinity-steps ahead were allowed. Now the users can set any steps they want.</li> <li>Polynomial and Fourier are supported for now. New basis functions will be added in next releases.</li> <li>No need to pass the number of inputs anymore.</li> <li>Improved docstring.</li> <li>Fixed minor grammatical and spelling mistakes.</li> <li>many under the hood changes.</li> </ul> </li> <li> <p>Major: Support for old methods removed.</p> <ul> <li>Now the old sysidentpy.PolynomialNarmax is not available anymore. All the old features are included in the new API with a lot of new features and performance improvements.</li> </ul> </li> <li> <p>API Change (new): sysidentpy.general_estimators.ModelPrediction</p> <ul> <li>ModelPrediction class was adapted to support General Estimators as a stand-alone class.</li> <li>predict: base method for prediction. Support infinity_steps ahead, one-step ahead and n-steps ahead prediction and any basis function.</li> <li>_one_step_ahead_prediction: Perform the 1-step-ahead prediction for any basis function.</li> <li>_n_step_ahead_prediction: Perform the n-step-ahead prediction for polynomial basis.</li> <li>_model_prediction: Perform the infinity-step-ahead prediction for polynomial basis.</li> <li>_narmax_predict: wrapper for NARMAX and NAR models.</li> <li>_nfir_predict: wrapper for NFIR models.</li> <li>_basis_function_predict: Perform the infinity-step-ahead prediction for basis functions other than polynomial.</li> <li>basis_function_n_step_prediction: Perform the n-step-ahead prediction for basis functions other than polynomial.</li> </ul> </li> <li> <p>API Change (new): sysidentpy.neural_network.ModelPrediction</p> <ul> <li>ModelPrediction class was adapted to support Neural NARX as a stand-alone class.</li> <li>predict: base method for prediction. Support infinity_steps ahead, one-step ahead and n-steps ahead prediction and any basis function.</li> <li>_one_step_ahead_prediction: Perform the 1-step-ahead prediction for any basis function.</li> <li>_n_step_ahead_prediction: Perform the n-step-ahead prediction for polynomial basis.</li> <li>_model_prediction: Perform the infinity-step-ahead prediction for polynomial basis.</li> <li>_narmax_predict: wrapper for NARMAX and NAR models.</li> <li>_nfir_predict: wrapper for NFIR models.</li> <li>_basis_function_predict: Perform the infinity-step-ahead prediction for basis functions other than polynomial.</li> <li>basis_function_n_step_prediction: Perform the n-step-ahead prediction for basis functions other than polynomial.</li> </ul> </li> <li> <p>API Change: Fit method for Neural NARX revamped.</p> <ul> <li>No need to convert the data to tensor before calling Fit method anymore.</li> </ul> </li> </ul> <p>API Change: Keyword and positional arguments     - Now users have to provide parameters with their names, as keyword arguments, instead of positional arguments. This is valid for every model class now.</p> <ul> <li> <p>API Change (new): sysidentpy.utils.narmax_tools</p> <ul> <li>New functions to help user getting useful information to build model. Now we have the regressor_code helper function to help to build neural NARX models.</li> </ul> </li> <li> <p>DOC: Improved Basic Steps notebook with new details about the prediction function.</p> </li> <li>DOC: NARX Neural Network notebook was updated following the new api and showing new features.</li> <li>DOC: General Estimators notebook was updated following the new api and showing new features.</li> <li>DOC: Fixed minor grammatical and spelling mistakes, including Issues #77 and #78.</li> <li>DOC: Fix issues related to html on Jupyter notebooks examples on documentation.</li> </ul>"},{"location":"changelog/changelog/#v019","title":"v0.1.9","text":""},{"location":"changelog/changelog/#contributors_8","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> <li>samirmartins</li> </ul>"},{"location":"changelog/changelog/#changes_8","title":"CHANGES","text":"<ul> <li> <p>The update v0.1.9  has been released with additional feature, minor API changes and fixes of the new features added in v0.1.7.</p> </li> <li> <p>MAJOR: Entropic Regression Algorithm</p> <ul> <li>Added the new class ER to build NARX models using the Entropic Regression algorithm.</li> <li>Only the Mutual Information KNN is implemented in this version and it may take too long to run on a high number of regressor, so the user should be careful regarding the number of candidates to put in the model.</li> </ul> </li> <li> <p>API: save_load</p> <ul> <li>Added a function to save and load models from file.</li> </ul> </li> <li> <p>API: Added tests for python 3.9</p> </li> <li> <p>Fix : Change condition for n_info_values in FROLS. Now the value defined by the user is compared against X matrix shape instead of regressor space shape. This fix the Fourier basis function usage with more the 15 regressors in FROLS.</p> </li> <li> <p>DOC: Save and Load models</p> <ul> <li>Added a notebook showing how to use the save_load method.</li> </ul> </li> <li> <p>DOC: Entropic Regression example</p> <ul> <li>Added notebook with a simple example of how to use AOLS</li> </ul> </li> <li> <p>DOC: Fourier Basis Function Example</p> <ul> <li>Added notebook with a simple example of how to use Fourier Basis Function</li> </ul> </li> <li> <p>DOC: PV forecasting benchmark</p> <ul> <li>FIX AOLS prediction. The example was using the meta_mss model in prediction, so the results for AOLS were wrong.</li> </ul> </li> <li> <p>DOC: Fixed minor grammatical and spelling mistakes.</p> </li> <li> <p>DOC: Fix issues related to html on Jupyter notebooks examples on documentation.</p> </li> </ul>"},{"location":"changelog/changelog/#v018","title":"v0.1.8","text":""},{"location":"changelog/changelog/#contributors_9","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_9","title":"CHANGES","text":"<ul> <li> <p>The update v0.1.8  has been released with additional feature, minor API changes and fixes of the new features added in v0.1.7.</p> </li> <li> <p>MAJOR: Ensemble Basis Functions</p> <ul> <li>Now you can use different basis function together. For now we allow to use Fourier combined with Polynomial of different degrees.</li> </ul> </li> <li> <p>API change: Add \"ensemble\" parameter in basis function to combine the features of different basis function.</p> </li> <li> <p>Fix: N-steps ahead prediction for model_type=\"NAR\" is working properly now with different forecast horizon.</p> </li> <li> <p>DOC: Air passenger benchmark</p> <ul> <li>Remove unused code.</li> <li>Use default hyperparameter in SysIdentPy models.</li> </ul> </li> <li> <p>DOC: Load forecasting benchmark</p> <ul> <li>Remove unused code.</li> <li>Use default hyperparameter in SysIdentPy models.</li> </ul> </li> <li> <p>DOC: PV forecasting benchmark</p> <ul> <li>Remove unused code.</li> <li>Use default hyperparameter in SysIdentPy models.</li> </ul> </li> </ul>"},{"location":"changelog/changelog/#v017","title":"v0.1.7","text":""},{"location":"changelog/changelog/#contributors_10","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_10","title":"CHANGES","text":"<ul> <li> <p>The update v0.1.7  has been released with major changes and additional features. There are several API modifications and you will need to change your code to have the new (and upcoming) features. All modifications are meant to make future expansion easier.</p> </li> <li> <p>On the user's side, the changes are not that disruptive, but in the background there are many changes that allowed the inclusion of new features and bug fixes that would be complex to solve without the changes. Check the <code>documentation page &lt;http://sysidentpy.org/notebooks.html&gt;</code>__</p> </li> <li> <p>Many classes were basically rebuild it from scratch, so I suggest to look at the new examples of how to use the new version.</p> </li> <li> <p>I will present the main updates below in order to highlight features and usability and then all API changes will be reported.</p> </li> <li> <p>MAJOR: NARX models with Fourier basis function <code>Issue63 &lt;https://github.com/wilsonrljr/sysidentpy/issues/63&gt;</code>, <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/64&gt;</code></p> <ul> <li>The user can choose which basis they want by importing it from sysidentpy.basis_function. Check the notebooks with examples of how to use it.</li> <li>Polynomial and Fourier are supported for now. New basis functions will be added in next releases.</li> </ul> </li> <li> <p>MAJOR: NAR models <code>Issue58 &lt;https://github.com/wilsonrljr/sysidentpy/issues/58&gt;</code>__</p> <ul> <li>It was already possible to build Polynomial NAR models, but with some hacks. Now the user just need to pass model_type=\"NAR\" to build NAR models.</li> <li>The user doesn't need to pass a vector of zeros as input anymore.</li> <li>Works for any model structure selection algorithm (FROLS, AOLS, MetaMSS)</li> </ul> </li> <li> <p>Major: NFIR models <code>Issue59 &lt;https://github.com/wilsonrljr/sysidentpy/issues/59&gt;</code>__</p> <ul> <li>NFIR models are models where the output depends only on past inputs. It was already possible to build Polynomial NFIR models, but with a lot of code on the user's side (much more than NAR, btw). Now the user just need to pass model_type=\"NFIR\" to build NFIR models.</li> <li>Works for any model structure selection algorithm (FROLS, AOLS, MetaMSS)</li> </ul> </li> <li> <p>Major: Select the order for the residues lags to use in Extended Least Squares - elag</p> <ul> <li>The user can select the maximum lag of the residues to be used in the Extended Least Squares algorithm. In previous versions sysidentpy used a predefined subset of residual lags.</li> <li>The degree of the lags follows the degree of the basis function</li> </ul> </li> <li> <p>Major: Residual analysis methods <code>Issue60 &lt;https://github.com/wilsonrljr/sysidentpy/issues/60&gt;</code>__</p> <ul> <li>There are now specific functions to calculate the autocorrelation of the residuals and cross-correlation for the analysis of the residuals. In previous versions the calculation was limited to just two inputs, for example, limiting user usability.</li> </ul> </li> <li> <p>Major: Plotting methods <code>Issue61 &lt;https://github.com/wilsonrljr/sysidentpy/issues/61&gt;</code>__</p> <ul> <li>The plotting functions are now separated from the models objects, so there are more flexibility regarding what to plot.</li> <li>Residual plots were separated from the forecast plot</li> </ul> </li> <li> <p>API Change: sysidentpy.polynomial_basis.PolynomialNarmax is deprecated. Use sysidentpy.model_structure_selection.FROLS instead. <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/62&gt;</code>__</p> <ul> <li>Now the user doesn't need to pass the number of inputs as a parameter.</li> <li>Added the elag parameter for unbiased_estimator. Now the user can define the number of lags of the residues for parameter estimation using the Extended Least Squares algorithm.</li> <li>model_type parameter: now the user can select the model type to be built. The options are \"NARMAX\", \"NAR\" and \"NFIR\". \"NARMAX\" is the default. If you want to build a NAR model without any \"hack\", just set model_type=\"NAR\". The same for \"NFIR\" models.</li> </ul> </li> <li> <p>API Change: sysidentpy.polynomial_basis.MetaMSS is deprecated. Use sysidentpy.model_structure_selection.MetaMSS instead. <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/64&gt;</code>__</p> <ul> <li>Now the user doesn't need to pass the number of inputs as a parameter.</li> <li>Added the elag parameter for unbiased_estimator. Now the user can define the number of lags of the residues for parameter estimation using the Extended Least Squares algorithm.</li> </ul> </li> <li> <p>API Change: sysidentpy.polynomial_basis.AOLS is deprecated. Use sysidentpy.model_structure_selection.AOLS instead. <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/64&gt;</code>__</p> </li> <li> <p>API Change: sysidentpy.polynomial_basis.SimulatePolynomialNarmax is deprecated. Use sysidentpy.simulation.SimulateNARMAX instead.</p> </li> <li> <p>API Change: Introducing sysidentpy.basis_function. Because NARMAX models can be built on different basis function, a new module is added to make easier to implement new basis functions in future updates <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/64&gt;</code>__.</p> <ul> <li>Each basis function class must have a fit and predict method to be used in training and prediction respectively.</li> </ul> </li> <li> <p>API Change: unbiased_estimator method moved to Estimators class.</p> <ul> <li>added elag option</li> <li>change the build_information_matrix method to build_output_matrix</li> </ul> </li> <li> <p>API Change (new): sysidentpy.narmax_base</p> <ul> <li>This is the new base for building NARMAX models. The classes have been rewritten to make it easier to expand functionality.</li> </ul> </li> <li> <p>API Change (new): sysidentpy.narmax_base.GenerateRegressors</p> <ul> <li>create_narmax_code: Creates the base coding that allows representation for the NARMAX, NAR, and NFIR models.</li> <li>regressor_space: Creates the encoding representation for the NARMAX, NAR, and NFIR models.</li> </ul> </li> <li> <p>API Change (new): sysidentpy.narmax_base.ModelInformation</p> <ul> <li>_get_index_from_regressor_code: Get the index of the model code representation in regressor space.</li> <li>_list_output_regressor_code: Create a flattened array of output regressors.</li> <li>_list_input_regressor_code: Create a flattened array of input regressors.</li> <li>_get_lag_from_regressor_code: Get the maximum lag from array of regressors.</li> <li>_get_max_lag_from_model_code: the name says it all.</li> <li>_get_max_lag: Get the maximum lag from ylag and xlag.</li> </ul> </li> <li> <p>API Change (new): sysidentpy.narmax_base.InformationMatrix</p> <ul> <li>_create_lagged_X: Create a lagged matrix of inputs without combinations.</li> <li>_create_lagged_y: Create a lagged matrix of the output without combinations.</li> <li>build_output_matrix: Build the information matrix of output values.</li> <li>build_input_matrix: Build the information matrix of input values.</li> <li>build_input_output_matrix: Build the information matrix of input and output values.</li> </ul> </li> <li> <p>API Change (new): sysidentpy.narmax_base.ModelPrediction</p> <ul> <li>predict: base method for prediction. Support infinity_steps ahead, one-step ahead and n-steps ahead prediction and any basis function.</li> <li>_one_step_ahead_prediction: Perform the 1-step-ahead prediction for any basis function.</li> <li>_n_step_ahead_prediction: Perform the n-step-ahead prediction for polynomial basis.</li> <li>_model_prediction: Perform the infinity-step-ahead prediction for polynomial basis.</li> <li>_narmax_predict: wrapper for NARMAX and NAR models.</li> <li>_nfir_predict: wrapper for NFIR models.</li> <li>_basis_function_predict: Perform the infinity-step-ahead prediction for basis functions other than polynomial.</li> <li>basis_function_n_step_prediction: Perform the n-step-ahead prediction for basis functions other than polynomial.</li> </ul> </li> <li> <p>API Change (new): sysidentpy.model_structure_selection.FROLS <code>Issue62 &lt;https://github.com/wilsonrljr/sysidentpy/issues/62&gt;</code>, <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/64&gt;</code></p> <ul> <li>Based on the old sysidentpy.polynomial_basis.PolynomialNARMAX. The class has been rebuilt with new functions and optimized code.</li> <li>Enforcing keyword-only arguments. This is an effort to promote clear and non-ambiguous use of the library.</li> <li>Add support for new basis functions.</li> <li>The user can choose the residual lags.</li> <li>No need to pass the number of inputs anymore.</li> <li>Improved docstring.</li> <li>Fixed minor grammatical and spelling mistakes.</li> <li>New prediction method.</li> <li>many under the hood changes.</li> </ul> </li> <li> <p>API Change (new): sysidentpy.model_structure_selection.MetaMSS <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/64&gt;</code>__</p> <ul> <li>Based on the old sysidentpy.polynomial_basis.MetaMSS. The class has been rebuilt with new functions and optimized code.</li> <li>Enforcing keyword-only arguments. This is an effort to promote clear and non-ambiguous use of the library.</li> <li>The user can choose the residual lags.</li> <li>Extended Least Squares support.</li> <li>Add support for new basis functions.</li> <li>No need to pass the number of inputs anymore.</li> <li>Improved docstring.</li> <li>Fixed minor grammatical and spelling mistakes.</li> <li>New prediction method.</li> <li>many under the hood changes.</li> </ul> </li> <li> <p>API Change (new): sysidentpy.model_structure_selection.AOLS <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/64&gt;</code>__</p> <ul> <li>Based on the old sysidentpy.polynomial_basis.AOLS. The class has been rebuilt with new functions and optimized code.</li> <li>Enforcing keyword-only arguments. This is an effort to promote clear and non-ambiguous use of the library.</li> <li>Add support for new basis functions.</li> <li>No need to pass the number of inputs anymore.</li> <li>Improved docstring.</li> <li>Change \"l\" parameter to \"L\".</li> <li>Fixed minor grammatical and spelling mistakes.</li> <li>New prediction method.</li> <li>many under the hood changes.</li> </ul> </li> <li> <p>API Change (new): sysidentpy.simulation.SimulateNARMAX</p> <ul> <li>Based on the old sysidentpy.polynomial_basis.SimulatePolynomialNarmax. The class has been rebuilt with new functions and optimized code.</li> <li>Fix the Extended Least Squares support.</li> <li>Fix n-steps ahead prediction and 1-step ahead prediction.</li> <li>Enforcing keyword-only arguments. This is an effort to promote clear and non-ambiguous use of the library.</li> <li>The user can choose the residual lags.</li> <li>Improved docstring.</li> <li>Fixed minor grammatical and spelling mistakes.</li> <li>New prediction method.</li> <li>Do not inherit from the structure selection algorithm anymore, only from narmax_base. Avoid circular import and other issues.</li> <li>many under the hood changes.</li> </ul> </li> <li> <p>API Change (new): sysidentpy.residues</p> <ul> <li>compute_residues_autocorrelation: the name says it all.</li> <li>calculate_residues: get the residues from y and yhat.</li> <li>get_unnormalized_e_acf: compute the unnormalized autocorrelation of the residues.</li> <li>compute_cross_correlation: compute cross correlation between two signals.</li> <li>_input_ccf</li> <li>_normalized_correlation: compute the normalized correlation between two signals.</li> </ul> </li> <li> <p>API Change (new): sysidentpy.utils.plotting</p> <ul> <li>plot_results: plot the forecast</li> <li>plot_residues_correlation: the name says it all.</li> </ul> </li> <li> <p>API Change (new): sysidentpy.utils.display_results</p> <ul> <li>results: return the model regressors, estimated parameter and ERR index of the fitted model in a table.</li> </ul> </li> <li> <p>DOC: Air passenger benchmark <code>Issue65 &lt;https://github.com/wilsonrljr/sysidentpy/issues/65&gt;</code>__</p> <ul> <li>Added notebook with Air passenger forecasting benchmark.</li> <li>We compare SysIdentPy against prophet, neuralprophet, autoarima, tbats and many more.</li> </ul> </li> <li> <p>DOC: Load forecasting benchmark <code>Issue65 &lt;https://github.com/wilsonrljr/sysidentpy/issues/65&gt;</code>__</p> <ul> <li>Added notebook with load forecasting benchmark.</li> </ul> </li> <li> <p>DOC: PV forecasting benchmark <code>Issue65 &lt;https://github.com/wilsonrljr/sysidentpy/issues/65&gt;</code>__</p> <ul> <li>Added notebook with PV forecasting benchmark.</li> </ul> </li> <li> <p>DOC: Presenting main functionality</p> <ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li> <p>DOC: Multiple Inputs usage</p> <ul> <li>Example rewritten following the new api</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li> <p>DOC: Information Criteria - Examples</p> <ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li> <p>DOC: Important notes and examples of how to use Extended Least Squares</p> <ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li> <p>DOC: Setting specific lags</p> <ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li> <p>DOC: Parameter Estimation</p> <ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li> <p>DOC: Using the Meta-Model Structure Selection (MetaMSS) algorithm for building Polynomial NARX models</p> <ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li> <p>DOC: Using the Accelerated Orthogonal Least-Squares algorithm for building Polynomial NARX models</p> <ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li> <p>DOC: Example: F-16 Ground Vibration Test benchmark</p> <ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li> <p>DOC: Building NARX Neural Network using Sysidentpy</p> <ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li> <p>DOC: Building NARX models using general estimators</p> <ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li> <p>DOC: Simulate a Predefined Model</p> <ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li> <p>DOC: System Identification Using Adaptive Filters</p> <ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li> <p>DOC: Identification of an electromechanical system</p> <ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li> <p>DOC: Example: N-steps-ahead prediction - F-16 Ground Vibration Test benchmark</p> <ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li> <p>DOC: Introduction to NARMAX models</p> <ul> <li>Fixed grammatical and spelling mistakes.</li> </ul> </li> </ul>"},{"location":"changelog/changelog/#v016","title":"v0.1.6","text":""},{"location":"changelog/changelog/#contributors_11","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_11","title":"CHANGES","text":"<ul> <li> <p>MAJOR: Meta-Model Structure Selection Algorithm (Meta-MSS).</p> <ul> <li>A new method for build NARMAX models based on metaheuristics. The algorithm uses a Binary hybrid Particle Swarm Optimization and Gravitational Search Algorithm with a new cost function to build parsimonious models.</li> <li>New class for the BPSOGSA algorithm. New algorithms can be adapted in the Meta-MSS framework.</li> <li>Future updates will add NARX models for classification and multiobjective model structure selection.</li> </ul> </li> <li> <p>MAJOR: Accelerated Orthogonal Least-Squares algorithm.</p> <ul> <li>Added the new class AOLS to build NARX models using the Accelerated Orthogonal Least-Squares algorithm.</li> <li>At the best of my knowledge, this is the first time this algorithm is used in the NARMAX framework. The tests I've made are promising, but use it with caution until the results are formalized into a research paper.</li> </ul> </li> <li> <p>Added notebook with a simple example of how to use MetaMSS and a simple model comparison of the Electromechanical system.</p> </li> <li> <p>Added notebook with a simple example of how to use AOLS</p> </li> <li> <p>Added ModelInformation class. This class have methods to return model information such as max_lag of a model code.</p> <ul> <li>added _list_output_regressor_code</li> <li>added _list_input_regressor_code</li> <li>added _get_lag_from_regressor_code</li> <li>added _get_max_lag_from_model_code</li> </ul> </li> <li> <p>Minor performance improvement: added the argument \"predefined_regressors\" in build_information_matrix function on base.py to improve the performance of the Simulation method.</p> </li> <li> <p>Pytorch is now an optional dependency. Use pip install sysidentpy['full']</p> </li> <li> <p>Fix code format issues.</p> </li> <li> <p>Fixed minor grammatical and spelling mistakes.</p> </li> <li> <p>Fix issues related to html on Jupyter notebooks examples on documentation.</p> </li> <li> <p>Updated Readme with examples of how to use.</p> </li> <li> <p>Improved descriptions and comments in methods.</p> </li> <li> <p>metaheuristics.bpsogsa (detailed description on code docstring)</p> <ul> <li>added evaluate_objective_function</li> <li>added optimize</li> <li>added generate_random_population</li> <li>added mass_calculation</li> <li>added calculate_gravitational_constant</li> <li>added calculate_acceleration</li> <li>added update_velocity_position</li> </ul> </li> <li> <p>FIX issue #52</p> </li> </ul>"},{"location":"changelog/changelog/#v015","title":"v0.1.5","text":""},{"location":"changelog/changelog/#contributors_12","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_12","title":"CHANGES","text":"<ul> <li> <p>MAJOR: n-steps-ahead prediction.</p> <ul> <li>Now you can define the numbers of steps ahead in the predict function.</li> <li>Only for Polynomial models for now. Next update will bring this functionality to Neural NARX and General Estimators.</li> </ul> </li> <li> <p>MAJOR: Simulating predefined models.</p> <ul> <li>Added the new class SimulatePolynomialNarmax to handle the simulation of known model structures.</li> <li>Now you can simulate predefined models by just passing the model structure codification. Check the notebook examples.</li> </ul> </li> <li> <p>Added 4 new notebooks in the example section.</p> </li> <li> <p>Added iterative notebooks. Now you can run the notebooks in Jupyter notebook section of the documentation in Colab.</p> </li> <li> <p>Fix code format issues.</p> </li> <li> <p>Added new tests for SimulatePolynomialNarmax and generate_data.</p> </li> <li> <p>Started changes related to numpy 1.19.4 update. There are still some Deprecation warnings that will be fixed in next update.</p> </li> <li> <p>Fix issues related to html on Jupyter notebooks examples on documentation.</p> </li> <li> <p>Updated Readme with examples of how to use.</p> </li> </ul>"},{"location":"changelog/changelog/#v014","title":"v0.1.4","text":""},{"location":"changelog/changelog/#contributors_13","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_13","title":"CHANGES","text":"<ul> <li> <p>MAJOR: Introducing NARX Neural Network in SysIdentPy.</p> <ul> <li>Now you can build NARX Neural Network on SysIdentPy.</li> <li>This feature is built on top of Pytorch. See the docs for more details and examples of how to use.</li> </ul> </li> <li> <p>MAJOR: Introducing general estimators in SysIdentPy.</p> <ul> <li>Now you are able to use any estimator that have Fit/Predict methods (estimators from Sklearn and Catboost, for example) and build NARX models based on those estimators.</li> <li>We use the core functions of SysIdentPy and keep the Fit/Predict approach from those estimators to keep the process easy to use.</li> <li>More estimators are coming soon like XGboost.</li> </ul> </li> <li> <p>Added notebooks to show how to build NARX neural Network.</p> </li> <li> <p>Added notebooks to show how to build NARX models using general estimators.</p> </li> <li> <p>Changed the default parameters of the plot_results function.</p> </li> <li> <p>NOTE: We will keeping improving the Polynomial NARX models (new model structure selection algorithms and multiobjective identification is on our roadmap). These recent modifications will allow us to introduce new NARX models like PWARX models very soon.</p> </li> <li> <p>New template for the documentation site.</p> </li> <li> <p>Fix issues related to html on Jupyter notebooks examples on documentation.</p> </li> <li> <p>Updated Readme with examples of how to use.</p> </li> </ul>"},{"location":"changelog/changelog/#v013","title":"v0.1.3","text":""},{"location":"changelog/changelog/#contributors_14","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> <li>renard162</li> </ul>"},{"location":"changelog/changelog/#changes_14","title":"CHANGES","text":"<ul> <li>Fixed a bug concerning the xlag and ylag in multiple input scenarios.</li> <li>Refactored predict function. Improved performance up to 87% depending on the number of regressors.</li> <li>You can set lags with different size for each input.</li> <li>Added a new function to get the max value of xlag and ylag. Work with int, list, nested lists.</li> <li>Fixed tests for information criteria.</li> <li>Added SysIdentPy logo.</li> <li>Refactored code of all classes following PEP 8 guidelines to improve readability.</li> <li>Added Citation information on Readme.</li> <li>Changes on information Criteria tests.</li> <li>Added workflow to run the tests when merge branch into master.</li> <li>Added new site domain.</li> <li>Updated docs.</li> </ul>"},{"location":"code/aols/","title":"Documentation for <code>AOLS</code>","text":"<p>NARMAX Models using the Accelerated Orthogonal Least-Squares algorithm.</p>"},{"location":"code/aols/#sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS","title":"<code>AOLS</code>","text":"<p>               Bases: <code>BaseMSS</code></p> <p>Accelerated Orthogonal Least Squares Algorithm.</p> <p>Build Polynomial NARMAX model using the Accelerated Orthogonal Least-Squares ([1]_). This algorithm is based on the Matlab code available on: https://github.com/realabolfazl/AOLS/</p> <p>The NARMAX model is described as:</p> \\[     y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1},     \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>where \\(n_y\\in \\mathbb{N}^*\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\), are the maximum lags for the system output and input respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}^\\ell\\) is some nonlinear function of the input and output regressors with nonlinearity degree \\(\\ell \\in \\mathbb{N}\\) and \\(d\\) is a time delay typically set to \\(d=1\\).</p>"},{"location":"code/aols/#sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS--parameters","title":"Parameters","text":"<p>ylag : int, default=2     The maximum lag of the output. xlag : int, default=2     The maximum lag of the input. k : int, default=1     The sparsity level. L : int, default=1     Number of selected indices per iteration. threshold : float, default=10e10     The desired accuracy.</p>"},{"location":"code/aols/#sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS--examples","title":"Examples","text":"<p>import numpy as np import matplotlib.pyplot as plt from sysidentpy.model_structure_selection import AOLS from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.utils.display_results import results from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.generate_data import get_miso_data, get_siso_data x_train, x_valid, y_train, y_valid = get_siso_data(n=1000, ...                                                    colored_noise=True, ...                                                    sigma=0.2, ...                                                    train_percentage=90) basis_function = Polynomial(degree=2) model = AOLS(basis_function=basis_function, ...              ylag=2, xlag=2 ...              ) model.fit(x_train, y_train) yhat = model.predict(x_valid, y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse) 0.001993603325328823 r = pd.DataFrame( ...     results( ...         model.final_model, model.theta, model.err, ...         model.n_terms, err_precision=8, dtype='sci' ...         ), ...     columns=['Regressors', 'Parameters', 'ERR']) print\u00ae     Regressors Parameters         ERR 0        x1(k-2)     0.9000       0.0 1         y(k-1)     0.1999       0.0 2  x1(k-1)y(k-1)     0.1000       0.0</p>"},{"location":"code/aols/#sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS--references","title":"References","text":"<ul> <li>Manuscript: Accelerated Orthogonal Least-Squares for Large-Scale    Sparse Reconstruction    https://www.sciencedirect.com/science/article/abs/pii/S1051200418305311</li> <li>Code:    https://github.com/realabolfazl/AOLS/</li> </ul> Source code in <code>sysidentpy/model_structure_selection/accelerated_orthogonal_least_squares.py</code> <pre><code>class AOLS(BaseMSS):\n    r\"\"\"Accelerated Orthogonal Least Squares Algorithm.\n\n    Build Polynomial NARMAX model using the Accelerated Orthogonal Least-Squares ([1]_).\n    This algorithm is based on the Matlab code available on:\n    https://github.com/realabolfazl/AOLS/\n\n    The NARMAX model is described as:\n\n    $$\n        y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1},\n        \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k\n    $$\n\n    where $n_y\\in \\mathbb{N}^*$, $n_x \\in \\mathbb{N}$, $n_e \\in \\mathbb{N}$,\n    are the maximum lags for the system output and input respectively;\n    $x_k \\in \\mathbb{R}^{n_x}$ is the system input and $y_k \\in \\mathbb{R}^{n_y}$\n    is the system output at discrete time $k \\in \\mathbb{N}^n$;\n    $e_k \\in \\mathbb{R}^{n_e}$ stands for uncertainties and possible noise\n    at discrete time $k$. In this case, $\\mathcal{F}^\\ell$ is some nonlinear function\n    of the input and output regressors with nonlinearity degree $\\ell \\in \\mathbb{N}$\n    and $d$ is a time delay typically set to $d=1$.\n\n    Parameters\n    ----------\n    ylag : int, default=2\n        The maximum lag of the output.\n    xlag : int, default=2\n        The maximum lag of the input.\n    k : int, default=1\n        The sparsity level.\n    L : int, default=1\n        Number of selected indices per iteration.\n    threshold : float, default=10e10\n        The desired accuracy.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from sysidentpy.model_structure_selection import AOLS\n    &gt;&gt;&gt; from sysidentpy.basis_function._basis_function import Polynomial\n    &gt;&gt;&gt; from sysidentpy.utils.display_results import results\n    &gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_miso_data, get_siso_data\n    &gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000,\n    ...                                                    colored_noise=True,\n    ...                                                    sigma=0.2,\n    ...                                                    train_percentage=90)\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; model = AOLS(basis_function=basis_function,\n    ...              ylag=2, xlag=2\n    ...              )\n    &gt;&gt;&gt; model.fit(x_train, y_train)\n    &gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n    &gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n    &gt;&gt;&gt; print(rrse)\n    0.001993603325328823\n    &gt;&gt;&gt; r = pd.DataFrame(\n    ...     results(\n    ...         model.final_model, model.theta, model.err,\n    ...         model.n_terms, err_precision=8, dtype='sci'\n    ...         ),\n    ...     columns=['Regressors', 'Parameters', 'ERR'])\n    &gt;&gt;&gt; print(r)\n        Regressors Parameters         ERR\n    0        x1(k-2)     0.9000       0.0\n    1         y(k-1)     0.1999       0.0\n    2  x1(k-1)y(k-1)     0.1000       0.0\n\n    References\n    ----------\n    - Manuscript: Accelerated Orthogonal Least-Squares for Large-Scale\n       Sparse Reconstruction\n       https://www.sciencedirect.com/science/article/abs/pii/S1051200418305311\n    - Code:\n       https://github.com/realabolfazl/AOLS/\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        ylag: Union[int, list] = 2,\n        xlag: Union[int, list] = 2,\n        k: int = 1,\n        L: int = 1,\n        threshold: float = 10e-10,\n        model_type: str = \"NARMAX\",\n        estimator: Estimators = LeastSquares(),\n        basis_function: Union[Polynomial, Fourier] = Polynomial(),\n    ):\n        self.basis_function = basis_function\n        self.model_type = model_type\n        self.build_matrix = self.get_build_io_method(model_type)\n        self.xlag = xlag\n        self.ylag = ylag\n        self.max_lag = self._get_max_lag()\n        self.k = k\n        self.L = L\n        self.estimator = estimator\n        self.threshold = threshold\n        self.res = None\n        self.n_inputs = None\n        self.theta = None\n        self.regressor_code = None\n        self.pivv = None\n        self.final_model = None\n        self.n_terms = None\n        self.err = None\n        self._validate_params()\n\n    def _validate_params(self):\n        \"\"\"Validate input params.\"\"\"\n        if isinstance(self.ylag, int) and self.ylag &lt; 1:\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if isinstance(self.xlag, int) and self.xlag &lt; 1:\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.xlag, (int, list)):\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.ylag, (int, list)):\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if not isinstance(self.k, int) or self.k &lt; 1:\n            raise ValueError(f\"k must be integer and &gt; zero. Got {self.k}\")\n\n        if not isinstance(self.L, int) or self.L &lt; 1:\n            raise ValueError(f\"k must be integer and &gt; zero. Got {self.L}\")\n\n        if not isinstance(self.threshold, (int, float)) or self.threshold &lt; 0:\n            raise ValueError(\n                f\"threshold must be integer and &gt; zero. Got {self.threshold}\"\n            )\n\n    def aols(\n        self, psi: np.ndarray, y: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Perform the Accelerated Orthogonal Least-Squares algorithm.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = n_samples\n            The target data used in the identification process.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The respective ERR calculated for each regressor.\n        piv : array-like of shape = number_of_model_elements\n            Contains the index to put the regressors in the correct order\n            based on err values.\n        residual_norm : float\n            The final residual norm.\n\n        References\n        ----------\n        - Manuscript: Accelerated Orthogonal Least-Squares for Large-Scale\n           Sparse Reconstruction\n           https://www.sciencedirect.com/science/article/abs/pii/S1051200418305311\n\n        \"\"\"\n        n, m = psi.shape\n        theta = np.zeros([m, 1])\n        r = y[self.max_lag :].reshape(-1, 1).copy()\n        it = 0\n        max_iter = int(min(self.k, np.floor(n / self.L)))\n        aols_index = np.zeros(max_iter * self.L)\n        U = np.zeros([n, max_iter * self.L])\n        T = psi.copy()\n        while LA.norm(r) &gt; self.threshold and it &lt; max_iter:\n            it = it + 1\n            temp_in = (it - 1) * self.L\n            if it &gt; 1:\n                T = T - U[:, temp_in].reshape(-1, 1) @ (\n                    U[:, temp_in].reshape(-1, 1).T @ psi\n                )\n\n            q = ((r.T @ psi) / np.sum(psi * T, axis=0)).ravel()\n            TT = np.sum(T**2, axis=0) * (q**2)\n            sub_ind = list(aols_index[:temp_in].astype(int))\n            TT[sub_ind] = 0\n            sorting_indices = np.argsort(TT)[::-1].ravel()\n            aols_index[temp_in : temp_in + self.L] = sorting_indices[: self.L]\n            for i in range(self.L):\n                TEMP = T[:, sorting_indices[i]].reshape(-1, 1) * q[sorting_indices[i]]\n                U[:, temp_in + i] = (TEMP / np.linalg.norm(TEMP, axis=0)).ravel()\n                r = r - TEMP\n                if i == self.L:\n                    break\n\n                T = T - U[:, temp_in + i].reshape(-1, 1) @ (\n                    U[:, temp_in + i].reshape(-1, 1).T @ psi\n                )\n                q = ((r.T @ psi) / np.sum(psi * T, axis=0)).ravel()\n\n        aols_index = aols_index[aols_index &gt; 0].ravel().astype(int)\n        residual_norm = LA.norm(r)\n        theta[aols_index] = self.estimator.optimize(\n            psi[:, aols_index], y[self.max_lag :, 0].reshape(-1, 1)\n        )\n        if self.L &gt; 1:\n            sorting_indices = np.argsort(np.abs(theta))[::-1]\n            aols_index = sorting_indices[: self.k].ravel().astype(int)\n            theta[aols_index] = self.estimator.optimize(\n                psi[:, aols_index], y[self.max_lag :, 0].reshape(-1, 1)\n            )\n            residual_norm = LA.norm(\n                y[self.max_lag :].reshape(-1, 1)\n                - psi[:, aols_index] @ theta[aols_index]\n            )\n\n        pivv = np.argwhere(theta.ravel() != 0).ravel()\n        theta = theta[theta != 0]\n        return theta.reshape(-1, 1), pivv, residual_norm\n\n    def fit(self, *, X: Optional[np.ndarray] = None, y: Optional[np.ndarray] = None):\n        \"\"\"Fit polynomial NARMAX model using AOLS algorithm.\n\n        The 'fit' function allows a friendly usage by the user.\n        Given two arguments, X and y, fit training data.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the training process.\n        y : ndarray of floats\n            The output data to be used in the training process.\n\n        Returns\n        -------\n        model : ndarray of int\n            The model code representation.\n        piv : array-like of shape = number_of_model_elements\n            Contains the index to put the regressors in the correct order\n            based on err values.\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n        err : array-like of shape = number_of_model_elements\n            The respective ERR calculated for each regressor.\n        info_values : array-like of shape = n_regressor\n            Vector with values of akaike's information criterion\n            for models with N terms (where N is the\n            vector position + 1).\n\n        \"\"\"\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        self.max_lag = self._get_max_lag()\n        lagged_data = self.build_matrix(X, y)\n        reg_matrix = self.basis_function.fit(\n            lagged_data, self.max_lag, predefined_regressors=None\n        )\n\n        if X is not None:\n            self.n_inputs = _num_features(X)\n        else:\n            self.n_inputs = 1  # just to create the regressor space base\n\n        self.regressor_code = self.regressor_space(self.n_inputs)\n        (self.theta, self.pivv, self.res) = self.aols(reg_matrix, y)\n        repetition = len(reg_matrix)\n        if isinstance(self.basis_function, Polynomial):\n            self.final_model = self.regressor_code[self.pivv, :].copy()\n        else:\n            self.regressor_code = np.sort(\n                np.tile(self.regressor_code[1:, :], (repetition, 1)),\n                axis=0,\n            )\n            self.final_model = self.regressor_code[self.pivv, :].copy()\n\n        self.n_terms = len(\n            self.theta\n        )  # the number of terms we selected (necessary in the 'results' methods)\n        self.err = self.n_terms * [\n            0\n        ]  # just to use the `results` method. Will be changed in next update.\n        return self\n\n    def predict(\n        self,\n        *,\n        X: Optional[np.ndarray] = None,\n        y: Optional[np.ndarray] = None,\n        steps_ahead: Optional[int] = None,\n        forecast_horizon: int = 0,\n    ) -&gt; np.ndarray:\n        \"\"\"Return the predicted values given an input.\n\n        The predict function allows a friendly usage by the user.\n        Given a previously trained model, predict values given\n        a new set of data.\n\n        This method accept y values mainly for prediction n-steps ahead\n        (to be implemented in the future)\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            The predicted values of the model.\n\n        \"\"\"\n        if self.basis_function.__class__.__name__ == \"Polynomial\":\n            if steps_ahead is None:\n                yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n            if steps_ahead == 1:\n                yhat = self._one_step_ahead_prediction(X, y)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n\n            _check_positive_int(steps_ahead, \"steps_ahead\")\n            yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        if steps_ahead is None:\n            yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        yhat = self._basis_function_n_step_prediction(\n            X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n        )\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    def _one_step_ahead_prediction(\n        self, X: Optional[np.ndarray], y: Optional[np.ndarray]\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        lagged_data = self.build_matrix(X, y)\n        X_base = self.basis_function.transform(\n            lagged_data,\n            self.max_lag,\n            predefined_regressors=self.pivv[: len(self.final_model)],\n        )\n\n        yhat = super()._one_step_ahead_prediction(X_base)\n        return yhat.reshape(-1, 1)\n\n    def _n_step_ahead_prediction(\n        self,\n        X: Optional[np.ndarray],\n        y: Optional[np.ndarray],\n        steps_ahead: Optional[int],\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        yhat = super()._n_step_ahead_prediction(X, y, steps_ahead)\n        return yhat\n\n    def _model_prediction(\n        self,\n        X: Optional[np.ndarray],\n        y_initial: Optional[np.ndarray],\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the infinity steps-ahead simulation of a model.\n\n        Parameters\n        ----------\n        y_initial : array-like of shape = max_lag\n            Number of initial conditions values of output\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The predicted values of the model.\n\n        \"\"\"\n        if self.model_type in [\"NARMAX\", \"NAR\"]:\n            return self._narmax_predict(X, y_initial, forecast_horizon)\n        if self.model_type == \"NFIR\":\n            return self._nfir_predict(X, y_initial)\n\n        raise ValueError(\n            f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n        )\n\n    def _narmax_predict(\n        self,\n        X: Optional[np.ndarray],\n        y_initial: Optional[np.ndarray],\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        if len(y_initial) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if X is not None:\n            forecast_horizon = X.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        y_output = super()._narmax_predict(X, y_initial, forecast_horizon)\n        return y_output\n\n    def _nfir_predict(\n        self, X: Optional[np.ndarray], y_initial: Optional[np.ndarray]\n    ) -&gt; np.ndarray:\n        y_output = super()._nfir_predict(X, y_initial)\n        return y_output\n\n    def _basis_function_predict(\n        self,\n        X: Optional[np.ndarray],\n        y_initial: Optional[np.ndarray],\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        if X is not None:\n            forecast_horizon = X.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        yhat = super()._basis_function_predict(X, y_initial, forecast_horizon)\n        return yhat.reshape(-1, 1)\n\n    def _basis_function_n_step_prediction(\n        self,\n        X: Optional[np.ndarray],\n        y: Optional[np.ndarray],\n        steps_ahead: Optional[int],\n        forecast_horizon: int,\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if X is not None:\n            forecast_horizon = X.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        yhat = super()._basis_function_n_step_prediction(\n            X, y, steps_ahead, forecast_horizon\n        )\n        return yhat.reshape(-1, 1)\n\n    def _basis_function_n_steps_horizon(\n        self,\n        X: Optional[np.ndarray],\n        y: Optional[np.ndarray],\n        steps_ahead: Optional[int],\n        forecast_horizon: int,\n    ) -&gt; np.ndarray:\n        yhat = super()._basis_function_n_steps_horizon(\n            X, y, steps_ahead, forecast_horizon\n        )\n        return yhat.reshape(-1, 1)\n</code></pre>"},{"location":"code/aols/#sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS.aols","title":"<code>aols(psi, y)</code>","text":"<p>Perform the Accelerated Orthogonal Least-Squares algorithm.</p>"},{"location":"code/aols/#sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS.aols--parameters","title":"Parameters","text":"<p>psi : ndarray of floats     The information matrix of the model. y : array-like of shape = n_samples     The target data used in the identification process.</p>"},{"location":"code/aols/#sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS.aols--returns","title":"Returns","text":"<p>theta : array-like of shape = number_of_model_elements     The respective ERR calculated for each regressor. piv : array-like of shape = number_of_model_elements     Contains the index to put the regressors in the correct order     based on err values. residual_norm : float     The final residual norm.</p>"},{"location":"code/aols/#sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS.aols--references","title":"References","text":"<ul> <li>Manuscript: Accelerated Orthogonal Least-Squares for Large-Scale    Sparse Reconstruction    https://www.sciencedirect.com/science/article/abs/pii/S1051200418305311</li> </ul> Source code in <code>sysidentpy/model_structure_selection/accelerated_orthogonal_least_squares.py</code> <pre><code>def aols(\n    self, psi: np.ndarray, y: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Perform the Accelerated Orthogonal Least-Squares algorithm.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = n_samples\n        The target data used in the identification process.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The respective ERR calculated for each regressor.\n    piv : array-like of shape = number_of_model_elements\n        Contains the index to put the regressors in the correct order\n        based on err values.\n    residual_norm : float\n        The final residual norm.\n\n    References\n    ----------\n    - Manuscript: Accelerated Orthogonal Least-Squares for Large-Scale\n       Sparse Reconstruction\n       https://www.sciencedirect.com/science/article/abs/pii/S1051200418305311\n\n    \"\"\"\n    n, m = psi.shape\n    theta = np.zeros([m, 1])\n    r = y[self.max_lag :].reshape(-1, 1).copy()\n    it = 0\n    max_iter = int(min(self.k, np.floor(n / self.L)))\n    aols_index = np.zeros(max_iter * self.L)\n    U = np.zeros([n, max_iter * self.L])\n    T = psi.copy()\n    while LA.norm(r) &gt; self.threshold and it &lt; max_iter:\n        it = it + 1\n        temp_in = (it - 1) * self.L\n        if it &gt; 1:\n            T = T - U[:, temp_in].reshape(-1, 1) @ (\n                U[:, temp_in].reshape(-1, 1).T @ psi\n            )\n\n        q = ((r.T @ psi) / np.sum(psi * T, axis=0)).ravel()\n        TT = np.sum(T**2, axis=0) * (q**2)\n        sub_ind = list(aols_index[:temp_in].astype(int))\n        TT[sub_ind] = 0\n        sorting_indices = np.argsort(TT)[::-1].ravel()\n        aols_index[temp_in : temp_in + self.L] = sorting_indices[: self.L]\n        for i in range(self.L):\n            TEMP = T[:, sorting_indices[i]].reshape(-1, 1) * q[sorting_indices[i]]\n            U[:, temp_in + i] = (TEMP / np.linalg.norm(TEMP, axis=0)).ravel()\n            r = r - TEMP\n            if i == self.L:\n                break\n\n            T = T - U[:, temp_in + i].reshape(-1, 1) @ (\n                U[:, temp_in + i].reshape(-1, 1).T @ psi\n            )\n            q = ((r.T @ psi) / np.sum(psi * T, axis=0)).ravel()\n\n    aols_index = aols_index[aols_index &gt; 0].ravel().astype(int)\n    residual_norm = LA.norm(r)\n    theta[aols_index] = self.estimator.optimize(\n        psi[:, aols_index], y[self.max_lag :, 0].reshape(-1, 1)\n    )\n    if self.L &gt; 1:\n        sorting_indices = np.argsort(np.abs(theta))[::-1]\n        aols_index = sorting_indices[: self.k].ravel().astype(int)\n        theta[aols_index] = self.estimator.optimize(\n            psi[:, aols_index], y[self.max_lag :, 0].reshape(-1, 1)\n        )\n        residual_norm = LA.norm(\n            y[self.max_lag :].reshape(-1, 1)\n            - psi[:, aols_index] @ theta[aols_index]\n        )\n\n    pivv = np.argwhere(theta.ravel() != 0).ravel()\n    theta = theta[theta != 0]\n    return theta.reshape(-1, 1), pivv, residual_norm\n</code></pre>"},{"location":"code/aols/#sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS.fit","title":"<code>fit(*, X=None, y=None)</code>","text":"<p>Fit polynomial NARMAX model using AOLS algorithm.</p> <p>The 'fit' function allows a friendly usage by the user. Given two arguments, X and y, fit training data.</p>"},{"location":"code/aols/#sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS.fit--parameters","title":"Parameters","text":"<p>X : ndarray of floats     The input data to be used in the training process. y : ndarray of floats     The output data to be used in the training process.</p>"},{"location":"code/aols/#sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS.fit--returns","title":"Returns","text":"<p>model : ndarray of int     The model code representation. piv : array-like of shape = number_of_model_elements     Contains the index to put the regressors in the correct order     based on err values. theta : array-like of shape = number_of_model_elements     The estimated parameters of the model. err : array-like of shape = number_of_model_elements     The respective ERR calculated for each regressor. info_values : array-like of shape = n_regressor     Vector with values of akaike's information criterion     for models with N terms (where N is the     vector position + 1).</p> Source code in <code>sysidentpy/model_structure_selection/accelerated_orthogonal_least_squares.py</code> <pre><code>def fit(self, *, X: Optional[np.ndarray] = None, y: Optional[np.ndarray] = None):\n    \"\"\"Fit polynomial NARMAX model using AOLS algorithm.\n\n    The 'fit' function allows a friendly usage by the user.\n    Given two arguments, X and y, fit training data.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the training process.\n    y : ndarray of floats\n        The output data to be used in the training process.\n\n    Returns\n    -------\n    model : ndarray of int\n        The model code representation.\n    piv : array-like of shape = number_of_model_elements\n        Contains the index to put the regressors in the correct order\n        based on err values.\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n    err : array-like of shape = number_of_model_elements\n        The respective ERR calculated for each regressor.\n    info_values : array-like of shape = n_regressor\n        Vector with values of akaike's information criterion\n        for models with N terms (where N is the\n        vector position + 1).\n\n    \"\"\"\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    self.max_lag = self._get_max_lag()\n    lagged_data = self.build_matrix(X, y)\n    reg_matrix = self.basis_function.fit(\n        lagged_data, self.max_lag, predefined_regressors=None\n    )\n\n    if X is not None:\n        self.n_inputs = _num_features(X)\n    else:\n        self.n_inputs = 1  # just to create the regressor space base\n\n    self.regressor_code = self.regressor_space(self.n_inputs)\n    (self.theta, self.pivv, self.res) = self.aols(reg_matrix, y)\n    repetition = len(reg_matrix)\n    if isinstance(self.basis_function, Polynomial):\n        self.final_model = self.regressor_code[self.pivv, :].copy()\n    else:\n        self.regressor_code = np.sort(\n            np.tile(self.regressor_code[1:, :], (repetition, 1)),\n            axis=0,\n        )\n        self.final_model = self.regressor_code[self.pivv, :].copy()\n\n    self.n_terms = len(\n        self.theta\n    )  # the number of terms we selected (necessary in the 'results' methods)\n    self.err = self.n_terms * [\n        0\n    ]  # just to use the `results` method. Will be changed in next update.\n    return self\n</code></pre>"},{"location":"code/aols/#sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS.predict","title":"<code>predict(*, X=None, y=None, steps_ahead=None, forecast_horizon=0)</code>","text":"<p>Return the predicted values given an input.</p> <p>The predict function allows a friendly usage by the user. Given a previously trained model, predict values given a new set of data.</p> <p>This method accept y values mainly for prediction n-steps ahead (to be implemented in the future)</p>"},{"location":"code/aols/#sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS.predict--parameters","title":"Parameters","text":"<p>X : ndarray of floats     The input data to be used in the prediction process. y : ndarray of floats     The output data to be used in the prediction process. steps_ahead : int (default = None)     The user can use free run simulation, one-step ahead prediction     and n-step ahead prediction. forecast_horizon : int, default=None     The number of predictions over the time.</p>"},{"location":"code/aols/#sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS.predict--returns","title":"Returns","text":"<p>yhat : ndarray of floats     The predicted values of the model.</p> Source code in <code>sysidentpy/model_structure_selection/accelerated_orthogonal_least_squares.py</code> <pre><code>def predict(\n    self,\n    *,\n    X: Optional[np.ndarray] = None,\n    y: Optional[np.ndarray] = None,\n    steps_ahead: Optional[int] = None,\n    forecast_horizon: int = 0,\n) -&gt; np.ndarray:\n    \"\"\"Return the predicted values given an input.\n\n    The predict function allows a friendly usage by the user.\n    Given a previously trained model, predict values given\n    a new set of data.\n\n    This method accept y values mainly for prediction n-steps ahead\n    (to be implemented in the future)\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    steps_ahead : int (default = None)\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction.\n    forecast_horizon : int, default=None\n        The number of predictions over the time.\n\n    Returns\n    -------\n    yhat : ndarray of floats\n        The predicted values of the model.\n\n    \"\"\"\n    if self.basis_function.__class__.__name__ == \"Polynomial\":\n        if steps_ahead is None:\n            yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        _check_positive_int(steps_ahead, \"steps_ahead\")\n        yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    if steps_ahead is None:\n        yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n    if steps_ahead == 1:\n        yhat = self._one_step_ahead_prediction(X, y)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    yhat = self._basis_function_n_step_prediction(\n        X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n    )\n    yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n    return yhat\n</code></pre>"},{"location":"code/basis-function/","title":"Documentation for <code>Basis Functions</code>","text":""},{"location":"code/entropic-regression/","title":"Documentation for <code>Entropic Regression</code>","text":"<p>Build Polynomial NARMAX Models using the Entropic Regression algorithm.</p>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER","title":"<code>ER</code>","text":"<p>               Bases: <code>BaseMSS</code></p> <p>Entropic Regression Algorithm.</p> <p>Build Polynomial NARMAX model using the Entropic Regression Algorithm ([1]_). This algorithm is based on the Matlab package available on: https://github.com/almomaa/ERFit-Package</p> <p>The NARMAX model is described as:</p> \\[     y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x},     e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>where \\(n_y\\in \\mathbb{N}^*\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\), are the maximum lags for the system output and input respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}^\\ell\\) is some nonlinear function of the input and output regressors with nonlinearity degree \\(\\ell \\in \\mathbb{N}\\) and \\(d\\) is a time delay typically set to \\(d=1\\).</p>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER--parameters","title":"Parameters","text":"<p>ylag : int, default=2     The maximum lag of the output. xlag : int, default=2     The maximum lag of the input. k : int, default=2     The kth nearest neighbor to be used in estimation. q : float, default=0.99     Quantile to compute, which must be between 0 and 1 inclusive. p : default=inf,     Lp Measure of the distance in Knn estimator. n_perm: int, default=200     Number of permutation to be used in shuffle test estimator : str, default=\"least_squares\"     The parameter estimation method. skip_forward = bool, default=False     To be used for difficult and highly uncertain problems.     Skipping the forward selection results in more accurate solution,     but comes with higher computational cost. model_type: str, default=\"NARMAX\"     The user can choose \"NARMAX\", \"NAR\" and \"NFIR\" models</p>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER--examples","title":"Examples","text":"<p>import numpy as np import matplotlib.pyplot as plt from sysidentpy.model_structure_selection import ER from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.utils.display_results import results from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.generate_data import get_miso_data, get_siso_data x_train, x_valid, y_train, y_valid = get_siso_data(n=1000, ...                                                    colored_noise=True, ...                                                    sigma=0.2, ...                                                    train_percentage=90) basis_function = Polynomial(degree=2) model = ER(basis_function=basis_function, ...              ylag=2, xlag=2 ...              ) model.fit(x_train, y_train) yhat = model.predict(x_valid, y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse) 0.001993603325328823 r = pd.DataFrame( ...     results( ...         model.final_model, model.theta, model.err, ...         model.n_terms, err_precision=8, dtype='sci' ...         ), ...     columns=['Regressors', 'Parameters', 'ERR']) print\u00ae     Regressors Parameters         ERR 0        x1(k-2)     0.9000       0.0 1         y(k-1)     0.1999       0.0 2  x1(k-1)y(k-1)     0.1000       0.0</p>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER--references","title":"References","text":"<ul> <li>Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic     Regression Beats the Outliers Problem in Nonlinear System     Identification. Chaos 30, 013107 (2020).</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> </ul> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>@deprecated(\n    version=\"v0.3.0\",\n    future_version=\"v0.4.0\",\n    message=(\n        \"Passing a string to define the estimator will rise an error in v0.4.0.\"\n        \" \\n You'll have to use ER(estimator=LeastSquares()) instead. \\n The\"\n        \" only change is that you'll have to define the estimator first instead\"\n        \" of passing a string like 'least_squares'. \\n This change will make\"\n        \" easier to implement new estimators and it'll improve code\"\n        \" readability.\"\n    ),\n)\nclass ER(BaseMSS):\n    r\"\"\"Entropic Regression Algorithm.\n\n    Build Polynomial NARMAX model using the Entropic Regression Algorithm ([1]_).\n    This algorithm is based on the Matlab package available on:\n    https://github.com/almomaa/ERFit-Package\n\n    The NARMAX model is described as:\n\n    $$\n        y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x},\n        e_{k-1}, \\dotsc, e_{k-n_e}] + e_k\n    $$\n\n    where $n_y\\in \\mathbb{N}^*$, $n_x \\in \\mathbb{N}$, $n_e \\in \\mathbb{N}$,\n    are the maximum lags for the system output and input respectively;\n    $x_k \\in \\mathbb{R}^{n_x}$ is the system input and $y_k \\in \\mathbb{R}^{n_y}$\n    is the system output at discrete time $k \\in \\mathbb{N}^n$;\n    $e_k \\in \\mathbb{R}^{n_e}$ stands for uncertainties and possible noise\n    at discrete time $k$. In this case, $\\mathcal{F}^\\ell$ is some nonlinear function\n    of the input and output regressors with nonlinearity degree $\\ell \\in \\mathbb{N}$\n    and $d$ is a time delay typically set to $d=1$.\n\n    Parameters\n    ----------\n    ylag : int, default=2\n        The maximum lag of the output.\n    xlag : int, default=2\n        The maximum lag of the input.\n    k : int, default=2\n        The kth nearest neighbor to be used in estimation.\n    q : float, default=0.99\n        Quantile to compute, which must be between 0 and 1 inclusive.\n    p : default=inf,\n        Lp Measure of the distance in Knn estimator.\n    n_perm: int, default=200\n        Number of permutation to be used in shuffle test\n    estimator : str, default=\"least_squares\"\n        The parameter estimation method.\n    skip_forward = bool, default=False\n        To be used for difficult and highly uncertain problems.\n        Skipping the forward selection results in more accurate solution,\n        but comes with higher computational cost.\n    model_type: str, default=\"NARMAX\"\n        The user can choose \"NARMAX\", \"NAR\" and \"NFIR\" models\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from sysidentpy.model_structure_selection import ER\n    &gt;&gt;&gt; from sysidentpy.basis_function._basis_function import Polynomial\n    &gt;&gt;&gt; from sysidentpy.utils.display_results import results\n    &gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_miso_data, get_siso_data\n    &gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000,\n    ...                                                    colored_noise=True,\n    ...                                                    sigma=0.2,\n    ...                                                    train_percentage=90)\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; model = ER(basis_function=basis_function,\n    ...              ylag=2, xlag=2\n    ...              )\n    &gt;&gt;&gt; model.fit(x_train, y_train)\n    &gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n    &gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n    &gt;&gt;&gt; print(rrse)\n    0.001993603325328823\n    &gt;&gt;&gt; r = pd.DataFrame(\n    ...     results(\n    ...         model.final_model, model.theta, model.err,\n    ...         model.n_terms, err_precision=8, dtype='sci'\n    ...         ),\n    ...     columns=['Regressors', 'Parameters', 'ERR'])\n    &gt;&gt;&gt; print(r)\n        Regressors Parameters         ERR\n    0        x1(k-2)     0.9000       0.0\n    1         y(k-1)     0.1999       0.0\n    2  x1(k-1)y(k-1)     0.1000       0.0\n\n    References\n    ----------\n    - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n        Regression Beats the Outliers Problem in Nonlinear System\n        Identification. Chaos 30, 013107 (2020).\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        ylag: Union[int, list] = 1,\n        xlag: Union[int, list] = 1,\n        q: float = 0.99,\n        estimator: Estimators_Union = LeastSquares(),\n        extended_least_squares: bool = False,\n        h: float = 0.01,\n        k: int = 2,\n        mutual_information_estimator: str = \"mutual_information_knn\",\n        n_perm: int = 200,\n        p: float = np.inf,\n        skip_forward: bool = False,\n        model_type: str = \"NARMAX\",\n        basis_function: Union[Polynomial, Fourier] = Polynomial(),\n        random_state: Optional[int] = None,\n    ):\n        self.basis_function = basis_function\n        self.model_type = model_type\n        self.build_matrix = self.get_build_io_method(model_type)\n        self.xlag = xlag\n        self.ylag = ylag\n        self.non_degree = basis_function.degree\n        self.max_lag = self._get_max_lag()\n        self.k = k\n        self.estimator = estimator\n        self.extended_least_squares = extended_least_squares\n        self.q = q\n        self.h = h\n        self.mutual_information_estimator = mutual_information_estimator\n        self.n_perm = n_perm\n        self.p = p\n        self.skip_forward = skip_forward\n        self.random_state = random_state\n        self.rng = check_random_state(random_state)\n        self.tol = None\n        # self.ensemble = None\n        self.n_inputs = None\n        self.estimated_tolerance = None\n        self.regressor_code = None\n        self.final_model = None\n        self.theta = None\n        self.n_terms = None\n        self.err = None\n        self.pivv = None\n        self._validate_params()\n\n    def _validate_params(self):\n        \"\"\"Validate input params.\"\"\"\n        if isinstance(self.ylag, int) and self.ylag &lt; 1:\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if isinstance(self.xlag, int) and self.xlag &lt; 1:\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.xlag, (int, list)):\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.ylag, (int, list)):\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if not isinstance(self.k, int) or self.k &lt; 1:\n            raise ValueError(f\"k must be integer and &gt; zero. Got {self.k}\")\n\n        if not isinstance(self.n_perm, int) or self.n_perm &lt; 1:\n            raise ValueError(f\"n_perm must be integer and &gt; zero. Got {self.n_perm}\")\n\n        if not isinstance(self.q, float) or self.q &gt; 1 or self.q &lt;= 0:\n            raise ValueError(\n                f\"q must be float and must be between 0 and 1 inclusive. Got {self.q}\"\n            )\n\n        if not isinstance(self.skip_forward, bool):\n            raise TypeError(\n                f\"skip_forward must be False or True. Got {self.skip_forward}\"\n            )\n\n        if not isinstance(self.extended_least_squares, bool):\n            raise TypeError(\n                \"extended_least_squares must be False or True. Got\"\n                f\" {self.extended_least_squares}\"\n            )\n\n        if self.model_type not in [\"NARMAX\", \"NAR\", \"NFIR\"]:\n            raise ValueError(\n                f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n            )\n\n    def mutual_information_knn(self, y, y_perm):\n        \"\"\"Find the mutual information.\n\n        Finds the mutual information between $x$ and $y$ given $z$.\n\n        This code is based on Matlab Entropic Regression package.\n\n        Parameters\n        ----------\n        y : ndarray of floats\n            The source signal.\n        y_perm : ndarray of floats\n            The destination signal.\n\n        Returns\n        -------\n        ksg_estimation : float\n            The conditioned mutual information.\n\n        References\n        ----------\n        - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n            Regression Beats the Outliers Problem in Nonlinear System\n            Identification. Chaos 30, 013107 (2020).\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n\n        \"\"\"\n        joint_space = np.concatenate([y, y_perm], axis=1)\n        smallest_distance = np.sort(\n            cdist(joint_space, joint_space, \"minkowski\", p=self.p).T\n        )\n        idx = np.argpartition(smallest_distance[-1, :], self.k + 1)[: self.k + 1]\n        smallest_distance = smallest_distance[:, idx]\n        epsilon = smallest_distance[:, -1].reshape(-1, 1)\n        smallest_distance_y = cdist(y, y, \"minkowski\", p=self.p)\n        less_than_array_nx = np.array((smallest_distance_y &lt; epsilon)).astype(int)\n        nx = (np.sum(less_than_array_nx, axis=1) - 1).reshape(-1, 1)\n        smallest_distance_y_perm = cdist(y_perm, y_perm, \"minkowski\", p=self.p)\n        less_than_array_ny = np.array((smallest_distance_y_perm &lt; epsilon)).astype(int)\n        ny = (np.sum(less_than_array_ny, axis=1) - 1).reshape(-1, 1)\n        arr = psi(nx + 1) + psi(ny + 1)\n        ksg_estimation = (\n            psi(self.k) + psi(y.shape[0]) - np.nanmean(arr[np.isfinite(arr)])\n        )\n        return ksg_estimation\n\n    def entropic_regression_backward(self, reg_matrix, y, piv):\n        \"\"\"Entropic Regression Backward Greedy Feature Elimination.\n\n        This algorithm is based on the Matlab package available on:\n        https://github.com/almomaa/ERFit-Package\n\n        Parameters\n        ----------\n        reg_matrix : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        piv : ndarray of ints\n            The set of indices to investigate\n\n        Returns\n        -------\n        piv : ndarray of ints\n            The set of remaining indices after the\n            Backward Greedy Feature Elimination.\n\n        \"\"\"\n        min_value = -np.inf\n        piv = np.array(piv)\n        ix = []\n        while (min_value &lt;= self.tol) and (len(piv) &gt; 1):\n            initial_array = np.full((1, len(piv)), np.inf)\n            for i in range(initial_array.shape[1]):\n                if piv[i] not in []:  # if you want to keep any regressor\n                    rem = np.setdiff1d(piv, piv[i])\n                    f1 = reg_matrix[:, piv] @ LA.pinv(reg_matrix[:, piv]) @ y\n                    f2 = reg_matrix[:, rem] @ LA.pinv(reg_matrix[:, rem]) @ y\n                    initial_array[0, i] = self.conditional_mutual_information(y, f1, f2)\n\n            ix = np.argmin(initial_array)\n            min_value = initial_array[0, ix]\n            piv = np.delete(piv, ix)\n\n        return piv\n\n    def entropic_regression_forward(self, reg_matrix, y):\n        \"\"\"Entropic Regression Forward Greedy Feature Selection.\n\n        This algorithm is based on the Matlab package available on:\n        https://github.com/almomaa/ERFit-Package\n\n        Parameters\n        ----------\n        reg_matrix : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n\n        Returns\n        -------\n        selected_terms : ndarray of ints\n            The set of selected regressors after the\n            Forward Greedy Feature Selection.\n        success : boolean\n            Indicate if the forward selection succeed.\n            If high degree of uncertainty is detected, and many parameters are\n            selected, the success flag will be set to false. Then, the\n            backward elimination will be applied for all indices.\n\n        \"\"\"\n        success = True\n        ix = []\n        selected_terms = []\n        reg_matrix_columns = np.array(list(range(reg_matrix.shape[1])))\n        self.tol = self.tolerance_estimator(y)\n        ksg_max = getattr(self, self.mutual_information_estimator)(\n            y, reg_matrix @ LA.pinv(reg_matrix) @ y\n        )\n        stop_criteria = False\n        while stop_criteria is False:\n            selected_terms = np.ravel(\n                [*selected_terms, *np.array([reg_matrix_columns[ix]])]\n            )\n            if len(selected_terms) != 0:\n                ksg_local = getattr(self, self.mutual_information_estimator)(\n                    y,\n                    reg_matrix[:, selected_terms]\n                    @ LA.pinv(reg_matrix[:, selected_terms])\n                    @ y,\n                )\n            else:\n                ksg_local = getattr(self, self.mutual_information_estimator)(\n                    y, np.zeros_like(y)\n                )\n\n            initial_vector = np.full((1, reg_matrix.shape[1]), -np.inf)\n            for i in range(reg_matrix.shape[1]):\n                if reg_matrix_columns[i] not in selected_terms:\n                    f1 = (\n                        reg_matrix[:, [*selected_terms, reg_matrix_columns[i]]]\n                        @ LA.pinv(\n                            reg_matrix[:, [*selected_terms, reg_matrix_columns[i]]]\n                        )\n                        @ y\n                    )\n                    if len(selected_terms) != 0:\n                        f2 = (\n                            reg_matrix[:, selected_terms]\n                            @ LA.pinv(reg_matrix[:, selected_terms])\n                            @ y\n                        )\n                    else:\n                        f2 = np.zeros_like(y)\n                    vp_estimation = self.conditional_mutual_information(y, f1, f2)\n                    initial_vector[0, i] = vp_estimation\n                else:\n                    continue\n\n            ix = np.nanargmax(initial_vector)\n            max_value = initial_vector[0, ix]\n\n            if (ksg_max - ksg_local &lt;= self.tol) or (max_value &lt;= self.tol):\n                stop_criteria = True\n            elif len(selected_terms) &gt; np.max([8, reg_matrix.shape[1] / 2]):\n                success = False\n                stop_criteria = True\n\n        return selected_terms, success\n\n    def conditional_mutual_information(self, y, f1, f2):\n        \"\"\"Find the conditional mutual information.\n\n        Finds the conditioned mutual information between $y$ and $f1$ given $f2$.\n\n        This code is based on Matlab Entropic Regression package.\n        https://github.com/almomaa/ERFit-Package\n\n        Parameters\n        ----------\n        y : ndarray of floats\n            The source signal.\n        f1 : ndarray of floats\n            The destination signal.\n        f2 : ndarray of floats\n            The condition set.\n\n        Returns\n        -------\n        vp_estimation : float\n            The conditioned mutual information.\n\n        References\n        ----------\n        - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n            Regression Beats the Outliers Problem in Nonlinear System\n            Identification. Chaos 30, 013107 (2020).\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n\n        \"\"\"\n        joint_space = np.concatenate([y, f1, f2], axis=1)\n        smallest_distance = np.sort(\n            cdist(joint_space, joint_space, \"minkowski\", p=self.p).T\n        )\n        idx = np.argpartition(smallest_distance[-1, :], self.k + 1)[: self.k + 1]\n        smallest_distance = smallest_distance[:, idx]\n        epsilon = smallest_distance[:, -1].reshape(-1, 1)\n        # Find number of points from (y,f2), (f1,f2), and (f2,f2) that lies withing the\n        # k^{th} nearest neighbor distance from each point of themselves.\n        smallest_distance_y_f2 = cdist(\n            np.concatenate([y, f2], axis=1),\n            np.concatenate([y, f2], axis=1),\n            \"minkowski\",\n            p=self.p,\n        )\n        less_than_array_y_f2 = np.array((smallest_distance_y_f2 &lt; epsilon)).astype(int)\n        y_f2 = (np.sum(less_than_array_y_f2, axis=1) - 1).reshape(-1, 1)\n\n        smallest_distance_f1_f2 = cdist(\n            np.concatenate([f1, f2], axis=1),\n            np.concatenate([f1, f2], axis=1),\n            \"minkowski\",\n            p=self.p,\n        )\n        less_than_array_f1_f2 = np.array((smallest_distance_f1_f2 &lt; epsilon)).astype(\n            int\n        )\n        f1_f2 = (np.sum(less_than_array_f1_f2, axis=1) - 1).reshape(-1, 1)\n\n        smallest_distance_f2 = cdist(f2, f2, \"minkowski\", p=self.p)\n        less_than_array_f2 = np.array((smallest_distance_f2 &lt; epsilon)).astype(int)\n        f2_f2 = (np.sum(less_than_array_f2, axis=1) - 1).reshape(-1, 1)\n        arr = psi(y_f2 + 1) + psi(f1_f2 + 1) - psi(f2_f2 + 1)\n        vp_estimation = psi(self.k) - np.nanmean(arr[np.isfinite(arr)])\n        return vp_estimation\n\n    def tolerance_estimator(self, y):\n        \"\"\"Tolerance Estimation for mutual independence test.\n\n        Finds the conditioned mutual information between $y$ and $f1$ given $f2$.\n\n        This code is based on Matlab Entropic Regression package.\n        https://github.com/almomaa/ERFit-Package\n\n        Parameters\n        ----------\n        y : ndarray of floats\n            The source signal.\n\n        Returns\n        -------\n        tol : float\n            The tolerance value given q.\n\n        References\n        ----------\n        - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n            Regression Beats the Outliers Problem in Nonlinear System\n            Identification. Chaos 30, 013107 (2020).\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n\n        \"\"\"\n        ksg_estimation = []\n        for _ in range(self.n_perm):\n            mutual_information_output = getattr(\n                self, self.mutual_information_estimator\n            )(y, self.rng.permutation(y))\n\n            ksg_estimation.append(mutual_information_output)\n\n        ksg_estimation = np.array(ksg_estimation)\n        tol = np.quantile(ksg_estimation, self.q)\n        return tol\n\n    def fit(self, *, X=None, y=None):\n        \"\"\"Fit polynomial NARMAX model using AOLS algorithm.\n\n        The 'fit' function allows a friendly usage by the user.\n        Given two arguments, X and y, fit training data.\n\n        The Entropic Regression algorithm is based on the Matlab package available on:\n        https://github.com/almomaa/ERFit-Package\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the training process.\n        y : ndarray of floats\n            The output data to be used in the training process.\n\n        Returns\n        -------\n        model : ndarray of int\n            The model code representation.\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        References\n        ----------\n        - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n            Regression Beats the Outliers Problem in Nonlinear System\n            Identification. Chaos 30, 013107 (2020).\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n\n        \"\"\"\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        self.max_lag = self._get_max_lag()\n        lagged_data = self.build_matrix(X, y)\n\n        reg_matrix = self.basis_function.fit(\n            lagged_data, self.max_lag, predefined_regressors=None\n        )\n\n        if X is not None:\n            self.n_inputs = _num_features(X)\n        else:\n            self.n_inputs = 1  # just to create the regressor space base\n\n        self.regressor_code = self.regressor_space(self.n_inputs)\n\n        if self.regressor_code.shape[0] &gt; 90:\n            warnings.warn(\n                \"Given the higher number of possible regressors\"\n                f\" ({self.regressor_code.shape[0]}), the Entropic Regression\"\n                \" algorithm may take long time to run. Consider reducing the\"\n                \" number of regressors \",\n                stacklevel=2,\n            )\n\n        y_full = y.copy()\n        y = y[self.max_lag :].reshape(-1, 1)\n        self.tol = 0\n        ksg_estimation = []\n        for _ in range(self.n_perm):\n            mutual_information_output = getattr(\n                self, self.mutual_information_estimator\n            )(y, self.rng.permutation(y))\n            ksg_estimation.append(mutual_information_output)\n\n        ksg_estimation = np.array(ksg_estimation).reshape(-1, 1)\n        self.tol = np.quantile(ksg_estimation, self.q)\n        self.estimated_tolerance = self.tol\n        success = False\n        if not self.skip_forward:\n            selected_terms, success = self.entropic_regression_forward(reg_matrix, y)\n\n        if not success or self.skip_forward:\n            selected_terms = np.array(list(range(reg_matrix.shape[1])))\n\n        selected_terms_backward = self.entropic_regression_backward(\n            reg_matrix[:, selected_terms], y, list(range(len(selected_terms)))\n        )\n\n        final_model = selected_terms[selected_terms_backward]\n        # re-check for the constant term (add it to the estimated indices)\n        if 0 not in final_model:\n            final_model = np.array([0, *final_model])\n\n        repetition = len(reg_matrix)\n        if isinstance(self.basis_function, Polynomial):\n            self.final_model = self.regressor_code[final_model, :].copy()\n        else:\n            self.regressor_code = np.sort(\n                np.tile(self.regressor_code[1:, :], (repetition, 1)),\n                axis=0,\n            )\n            self.final_model = self.regressor_code[final_model, :].copy()\n\n        # self.theta = getattr(self, self.estimator)(reg_matrix[:, final_model], y_full)\n        self.theta = self.estimator.optimize(\n            reg_matrix[:, final_model], y_full[self.max_lag :, 0].reshape(-1, 1)\n        )\n        if (np.abs(self.theta[0]) &lt; self.h) and (\n            np.sum((self.theta != 0).astype(int)) &gt; 1\n        ):\n            self.theta = self.theta[1:].reshape(-1, 1)\n            self.final_model = self.final_model[1:, :]\n            final_model = final_model[1:]\n\n        self.n_terms = len(\n            self.theta\n        )  # the number of terms we selected (necessary in the 'results' methods)\n        self.err = self.n_terms * [\n            0\n        ]  # just to use the `results` method. Will be changed in next update.\n        self.pivv = final_model\n        return self\n\n    def predict(self, *, X=None, y=None, steps_ahead=None, forecast_horizon=None):\n        \"\"\"Return the predicted values given an input.\n\n        The predict function allows a friendly usage by the user.\n        Given a previously trained model, predict values given\n        a new set of data.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            The predicted values of the model.\n\n        \"\"\"\n        if self.basis_function.__class__.__name__ == \"Polynomial\":\n            if steps_ahead is None:\n                yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n            if steps_ahead == 1:\n                yhat = self._one_step_ahead_prediction(X, y)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n\n            _check_positive_int(steps_ahead, \"steps_ahead\")\n            yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        if steps_ahead is None:\n            yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        yhat = self._basis_function_n_step_prediction(\n            X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n        )\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    def _one_step_ahead_prediction(self, X, y):\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        lagged_data = self.build_matrix(X, y)\n\n        X_base = self.basis_function.transform(\n            lagged_data,\n            self.max_lag,\n            predefined_regressors=self.pivv[: len(self.final_model)],\n        )\n\n        yhat = super()._one_step_ahead_prediction(X_base)\n        return yhat.reshape(-1, 1)\n\n    def _n_step_ahead_prediction(self, X, y, steps_ahead):\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        yhat = super()._n_step_ahead_prediction(X, y, steps_ahead)\n        return yhat\n\n    def _model_prediction(self, X, y_initial, forecast_horizon=None):\n        \"\"\"Perform the infinity steps-ahead simulation of a model.\n\n        Parameters\n        ----------\n        y_initial : array-like of shape = max_lag\n            Number of initial conditions values of output\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The predicted values of the model.\n\n        \"\"\"\n        if self.model_type in [\"NARMAX\", \"NAR\"]:\n            return self._narmax_predict(X, y_initial, forecast_horizon)\n        elif self.model_type == \"NFIR\":\n            return self._nfir_predict(X, y_initial)\n        else:\n            raise ValueError(\n                f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n            )\n\n    def _narmax_predict(self, X, y_initial, forecast_horizon):\n        if len(y_initial) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if X is not None:\n            forecast_horizon = X.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        y_output = super()._narmax_predict(X, y_initial, forecast_horizon)\n        return y_output\n\n    def _nfir_predict(self, X, y_initial):\n        y_output = super()._nfir_predict(X, y_initial)\n        return y_output\n\n    def _basis_function_predict(self, X, y_initial, forecast_horizon=None):\n        if X is not None:\n            forecast_horizon = X.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        yhat = super()._basis_function_predict(X, y_initial, forecast_horizon)\n        return yhat.reshape(-1, 1)\n\n    def _basis_function_n_step_prediction(self, X, y, steps_ahead, forecast_horizon):\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if X is not None:\n            forecast_horizon = X.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        yhat = super()._basis_function_n_step_prediction(\n            X, y, steps_ahead, forecast_horizon\n        )\n        return yhat.reshape(-1, 1)\n\n    def _basis_function_n_steps_horizon(self, X, y, steps_ahead, forecast_horizon):\n        yhat = super()._basis_function_n_steps_horizon(\n            X, y, steps_ahead, forecast_horizon\n        )\n        return yhat.reshape(-1, 1)\n</code></pre>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.conditional_mutual_information","title":"<code>conditional_mutual_information(y, f1, f2)</code>","text":"<p>Find the conditional mutual information.</p> <p>Finds the conditioned mutual information between \\(y\\) and \\(f1\\) given \\(f2\\).</p> <p>This code is based on Matlab Entropic Regression package. https://github.com/almomaa/ERFit-Package</p>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.conditional_mutual_information--parameters","title":"Parameters","text":"<p>y : ndarray of floats     The source signal. f1 : ndarray of floats     The destination signal. f2 : ndarray of floats     The condition set.</p>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.conditional_mutual_information--returns","title":"Returns","text":"<p>vp_estimation : float     The conditioned mutual information.</p>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.conditional_mutual_information--references","title":"References","text":"<ul> <li>Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic     Regression Beats the Outliers Problem in Nonlinear System     Identification. Chaos 30, 013107 (2020).</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> </ul> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>def conditional_mutual_information(self, y, f1, f2):\n    \"\"\"Find the conditional mutual information.\n\n    Finds the conditioned mutual information between $y$ and $f1$ given $f2$.\n\n    This code is based on Matlab Entropic Regression package.\n    https://github.com/almomaa/ERFit-Package\n\n    Parameters\n    ----------\n    y : ndarray of floats\n        The source signal.\n    f1 : ndarray of floats\n        The destination signal.\n    f2 : ndarray of floats\n        The condition set.\n\n    Returns\n    -------\n    vp_estimation : float\n        The conditioned mutual information.\n\n    References\n    ----------\n    - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n        Regression Beats the Outliers Problem in Nonlinear System\n        Identification. Chaos 30, 013107 (2020).\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n\n    \"\"\"\n    joint_space = np.concatenate([y, f1, f2], axis=1)\n    smallest_distance = np.sort(\n        cdist(joint_space, joint_space, \"minkowski\", p=self.p).T\n    )\n    idx = np.argpartition(smallest_distance[-1, :], self.k + 1)[: self.k + 1]\n    smallest_distance = smallest_distance[:, idx]\n    epsilon = smallest_distance[:, -1].reshape(-1, 1)\n    # Find number of points from (y,f2), (f1,f2), and (f2,f2) that lies withing the\n    # k^{th} nearest neighbor distance from each point of themselves.\n    smallest_distance_y_f2 = cdist(\n        np.concatenate([y, f2], axis=1),\n        np.concatenate([y, f2], axis=1),\n        \"minkowski\",\n        p=self.p,\n    )\n    less_than_array_y_f2 = np.array((smallest_distance_y_f2 &lt; epsilon)).astype(int)\n    y_f2 = (np.sum(less_than_array_y_f2, axis=1) - 1).reshape(-1, 1)\n\n    smallest_distance_f1_f2 = cdist(\n        np.concatenate([f1, f2], axis=1),\n        np.concatenate([f1, f2], axis=1),\n        \"minkowski\",\n        p=self.p,\n    )\n    less_than_array_f1_f2 = np.array((smallest_distance_f1_f2 &lt; epsilon)).astype(\n        int\n    )\n    f1_f2 = (np.sum(less_than_array_f1_f2, axis=1) - 1).reshape(-1, 1)\n\n    smallest_distance_f2 = cdist(f2, f2, \"minkowski\", p=self.p)\n    less_than_array_f2 = np.array((smallest_distance_f2 &lt; epsilon)).astype(int)\n    f2_f2 = (np.sum(less_than_array_f2, axis=1) - 1).reshape(-1, 1)\n    arr = psi(y_f2 + 1) + psi(f1_f2 + 1) - psi(f2_f2 + 1)\n    vp_estimation = psi(self.k) - np.nanmean(arr[np.isfinite(arr)])\n    return vp_estimation\n</code></pre>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.entropic_regression_backward","title":"<code>entropic_regression_backward(reg_matrix, y, piv)</code>","text":"<p>Entropic Regression Backward Greedy Feature Elimination.</p> <p>This algorithm is based on the Matlab package available on: https://github.com/almomaa/ERFit-Package</p>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.entropic_regression_backward--parameters","title":"Parameters","text":"<p>reg_matrix : ndarray of floats     The input data to be used in the prediction process. y : ndarray of floats     The output data to be used in the prediction process. piv : ndarray of ints     The set of indices to investigate</p>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.entropic_regression_backward--returns","title":"Returns","text":"<p>piv : ndarray of ints     The set of remaining indices after the     Backward Greedy Feature Elimination.</p> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>def entropic_regression_backward(self, reg_matrix, y, piv):\n    \"\"\"Entropic Regression Backward Greedy Feature Elimination.\n\n    This algorithm is based on the Matlab package available on:\n    https://github.com/almomaa/ERFit-Package\n\n    Parameters\n    ----------\n    reg_matrix : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    piv : ndarray of ints\n        The set of indices to investigate\n\n    Returns\n    -------\n    piv : ndarray of ints\n        The set of remaining indices after the\n        Backward Greedy Feature Elimination.\n\n    \"\"\"\n    min_value = -np.inf\n    piv = np.array(piv)\n    ix = []\n    while (min_value &lt;= self.tol) and (len(piv) &gt; 1):\n        initial_array = np.full((1, len(piv)), np.inf)\n        for i in range(initial_array.shape[1]):\n            if piv[i] not in []:  # if you want to keep any regressor\n                rem = np.setdiff1d(piv, piv[i])\n                f1 = reg_matrix[:, piv] @ LA.pinv(reg_matrix[:, piv]) @ y\n                f2 = reg_matrix[:, rem] @ LA.pinv(reg_matrix[:, rem]) @ y\n                initial_array[0, i] = self.conditional_mutual_information(y, f1, f2)\n\n        ix = np.argmin(initial_array)\n        min_value = initial_array[0, ix]\n        piv = np.delete(piv, ix)\n\n    return piv\n</code></pre>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.entropic_regression_forward","title":"<code>entropic_regression_forward(reg_matrix, y)</code>","text":"<p>Entropic Regression Forward Greedy Feature Selection.</p> <p>This algorithm is based on the Matlab package available on: https://github.com/almomaa/ERFit-Package</p>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.entropic_regression_forward--parameters","title":"Parameters","text":"<p>reg_matrix : ndarray of floats     The input data to be used in the prediction process. y : ndarray of floats     The output data to be used in the prediction process.</p>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.entropic_regression_forward--returns","title":"Returns","text":"<p>selected_terms : ndarray of ints     The set of selected regressors after the     Forward Greedy Feature Selection. success : boolean     Indicate if the forward selection succeed.     If high degree of uncertainty is detected, and many parameters are     selected, the success flag will be set to false. Then, the     backward elimination will be applied for all indices.</p> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>def entropic_regression_forward(self, reg_matrix, y):\n    \"\"\"Entropic Regression Forward Greedy Feature Selection.\n\n    This algorithm is based on the Matlab package available on:\n    https://github.com/almomaa/ERFit-Package\n\n    Parameters\n    ----------\n    reg_matrix : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n\n    Returns\n    -------\n    selected_terms : ndarray of ints\n        The set of selected regressors after the\n        Forward Greedy Feature Selection.\n    success : boolean\n        Indicate if the forward selection succeed.\n        If high degree of uncertainty is detected, and many parameters are\n        selected, the success flag will be set to false. Then, the\n        backward elimination will be applied for all indices.\n\n    \"\"\"\n    success = True\n    ix = []\n    selected_terms = []\n    reg_matrix_columns = np.array(list(range(reg_matrix.shape[1])))\n    self.tol = self.tolerance_estimator(y)\n    ksg_max = getattr(self, self.mutual_information_estimator)(\n        y, reg_matrix @ LA.pinv(reg_matrix) @ y\n    )\n    stop_criteria = False\n    while stop_criteria is False:\n        selected_terms = np.ravel(\n            [*selected_terms, *np.array([reg_matrix_columns[ix]])]\n        )\n        if len(selected_terms) != 0:\n            ksg_local = getattr(self, self.mutual_information_estimator)(\n                y,\n                reg_matrix[:, selected_terms]\n                @ LA.pinv(reg_matrix[:, selected_terms])\n                @ y,\n            )\n        else:\n            ksg_local = getattr(self, self.mutual_information_estimator)(\n                y, np.zeros_like(y)\n            )\n\n        initial_vector = np.full((1, reg_matrix.shape[1]), -np.inf)\n        for i in range(reg_matrix.shape[1]):\n            if reg_matrix_columns[i] not in selected_terms:\n                f1 = (\n                    reg_matrix[:, [*selected_terms, reg_matrix_columns[i]]]\n                    @ LA.pinv(\n                        reg_matrix[:, [*selected_terms, reg_matrix_columns[i]]]\n                    )\n                    @ y\n                )\n                if len(selected_terms) != 0:\n                    f2 = (\n                        reg_matrix[:, selected_terms]\n                        @ LA.pinv(reg_matrix[:, selected_terms])\n                        @ y\n                    )\n                else:\n                    f2 = np.zeros_like(y)\n                vp_estimation = self.conditional_mutual_information(y, f1, f2)\n                initial_vector[0, i] = vp_estimation\n            else:\n                continue\n\n        ix = np.nanargmax(initial_vector)\n        max_value = initial_vector[0, ix]\n\n        if (ksg_max - ksg_local &lt;= self.tol) or (max_value &lt;= self.tol):\n            stop_criteria = True\n        elif len(selected_terms) &gt; np.max([8, reg_matrix.shape[1] / 2]):\n            success = False\n            stop_criteria = True\n\n    return selected_terms, success\n</code></pre>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.fit","title":"<code>fit(*, X=None, y=None)</code>","text":"<p>Fit polynomial NARMAX model using AOLS algorithm.</p> <p>The 'fit' function allows a friendly usage by the user. Given two arguments, X and y, fit training data.</p> <p>The Entropic Regression algorithm is based on the Matlab package available on: https://github.com/almomaa/ERFit-Package</p>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.fit--parameters","title":"Parameters","text":"<p>X : ndarray of floats     The input data to be used in the training process. y : ndarray of floats     The output data to be used in the training process.</p>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.fit--returns","title":"Returns","text":"<p>model : ndarray of int     The model code representation. theta : array-like of shape = number_of_model_elements     The estimated parameters of the model.</p>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.fit--references","title":"References","text":"<ul> <li>Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic     Regression Beats the Outliers Problem in Nonlinear System     Identification. Chaos 30, 013107 (2020).</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> </ul> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>def fit(self, *, X=None, y=None):\n    \"\"\"Fit polynomial NARMAX model using AOLS algorithm.\n\n    The 'fit' function allows a friendly usage by the user.\n    Given two arguments, X and y, fit training data.\n\n    The Entropic Regression algorithm is based on the Matlab package available on:\n    https://github.com/almomaa/ERFit-Package\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the training process.\n    y : ndarray of floats\n        The output data to be used in the training process.\n\n    Returns\n    -------\n    model : ndarray of int\n        The model code representation.\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    References\n    ----------\n    - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n        Regression Beats the Outliers Problem in Nonlinear System\n        Identification. Chaos 30, 013107 (2020).\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n\n    \"\"\"\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    self.max_lag = self._get_max_lag()\n    lagged_data = self.build_matrix(X, y)\n\n    reg_matrix = self.basis_function.fit(\n        lagged_data, self.max_lag, predefined_regressors=None\n    )\n\n    if X is not None:\n        self.n_inputs = _num_features(X)\n    else:\n        self.n_inputs = 1  # just to create the regressor space base\n\n    self.regressor_code = self.regressor_space(self.n_inputs)\n\n    if self.regressor_code.shape[0] &gt; 90:\n        warnings.warn(\n            \"Given the higher number of possible regressors\"\n            f\" ({self.regressor_code.shape[0]}), the Entropic Regression\"\n            \" algorithm may take long time to run. Consider reducing the\"\n            \" number of regressors \",\n            stacklevel=2,\n        )\n\n    y_full = y.copy()\n    y = y[self.max_lag :].reshape(-1, 1)\n    self.tol = 0\n    ksg_estimation = []\n    for _ in range(self.n_perm):\n        mutual_information_output = getattr(\n            self, self.mutual_information_estimator\n        )(y, self.rng.permutation(y))\n        ksg_estimation.append(mutual_information_output)\n\n    ksg_estimation = np.array(ksg_estimation).reshape(-1, 1)\n    self.tol = np.quantile(ksg_estimation, self.q)\n    self.estimated_tolerance = self.tol\n    success = False\n    if not self.skip_forward:\n        selected_terms, success = self.entropic_regression_forward(reg_matrix, y)\n\n    if not success or self.skip_forward:\n        selected_terms = np.array(list(range(reg_matrix.shape[1])))\n\n    selected_terms_backward = self.entropic_regression_backward(\n        reg_matrix[:, selected_terms], y, list(range(len(selected_terms)))\n    )\n\n    final_model = selected_terms[selected_terms_backward]\n    # re-check for the constant term (add it to the estimated indices)\n    if 0 not in final_model:\n        final_model = np.array([0, *final_model])\n\n    repetition = len(reg_matrix)\n    if isinstance(self.basis_function, Polynomial):\n        self.final_model = self.regressor_code[final_model, :].copy()\n    else:\n        self.regressor_code = np.sort(\n            np.tile(self.regressor_code[1:, :], (repetition, 1)),\n            axis=0,\n        )\n        self.final_model = self.regressor_code[final_model, :].copy()\n\n    # self.theta = getattr(self, self.estimator)(reg_matrix[:, final_model], y_full)\n    self.theta = self.estimator.optimize(\n        reg_matrix[:, final_model], y_full[self.max_lag :, 0].reshape(-1, 1)\n    )\n    if (np.abs(self.theta[0]) &lt; self.h) and (\n        np.sum((self.theta != 0).astype(int)) &gt; 1\n    ):\n        self.theta = self.theta[1:].reshape(-1, 1)\n        self.final_model = self.final_model[1:, :]\n        final_model = final_model[1:]\n\n    self.n_terms = len(\n        self.theta\n    )  # the number of terms we selected (necessary in the 'results' methods)\n    self.err = self.n_terms * [\n        0\n    ]  # just to use the `results` method. Will be changed in next update.\n    self.pivv = final_model\n    return self\n</code></pre>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.mutual_information_knn","title":"<code>mutual_information_knn(y, y_perm)</code>","text":"<p>Find the mutual information.</p> <p>Finds the mutual information between \\(x\\) and \\(y\\) given \\(z\\).</p> <p>This code is based on Matlab Entropic Regression package.</p>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.mutual_information_knn--parameters","title":"Parameters","text":"<p>y : ndarray of floats     The source signal. y_perm : ndarray of floats     The destination signal.</p>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.mutual_information_knn--returns","title":"Returns","text":"<p>ksg_estimation : float     The conditioned mutual information.</p>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.mutual_information_knn--references","title":"References","text":"<ul> <li>Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic     Regression Beats the Outliers Problem in Nonlinear System     Identification. Chaos 30, 013107 (2020).</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> </ul> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>def mutual_information_knn(self, y, y_perm):\n    \"\"\"Find the mutual information.\n\n    Finds the mutual information between $x$ and $y$ given $z$.\n\n    This code is based on Matlab Entropic Regression package.\n\n    Parameters\n    ----------\n    y : ndarray of floats\n        The source signal.\n    y_perm : ndarray of floats\n        The destination signal.\n\n    Returns\n    -------\n    ksg_estimation : float\n        The conditioned mutual information.\n\n    References\n    ----------\n    - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n        Regression Beats the Outliers Problem in Nonlinear System\n        Identification. Chaos 30, 013107 (2020).\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n\n    \"\"\"\n    joint_space = np.concatenate([y, y_perm], axis=1)\n    smallest_distance = np.sort(\n        cdist(joint_space, joint_space, \"minkowski\", p=self.p).T\n    )\n    idx = np.argpartition(smallest_distance[-1, :], self.k + 1)[: self.k + 1]\n    smallest_distance = smallest_distance[:, idx]\n    epsilon = smallest_distance[:, -1].reshape(-1, 1)\n    smallest_distance_y = cdist(y, y, \"minkowski\", p=self.p)\n    less_than_array_nx = np.array((smallest_distance_y &lt; epsilon)).astype(int)\n    nx = (np.sum(less_than_array_nx, axis=1) - 1).reshape(-1, 1)\n    smallest_distance_y_perm = cdist(y_perm, y_perm, \"minkowski\", p=self.p)\n    less_than_array_ny = np.array((smallest_distance_y_perm &lt; epsilon)).astype(int)\n    ny = (np.sum(less_than_array_ny, axis=1) - 1).reshape(-1, 1)\n    arr = psi(nx + 1) + psi(ny + 1)\n    ksg_estimation = (\n        psi(self.k) + psi(y.shape[0]) - np.nanmean(arr[np.isfinite(arr)])\n    )\n    return ksg_estimation\n</code></pre>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.predict","title":"<code>predict(*, X=None, y=None, steps_ahead=None, forecast_horizon=None)</code>","text":"<p>Return the predicted values given an input.</p> <p>The predict function allows a friendly usage by the user. Given a previously trained model, predict values given a new set of data.</p>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.predict--parameters","title":"Parameters","text":"<p>X : ndarray of floats     The input data to be used in the prediction process. y : ndarray of floats     The output data to be used in the prediction process. steps_ahead : int (default = None)     The user can use free run simulation, one-step ahead prediction     and n-step ahead prediction. forecast_horizon : int, default=None     The number of predictions over the time.</p>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.predict--returns","title":"Returns","text":"<p>yhat : ndarray of floats     The predicted values of the model.</p> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>def predict(self, *, X=None, y=None, steps_ahead=None, forecast_horizon=None):\n    \"\"\"Return the predicted values given an input.\n\n    The predict function allows a friendly usage by the user.\n    Given a previously trained model, predict values given\n    a new set of data.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    steps_ahead : int (default = None)\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction.\n    forecast_horizon : int, default=None\n        The number of predictions over the time.\n\n    Returns\n    -------\n    yhat : ndarray of floats\n        The predicted values of the model.\n\n    \"\"\"\n    if self.basis_function.__class__.__name__ == \"Polynomial\":\n        if steps_ahead is None:\n            yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        _check_positive_int(steps_ahead, \"steps_ahead\")\n        yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    if steps_ahead is None:\n        yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n    if steps_ahead == 1:\n        yhat = self._one_step_ahead_prediction(X, y)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    yhat = self._basis_function_n_step_prediction(\n        X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n    )\n    yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n    return yhat\n</code></pre>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.tolerance_estimator","title":"<code>tolerance_estimator(y)</code>","text":"<p>Tolerance Estimation for mutual independence test.</p> <p>Finds the conditioned mutual information between \\(y\\) and \\(f1\\) given \\(f2\\).</p> <p>This code is based on Matlab Entropic Regression package. https://github.com/almomaa/ERFit-Package</p>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.tolerance_estimator--parameters","title":"Parameters","text":"<p>y : ndarray of floats     The source signal.</p>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.tolerance_estimator--returns","title":"Returns","text":"<p>tol : float     The tolerance value given q.</p>"},{"location":"code/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.tolerance_estimator--references","title":"References","text":"<ul> <li>Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic     Regression Beats the Outliers Problem in Nonlinear System     Identification. Chaos 30, 013107 (2020).</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> </ul> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>def tolerance_estimator(self, y):\n    \"\"\"Tolerance Estimation for mutual independence test.\n\n    Finds the conditioned mutual information between $y$ and $f1$ given $f2$.\n\n    This code is based on Matlab Entropic Regression package.\n    https://github.com/almomaa/ERFit-Package\n\n    Parameters\n    ----------\n    y : ndarray of floats\n        The source signal.\n\n    Returns\n    -------\n    tol : float\n        The tolerance value given q.\n\n    References\n    ----------\n    - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n        Regression Beats the Outliers Problem in Nonlinear System\n        Identification. Chaos 30, 013107 (2020).\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n\n    \"\"\"\n    ksg_estimation = []\n    for _ in range(self.n_perm):\n        mutual_information_output = getattr(\n            self, self.mutual_information_estimator\n        )(y, self.rng.permutation(y))\n\n        ksg_estimation.append(mutual_information_output)\n\n    ksg_estimation = np.array(ksg_estimation)\n    tol = np.quantile(ksg_estimation, self.q)\n    return tol\n</code></pre>"},{"location":"code/frols/","title":"Documentation for <code>FROLS</code>","text":"<p>Build Polynomial NARMAX Models using FROLS algorithm.</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS","title":"<code>FROLS</code>","text":"<p>               Bases: <code>BaseMSS</code></p> <p>Forward Regression Orthogonal Least Squares algorithm.</p> <p>This class uses the FROLS algorithm ([1], [2]) to build NARMAX models. The NARMAX model is described as:</p> \\[     y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1},     \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>where \\(n_y\\in \\mathbb{N}^*\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\), are the maximum lags for the system output and input respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); $e_k \\in \\mathbb{R}^{n_e}4 stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}^\\ell\\) is some nonlinear function of the input and output regressors with nonlinearity degree \\(\\ell \\in \\mathbb{N}\\) and \\(d\\) is a time delay typically set to \\(d=1\\).</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS--parameters","title":"Parameters","text":"<p>ylag : int, default=2     The maximum lag of the output. xlag : int, default=2     The maximum lag of the input. elag : int, default=2     The maximum lag of the residues regressors. order_selection: bool, default=False     Whether to use information criteria for order selection. info_criteria : str, default=\"aic\"     The information criteria method to be used. n_terms : int, default=None     The number of the model terms to be selected.     Note that n_terms overwrite the information criteria     values. n_info_values : int, default=10     The number of iterations of the information     criteria method. estimator : str, default=\"least_squares\"     The parameter estimation method. model_type: str, default=\"NARMAX\"     The user can choose \"NARMAX\", \"NAR\" and \"NFIR\" models eps : float, default=np.finfo(np.float64).eps     Normalization factor of the normalized filters. alpha : float, default=np.finfo(np.float64).eps     Regularization parameter used in ridge regression.     Ridge regression parameter that regularizes the algorithm to prevent over     fitting. If the input is a noisy signal, the ridge parameter is likely to be     set close to the noise level, at least as a starting point.     Entered through the self data structure.</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS--examples","title":"Examples","text":"<p>import numpy as np import matplotlib.pyplot as plt from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.utils.display_results import results from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.generate_data import get_miso_data, get_siso_data x_train, x_valid, y_train, y_valid = get_siso_data(n=1000, ...                                                    colored_noise=True, ...                                                    sigma=0.2, ...                                                    train_percentage=90) basis_function = Polynomial(degree=2) model = PolynomialNarmax(basis_function=basis_function, ...                          order_selection=True, ...                          n_info_values=10, ...                          extended_least_squares=False, ...                          ylag=2, xlag=2, ...                          info_criteria='aic', ...                          estimator='least_squares', ...                          ) model.fit(x_train, y_train) yhat = model.predict(x_valid, y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse) 0.001993603325328823 r = pd.DataFrame( ...     results( ...         model.final_model, model.theta, model.err, ...         model.n_terms, err_precision=8, dtype='sci' ...         ), ...     columns=['Regressors', 'Parameters', 'ERR']) print\u00ae     Regressors Parameters         ERR 0        x1(k-2)     0.9000       0.0 1         y(k-1)     0.1999       0.0 2  x1(k-1)y(k-1)     0.1000       0.0</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS--references","title":"References","text":"<ul> <li>Manuscript: Orthogonal least squares methods and their application    to non-linear system identification    https://eprints.soton.ac.uk/251147/1/778742007_content.pdf</li> <li>Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares    Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o    e Novos Resultados</li> </ul> Source code in <code>sysidentpy/model_structure_selection/forward_regression_orthogonal_least_squares.py</code> <pre><code>class FROLS(BaseMSS):\n    r\"\"\"Forward Regression Orthogonal Least Squares algorithm.\n\n    This class uses the FROLS algorithm ([1]_, [2]_) to build NARMAX models.\n    The NARMAX model is described as:\n\n    $$\n        y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1},\n        \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k\n    $$\n\n    where $n_y\\in \\mathbb{N}^*$, $n_x \\in \\mathbb{N}$, $n_e \\in \\mathbb{N}$,\n    are the maximum lags for the system output and input respectively;\n    $x_k \\in \\mathbb{R}^{n_x}$ is the system input and $y_k \\in \\mathbb{R}^{n_y}$\n    is the system output at discrete time $k \\in \\mathbb{N}^n$;\n    $e_k \\in \\mathbb{R}^{n_e}4 stands for uncertainties and possible noise\n    at discrete time $k$. In this case, $\\mathcal{F}^\\ell$ is some nonlinear function\n    of the input and output regressors with nonlinearity degree $\\ell \\in \\mathbb{N}$\n    and $d$ is a time delay typically set to $d=1$.\n\n    Parameters\n    ----------\n    ylag : int, default=2\n        The maximum lag of the output.\n    xlag : int, default=2\n        The maximum lag of the input.\n    elag : int, default=2\n        The maximum lag of the residues regressors.\n    order_selection: bool, default=False\n        Whether to use information criteria for order selection.\n    info_criteria : str, default=\"aic\"\n        The information criteria method to be used.\n    n_terms : int, default=None\n        The number of the model terms to be selected.\n        Note that n_terms overwrite the information criteria\n        values.\n    n_info_values : int, default=10\n        The number of iterations of the information\n        criteria method.\n    estimator : str, default=\"least_squares\"\n        The parameter estimation method.\n    model_type: str, default=\"NARMAX\"\n        The user can choose \"NARMAX\", \"NAR\" and \"NFIR\" models\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n    alpha : float, default=np.finfo(np.float64).eps\n        Regularization parameter used in ridge regression.\n        Ridge regression parameter that regularizes the algorithm to prevent over\n        fitting. If the input is a noisy signal, the ridge parameter is likely to be\n        set close to the noise level, at least as a starting point.\n        Entered through the self data structure.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from sysidentpy.model_structure_selection import FROLS\n    &gt;&gt;&gt; from sysidentpy.basis_function._basis_function import Polynomial\n    &gt;&gt;&gt; from sysidentpy.utils.display_results import results\n    &gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_miso_data, get_siso_data\n    &gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000,\n    ...                                                    colored_noise=True,\n    ...                                                    sigma=0.2,\n    ...                                                    train_percentage=90)\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; model = PolynomialNarmax(basis_function=basis_function,\n    ...                          order_selection=True,\n    ...                          n_info_values=10,\n    ...                          extended_least_squares=False,\n    ...                          ylag=2, xlag=2,\n    ...                          info_criteria='aic',\n    ...                          estimator='least_squares',\n    ...                          )\n    &gt;&gt;&gt; model.fit(x_train, y_train)\n    &gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n    &gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n    &gt;&gt;&gt; print(rrse)\n    0.001993603325328823\n    &gt;&gt;&gt; r = pd.DataFrame(\n    ...     results(\n    ...         model.final_model, model.theta, model.err,\n    ...         model.n_terms, err_precision=8, dtype='sci'\n    ...         ),\n    ...     columns=['Regressors', 'Parameters', 'ERR'])\n    &gt;&gt;&gt; print(r)\n        Regressors Parameters         ERR\n    0        x1(k-2)     0.9000       0.0\n    1         y(k-1)     0.1999       0.0\n    2  x1(k-1)y(k-1)     0.1000       0.0\n\n    References\n    ----------\n    - Manuscript: Orthogonal least squares methods and their application\n       to non-linear system identification\n       https://eprints.soton.ac.uk/251147/1/778742007_content.pdf\n    - Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares\n       Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o\n       e Novos Resultados\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        ylag: Union[int, list] = 2,\n        xlag: Union[int, list] = 2,\n        elag: Union[int, list] = 2,\n        order_selection: bool = True,\n        info_criteria: str = \"aic\",\n        n_terms: Union[int, None] = None,\n        n_info_values: int = 15,\n        estimator: Estimators = RecursiveLeastSquares(),\n        basis_function: Union[Polynomial, Fourier] = Polynomial(),\n        model_type: str = \"NARMAX\",\n        eps: np.float64 = np.finfo(np.float64).eps,\n        alpha: float = 0,\n        err_tol: Optional[float] = None,\n    ):\n        self.order_selection = order_selection\n        self.ylag = ylag\n        self.xlag = xlag\n        self.max_lag = self._get_max_lag()\n        self.info_criteria = info_criteria\n        self.info_criteria_function = self.get_info_criteria(info_criteria)\n        self.n_info_values = n_info_values\n        self.n_terms = n_terms\n        self.estimator = estimator\n        self.elag = elag\n        self.model_type = model_type\n        self.build_matrix = self.get_build_io_method(model_type)\n        self.basis_function = basis_function\n        self.eps = eps\n        if isinstance(self.estimator, RidgeRegression):\n            self.alpha = self.estimator.alpha\n        else:\n            self.alpha = alpha\n\n        self.err_tol = err_tol\n        self._validate_params()\n        self.n_inputs = None\n        self.regressor_code = None\n        self.info_values = None\n        self.err = None\n        self.final_model = None\n        self.theta = None\n        self.pivv = None\n\n    def _validate_params(self):\n        \"\"\"Validate input params.\"\"\"\n        if not isinstance(self.n_info_values, int) or self.n_info_values &lt; 1:\n            raise ValueError(\n                f\"n_info_values must be integer and &gt; zero. Got {self.n_info_values}\"\n            )\n\n        if isinstance(self.ylag, int) and self.ylag &lt; 1:\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if isinstance(self.xlag, int) and self.xlag &lt; 1:\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.xlag, (int, list)):\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.ylag, (int, list)):\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if not isinstance(self.order_selection, bool):\n            raise TypeError(\n                f\"order_selection must be False or True. Got {self.order_selection}\"\n            )\n\n        if self.info_criteria not in [\"aic\", \"aicc\", \"bic\", \"fpe\", \"lilc\"]:\n            raise ValueError(\n                f\"info_criteria must be aic, bic, fpe or lilc. Got {self.info_criteria}\"\n            )\n\n        if self.model_type not in [\"NARMAX\", \"NAR\", \"NFIR\"]:\n            raise ValueError(\n                f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n            )\n\n        if (\n            not isinstance(self.n_terms, int) or self.n_terms &lt; 1\n        ) and self.n_terms is not None:\n            raise ValueError(f\"n_terms must be integer and &gt; zero. Got {self.n_terms}\")\n\n    def error_reduction_ratio(\n        self, psi: np.ndarray, y: np.ndarray, process_term_number: int\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Perform the Error Reduction Ration algorithm.\n\n        Parameters\n        ----------\n        y : array-like of shape = n_samples\n            The target data used in the identification process.\n        psi : ndarray of floats\n            The information matrix of the model.\n        process_term_number : int\n            Number of Process Terms defined by the user.\n\n        Returns\n        -------\n        err : array-like of shape = number_of_model_elements\n            The respective ERR calculated for each regressor.\n        piv : array-like of shape = number_of_model_elements\n            Contains the index to put the regressors in the correct order\n            based on err values.\n        psi_orthogonal : ndarray of floats\n            The updated and orthogonal information matrix.\n\n        References\n        ----------\n        - Manuscript: Orthogonal least squares methods and their application\n           to non-linear system identification\n           https://eprints.soton.ac.uk/251147/1/778742007_content.pdf\n        - Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares\n           Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o\n           e Novos Resultados\n\n        \"\"\"\n        squared_y = np.dot(y[self.max_lag :].T, y[self.max_lag :])\n        tmp_psi = psi.copy()\n        y = y[self.max_lag :, 0].reshape(-1, 1)\n        tmp_y = y.copy()\n        dimension = tmp_psi.shape[1]\n        piv = np.arange(dimension)\n        tmp_err = np.zeros(dimension)\n        err = np.zeros(dimension)\n\n        for i in np.arange(0, dimension):\n            for j in np.arange(i, dimension):\n                # Add `eps` in the denominator to omit division by zero if\n                # denominator is zero\n                # To implement regularized regression (ridge regression), add\n                # alpha to psi.T @ psi.   See S. Chen, Local regularization assisted\n                # orthogonal least squares regression, Neurocomputing 69 (2006) 559-585.\n                # The version implemented below uses the same regularization for every\n                # feature, # What Chen refers to Uniform regularized orthogonal least\n                # squares (UROLS) Set to tiny (self.eps) when you are not regularizing.\n                # alpha = eps is the default.\n                tmp_err[j] = (\n                    (np.dot(tmp_psi[i:, j].T, tmp_y[i:]) ** 2)\n                    / (\n                        (np.dot(tmp_psi[i:, j].T, tmp_psi[i:, j]) + self.alpha)\n                        * squared_y\n                    )\n                    + self.eps\n                )[0, 0]\n\n            piv_index = np.argmax(tmp_err[i:]) + i\n            err[i] = tmp_err[piv_index]\n            if i == process_term_number:\n                break\n\n            if (self.err_tol is not None) and (err.cumsum()[i] &gt;= self.err_tol):\n                self.n_terms = i + 1\n                process_term_number = i + 1\n                break\n\n            tmp_psi[:, [piv_index, i]] = tmp_psi[:, [i, piv_index]]\n            piv[[piv_index, i]] = piv[[i, piv_index]]\n            v = Orthogonalization().house(tmp_psi[i:, i])\n            row_result = Orthogonalization().rowhouse(tmp_psi[i:, i:], v)\n            tmp_y[i:] = Orthogonalization().rowhouse(tmp_y[i:], v)\n            tmp_psi[i:, i:] = np.copy(row_result)\n\n        tmp_piv = piv[0:process_term_number]\n        psi_orthogonal = psi[:, tmp_piv]\n        return err, tmp_piv, psi_orthogonal\n\n    def information_criterion(self, X: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Determine the model order.\n\n        This function uses a information criterion to determine the model size.\n        'Akaike'-  Akaike's Information Criterion with\n                   critical value 2 (AIC) (default).\n        'Bayes' -  Bayes Information Criterion (BIC).\n        'FPE'   -  Final Prediction Error (FPE).\n        'LILC'  -  Khundrin's law ofiterated logarithm criterion (LILC).\n\n        Parameters\n        ----------\n        y : array-like of shape = n_samples\n            Target values of the system.\n        X : array-like of shape = n_samples\n            Input system values measured by the user.\n\n        Returns\n        -------\n        output_vector : array-like of shape = n_regressor\n            Vector with values of akaike's information criterion\n            for models with N terms (where N is the\n            vector position + 1).\n\n        \"\"\"\n        if self.n_info_values is not None and self.n_info_values &gt; X.shape[1]:\n            self.n_info_values = X.shape[1]\n            warnings.warn(\n                \"n_info_values is greater than the maximum number of all\"\n                \" regressors space considering the chosen y_lag, u_lag, and\"\n                f\" non_degree. We set as {X.shape[1]}\",\n                stacklevel=2,\n            )\n\n        output_vector = np.zeros(self.n_info_values)\n        output_vector[:] = np.nan\n\n        n_samples = len(y) - self.max_lag\n\n        for i in range(self.n_info_values):\n            n_theta = i + 1\n            regressor_matrix = self.error_reduction_ratio(X, y, n_theta)[2]\n\n            tmp_theta = self.estimator.optimize(\n                regressor_matrix, y[self.max_lag :, 0].reshape(-1, 1)\n            )\n\n            tmp_yhat = np.dot(regressor_matrix, tmp_theta)\n            tmp_residual = y[self.max_lag :] - tmp_yhat\n            e_var = np.var(tmp_residual, ddof=1)\n            output_vector[i] = self.info_criteria_function(n_theta, n_samples, e_var)\n\n        return output_vector\n\n    def get_info_criteria(self, info_criteria: str):\n        \"\"\"Get info criteria.\"\"\"\n        info_criteria_options = {\n            \"aic\": self.aic,\n            \"aicc\": self.aicc,\n            \"bic\": self.bic,\n            \"fpe\": self.fpe,\n            \"lilc\": self.lilc,\n        }\n        return info_criteria_options.get(info_criteria)\n\n    def bic(self, n_theta: int, n_samples: int, e_var: float) -&gt; float:\n        \"\"\"Compute the Bayesian information criteria value.\n\n        Parameters\n        ----------\n        n_theta : int\n            Number of parameters of the model.\n        n_samples : int\n            Number of samples given the maximum lag.\n        e_var : float\n            Variance of the residues\n\n        Returns\n        -------\n        info_criteria_value : float\n            The computed value given the information criteria selected by the\n            user.\n\n        \"\"\"\n        model_factor = n_theta * np.log(n_samples)\n        e_factor = n_samples * np.log(e_var)\n        info_criteria_value = e_factor + model_factor\n\n        return info_criteria_value\n\n    def aic(self, n_theta: int, n_samples: int, e_var: float) -&gt; float:\n        \"\"\"Compute the Akaike information criteria value.\n\n        Parameters\n        ----------\n        n_theta : int\n            Number of parameters of the model.\n        n_samples : int\n            Number of samples given the maximum lag.\n        e_var : float\n            Variance of the residues\n\n        Returns\n        -------\n        info_criteria_value : float\n            The computed value given the information criteria selected by the\n            user.\n\n        \"\"\"\n        model_factor = 2 * n_theta\n        e_factor = n_samples * np.log(e_var)\n        info_criteria_value = e_factor + model_factor\n\n        return info_criteria_value\n\n    def aicc(self, n_theta: int, n_samples: int, e_var: float) -&gt; float:\n        \"\"\"Compute the Akaike information Criteria corrected value.\n\n        Parameters\n        ----------\n        n_theta : int\n            Number of parameters of the model.\n        n_samples : int\n            Number of samples given the maximum lag.\n        e_var : float\n            Variance of the residues\n\n        Returns\n        -------\n        aicc : float\n            The computed aicc value.\n\n        References\n        ----------\n        - https://www.mathworks.com/help/ident/ref/idmodel.aic.html\n\n        \"\"\"\n        aic = self.aic(n_theta, n_samples, e_var)\n        aicc = aic + (2 * n_theta * (n_theta + 1) / (n_samples - n_theta - 1))\n\n        return aicc\n\n    def fpe(self, n_theta: int, n_samples: int, e_var: float) -&gt; float:\n        \"\"\"Compute the Final Error Prediction value.\n\n        Parameters\n        ----------\n        n_theta : int\n            Number of parameters of the model.\n        n_samples : int\n            Number of samples given the maximum lag.\n        e_var : float\n            Variance of the residues\n\n        Returns\n        -------\n        info_criteria_value : float\n            The computed value given the information criteria selected by the\n            user.\n\n        \"\"\"\n        model_factor = n_samples * np.log((n_samples + n_theta) / (n_samples - n_theta))\n        e_factor = n_samples * np.log(e_var)\n        info_criteria_value = e_factor + model_factor\n\n        return info_criteria_value\n\n    def lilc(self, n_theta: int, n_samples: int, e_var: float) -&gt; float:\n        \"\"\"Compute the Lilc information criteria value.\n\n        Parameters\n        ----------\n        n_theta : int\n            Number of parameters of the model.\n        n_samples : int\n            Number of samples given the maximum lag.\n        e_var : float\n            Variance of the residues\n\n        Returns\n        -------\n        info_criteria_value : float\n            The computed value given the information criteria selected by the\n            user.\n\n        \"\"\"\n        model_factor = 2 * n_theta * np.log(np.log(n_samples))\n        e_factor = n_samples * np.log(e_var)\n        info_criteria_value = e_factor + model_factor\n\n        return info_criteria_value\n\n    def get_min_info_value(self, info_values):\n        \"\"\"Find the index of the first increasing value in an array.\n\n        Parameters\n        ----------\n        info_values : array-like\n            A sequence of numeric values to be analyzed.\n\n        Returns\n        -------\n        int\n            The index of the first element where the values start to increase\n            monotonically. If no such element exists, the length of\n            `info_values` is returned.\n\n        Notes\n        -----\n        - The function assumes that `info_values` is a 1-dimensional array-like\n        structure.\n        - The function uses `np.diff` to compute the difference between consecutive\n        elements in the sequence.\n        - The function checks if any differences are positive, indicating an increase\n        in value.\n\n        Examples\n        --------\n        &gt;&gt;&gt; class MyClass:\n        ...     def __init__(self, values):\n        ...         self.info_values = values\n        ...     def get_min_info_value(self):\n        ...         is_monotonique = np.diff(self.info_values) &gt; 0\n        ...         if any(is_monotonique):\n        ...             return np.where(is_monotonique)[0][0] + 1\n        ...         return len(self.info_values)\n        &gt;&gt;&gt; instance = MyClass([3, 2, 1, 4, 5])\n        &gt;&gt;&gt; instance.get_min_info_value()\n        3\n        \"\"\"\n        is_monotonique = np.diff(info_values) &gt; 0\n        if any(is_monotonique):\n            return np.where(is_monotonique)[0][0] + 1\n        return len(info_values)\n\n    def fit(self, *, X: Optional[np.ndarray] = None, y: np.ndarray):\n        \"\"\"Fit polynomial NARMAX model.\n\n        This is an 'alpha' version of the 'fit' function which allows\n        a friendly usage by the user. Given two arguments, X and y, fit\n        training data.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the training process.\n        y : ndarray of floats\n            The output data to be used in the training process.\n\n        Returns\n        -------\n        model : ndarray of int\n            The model code representation.\n        piv : array-like of shape = number_of_model_elements\n            Contains the index to put the regressors in the correct order\n            based on err values.\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n        err : array-like of shape = number_of_model_elements\n            The respective ERR calculated for each regressor.\n        info_values : array-like of shape = n_regressor\n            Vector with values of akaike's information criterion\n            for models with N terms (where N is the\n            vector position + 1).\n\n        \"\"\"\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        self.max_lag = self._get_max_lag()\n        lagged_data = self.build_matrix(X, y)\n\n        reg_matrix = self.basis_function.fit(\n            lagged_data, self.max_lag, predefined_regressors=None\n        )\n\n        if X is not None:\n            self.n_inputs = _num_features(X)\n        else:\n            self.n_inputs = 1  # just to create the regressor space base\n\n        self.regressor_code = self.regressor_space(self.n_inputs)\n\n        if self.order_selection is True:\n            self.info_values = self.information_criterion(reg_matrix, y)\n\n        if self.n_terms is None and self.order_selection is True:\n            model_length = self.get_min_info_value(self.info_values)\n            self.n_terms = model_length\n        elif self.n_terms is None and self.order_selection is not True:\n            raise ValueError(\n                \"If order_selection is False, you must define n_terms value.\"\n            )\n        else:\n            model_length = self.n_terms\n\n        (self.err, self.pivv, psi) = self.error_reduction_ratio(\n            reg_matrix, y, model_length\n        )\n\n        tmp_piv = self.pivv[0:model_length]\n        repetition = len(reg_matrix)\n        if isinstance(self.basis_function, Polynomial):\n            self.final_model = self.regressor_code[tmp_piv, :].copy()\n        else:\n            self.regressor_code = np.sort(\n                np.tile(self.regressor_code[1:, :], (repetition, 1)),\n                axis=0,\n            )\n            self.final_model = self.regressor_code[tmp_piv, :].copy()\n\n        self.theta = self.estimator.optimize(psi, y[self.max_lag :, 0].reshape(-1, 1))\n        if self.estimator.unbiased is True:\n            self.theta = self.estimator.unbiased_estimator(\n                psi,\n                y[self.max_lag :, 0].reshape(-1, 1),\n                self.theta,\n                self.elag,\n                self.max_lag,\n                self.estimator,\n                self.basis_function,\n                self.estimator.uiter,\n            )\n        return self\n\n    def predict(\n        self,\n        *,\n        X: Optional[np.ndarray] = None,\n        y: np.ndarray,\n        steps_ahead: Optional[int] = None,\n        forecast_horizon: Optional[int] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Return the predicted values given an input.\n\n        The predict function allows a friendly usage by the user.\n        Given a previously trained model, predict values given\n        a new set of data.\n\n        This method accept y values mainly for prediction n-steps ahead\n        (to be implemented in the future)\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            The predicted values of the model.\n\n        \"\"\"\n        if self.basis_function.__class__.__name__ == \"Polynomial\":\n            if steps_ahead is None:\n                yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n            if steps_ahead == 1:\n                yhat = self._one_step_ahead_prediction(X, y)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n\n            _check_positive_int(steps_ahead, \"steps_ahead\")\n            yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        if steps_ahead is None:\n            yhat = self._basis_function_predict(X, y, forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        yhat = self._basis_function_n_step_prediction(\n            X, y, steps_ahead, forecast_horizon\n        )\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    def _one_step_ahead_prediction(\n        self, X: Optional[np.ndarray], y: Optional[np.ndarray]\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        lagged_data = self.build_matrix(X, y)\n\n        X_base = self.basis_function.transform(\n            lagged_data,\n            self.max_lag,\n            predefined_regressors=self.pivv[: len(self.final_model)],\n        )\n\n        yhat = super()._one_step_ahead_prediction(X_base)\n        return yhat.reshape(-1, 1)\n\n    def _n_step_ahead_prediction(\n        self, X: Optional[np.ndarray], y: Optional[np.ndarray], steps_ahead: int\n    ) -&gt; float:\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        yhat = super()._n_step_ahead_prediction(X, y, steps_ahead)\n        return yhat\n\n    def _model_prediction(\n        self,\n        X: Optional[np.ndarray],\n        y_initial: np.ndarray,\n        forecast_horizon: int = 0,\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the infinity steps-ahead simulation of a model.\n\n        Parameters\n        ----------\n        y_initial : array-like of shape = max_lag\n            Number of initial conditions values of output\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The predicted values of the model.\n\n        \"\"\"\n        if self.model_type in [\"NARMAX\", \"NAR\"]:\n            return self._narmax_predict(X, y_initial, forecast_horizon)\n\n        if self.model_type == \"NFIR\":\n            return self._nfir_predict(X, y_initial)\n\n        raise ValueError(\n            f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n        )\n\n    def _narmax_predict(\n        self,\n        X: Optional[np.ndarray],\n        y_initial: np.ndarray,\n        forecast_horizon: int = 0,\n    ) -&gt; np.ndarray:\n        if len(y_initial) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if X is not None:\n            forecast_horizon = X.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        y_output = super()._narmax_predict(X, y_initial, forecast_horizon)\n        return y_output\n\n    def _nfir_predict(\n        self, X: Optional[np.ndarray], y_initial: Optional[np.ndarray]\n    ) -&gt; np.ndarray:\n        y_output = super()._nfir_predict(X, y_initial)\n        return y_output\n\n    def _basis_function_predict(\n        self,\n        X: Optional[np.ndarray],\n        y_initial: Optional[np.ndarray],\n        forecast_horizon: int = 0,\n    ) -&gt; np.ndarray:\n        if X is not None:\n            forecast_horizon = X.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        yhat = super()._basis_function_predict(X, y_initial, forecast_horizon)\n        return yhat.reshape(-1, 1)\n\n    def _basis_function_n_step_prediction(\n        self,\n        X: Optional[np.ndarray],\n        y: np.ndarray,\n        steps_ahead: Optional[int],\n        forecast_horizon: int,\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if X is not None:\n            forecast_horizon = X.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        yhat = super()._basis_function_n_step_prediction(\n            X, y, steps_ahead, forecast_horizon\n        )\n        return yhat.reshape(-1, 1)\n\n    def _basis_function_n_steps_horizon(\n        self,\n        X: Optional[np.ndarray],\n        y: Optional[np.ndarray],\n        steps_ahead: Optional[int],\n        forecast_horizon: int,\n    ) -&gt; np.ndarray:\n        yhat = super()._basis_function_n_steps_horizon(\n            X, y, steps_ahead, forecast_horizon\n        )\n        return yhat.reshape(-1, 1)\n</code></pre>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.aic","title":"<code>aic(n_theta, n_samples, e_var)</code>","text":"<p>Compute the Akaike information criteria value.</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.aic--parameters","title":"Parameters","text":"<p>n_theta : int     Number of parameters of the model. n_samples : int     Number of samples given the maximum lag. e_var : float     Variance of the residues</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.aic--returns","title":"Returns","text":"<p>info_criteria_value : float     The computed value given the information criteria selected by the     user.</p> Source code in <code>sysidentpy/model_structure_selection/forward_regression_orthogonal_least_squares.py</code> <pre><code>def aic(self, n_theta: int, n_samples: int, e_var: float) -&gt; float:\n    \"\"\"Compute the Akaike information criteria value.\n\n    Parameters\n    ----------\n    n_theta : int\n        Number of parameters of the model.\n    n_samples : int\n        Number of samples given the maximum lag.\n    e_var : float\n        Variance of the residues\n\n    Returns\n    -------\n    info_criteria_value : float\n        The computed value given the information criteria selected by the\n        user.\n\n    \"\"\"\n    model_factor = 2 * n_theta\n    e_factor = n_samples * np.log(e_var)\n    info_criteria_value = e_factor + model_factor\n\n    return info_criteria_value\n</code></pre>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.aicc","title":"<code>aicc(n_theta, n_samples, e_var)</code>","text":"<p>Compute the Akaike information Criteria corrected value.</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.aicc--parameters","title":"Parameters","text":"<p>n_theta : int     Number of parameters of the model. n_samples : int     Number of samples given the maximum lag. e_var : float     Variance of the residues</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.aicc--returns","title":"Returns","text":"<p>aicc : float     The computed aicc value.</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.aicc--references","title":"References","text":"<ul> <li>https://www.mathworks.com/help/ident/ref/idmodel.aic.html</li> </ul> Source code in <code>sysidentpy/model_structure_selection/forward_regression_orthogonal_least_squares.py</code> <pre><code>def aicc(self, n_theta: int, n_samples: int, e_var: float) -&gt; float:\n    \"\"\"Compute the Akaike information Criteria corrected value.\n\n    Parameters\n    ----------\n    n_theta : int\n        Number of parameters of the model.\n    n_samples : int\n        Number of samples given the maximum lag.\n    e_var : float\n        Variance of the residues\n\n    Returns\n    -------\n    aicc : float\n        The computed aicc value.\n\n    References\n    ----------\n    - https://www.mathworks.com/help/ident/ref/idmodel.aic.html\n\n    \"\"\"\n    aic = self.aic(n_theta, n_samples, e_var)\n    aicc = aic + (2 * n_theta * (n_theta + 1) / (n_samples - n_theta - 1))\n\n    return aicc\n</code></pre>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.bic","title":"<code>bic(n_theta, n_samples, e_var)</code>","text":"<p>Compute the Bayesian information criteria value.</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.bic--parameters","title":"Parameters","text":"<p>n_theta : int     Number of parameters of the model. n_samples : int     Number of samples given the maximum lag. e_var : float     Variance of the residues</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.bic--returns","title":"Returns","text":"<p>info_criteria_value : float     The computed value given the information criteria selected by the     user.</p> Source code in <code>sysidentpy/model_structure_selection/forward_regression_orthogonal_least_squares.py</code> <pre><code>def bic(self, n_theta: int, n_samples: int, e_var: float) -&gt; float:\n    \"\"\"Compute the Bayesian information criteria value.\n\n    Parameters\n    ----------\n    n_theta : int\n        Number of parameters of the model.\n    n_samples : int\n        Number of samples given the maximum lag.\n    e_var : float\n        Variance of the residues\n\n    Returns\n    -------\n    info_criteria_value : float\n        The computed value given the information criteria selected by the\n        user.\n\n    \"\"\"\n    model_factor = n_theta * np.log(n_samples)\n    e_factor = n_samples * np.log(e_var)\n    info_criteria_value = e_factor + model_factor\n\n    return info_criteria_value\n</code></pre>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.error_reduction_ratio","title":"<code>error_reduction_ratio(psi, y, process_term_number)</code>","text":"<p>Perform the Error Reduction Ration algorithm.</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.error_reduction_ratio--parameters","title":"Parameters","text":"<p>y : array-like of shape = n_samples     The target data used in the identification process. psi : ndarray of floats     The information matrix of the model. process_term_number : int     Number of Process Terms defined by the user.</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.error_reduction_ratio--returns","title":"Returns","text":"<p>err : array-like of shape = number_of_model_elements     The respective ERR calculated for each regressor. piv : array-like of shape = number_of_model_elements     Contains the index to put the regressors in the correct order     based on err values. psi_orthogonal : ndarray of floats     The updated and orthogonal information matrix.</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.error_reduction_ratio--references","title":"References","text":"<ul> <li>Manuscript: Orthogonal least squares methods and their application    to non-linear system identification    https://eprints.soton.ac.uk/251147/1/778742007_content.pdf</li> <li>Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares    Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o    e Novos Resultados</li> </ul> Source code in <code>sysidentpy/model_structure_selection/forward_regression_orthogonal_least_squares.py</code> <pre><code>def error_reduction_ratio(\n    self, psi: np.ndarray, y: np.ndarray, process_term_number: int\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Perform the Error Reduction Ration algorithm.\n\n    Parameters\n    ----------\n    y : array-like of shape = n_samples\n        The target data used in the identification process.\n    psi : ndarray of floats\n        The information matrix of the model.\n    process_term_number : int\n        Number of Process Terms defined by the user.\n\n    Returns\n    -------\n    err : array-like of shape = number_of_model_elements\n        The respective ERR calculated for each regressor.\n    piv : array-like of shape = number_of_model_elements\n        Contains the index to put the regressors in the correct order\n        based on err values.\n    psi_orthogonal : ndarray of floats\n        The updated and orthogonal information matrix.\n\n    References\n    ----------\n    - Manuscript: Orthogonal least squares methods and their application\n       to non-linear system identification\n       https://eprints.soton.ac.uk/251147/1/778742007_content.pdf\n    - Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares\n       Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o\n       e Novos Resultados\n\n    \"\"\"\n    squared_y = np.dot(y[self.max_lag :].T, y[self.max_lag :])\n    tmp_psi = psi.copy()\n    y = y[self.max_lag :, 0].reshape(-1, 1)\n    tmp_y = y.copy()\n    dimension = tmp_psi.shape[1]\n    piv = np.arange(dimension)\n    tmp_err = np.zeros(dimension)\n    err = np.zeros(dimension)\n\n    for i in np.arange(0, dimension):\n        for j in np.arange(i, dimension):\n            # Add `eps` in the denominator to omit division by zero if\n            # denominator is zero\n            # To implement regularized regression (ridge regression), add\n            # alpha to psi.T @ psi.   See S. Chen, Local regularization assisted\n            # orthogonal least squares regression, Neurocomputing 69 (2006) 559-585.\n            # The version implemented below uses the same regularization for every\n            # feature, # What Chen refers to Uniform regularized orthogonal least\n            # squares (UROLS) Set to tiny (self.eps) when you are not regularizing.\n            # alpha = eps is the default.\n            tmp_err[j] = (\n                (np.dot(tmp_psi[i:, j].T, tmp_y[i:]) ** 2)\n                / (\n                    (np.dot(tmp_psi[i:, j].T, tmp_psi[i:, j]) + self.alpha)\n                    * squared_y\n                )\n                + self.eps\n            )[0, 0]\n\n        piv_index = np.argmax(tmp_err[i:]) + i\n        err[i] = tmp_err[piv_index]\n        if i == process_term_number:\n            break\n\n        if (self.err_tol is not None) and (err.cumsum()[i] &gt;= self.err_tol):\n            self.n_terms = i + 1\n            process_term_number = i + 1\n            break\n\n        tmp_psi[:, [piv_index, i]] = tmp_psi[:, [i, piv_index]]\n        piv[[piv_index, i]] = piv[[i, piv_index]]\n        v = Orthogonalization().house(tmp_psi[i:, i])\n        row_result = Orthogonalization().rowhouse(tmp_psi[i:, i:], v)\n        tmp_y[i:] = Orthogonalization().rowhouse(tmp_y[i:], v)\n        tmp_psi[i:, i:] = np.copy(row_result)\n\n    tmp_piv = piv[0:process_term_number]\n    psi_orthogonal = psi[:, tmp_piv]\n    return err, tmp_piv, psi_orthogonal\n</code></pre>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.fit","title":"<code>fit(*, X=None, y)</code>","text":"<p>Fit polynomial NARMAX model.</p> <p>This is an 'alpha' version of the 'fit' function which allows a friendly usage by the user. Given two arguments, X and y, fit training data.</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.fit--parameters","title":"Parameters","text":"<p>X : ndarray of floats     The input data to be used in the training process. y : ndarray of floats     The output data to be used in the training process.</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.fit--returns","title":"Returns","text":"<p>model : ndarray of int     The model code representation. piv : array-like of shape = number_of_model_elements     Contains the index to put the regressors in the correct order     based on err values. theta : array-like of shape = number_of_model_elements     The estimated parameters of the model. err : array-like of shape = number_of_model_elements     The respective ERR calculated for each regressor. info_values : array-like of shape = n_regressor     Vector with values of akaike's information criterion     for models with N terms (where N is the     vector position + 1).</p> Source code in <code>sysidentpy/model_structure_selection/forward_regression_orthogonal_least_squares.py</code> <pre><code>def fit(self, *, X: Optional[np.ndarray] = None, y: np.ndarray):\n    \"\"\"Fit polynomial NARMAX model.\n\n    This is an 'alpha' version of the 'fit' function which allows\n    a friendly usage by the user. Given two arguments, X and y, fit\n    training data.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the training process.\n    y : ndarray of floats\n        The output data to be used in the training process.\n\n    Returns\n    -------\n    model : ndarray of int\n        The model code representation.\n    piv : array-like of shape = number_of_model_elements\n        Contains the index to put the regressors in the correct order\n        based on err values.\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n    err : array-like of shape = number_of_model_elements\n        The respective ERR calculated for each regressor.\n    info_values : array-like of shape = n_regressor\n        Vector with values of akaike's information criterion\n        for models with N terms (where N is the\n        vector position + 1).\n\n    \"\"\"\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    self.max_lag = self._get_max_lag()\n    lagged_data = self.build_matrix(X, y)\n\n    reg_matrix = self.basis_function.fit(\n        lagged_data, self.max_lag, predefined_regressors=None\n    )\n\n    if X is not None:\n        self.n_inputs = _num_features(X)\n    else:\n        self.n_inputs = 1  # just to create the regressor space base\n\n    self.regressor_code = self.regressor_space(self.n_inputs)\n\n    if self.order_selection is True:\n        self.info_values = self.information_criterion(reg_matrix, y)\n\n    if self.n_terms is None and self.order_selection is True:\n        model_length = self.get_min_info_value(self.info_values)\n        self.n_terms = model_length\n    elif self.n_terms is None and self.order_selection is not True:\n        raise ValueError(\n            \"If order_selection is False, you must define n_terms value.\"\n        )\n    else:\n        model_length = self.n_terms\n\n    (self.err, self.pivv, psi) = self.error_reduction_ratio(\n        reg_matrix, y, model_length\n    )\n\n    tmp_piv = self.pivv[0:model_length]\n    repetition = len(reg_matrix)\n    if isinstance(self.basis_function, Polynomial):\n        self.final_model = self.regressor_code[tmp_piv, :].copy()\n    else:\n        self.regressor_code = np.sort(\n            np.tile(self.regressor_code[1:, :], (repetition, 1)),\n            axis=0,\n        )\n        self.final_model = self.regressor_code[tmp_piv, :].copy()\n\n    self.theta = self.estimator.optimize(psi, y[self.max_lag :, 0].reshape(-1, 1))\n    if self.estimator.unbiased is True:\n        self.theta = self.estimator.unbiased_estimator(\n            psi,\n            y[self.max_lag :, 0].reshape(-1, 1),\n            self.theta,\n            self.elag,\n            self.max_lag,\n            self.estimator,\n            self.basis_function,\n            self.estimator.uiter,\n        )\n    return self\n</code></pre>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.fpe","title":"<code>fpe(n_theta, n_samples, e_var)</code>","text":"<p>Compute the Final Error Prediction value.</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.fpe--parameters","title":"Parameters","text":"<p>n_theta : int     Number of parameters of the model. n_samples : int     Number of samples given the maximum lag. e_var : float     Variance of the residues</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.fpe--returns","title":"Returns","text":"<p>info_criteria_value : float     The computed value given the information criteria selected by the     user.</p> Source code in <code>sysidentpy/model_structure_selection/forward_regression_orthogonal_least_squares.py</code> <pre><code>def fpe(self, n_theta: int, n_samples: int, e_var: float) -&gt; float:\n    \"\"\"Compute the Final Error Prediction value.\n\n    Parameters\n    ----------\n    n_theta : int\n        Number of parameters of the model.\n    n_samples : int\n        Number of samples given the maximum lag.\n    e_var : float\n        Variance of the residues\n\n    Returns\n    -------\n    info_criteria_value : float\n        The computed value given the information criteria selected by the\n        user.\n\n    \"\"\"\n    model_factor = n_samples * np.log((n_samples + n_theta) / (n_samples - n_theta))\n    e_factor = n_samples * np.log(e_var)\n    info_criteria_value = e_factor + model_factor\n\n    return info_criteria_value\n</code></pre>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.get_info_criteria","title":"<code>get_info_criteria(info_criteria)</code>","text":"<p>Get info criteria.</p> Source code in <code>sysidentpy/model_structure_selection/forward_regression_orthogonal_least_squares.py</code> <pre><code>def get_info_criteria(self, info_criteria: str):\n    \"\"\"Get info criteria.\"\"\"\n    info_criteria_options = {\n        \"aic\": self.aic,\n        \"aicc\": self.aicc,\n        \"bic\": self.bic,\n        \"fpe\": self.fpe,\n        \"lilc\": self.lilc,\n    }\n    return info_criteria_options.get(info_criteria)\n</code></pre>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.get_min_info_value","title":"<code>get_min_info_value(info_values)</code>","text":"<p>Find the index of the first increasing value in an array.</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.get_min_info_value--parameters","title":"Parameters","text":"<p>info_values : array-like     A sequence of numeric values to be analyzed.</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.get_min_info_value--returns","title":"Returns","text":"<p>int     The index of the first element where the values start to increase     monotonically. If no such element exists, the length of     <code>info_values</code> is returned.</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.get_min_info_value--notes","title":"Notes","text":"<ul> <li>The function assumes that <code>info_values</code> is a 1-dimensional array-like structure.</li> <li>The function uses <code>np.diff</code> to compute the difference between consecutive elements in the sequence.</li> <li>The function checks if any differences are positive, indicating an increase in value.</li> </ul>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.get_min_info_value--examples","title":"Examples","text":"<p>class MyClass: ...     def init(self, values): ...         self.info_values = values ...     def get_min_info_value(self): ...         is_monotonique = np.diff(self.info_values) &gt; 0 ...         if any(is_monotonique): ...             return np.where(is_monotonique)0 + 1 ...         return len(self.info_values) instance = MyClass([3, 2, 1, 4, 5]) instance.get_min_info_value() 3</p> Source code in <code>sysidentpy/model_structure_selection/forward_regression_orthogonal_least_squares.py</code> <pre><code>def get_min_info_value(self, info_values):\n    \"\"\"Find the index of the first increasing value in an array.\n\n    Parameters\n    ----------\n    info_values : array-like\n        A sequence of numeric values to be analyzed.\n\n    Returns\n    -------\n    int\n        The index of the first element where the values start to increase\n        monotonically. If no such element exists, the length of\n        `info_values` is returned.\n\n    Notes\n    -----\n    - The function assumes that `info_values` is a 1-dimensional array-like\n    structure.\n    - The function uses `np.diff` to compute the difference between consecutive\n    elements in the sequence.\n    - The function checks if any differences are positive, indicating an increase\n    in value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; class MyClass:\n    ...     def __init__(self, values):\n    ...         self.info_values = values\n    ...     def get_min_info_value(self):\n    ...         is_monotonique = np.diff(self.info_values) &gt; 0\n    ...         if any(is_monotonique):\n    ...             return np.where(is_monotonique)[0][0] + 1\n    ...         return len(self.info_values)\n    &gt;&gt;&gt; instance = MyClass([3, 2, 1, 4, 5])\n    &gt;&gt;&gt; instance.get_min_info_value()\n    3\n    \"\"\"\n    is_monotonique = np.diff(info_values) &gt; 0\n    if any(is_monotonique):\n        return np.where(is_monotonique)[0][0] + 1\n    return len(info_values)\n</code></pre>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.information_criterion","title":"<code>information_criterion(X, y)</code>","text":"<p>Determine the model order.</p> <p>This function uses a information criterion to determine the model size. 'Akaike'-  Akaike's Information Criterion with            critical value 2 (AIC) (default). 'Bayes' -  Bayes Information Criterion (BIC). 'FPE'   -  Final Prediction Error (FPE). 'LILC'  -  Khundrin's law ofiterated logarithm criterion (LILC).</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.information_criterion--parameters","title":"Parameters","text":"<p>y : array-like of shape = n_samples     Target values of the system. X : array-like of shape = n_samples     Input system values measured by the user.</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.information_criterion--returns","title":"Returns","text":"<p>output_vector : array-like of shape = n_regressor     Vector with values of akaike's information criterion     for models with N terms (where N is the     vector position + 1).</p> Source code in <code>sysidentpy/model_structure_selection/forward_regression_orthogonal_least_squares.py</code> <pre><code>def information_criterion(self, X: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Determine the model order.\n\n    This function uses a information criterion to determine the model size.\n    'Akaike'-  Akaike's Information Criterion with\n               critical value 2 (AIC) (default).\n    'Bayes' -  Bayes Information Criterion (BIC).\n    'FPE'   -  Final Prediction Error (FPE).\n    'LILC'  -  Khundrin's law ofiterated logarithm criterion (LILC).\n\n    Parameters\n    ----------\n    y : array-like of shape = n_samples\n        Target values of the system.\n    X : array-like of shape = n_samples\n        Input system values measured by the user.\n\n    Returns\n    -------\n    output_vector : array-like of shape = n_regressor\n        Vector with values of akaike's information criterion\n        for models with N terms (where N is the\n        vector position + 1).\n\n    \"\"\"\n    if self.n_info_values is not None and self.n_info_values &gt; X.shape[1]:\n        self.n_info_values = X.shape[1]\n        warnings.warn(\n            \"n_info_values is greater than the maximum number of all\"\n            \" regressors space considering the chosen y_lag, u_lag, and\"\n            f\" non_degree. We set as {X.shape[1]}\",\n            stacklevel=2,\n        )\n\n    output_vector = np.zeros(self.n_info_values)\n    output_vector[:] = np.nan\n\n    n_samples = len(y) - self.max_lag\n\n    for i in range(self.n_info_values):\n        n_theta = i + 1\n        regressor_matrix = self.error_reduction_ratio(X, y, n_theta)[2]\n\n        tmp_theta = self.estimator.optimize(\n            regressor_matrix, y[self.max_lag :, 0].reshape(-1, 1)\n        )\n\n        tmp_yhat = np.dot(regressor_matrix, tmp_theta)\n        tmp_residual = y[self.max_lag :] - tmp_yhat\n        e_var = np.var(tmp_residual, ddof=1)\n        output_vector[i] = self.info_criteria_function(n_theta, n_samples, e_var)\n\n    return output_vector\n</code></pre>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.lilc","title":"<code>lilc(n_theta, n_samples, e_var)</code>","text":"<p>Compute the Lilc information criteria value.</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.lilc--parameters","title":"Parameters","text":"<p>n_theta : int     Number of parameters of the model. n_samples : int     Number of samples given the maximum lag. e_var : float     Variance of the residues</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.lilc--returns","title":"Returns","text":"<p>info_criteria_value : float     The computed value given the information criteria selected by the     user.</p> Source code in <code>sysidentpy/model_structure_selection/forward_regression_orthogonal_least_squares.py</code> <pre><code>def lilc(self, n_theta: int, n_samples: int, e_var: float) -&gt; float:\n    \"\"\"Compute the Lilc information criteria value.\n\n    Parameters\n    ----------\n    n_theta : int\n        Number of parameters of the model.\n    n_samples : int\n        Number of samples given the maximum lag.\n    e_var : float\n        Variance of the residues\n\n    Returns\n    -------\n    info_criteria_value : float\n        The computed value given the information criteria selected by the\n        user.\n\n    \"\"\"\n    model_factor = 2 * n_theta * np.log(np.log(n_samples))\n    e_factor = n_samples * np.log(e_var)\n    info_criteria_value = e_factor + model_factor\n\n    return info_criteria_value\n</code></pre>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.predict","title":"<code>predict(*, X=None, y, steps_ahead=None, forecast_horizon=None)</code>","text":"<p>Return the predicted values given an input.</p> <p>The predict function allows a friendly usage by the user. Given a previously trained model, predict values given a new set of data.</p> <p>This method accept y values mainly for prediction n-steps ahead (to be implemented in the future)</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.predict--parameters","title":"Parameters","text":"<p>X : ndarray of floats     The input data to be used in the prediction process. y : ndarray of floats     The output data to be used in the prediction process. steps_ahead : int (default = None)     The user can use free run simulation, one-step ahead prediction     and n-step ahead prediction. forecast_horizon : int, default=None     The number of predictions over the time.</p>"},{"location":"code/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.predict--returns","title":"Returns","text":"<p>yhat : ndarray of floats     The predicted values of the model.</p> Source code in <code>sysidentpy/model_structure_selection/forward_regression_orthogonal_least_squares.py</code> <pre><code>def predict(\n    self,\n    *,\n    X: Optional[np.ndarray] = None,\n    y: np.ndarray,\n    steps_ahead: Optional[int] = None,\n    forecast_horizon: Optional[int] = None,\n) -&gt; np.ndarray:\n    \"\"\"Return the predicted values given an input.\n\n    The predict function allows a friendly usage by the user.\n    Given a previously trained model, predict values given\n    a new set of data.\n\n    This method accept y values mainly for prediction n-steps ahead\n    (to be implemented in the future)\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    steps_ahead : int (default = None)\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction.\n    forecast_horizon : int, default=None\n        The number of predictions over the time.\n\n    Returns\n    -------\n    yhat : ndarray of floats\n        The predicted values of the model.\n\n    \"\"\"\n    if self.basis_function.__class__.__name__ == \"Polynomial\":\n        if steps_ahead is None:\n            yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        _check_positive_int(steps_ahead, \"steps_ahead\")\n        yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    if steps_ahead is None:\n        yhat = self._basis_function_predict(X, y, forecast_horizon)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n    if steps_ahead == 1:\n        yhat = self._one_step_ahead_prediction(X, y)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    yhat = self._basis_function_n_step_prediction(\n        X, y, steps_ahead, forecast_horizon\n    )\n    yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n    return yhat\n</code></pre>"},{"location":"code/general-estimators/","title":"Documentation for <code>General Estimators</code>","text":"<p>Build NARX Models Using general estimators.</p>"},{"location":"code/general-estimators/#sysidentpy.general_estimators.narx.NARX","title":"<code>NARX</code>","text":"<p>               Bases: <code>BaseMSS</code></p> <p>NARX model build on top of general estimators.</p> <p>Currently is possible to use any estimator that have a fit/predict as an Autoregressive Model. We use our GenerateRegressors and InformationMatrix classes to handle the creation of the lagged features and we are able to use a simple fit and prediction function to run infinity-steps-ahead prediction.</p>"},{"location":"code/general-estimators/#sysidentpy.general_estimators.narx.NARX--parameters","title":"Parameters","text":"<p>ylag : int, default=2     The maximum lag of the output. xlag : int, default=2     The maximum lag of the input. fit_params : dict, default=None     Optional parameters of the fit function of the baseline estimator base_estimator : default=None     The defined base estimator of the sklearn</p>"},{"location":"code/general-estimators/#sysidentpy.general_estimators.narx.NARX--examples","title":"Examples","text":"<p>import numpy as np import pandas as pd import matplotlib.pyplot as plt from sysidentpy.metrics import mean_squared_error from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.general_estimators import NARX from sklearn.linear_model import BayesianRidge from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import( ...    compute_residues_autocorrelation, ...    compute_cross_correlation ... ) from sklearn.linear_model import BayesianRidge # to use as base estimator x_train, x_valid, y_train, y_valid = get_siso_data( ...    n=1000, ...    colored_noise=False, ...    sigma=0.01, ...    train_percentage=80 ... ) BayesianRidge_narx = NARX( ...     base_estimator=BayesianRidge(), ...     xlag=2, ...     ylag=2, ...     basis_function=basis_function, ...     model_type=\"NARMAX\", ... ) BayesianRidge_narx.fit(x_train, y_train) yhat = BayesianRidge_narx.predict(x_valid, y_valid) print(\"MSE: \", mean_squared_error(y_valid, yhat)) plot_results(y=y_valid, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"\\(e^2\\)\") x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"\\(x_1e\\)\") 0.000131</p> Source code in <code>sysidentpy/general_estimators/narx.py</code> <pre><code>class NARX(BaseMSS):\n    \"\"\"NARX model build on top of general estimators.\n\n    Currently is possible to use any estimator that have a fit/predict\n    as an Autoregressive Model. We use our GenerateRegressors and\n    InformationMatrix classes to handle the creation of the lagged\n    features and we are able to use a simple fit and prediction function\n    to run infinity-steps-ahead prediction.\n\n    Parameters\n    ----------\n    ylag : int, default=2\n        The maximum lag of the output.\n    xlag : int, default=2\n        The maximum lag of the input.\n    fit_params : dict, default=None\n        Optional parameters of the fit function of the baseline estimator\n    base_estimator : default=None\n        The defined base estimator of the sklearn\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from sysidentpy.metrics import mean_squared_error\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_siso_data\n    &gt;&gt;&gt; from sysidentpy.general_estimators import NARX\n    &gt;&gt;&gt; from sklearn.linear_model import BayesianRidge\n    &gt;&gt;&gt; from sysidentpy.basis_function._basis_function import Polynomial\n    &gt;&gt;&gt; from sysidentpy.utils.display_results import results\n    &gt;&gt;&gt; from sysidentpy.utils.plotting import plot_residues_correlation, plot_results\n    &gt;&gt;&gt; from sysidentpy.residues.residues_correlation import(\n    ...    compute_residues_autocorrelation,\n    ...    compute_cross_correlation\n    ... )\n    &gt;&gt;&gt; from sklearn.linear_model import BayesianRidge # to use as base estimator\n    &gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(\n    ...    n=1000,\n    ...    colored_noise=False,\n    ...    sigma=0.01,\n    ...    train_percentage=80\n    ... )\n    &gt;&gt;&gt; BayesianRidge_narx = NARX(\n    ...     base_estimator=BayesianRidge(),\n    ...     xlag=2,\n    ...     ylag=2,\n    ...     basis_function=basis_function,\n    ...     model_type=\"NARMAX\",\n    ... )\n    &gt;&gt;&gt; BayesianRidge_narx.fit(x_train, y_train)\n    &gt;&gt;&gt; yhat = BayesianRidge_narx.predict(x_valid, y_valid)\n    &gt;&gt;&gt; print(\"MSE: \", mean_squared_error(y_valid, yhat))\n    &gt;&gt;&gt; plot_results(y=y_valid, yhat=yhat, n=1000)\n    &gt;&gt;&gt; ee = compute_residues_autocorrelation(y_valid, yhat)\n    &gt;&gt;&gt; plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\n    &gt;&gt;&gt; x1e = compute_cross_correlation(y_valid, yhat, x_valid)\n    &gt;&gt;&gt; plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n    0.000131\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        ylag: Union[List[Any], Any] = 1,\n        xlag: Union[List[Any], Any] = 1,\n        model_type: str = \"NARMAX\",\n        basis_function: Union[Polynomial, Fourier] = Polynomial(),\n        base_estimator=None,\n        fit_params=None,\n    ):\n        self.basis_function = basis_function\n        self.model_type = model_type\n        self.build_matrix = self.get_build_io_method(model_type)\n        self.non_degree = basis_function.degree\n        self.ylag = ylag\n        self.xlag = xlag\n        self.max_lag = self._get_max_lag()\n        self.base_estimator = base_estimator\n        if fit_params is None:\n            fit_params = {}\n\n        self.fit_params = fit_params\n        self.ensemble = None\n        self.n_inputs = None\n        self.regressor_code = None\n        self._validate_params()\n\n    def _validate_params(self):\n        \"\"\"Validate input params.\"\"\"\n        if isinstance(self.ylag, int) and self.ylag &lt; 1:\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if isinstance(self.xlag, int) and self.xlag &lt; 1:\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.xlag, (int, list)):\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.ylag, (int, list)):\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if self.model_type not in [\"NARMAX\", \"NAR\", \"NFIR\"]:\n            raise ValueError(\n                f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n            )\n\n    def fit(self, *, X=None, y=None):\n        \"\"\"Train a NARX Neural Network model.\n\n        This is an training pipeline that allows a friendly usage\n        by the user. All the lagged features are built using the\n        SysIdentPy classes and we use the fit method of the base\n        estimator of the sklearn to fit the model.\n\n        Parameters\n        ----------\n        X : ndarrays of floats\n            The input data to be used in the training process.\n        y : ndarrays of floats\n            The output data to be used in the training process.\n\n        Returns\n        -------\n        base_estimator : sklearn estimator\n            The model fitted.\n\n        \"\"\"\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        self.max_lag = self._get_max_lag()\n        lagged_data = self.build_matrix(X, y)\n        reg_matrix = self.basis_function.fit(\n            lagged_data, self.max_lag, predefined_regressors=None\n        )\n\n        if X is not None:\n            self.n_inputs = _num_features(X)\n        else:\n            self.n_inputs = 1  # just to create the regressor space base\n\n        self.regressor_code = self.regressor_space(self.n_inputs)\n        self.final_model = self.regressor_code\n        y = y[self.max_lag :].ravel()\n\n        self.base_estimator.fit(reg_matrix, y, **self.fit_params)\n        return self\n\n    def predict(\n        self,\n        *,\n        X: Optional[NDArray] = None,\n        y: Optional[NDArray] = None,\n        steps_ahead: Optional[int] = None,\n        forecast_horizon: Optional[int] = 1,\n    ) -&gt; NDArray:\n        \"\"\"Return the predicted given an input and initial values.\n\n        The predict function allows a friendly usage by the user.\n        Given a trained model, predict values given\n        a new set of data.\n\n        This method accept y values mainly for prediction n-steps ahead\n        (to be implemented in the future).\n\n        Currently we only support infinity-steps-ahead prediction,\n        but run 1-step-ahead prediction manually is straightforward.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            The predicted values of the model.\n\n        \"\"\"\n        if self.basis_function.__class__.__name__ == \"Polynomial\":\n            if steps_ahead is None:\n                yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n\n            if steps_ahead == 1:\n                yhat = self._one_step_ahead_prediction(X, y)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n\n            _check_positive_int(steps_ahead, \"steps_ahead\")\n            yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        if steps_ahead is None:\n            yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        yhat = self._basis_function_n_step_prediction(\n            X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n        )\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    def _one_step_ahead_prediction(self, X, y):\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        lagged_data = self.build_matrix(X, y)\n        X_base = self.basis_function.transform(\n            lagged_data,\n            self.max_lag,\n            # predefined_regressors=self.pivv[: len(self.final_model)],\n        )\n\n        yhat = self.base_estimator.predict(X_base)\n        # yhat = np.concatenate([y[: self.max_lag].flatten(), yhat])  # delete this one\n        return yhat.reshape(-1, 1)\n\n    def _nar_step_ahead(self, y, steps_ahead):\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        to_remove = int(np.ceil((len(y) - self.max_lag) / steps_ahead))\n        yhat = np.zeros(len(y) + steps_ahead, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n        i = self.max_lag\n\n        steps = [step for step in range(0, to_remove * steps_ahead, steps_ahead)]\n        if len(steps) &gt; 1:\n            for step in steps[:-1]:\n                yhat[i : i + steps_ahead] = self._model_prediction(\n                    X=None, y_initial=y[step:i], forecast_horizon=steps_ahead\n                )[-steps_ahead:].ravel()\n                i += steps_ahead\n\n            steps_ahead = np.sum(np.isnan(yhat))\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                X=None, y_initial=y[steps[-1] : i]\n            )[-steps_ahead:].ravel()\n        else:\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                X=None, y_initial=y[0:i], forecast_horizon=steps_ahead\n            )[-steps_ahead:].ravel()\n\n        yhat = yhat.ravel()[self.max_lag : :]\n        return yhat.reshape(-1, 1)\n\n    def narmax_n_step_ahead(self, X, y, steps_ahead):\n        \"\"\"N steps ahead prediction method for NARMAX model.\"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        to_remove = int(np.ceil((len(y) - self.max_lag) / steps_ahead))\n        X = X.reshape(-1, self.n_inputs)\n        yhat = np.zeros(X.shape[0], dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n        i = self.max_lag\n        steps = [step for step in range(0, to_remove * steps_ahead, steps_ahead)]\n        if len(steps) &gt; 1:\n            for step in steps[:-1]:\n                yhat[i : i + steps_ahead] = self._model_prediction(\n                    X=X[step : i + steps_ahead],\n                    y_initial=y[step:i],\n                )[-steps_ahead:].ravel()\n                i += steps_ahead\n\n            steps_ahead = np.sum(np.isnan(yhat))\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                X=X[steps[-1] : i + steps_ahead],\n                y_initial=y[steps[-1] : i],\n            )[-steps_ahead:].ravel()\n        else:\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                X=X[0 : i + steps_ahead],\n                y_initial=y[0:i],\n            )[-steps_ahead:].ravel()\n\n        yhat = yhat.ravel()[self.max_lag : :]\n        return yhat.reshape(-1, 1)\n\n    def _n_step_ahead_prediction(self, X, y, steps_ahead):\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        if self.model_type == \"NARMAX\":\n            return self.narmax_n_step_ahead(X, y, steps_ahead)\n\n        if self.model_type == \"NAR\":\n            return self._nar_step_ahead(y, steps_ahead)\n\n    def _model_prediction(self, X, y_initial, forecast_horizon=None):\n        \"\"\"Perform the infinity steps-ahead simulation of a model.\n\n        Parameters\n        ----------\n        y_initial : array-like of shape = max_lag\n            Number of initial conditions values of output\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The predicted values of the model.\n\n        \"\"\"\n        if self.model_type in [\"NARMAX\", \"NAR\"]:\n            return self._narmax_predict(X, y_initial, forecast_horizon)\n        if self.model_type == \"NFIR\":\n            return self._nfir_predict(X, y_initial)\n\n        raise ValueError(\n            f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n        )\n\n    def _narmax_predict(self, X, y_initial, forecast_horizon):\n        if len(y_initial) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if X is not None:\n            forecast_horizon = X.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        y_output = np.zeros(forecast_horizon, dtype=float)\n        y_output.fill(np.nan)\n        y_output[: self.max_lag] = y_initial[: self.max_lag, 0]\n\n        model_exponents = [\n            self._code2exponents(code=model) for model in self.final_model\n        ]\n        raw_regressor = np.zeros(len(model_exponents[0]), dtype=float)\n        for i in range(self.max_lag, forecast_horizon):\n            init = 0\n            final = self.max_lag\n            k = int(i - self.max_lag)\n            raw_regressor[:final] = y_output[k:i]\n            for j in range(self.n_inputs):\n                init += self.max_lag\n                final += self.max_lag\n                raw_regressor[init:final] = X[k:i, j]\n\n            regressor_value = np.zeros(len(model_exponents))\n            for j, model_exponent in enumerate(model_exponents):\n                regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\n\n            y_output[i] = self.base_estimator.predict(regressor_value.reshape(1, -1))[0]\n        return y_output[self.max_lag : :].reshape(-1, 1)\n\n    def _nfir_predict(self, X, y_initial):\n        y_output = np.zeros(X.shape[0], dtype=float)\n        y_output.fill(np.nan)\n        y_output[: self.max_lag] = y_initial[: self.max_lag, 0]\n        X = X.reshape(-1, self.n_inputs)\n        model_exponents = [\n            self._code2exponents(code=model) for model in self.final_model\n        ]\n        raw_regressor = np.zeros(len(model_exponents[0]), dtype=float)\n        for i in range(self.max_lag, X.shape[0]):\n            init = 0\n            final = self.max_lag\n            k = int(i - self.max_lag)\n            raw_regressor[:final] = y_output[k:i]\n            for j in range(self.n_inputs):\n                init += self.max_lag\n                final += self.max_lag\n                raw_regressor[init:final] = X[k:i, j]\n\n            regressor_value = np.zeros(len(model_exponents))\n            for j, model_exponent in enumerate(model_exponents):\n                regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\n\n            y_output[i] = self.base_estimator.predict(regressor_value.reshape(1, -1))[0]\n        return y_output[self.max_lag : :].reshape(-1, 1)\n\n    def _basis_function_predict(self, X, y_initial, forecast_horizon=None):\n        if X is not None:\n            forecast_horizon = X.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y_initial[: self.max_lag, 0]\n\n        analyzed_elements_number = self.max_lag + 1\n\n        for i in range(forecast_horizon - self.max_lag):\n            lagged_data = self.build_matrix(\n                X[i : i + analyzed_elements_number],\n                yhat[i : i + analyzed_elements_number].reshape(-1, 1),\n            )\n            X_tmp = self.basis_function.transform(\n                lagged_data,\n                self.max_lag,\n                # predefined_regressors=self.pivv[: len(self.final_model)],\n            )\n\n            a = self.base_estimator.predict(X_tmp)\n            yhat[i + self.max_lag] = a[0]\n\n        return yhat[self.max_lag :].reshape(-1, 1)\n\n    def _basis_function_n_step_prediction(self, X, y, steps_ahead, forecast_horizon):\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if X is not None:\n            forecast_horizon = X.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n\n        i = self.max_lag\n\n        while i &lt; len(y):\n            k = int(i - self.max_lag)\n            if i + steps_ahead &gt; len(y):\n                steps_ahead = len(y) - i  # predicts the remaining values\n\n            if self.model_type == \"NARMAX\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    X[k : i + steps_ahead],\n                    y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-steps_ahead:].ravel()\n            elif self.model_type == \"NAR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    X=None,\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NFIR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    X=X[k : i + steps_ahead],\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-steps_ahead:].ravel()\n            else:\n                raise ValueError(\n                    f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n                )\n\n            i += steps_ahead\n\n        return yhat[self.max_lag : :].reshape(-1, 1)\n\n    def _basis_function_n_steps_horizon(self, X, y, steps_ahead, forecast_horizon):\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n\n        i = self.max_lag\n\n        while i &lt; len(y):\n            k = int(i - self.max_lag)\n            if i + steps_ahead &gt; len(y):\n                steps_ahead = len(y) - i  # predicts the remaining values\n\n            if self.model_type == \"NARMAX\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    X[k : i + steps_ahead],\n                    y[k : i + steps_ahead],\n                    forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NAR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    X=None,\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NFIR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    X=X[k : i + steps_ahead],\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            else:\n                raise ValueError(\n                    f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n                )\n\n            i += steps_ahead\n\n        yhat = yhat.ravel()\n        return yhat[self.max_lag : :].reshape(-1, 1)\n</code></pre>"},{"location":"code/general-estimators/#sysidentpy.general_estimators.narx.NARX.fit","title":"<code>fit(*, X=None, y=None)</code>","text":"<p>Train a NARX Neural Network model.</p> <p>This is an training pipeline that allows a friendly usage by the user. All the lagged features are built using the SysIdentPy classes and we use the fit method of the base estimator of the sklearn to fit the model.</p>"},{"location":"code/general-estimators/#sysidentpy.general_estimators.narx.NARX.fit--parameters","title":"Parameters","text":"<p>X : ndarrays of floats     The input data to be used in the training process. y : ndarrays of floats     The output data to be used in the training process.</p>"},{"location":"code/general-estimators/#sysidentpy.general_estimators.narx.NARX.fit--returns","title":"Returns","text":"<p>base_estimator : sklearn estimator     The model fitted.</p> Source code in <code>sysidentpy/general_estimators/narx.py</code> <pre><code>def fit(self, *, X=None, y=None):\n    \"\"\"Train a NARX Neural Network model.\n\n    This is an training pipeline that allows a friendly usage\n    by the user. All the lagged features are built using the\n    SysIdentPy classes and we use the fit method of the base\n    estimator of the sklearn to fit the model.\n\n    Parameters\n    ----------\n    X : ndarrays of floats\n        The input data to be used in the training process.\n    y : ndarrays of floats\n        The output data to be used in the training process.\n\n    Returns\n    -------\n    base_estimator : sklearn estimator\n        The model fitted.\n\n    \"\"\"\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    self.max_lag = self._get_max_lag()\n    lagged_data = self.build_matrix(X, y)\n    reg_matrix = self.basis_function.fit(\n        lagged_data, self.max_lag, predefined_regressors=None\n    )\n\n    if X is not None:\n        self.n_inputs = _num_features(X)\n    else:\n        self.n_inputs = 1  # just to create the regressor space base\n\n    self.regressor_code = self.regressor_space(self.n_inputs)\n    self.final_model = self.regressor_code\n    y = y[self.max_lag :].ravel()\n\n    self.base_estimator.fit(reg_matrix, y, **self.fit_params)\n    return self\n</code></pre>"},{"location":"code/general-estimators/#sysidentpy.general_estimators.narx.NARX.narmax_n_step_ahead","title":"<code>narmax_n_step_ahead(X, y, steps_ahead)</code>","text":"<p>N steps ahead prediction method for NARMAX model.</p> Source code in <code>sysidentpy/general_estimators/narx.py</code> <pre><code>def narmax_n_step_ahead(self, X, y, steps_ahead):\n    \"\"\"N steps ahead prediction method for NARMAX model.\"\"\"\n    if len(y) &lt; self.max_lag:\n        raise ValueError(\n            \"Insufficient initial condition elements! Expected at least\"\n            f\" {self.max_lag} elements.\"\n        )\n\n    to_remove = int(np.ceil((len(y) - self.max_lag) / steps_ahead))\n    X = X.reshape(-1, self.n_inputs)\n    yhat = np.zeros(X.shape[0], dtype=float)\n    yhat.fill(np.nan)\n    yhat[: self.max_lag] = y[: self.max_lag, 0]\n    i = self.max_lag\n    steps = [step for step in range(0, to_remove * steps_ahead, steps_ahead)]\n    if len(steps) &gt; 1:\n        for step in steps[:-1]:\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                X=X[step : i + steps_ahead],\n                y_initial=y[step:i],\n            )[-steps_ahead:].ravel()\n            i += steps_ahead\n\n        steps_ahead = np.sum(np.isnan(yhat))\n        yhat[i : i + steps_ahead] = self._model_prediction(\n            X=X[steps[-1] : i + steps_ahead],\n            y_initial=y[steps[-1] : i],\n        )[-steps_ahead:].ravel()\n    else:\n        yhat[i : i + steps_ahead] = self._model_prediction(\n            X=X[0 : i + steps_ahead],\n            y_initial=y[0:i],\n        )[-steps_ahead:].ravel()\n\n    yhat = yhat.ravel()[self.max_lag : :]\n    return yhat.reshape(-1, 1)\n</code></pre>"},{"location":"code/general-estimators/#sysidentpy.general_estimators.narx.NARX.predict","title":"<code>predict(*, X=None, y=None, steps_ahead=None, forecast_horizon=1)</code>","text":"<p>Return the predicted given an input and initial values.</p> <p>The predict function allows a friendly usage by the user. Given a trained model, predict values given a new set of data.</p> <p>This method accept y values mainly for prediction n-steps ahead (to be implemented in the future).</p> <p>Currently we only support infinity-steps-ahead prediction, but run 1-step-ahead prediction manually is straightforward.</p>"},{"location":"code/general-estimators/#sysidentpy.general_estimators.narx.NARX.predict--parameters","title":"Parameters","text":"<p>X : ndarray of floats     The input data to be used in the prediction process. y : ndarray of floats     The output data to be used in the prediction process. steps_ahead : int (default = None)     The user can use free run simulation, one-step ahead prediction     and n-step ahead prediction. forecast_horizon : int, default=None     The number of predictions over the time.</p>"},{"location":"code/general-estimators/#sysidentpy.general_estimators.narx.NARX.predict--returns","title":"Returns","text":"<p>yhat : ndarray of floats     The predicted values of the model.</p> Source code in <code>sysidentpy/general_estimators/narx.py</code> <pre><code>def predict(\n    self,\n    *,\n    X: Optional[NDArray] = None,\n    y: Optional[NDArray] = None,\n    steps_ahead: Optional[int] = None,\n    forecast_horizon: Optional[int] = 1,\n) -&gt; NDArray:\n    \"\"\"Return the predicted given an input and initial values.\n\n    The predict function allows a friendly usage by the user.\n    Given a trained model, predict values given\n    a new set of data.\n\n    This method accept y values mainly for prediction n-steps ahead\n    (to be implemented in the future).\n\n    Currently we only support infinity-steps-ahead prediction,\n    but run 1-step-ahead prediction manually is straightforward.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    steps_ahead : int (default = None)\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction.\n    forecast_horizon : int, default=None\n        The number of predictions over the time.\n\n    Returns\n    -------\n    yhat : ndarray of floats\n        The predicted values of the model.\n\n    \"\"\"\n    if self.basis_function.__class__.__name__ == \"Polynomial\":\n        if steps_ahead is None:\n            yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        _check_positive_int(steps_ahead, \"steps_ahead\")\n        yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    if steps_ahead is None:\n        yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n    if steps_ahead == 1:\n        yhat = self._one_step_ahead_prediction(X, y)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    yhat = self._basis_function_n_step_prediction(\n        X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n    )\n    yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n    return yhat\n</code></pre>"},{"location":"code/metaheuristics/","title":"Documentation for <code>Metaheuristics</code>","text":"<p>Binary Hybrid Particle Swarm Optimization and Gravitational Search Algorithm.</p>"},{"location":"code/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA","title":"<code>BPSOGSA</code>","text":"<p>Binary Hybrid Particle Swarm Optimization and Gravitational Search Algorithm.</p>"},{"location":"code/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA--parameters","title":"Parameters","text":"<p>maxiter : int, default=30     The maximum number of iterations. alpha : int, default=23     The descending coefficient of the gravitational constant. g_zero : int, default=100     The initial value of the gravitational constant. k_agents_percent: int, default=2     Percent of agents applying force to the others in the last iteration. norm : int, default=-2     The information criteria method to be used. power : int, default=2     The number of the model terms to be selected.     Note that n_terms overwrite the information criteria     values. n_agents : int, default=10     The number of agents to search the optimal solution. dimension : int, default=15     The dimension of the search space.     criteria method. p_zeros : float, default=0.5     The probability of getting ones in the construction of the population. p_zeros : float, default=0.5     The probability of getting zeros in the construction of the population.</p>"},{"location":"code/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA--examples","title":"Examples","text":"<p>import numpy as np import matplotlib.pyplot as plt from sysidentpy.metaheuristics import BPSOGSA opt = BPSOGSA(maxiter=100, ...               k_agents_percent=2, ...               n_agents=10, ...               dimension=20 ...               ) opt.optimize() plt.plot(opt.best_by_iter) plt.show() print(opt.optimal_fitness_value)</p>"},{"location":"code/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA--references","title":"References","text":"<ul> <li>A New Hybrid PSOGSA Algorithm for Function Optimization,    https://www.mathworks.com/matlabcentral/fileexchange/35939-hybrid-particle-swarm-optimization-and-gravitational-search-algorithm-psogsa</li> <li>Manuscript: Particle swarm optimization: developments, applications and resources.</li> <li>Manuscript: S-shaped versus v-shaped transfer functions for binary    particle swarm optimization</li> <li>Manuscript: BGSA: Binary Gravitational Search Algorithm.</li> <li>Manuscript: A taxonomy of hybrid metaheuristics</li> </ul> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>class BPSOGSA:\n    \"\"\"Binary Hybrid Particle Swarm Optimization and Gravitational Search Algorithm.\n\n    Parameters\n    ----------\n    maxiter : int, default=30\n        The maximum number of iterations.\n    alpha : int, default=23\n        The descending coefficient of the gravitational constant.\n    g_zero : int, default=100\n        The initial value of the gravitational constant.\n    k_agents_percent: int, default=2\n        Percent of agents applying force to the others in the last iteration.\n    norm : int, default=-2\n        The information criteria method to be used.\n    power : int, default=2\n        The number of the model terms to be selected.\n        Note that n_terms overwrite the information criteria\n        values.\n    n_agents : int, default=10\n        The number of agents to search the optimal solution.\n    dimension : int, default=15\n        The dimension of the search space.\n        criteria method.\n    p_zeros : float, default=0.5\n        The probability of getting ones in the construction of the population.\n    p_zeros : float, default=0.5\n        The probability of getting zeros in the construction of the population.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from sysidentpy.metaheuristics import BPSOGSA\n    &gt;&gt;&gt; opt = BPSOGSA(maxiter=100,\n    ...               k_agents_percent=2,\n    ...               n_agents=10,\n    ...               dimension=20\n    ...               )\n    &gt;&gt;&gt; opt.optimize()\n    &gt;&gt;&gt; plt.plot(opt.best_by_iter)\n    &gt;&gt;&gt; plt.show()\n    &gt;&gt;&gt; print(opt.optimal_fitness_value)\n\n    References\n    ----------\n    - A New Hybrid PSOGSA Algorithm for Function Optimization,\n       https://www.mathworks.com/matlabcentral/fileexchange/35939-hybrid-particle-swarm-optimization-and-gravitational-search-algorithm-psogsa\n    - Manuscript: Particle swarm optimization: developments, applications and resources.\n    - Manuscript: S-shaped versus v-shaped transfer functions for binary\n       particle swarm optimization\n    - Manuscript: BGSA: Binary Gravitational Search Algorithm.\n    - Manuscript: A taxonomy of hybrid metaheuristics\n\n    \"\"\"\n\n    def __init__(\n        self,\n        maxiter=30,\n        alpha=23,\n        g_zero=100,\n        k_agents_percent=2,\n        norm=-2,\n        power=2,\n        n_agents=10,\n        dimension=15,\n        p_zeros=0.5,\n        p_ones=0.5,\n    ):\n        self.dimension = dimension\n        self.n_agents = n_agents\n        self.maxiter = maxiter\n        self.g_zero = g_zero\n        self.alpha = alpha\n        self.k_agents_percent = k_agents_percent\n        self.norm = norm\n        self.power = power\n        self.p_zeros = p_zeros\n        self.p_ones = p_ones\n        self.best_by_iter = None\n        self.mean_by_iter = None\n        self.optimal_fitness_value = None\n        self.optimal_model = None\n        super(BPSOGSA, self).__init__()\n\n    def evaluate_objective_function(self, candidate_solution):\n        \"\"\"Define a function to be optimized.\"\"\"\n        total = 0\n        for candidate in candidate_solution:\n            total += candidate**2\n        return total\n\n    def optimize(self):\n        \"\"\"Run the BPSOGSA algorithm.\n\n        This algorithm is based on the Matlab implementation provided by the\n        author of the BPSOGSA algorithm.\n\n        References\n        ----------\n        - A New Hybrid PSOGSA Algorithm for Function Optimization.\n           https://www.mathworks.com/matlabcentral/fileexchange/35939-hybrid-particle-swarm-optimization-and-gravitational-search-algorithm-psogsa\n        - Manuscript: Particle swarm optimization: developments, applications and\n            resources.\n        - Manuscript: S-shaped versus v-shaped transfer functions for binary.\n           particle swarm optimization\n        - Manuscript: BGSA: Binary Gravitational Search Algorithm.\n        - Manuscript: A taxonomy of hybrid metaheuristics.\n\n        \"\"\"\n        velocity = np.zeros([self.dimension, self.n_agents])\n        population = self.generate_random_population()\n        self.best_by_iter = []\n        self.mean_by_iter = []\n        self.optimal_fitness_value = np.inf\n        self.optimal_model = None\n\n        for i in range(self.maxiter):\n            fitness = self.evaluate_objective_function(population)\n\n            column_of_best_solution = np.argmin(fitness)\n            current_best_fitness = fitness[column_of_best_solution]\n\n            if current_best_fitness &lt; self.optimal_fitness_value:\n                self.optimal_fitness_value = current_best_fitness\n                self.optimal_model = population[:, column_of_best_solution].copy()\n\n            self.best_by_iter.append(self.optimal_fitness_value)\n            self.mean_by_iter.append(np.mean(fitness))\n            agent_mass = self.mass_calculation(fitness)\n            gravitational_constant = self.calculate_gravitational_constant(i)\n            acceleration = self.calculate_acceleration(\n                population, agent_mass, gravitational_constant, i\n            )\n            velocity, population = self.update_velocity_position(\n                population,\n                acceleration,\n                velocity,\n                i,\n            )\n\n        return self\n\n    def generate_random_population(self, random_state=None):\n        \"\"\"Generate the initial population of agents randomly.\n\n        Returns\n        -------\n        population : ndarray of zeros and ones\n            The initial population of agents.\n\n        \"\"\"\n        rng = check_random_state(random_state)\n        population = rng.choice(\n            [0, 1], size=(self.dimension, self.n_agents), p=[self.p_zeros, self.p_ones]\n        )\n        return population\n\n    def mass_calculation(self, fitness_value):\n        \"\"\"Calculate the inertial masses of the agents.\n\n        Parameters\n        ----------\n        fitness_value : ndarray\n            The fitness value of each agent.\n\n        Returns\n        -------\n        agent_mass : ndarray of floats\n            The mass of each agent.\n\n        \"\"\"\n        highest_fitness_value = np.nanmax(fitness_value)\n        lowest_fitness_value = np.nanmin(fitness_value)\n\n        column_fitness = len(fitness_value)\n        if highest_fitness_value == lowest_fitness_value:\n            agent_mass = np.ones([column_fitness, 1])\n        else:\n            best_fitness_value = lowest_fitness_value\n            worst_fitness_value = highest_fitness_value\n            agent_mass = (fitness_value - 0.99 * worst_fitness_value) / (\n                best_fitness_value - worst_fitness_value\n            )\n\n        agent_mass = (5 * agent_mass) / np.sum(agent_mass)\n        return agent_mass\n\n    def calculate_gravitational_constant(self, iteration):\n        \"\"\"Update the gravitational constant.\n\n        Parameters\n        ----------\n        iteration : int\n            The specific time.\n\n        Returns\n        -------\n        gravitational_constant : float\n            The gravitational_constant at time defined by the iteration.\n\n        \"\"\"\n        gravitational_constant = self.g_zero * np.exp(\n            (-self.alpha * (iteration + 1)) / self.maxiter\n        )\n\n        return gravitational_constant\n\n    def calculate_acceleration(\n        self, population, agent_mass, gravitational_constant, iteration\n    ):\n        \"\"\"Calculate the acceleration of each agent.\n\n        Parameters\n        ----------\n        population : ndarray of zeros and ones\n            The population defined by the agents.\n        agent_mass : ndarray of floats\n            The mass of each agent.\n        gravitational_constant : float\n            The gravitational_constant at time defined by the iteration.\n        iteration : int\n            The current iteration.\n\n        Returns\n        -------\n        acceleration : ndarray of floats\n            The acceleration of each agent.\n\n        \"\"\"\n        k_best_agents = self.k_agents_percent + (1 - iteration / self.maxiter) * (\n            100 - self.k_agents_percent\n        )\n        k_best_agents = round(self.n_agents * k_best_agents / 100)\n\n        maximum_value_index = np.argsort(agent_mass)[::-1].ravel()\n        gravitational_force = np.zeros([self.dimension, self.n_agents])\n        for i in range(self.n_agents):\n            for j in range(k_best_agents):\n                if maximum_value_index[j] != i:\n                    euclidean_distance = np.linalg.norm(\n                        population[:, i] - population[:, maximum_value_index[j]],\n                        self.norm,\n                    )\n                    gravitational_force[:, i] = gravitational_force[\n                        :, i\n                    ] + np.random.rand(self.dimension) * agent_mass[\n                        maximum_value_index[j]\n                    ] * (\n                        population[:, maximum_value_index[j]] - population[:, i]\n                    ) / (\n                        euclidean_distance**self.power + np.finfo(np.float64).eps\n                    )\n\n        acceleration = gravitational_force * gravitational_constant\n        return acceleration\n\n    def update_velocity_position(\n        self,\n        population,\n        acceleration,\n        velocity,\n        iteration,\n    ):\n        \"\"\"Update the velocity and position of each agent.\n\n        Parameters\n        ----------\n        population : ndarray of zeros and ones\n            The population defined by the agents.\n        acceleration : ndarray of floats\n            The acceleration of each agent.\n        velocity : ndarray of floats\n            The velocity of each agent.\n        iteration : int\n            The current iteration.\n\n        Returns\n        -------\n        velocity : ndarray of floats\n            The updated velocity of each agent.\n        population : ndarray of zeros and ones\n            The updated population defined by the agents.\n\n        \"\"\"\n        c_factor_local_best = -2 * ((iteration**3) / (self.maxiter**3)) + 2\n        c_factor_global_best = 2 * ((iteration**3) / (self.maxiter**3)) + 2\n        global_best = np.repeat(self.optimal_model, self.n_agents, axis=0).reshape(\n            self.dimension, self.n_agents\n        )\n\n        velocity = (\n            np.random.rand(self.dimension, self.n_agents) * velocity\n            + c_factor_local_best * acceleration\n            + c_factor_global_best * (global_best - population)\n        )\n        r = np.random.rand(self.dimension, self.n_agents)\n        transform_to_binary = np.absolute(np.tanh((velocity)))\n        ind = np.where(r &lt; transform_to_binary)\n        population[ind] = 1 - population[ind]\n        return velocity, population\n</code></pre>"},{"location":"code/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.calculate_acceleration","title":"<code>calculate_acceleration(population, agent_mass, gravitational_constant, iteration)</code>","text":"<p>Calculate the acceleration of each agent.</p>"},{"location":"code/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.calculate_acceleration--parameters","title":"Parameters","text":"<p>population : ndarray of zeros and ones     The population defined by the agents. agent_mass : ndarray of floats     The mass of each agent. gravitational_constant : float     The gravitational_constant at time defined by the iteration. iteration : int     The current iteration.</p>"},{"location":"code/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.calculate_acceleration--returns","title":"Returns","text":"<p>acceleration : ndarray of floats     The acceleration of each agent.</p> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>def calculate_acceleration(\n    self, population, agent_mass, gravitational_constant, iteration\n):\n    \"\"\"Calculate the acceleration of each agent.\n\n    Parameters\n    ----------\n    population : ndarray of zeros and ones\n        The population defined by the agents.\n    agent_mass : ndarray of floats\n        The mass of each agent.\n    gravitational_constant : float\n        The gravitational_constant at time defined by the iteration.\n    iteration : int\n        The current iteration.\n\n    Returns\n    -------\n    acceleration : ndarray of floats\n        The acceleration of each agent.\n\n    \"\"\"\n    k_best_agents = self.k_agents_percent + (1 - iteration / self.maxiter) * (\n        100 - self.k_agents_percent\n    )\n    k_best_agents = round(self.n_agents * k_best_agents / 100)\n\n    maximum_value_index = np.argsort(agent_mass)[::-1].ravel()\n    gravitational_force = np.zeros([self.dimension, self.n_agents])\n    for i in range(self.n_agents):\n        for j in range(k_best_agents):\n            if maximum_value_index[j] != i:\n                euclidean_distance = np.linalg.norm(\n                    population[:, i] - population[:, maximum_value_index[j]],\n                    self.norm,\n                )\n                gravitational_force[:, i] = gravitational_force[\n                    :, i\n                ] + np.random.rand(self.dimension) * agent_mass[\n                    maximum_value_index[j]\n                ] * (\n                    population[:, maximum_value_index[j]] - population[:, i]\n                ) / (\n                    euclidean_distance**self.power + np.finfo(np.float64).eps\n                )\n\n    acceleration = gravitational_force * gravitational_constant\n    return acceleration\n</code></pre>"},{"location":"code/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.calculate_gravitational_constant","title":"<code>calculate_gravitational_constant(iteration)</code>","text":"<p>Update the gravitational constant.</p>"},{"location":"code/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.calculate_gravitational_constant--parameters","title":"Parameters","text":"<p>iteration : int     The specific time.</p>"},{"location":"code/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.calculate_gravitational_constant--returns","title":"Returns","text":"<p>gravitational_constant : float     The gravitational_constant at time defined by the iteration.</p> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>def calculate_gravitational_constant(self, iteration):\n    \"\"\"Update the gravitational constant.\n\n    Parameters\n    ----------\n    iteration : int\n        The specific time.\n\n    Returns\n    -------\n    gravitational_constant : float\n        The gravitational_constant at time defined by the iteration.\n\n    \"\"\"\n    gravitational_constant = self.g_zero * np.exp(\n        (-self.alpha * (iteration + 1)) / self.maxiter\n    )\n\n    return gravitational_constant\n</code></pre>"},{"location":"code/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.evaluate_objective_function","title":"<code>evaluate_objective_function(candidate_solution)</code>","text":"<p>Define a function to be optimized.</p> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>def evaluate_objective_function(self, candidate_solution):\n    \"\"\"Define a function to be optimized.\"\"\"\n    total = 0\n    for candidate in candidate_solution:\n        total += candidate**2\n    return total\n</code></pre>"},{"location":"code/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.generate_random_population","title":"<code>generate_random_population(random_state=None)</code>","text":"<p>Generate the initial population of agents randomly.</p>"},{"location":"code/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.generate_random_population--returns","title":"Returns","text":"<p>population : ndarray of zeros and ones     The initial population of agents.</p> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>def generate_random_population(self, random_state=None):\n    \"\"\"Generate the initial population of agents randomly.\n\n    Returns\n    -------\n    population : ndarray of zeros and ones\n        The initial population of agents.\n\n    \"\"\"\n    rng = check_random_state(random_state)\n    population = rng.choice(\n        [0, 1], size=(self.dimension, self.n_agents), p=[self.p_zeros, self.p_ones]\n    )\n    return population\n</code></pre>"},{"location":"code/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.mass_calculation","title":"<code>mass_calculation(fitness_value)</code>","text":"<p>Calculate the inertial masses of the agents.</p>"},{"location":"code/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.mass_calculation--parameters","title":"Parameters","text":"<p>fitness_value : ndarray     The fitness value of each agent.</p>"},{"location":"code/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.mass_calculation--returns","title":"Returns","text":"<p>agent_mass : ndarray of floats     The mass of each agent.</p> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>def mass_calculation(self, fitness_value):\n    \"\"\"Calculate the inertial masses of the agents.\n\n    Parameters\n    ----------\n    fitness_value : ndarray\n        The fitness value of each agent.\n\n    Returns\n    -------\n    agent_mass : ndarray of floats\n        The mass of each agent.\n\n    \"\"\"\n    highest_fitness_value = np.nanmax(fitness_value)\n    lowest_fitness_value = np.nanmin(fitness_value)\n\n    column_fitness = len(fitness_value)\n    if highest_fitness_value == lowest_fitness_value:\n        agent_mass = np.ones([column_fitness, 1])\n    else:\n        best_fitness_value = lowest_fitness_value\n        worst_fitness_value = highest_fitness_value\n        agent_mass = (fitness_value - 0.99 * worst_fitness_value) / (\n            best_fitness_value - worst_fitness_value\n        )\n\n    agent_mass = (5 * agent_mass) / np.sum(agent_mass)\n    return agent_mass\n</code></pre>"},{"location":"code/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.optimize","title":"<code>optimize()</code>","text":"<p>Run the BPSOGSA algorithm.</p> <p>This algorithm is based on the Matlab implementation provided by the author of the BPSOGSA algorithm.</p>"},{"location":"code/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.optimize--references","title":"References","text":"<ul> <li>A New Hybrid PSOGSA Algorithm for Function Optimization.    https://www.mathworks.com/matlabcentral/fileexchange/35939-hybrid-particle-swarm-optimization-and-gravitational-search-algorithm-psogsa</li> <li>Manuscript: Particle swarm optimization: developments, applications and     resources.</li> <li>Manuscript: S-shaped versus v-shaped transfer functions for binary.    particle swarm optimization</li> <li>Manuscript: BGSA: Binary Gravitational Search Algorithm.</li> <li>Manuscript: A taxonomy of hybrid metaheuristics.</li> </ul> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>def optimize(self):\n    \"\"\"Run the BPSOGSA algorithm.\n\n    This algorithm is based on the Matlab implementation provided by the\n    author of the BPSOGSA algorithm.\n\n    References\n    ----------\n    - A New Hybrid PSOGSA Algorithm for Function Optimization.\n       https://www.mathworks.com/matlabcentral/fileexchange/35939-hybrid-particle-swarm-optimization-and-gravitational-search-algorithm-psogsa\n    - Manuscript: Particle swarm optimization: developments, applications and\n        resources.\n    - Manuscript: S-shaped versus v-shaped transfer functions for binary.\n       particle swarm optimization\n    - Manuscript: BGSA: Binary Gravitational Search Algorithm.\n    - Manuscript: A taxonomy of hybrid metaheuristics.\n\n    \"\"\"\n    velocity = np.zeros([self.dimension, self.n_agents])\n    population = self.generate_random_population()\n    self.best_by_iter = []\n    self.mean_by_iter = []\n    self.optimal_fitness_value = np.inf\n    self.optimal_model = None\n\n    for i in range(self.maxiter):\n        fitness = self.evaluate_objective_function(population)\n\n        column_of_best_solution = np.argmin(fitness)\n        current_best_fitness = fitness[column_of_best_solution]\n\n        if current_best_fitness &lt; self.optimal_fitness_value:\n            self.optimal_fitness_value = current_best_fitness\n            self.optimal_model = population[:, column_of_best_solution].copy()\n\n        self.best_by_iter.append(self.optimal_fitness_value)\n        self.mean_by_iter.append(np.mean(fitness))\n        agent_mass = self.mass_calculation(fitness)\n        gravitational_constant = self.calculate_gravitational_constant(i)\n        acceleration = self.calculate_acceleration(\n            population, agent_mass, gravitational_constant, i\n        )\n        velocity, population = self.update_velocity_position(\n            population,\n            acceleration,\n            velocity,\n            i,\n        )\n\n    return self\n</code></pre>"},{"location":"code/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.update_velocity_position","title":"<code>update_velocity_position(population, acceleration, velocity, iteration)</code>","text":"<p>Update the velocity and position of each agent.</p>"},{"location":"code/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.update_velocity_position--parameters","title":"Parameters","text":"<p>population : ndarray of zeros and ones     The population defined by the agents. acceleration : ndarray of floats     The acceleration of each agent. velocity : ndarray of floats     The velocity of each agent. iteration : int     The current iteration.</p>"},{"location":"code/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.update_velocity_position--returns","title":"Returns","text":"<p>velocity : ndarray of floats     The updated velocity of each agent. population : ndarray of zeros and ones     The updated population defined by the agents.</p> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>def update_velocity_position(\n    self,\n    population,\n    acceleration,\n    velocity,\n    iteration,\n):\n    \"\"\"Update the velocity and position of each agent.\n\n    Parameters\n    ----------\n    population : ndarray of zeros and ones\n        The population defined by the agents.\n    acceleration : ndarray of floats\n        The acceleration of each agent.\n    velocity : ndarray of floats\n        The velocity of each agent.\n    iteration : int\n        The current iteration.\n\n    Returns\n    -------\n    velocity : ndarray of floats\n        The updated velocity of each agent.\n    population : ndarray of zeros and ones\n        The updated population defined by the agents.\n\n    \"\"\"\n    c_factor_local_best = -2 * ((iteration**3) / (self.maxiter**3)) + 2\n    c_factor_global_best = 2 * ((iteration**3) / (self.maxiter**3)) + 2\n    global_best = np.repeat(self.optimal_model, self.n_agents, axis=0).reshape(\n        self.dimension, self.n_agents\n    )\n\n    velocity = (\n        np.random.rand(self.dimension, self.n_agents) * velocity\n        + c_factor_local_best * acceleration\n        + c_factor_global_best * (global_best - population)\n    )\n    r = np.random.rand(self.dimension, self.n_agents)\n    transform_to_binary = np.absolute(np.tanh((velocity)))\n    ind = np.where(r &lt; transform_to_binary)\n    population[ind] = 1 - population[ind]\n    return velocity, population\n</code></pre>"},{"location":"code/metamss/","title":"Documentation for <code>MetaMSS</code>","text":"<p>Meta Model Structure Selection.</p>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS","title":"<code>MetaMSS</code>","text":"<p>               Bases: <code>SimulateNARMAX</code>, <code>BPSOGSA</code></p> <p>Meta-Model Structure Selection: Building Polynomial NARMAX model.</p> <p>This class uses the MetaMSS ([1], [2], [3]_) algorithm to build NARMAX models. The NARMAX model is described as:</p> \\[     y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x},     e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>where \\(n_y\\in \\mathbb{N}^*\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\), are the maximum lags for the system output and input respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}^\\ell\\) is some nonlinear function of the input and output regressors with nonlinearity degree \\(\\ell \\in \\mathbb{N}\\) and \\(d\\) is a time delay typically set to \\(d=1\\).</p>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS--parameters","title":"Parameters","text":"<p>ylag : int, default=2     The maximum lag of the output. xlag : int, default=2     The maximum lag of the input. loss_func : str, default=\"metamss_loss\"     The loss function to be minimized. estimator : str, default=\"least_squares\"     The parameter estimation method. estimate_parameter : bool, default=True     Whether to estimate the model parameters. eps : float     Normalization factor of the normalized filters. maxiter : int, default=30     The maximum number of iterations. alpha : int, default=23     The descending coefficient of the gravitational constant. g_zero : int, default=100     The initial value of the gravitational constant. k_agents_percent: int, default=2     Percent of agents applying force to the others in the last iteration. norm : int, default=-2     The information criteria method to be used. power : int, default=2     The number of the model terms to be selected.     Note that n_terms overwrite the information criteria     values. n_agents : int, default=10     The number of agents to search the optimal solution. p_zeros : float, default=0.5     The probability of getting ones in the construction of the population. p_zeros : float, default=0.5     The probability of getting zeros in the construction of the population.</p>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS--examples","title":"Examples","text":"<p>import numpy as np import matplotlib.pyplot as plt from sysidentpy.model_structure_selection import MetaMSS from sysidentpy.metrics import root_relative_squared_error from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.utils.display_results import results from sysidentpy.utils.generate_data import get_siso_data x_train, x_valid, y_train, y_valid = get_siso_data(n=400, ...                                                    colored_noise=False, ...                                                    sigma=0.001, ...                                                    train_percentage=80) basis_function = Polynomial(degree=2) model = MetaMSS( ...     basis_function=basis_function, ...     norm=-2, ...     xlag=7, ...     ylag=7, ...     estimator=\"least_squares\", ...     k_agents_percent=2, ...     estimate_parameter=True, ...     maxiter=30, ...     n_agents=10, ...     p_value=0.05, ...     loss_func='metamss_loss' ... ) model.fit(x_train, y_train, x_valid, y_valid) yhat = model.predict(x_valid, y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse) 0.001993603325328823 r = pd.DataFrame( ...     results( ...         model.final_model, model.theta, model.err, ...         model.n_terms, err_precision=8, dtype='sci' ...         ), ...     columns=['Regressors', 'Parameters', 'ERR']) print\u00ae     Regressors Parameters         ERR 0        x1(k-2)     0.9000       0.0 1         y(k-1)     0.1999       0.0 2  x1(k-1)y(k-1)     0.1000       0.0</p>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS--references","title":"References","text":"<ul> <li>Manuscript: Meta-Model Structure Selection: Building Polynomial NARX Model    for Regression and Classification    https://arxiv.org/pdf/2109.09917.pdf</li> <li>Manuscript (Portuguese): Identifica\u00e7\u00e3o de Sistemas N\u00e3o Lineares    Utilizando o Algoritmo H\u00edbrido e Bin\u00e1rio de Otimiza\u00e7\u00e3o por    Enxame de Part\u00edculas e Busca Gravitacional    DOI: 10.17648/sbai-2019-111317</li> <li>Master thesis: Meta model structure selection: an algorithm for    building polynomial NARX models for regression and classification</li> </ul> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>class MetaMSS(SimulateNARMAX, BPSOGSA):\n    r\"\"\"Meta-Model Structure Selection: Building Polynomial NARMAX model.\n\n    This class uses the MetaMSS ([1]_, [2]_, [3]_) algorithm to build NARMAX models.\n    The NARMAX model is described as:\n\n    $$\n        y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x},\n        e_{k-1}, \\dotsc, e_{k-n_e}] + e_k\n    $$\n\n    where $n_y\\in \\mathbb{N}^*$, $n_x \\in \\mathbb{N}$, $n_e \\in \\mathbb{N}$,\n    are the maximum lags for the system output and input respectively;\n    $x_k \\in \\mathbb{R}^{n_x}$ is the system input and $y_k \\in \\mathbb{R}^{n_y}$\n    is the system output at discrete time $k \\in \\mathbb{N}^n$;\n    $e_k \\in \\mathbb{R}^{n_e}$ stands for uncertainties and possible noise\n    at discrete time $k$. In this case, $\\mathcal{F}^\\ell$ is some nonlinear function\n    of the input and output regressors with nonlinearity degree $\\ell \\in \\mathbb{N}$\n    and $d$ is a time delay typically set to $d=1$.\n\n    Parameters\n    ----------\n    ylag : int, default=2\n        The maximum lag of the output.\n    xlag : int, default=2\n        The maximum lag of the input.\n    loss_func : str, default=\"metamss_loss\"\n        The loss function to be minimized.\n    estimator : str, default=\"least_squares\"\n        The parameter estimation method.\n    estimate_parameter : bool, default=True\n        Whether to estimate the model parameters.\n    eps : float\n        Normalization factor of the normalized filters.\n    maxiter : int, default=30\n        The maximum number of iterations.\n    alpha : int, default=23\n        The descending coefficient of the gravitational constant.\n    g_zero : int, default=100\n        The initial value of the gravitational constant.\n    k_agents_percent: int, default=2\n        Percent of agents applying force to the others in the last iteration.\n    norm : int, default=-2\n        The information criteria method to be used.\n    power : int, default=2\n        The number of the model terms to be selected.\n        Note that n_terms overwrite the information criteria\n        values.\n    n_agents : int, default=10\n        The number of agents to search the optimal solution.\n    p_zeros : float, default=0.5\n        The probability of getting ones in the construction of the population.\n    p_zeros : float, default=0.5\n        The probability of getting zeros in the construction of the population.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from sysidentpy.model_structure_selection import MetaMSS\n    &gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n    &gt;&gt;&gt; from sysidentpy.basis_function._basis_function import Polynomial\n    &gt;&gt;&gt; from sysidentpy.utils.display_results import results\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_siso_data\n    &gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=400,\n    ...                                                    colored_noise=False,\n    ...                                                    sigma=0.001,\n    ...                                                    train_percentage=80)\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; model = MetaMSS(\n    ...     basis_function=basis_function,\n    ...     norm=-2,\n    ...     xlag=7,\n    ...     ylag=7,\n    ...     estimator=\"least_squares\",\n    ...     k_agents_percent=2,\n    ...     estimate_parameter=True,\n    ...     maxiter=30,\n    ...     n_agents=10,\n    ...     p_value=0.05,\n    ...     loss_func='metamss_loss'\n    ... )\n    &gt;&gt;&gt; model.fit(x_train, y_train, x_valid, y_valid)\n    &gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n    &gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n    &gt;&gt;&gt; print(rrse)\n    0.001993603325328823\n    &gt;&gt;&gt; r = pd.DataFrame(\n    ...     results(\n    ...         model.final_model, model.theta, model.err,\n    ...         model.n_terms, err_precision=8, dtype='sci'\n    ...         ),\n    ...     columns=['Regressors', 'Parameters', 'ERR'])\n    &gt;&gt;&gt; print(r)\n        Regressors Parameters         ERR\n    0        x1(k-2)     0.9000       0.0\n    1         y(k-1)     0.1999       0.0\n    2  x1(k-1)y(k-1)     0.1000       0.0\n\n    References\n    ----------\n    - Manuscript: Meta-Model Structure Selection: Building Polynomial NARX Model\n       for Regression and Classification\n       https://arxiv.org/pdf/2109.09917.pdf\n    - Manuscript (Portuguese): Identifica\u00e7\u00e3o de Sistemas N\u00e3o Lineares\n       Utilizando o Algoritmo H\u00edbrido e Bin\u00e1rio de Otimiza\u00e7\u00e3o por\n       Enxame de Part\u00edculas e Busca Gravitacional\n       DOI: 10.17648/sbai-2019-111317\n    - Master thesis: Meta model structure selection: an algorithm for\n       building polynomial NARX models for regression and classification\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        maxiter: int = 30,\n        alpha: int = 23,\n        g_zero: int = 100,\n        k_agents_percent: int = 2,\n        norm: float = -2,\n        power: int = 2,\n        n_agents: int = 10,\n        p_zeros: float = 0.5,\n        p_ones: float = 0.5,\n        p_value: float = 0.05,\n        xlag: Union[int, list] = 1,\n        ylag: Union[int, list] = 1,\n        elag: Union[int, list] = 1,\n        estimator: Estimators = LeastSquares(),\n        eps: np.float64 = np.finfo(np.float64).eps,\n        estimate_parameter: bool = True,\n        loss_func: str = \"metamss_loss\",\n        model_type: str = \"NARMAX\",\n        basis_function: Polynomial = Polynomial(),\n        steps_ahead: Optional[int] = None,\n        random_state: Optional[int] = None,\n        test_size: float = 0.25,\n    ):\n        super().__init__(\n            estimator=estimator,\n            eps=eps,\n            estimate_parameter=estimate_parameter,\n            model_type=model_type,\n            basis_function=basis_function,\n        )\n\n        BPSOGSA.__init__(\n            self,\n            n_agents=n_agents,\n            maxiter=maxiter,\n            g_zero=g_zero,\n            alpha=alpha,\n            k_agents_percent=k_agents_percent,\n            norm=norm,\n            power=power,\n            p_zeros=p_zeros,\n            p_ones=p_ones,\n        )\n\n        self.xlag = xlag\n        self.ylag = ylag\n        self.elag = elag\n        self.p_value = p_value\n        self.estimator = estimator\n        self.estimate_parameter = estimate_parameter\n        self.loss_func = loss_func\n        self.steps_ahead = steps_ahead\n        self.random_state = random_state\n        self.test_size = test_size\n        self.build_matrix = self.get_build_io_method(model_type)\n        self.n_inputs = None\n        self.regressor_code = None\n        self.best_model_history = None\n        self.tested_models = None\n        self.final_model = None\n        self._validate_metamss_params()\n\n    def _validate_metamss_params(self):\n        if isinstance(self.ylag, int) and self.ylag &lt; 1:\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if isinstance(self.xlag, int) and self.xlag &lt; 1:\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.xlag, (int, list)):\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.ylag, (int, list)):\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n    def fit(\n        self,\n        *,\n        X: Optional[np.ndarray] = None,\n        y: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Fit the polynomial NARMAX model.\n\n        Parameters\n        ----------\n        X : ndarray, optional\n            The input data to be used in the training process.\n        y : ndarray\n            The output data to be used in the training process.\n\n        Returns\n        -------\n        self : returns an instance of self.\n\n        \"\"\"\n        if not isinstance(self.basis_function, Polynomial):\n            raise NotImplementedError(\n                \"Currently MetaMSS only supports polynomial models.\"\n            )\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        if X is not None:\n            check_X_y(X, y)\n            self.n_inputs = _num_features(X)\n        else:\n            self.n_inputs = 1  # just to create the regressor space base\n\n        self.max_lag = self._get_max_lag()\n        self.regressor_code = self.regressor_space(self.n_inputs)\n        self.dimension = self.regressor_code.shape[0]\n        velocity = np.zeros([self.dimension, self.n_agents])\n        self.random_state = check_random_state(self.random_state)\n        population = self.generate_random_population(self.random_state)\n        self.best_by_iter = []\n        self.mean_by_iter = []\n        self.optimal_fitness_value = np.inf\n        self.optimal_model = None\n        self.best_model_history = []\n        self.tested_models = []\n\n        X, X_test, y, y_test = train_test_split(X, y, test_size=self.test_size)\n\n        for i in range(self.maxiter):\n            fitness = self.evaluate_objective_function(X, y, X_test, y_test, population)\n            column_of_best_solution = np.nanargmin(fitness)\n            current_best_fitness = fitness[column_of_best_solution]\n\n            if current_best_fitness &lt; self.optimal_fitness_value:\n                self.optimal_fitness_value = current_best_fitness\n                self.optimal_model = population[:, column_of_best_solution].copy()\n                self.best_model_history.append(self.optimal_model)\n\n            self.best_by_iter.append(self.optimal_fitness_value)\n            self.mean_by_iter.append(np.mean(fitness))\n            agent_mass = self.mass_calculation(fitness)\n            gravitational_constant = self.calculate_gravitational_constant(i)\n            acceleration = self.calculate_acceleration(\n                population, agent_mass, gravitational_constant, i\n            )\n            velocity, population = self.update_velocity_position(\n                population,\n                acceleration,\n                velocity,\n                i,\n            )\n\n        self.final_model = self.regressor_code[self.optimal_model == 1].copy()\n        _ = self.simulate(\n            X_train=X,\n            y_train=y,\n            X_test=X_test,\n            y_test=y_test,\n            model_code=self.final_model,\n            steps_ahead=self.steps_ahead,\n        )\n        self.max_lag = self._get_max_lag()\n        return self\n\n    def evaluate_objective_function(\n        self,\n        X_train: Optional[np.ndarray],\n        y_train: Optional[np.ndarray],\n        X_test: Optional[np.ndarray],\n        y_test: Optional[np.ndarray],\n        population: np.ndarray,\n    ):\n        \"\"\"Fit the polynomial NARMAX model.\n\n        Parameters\n        ----------\n        X_train : ndarray of floats\n            The input data to be used in the training process.\n        y_train : ndarray of floats\n            The output data to be used in the training process.\n        X_test : ndarray of floats\n            The input data to be used in the prediction process.\n        y_test : ndarray of floats\n            The output data (initial conditions) to be used in the prediction process.\n        population : ndarray of zeros and ones\n            The initial population of agents.\n\n        Returns\n        -------\n        fitness_value : ndarray\n            The fitness value of each agent.\n        \"\"\"\n        fitness = []\n        for agent in population.T:\n            if np.all(agent == 0):\n                fitness.append(30)  # penalty for cases where there is no terms\n                continue\n\n            m = self.regressor_code[agent == 1].copy()\n            yhat = self.simulate(\n                X_train=X_train,\n                y_train=y_train,\n                X_test=X_test,\n                y_test=y_test,\n                model_code=m,\n                steps_ahead=self.steps_ahead,\n            )\n\n            residues = y_test - yhat\n            self.max_lag = self._get_max_lag()\n            lagged_data = self.build_matrix(X_train, y_train)\n\n            psi = self.basis_function.fit(\n                lagged_data, self.max_lag, predefined_regressors=self.pivv\n            )\n\n            pos_insignificant_terms, _, _ = self.perform_t_test(\n                psi, self.theta, residues\n            )\n\n            pos_aux = np.where(agent == 1)[0]\n            pos_aux = pos_aux[pos_insignificant_terms]\n            agent[pos_aux] = 0\n\n            m = self.regressor_code[agent == 1].copy()\n\n            if np.all(agent == 0):\n                fitness.append(1000)  # just a big number as penalty\n                continue\n\n            yhat = self.simulate(\n                X_train=X_train,\n                y_train=y_train,\n                X_test=X_test,\n                y_test=y_test,\n                model_code=m,\n                steps_ahead=self.steps_ahead,\n            )\n\n            self.final_model = m.copy()\n            self.tested_models.append(m)\n            if len(self.theta) == 0:\n                print(m)\n            d = getattr(self, self.loss_func)(y_test, yhat, len(self.theta))\n            fitness.append(d)\n\n        return fitness\n\n    def perform_t_test(\n        self, psi: np.ndarray, theta: np.ndarray, residues: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Perform the t-test given the p-value defined by the user.\n\n        Parameters\n        ----------\n        psi : array\n            the data matrix of regressors\n        theta : array\n            the parameters estimated via least squares algorithm\n        residues : array\n            the identification residues of the solution\n\n        Returns\n        -------\n        pos_insignificant_terms : array\n            these regressors in the actual candidate solution are removed\n            from the population since they are insignificant\n        t_test : array\n            the values of the p_value of each regressor of the model\n        tail2p: array\n            The calculated two-tailed p-value.\n\n        \"\"\"\n        sum_of_squared_residues = np.sum(residues**2)\n        variance_of_residues = (sum_of_squared_residues) / (\n            len(residues) - psi.shape[1]\n        )\n        if np.isnan(variance_of_residues):\n            variance_of_residues = 4.3645e05\n\n        skk = np.linalg.pinv(psi.T.dot(psi))\n        skk_diag = np.diag(skk)\n        var_e = variance_of_residues * skk_diag\n        se_theta = np.sqrt(var_e)\n        se_theta = se_theta.reshape(-1, 1)\n        t_test = theta / se_theta\n        degree_of_freedom = psi.shape[0] - psi.shape[1]\n\n        tail2p = 2 * t.cdf(-np.abs(t_test), degree_of_freedom)\n\n        pos_insignificant_terms = np.where(tail2p &gt; self.p_value)[0]\n        pos_insignificant_terms = pos_insignificant_terms.reshape(-1, 1).T\n        if pos_insignificant_terms.shape == 0:\n            return np.array([]), t_test, tail2p\n\n        return pos_insignificant_terms, t_test, tail2p\n\n    def aic(self, y_test: np.ndarray, yhat: np.ndarray, n_theta: int) -&gt; float:\n        \"\"\"Calculate the Akaike Information Criterion.\n\n        Parameters\n        ----------\n        y_test : ndarray of floats\n            The output data (initial conditions) to be used in the prediction process.\n        yhat : ndarray of floats\n            The n-steps-ahead predicted values of the model.\n        n_theta : ndarray of floats\n            The number of model parameters.\n\n        Returns\n        -------\n        aic : float\n            The Akaike Information Criterion\n\n        \"\"\"\n        mse = mean_squared_error(y_test, yhat)\n        n = y_test.shape[0]\n        return n * np.log(mse) + 2 * n_theta\n\n    def bic(self, y_test: np.ndarray, yhat: np.ndarray, n_theta: int) -&gt; float:\n        \"\"\"Calculate the Bayesian Information Criterion.\n\n        Parameters\n        ----------\n        y_test : ndarray of floats\n            The output data (initial conditions) to be used in the prediction process.\n        yhat : ndarray of floats\n            The n-steps-ahead predicted values of the model.\n        n_theta : ndarray of floats\n            The number of model parameters.\n\n        Returns\n        -------\n        bic : float\n            The Bayesian Information Criterion\n\n        \"\"\"\n        mse = mean_squared_error(y_test, yhat)\n        n = y_test.shape[0]\n        return n * np.log(mse) + n_theta + np.log(n)\n\n    def metamss_loss(self, y_test: np.ndarray, yhat: np.ndarray, n_terms: int) -&gt; float:\n        \"\"\"Calculate the MetaMSS loss function.\n\n        Parameters\n        ----------\n        y_test : ndarray of floats\n            The output data (initial conditions) to be used in the prediction process.\n        yhat : ndarray of floats\n            The n-steps-ahead predicted values of the model.\n        n_terms : ndarray of floats\n            The number of model parameters.\n\n        Returns\n        -------\n        metamss_loss : float\n            The MetaMSS loss function\n\n        \"\"\"\n        penalty_count = np.arange(0, self.dimension)\n        penalty_distribution = (np.log(n_terms + 1) ** (-1)) / self.dimension\n        penalty = self.sigmoid_linear_unit_derivative(\n            penalty_count, self.dimension / 2, penalty_distribution\n        )\n\n        penalty = penalty - np.min(penalty)\n        rmse = root_relative_squared_error(y_test, yhat)\n        fitness = rmse * penalty[n_terms]\n        if np.isnan(fitness):\n            fitness = 30\n\n        return fitness\n\n    def sigmoid_linear_unit_derivative(self, x, c, a):\n        \"\"\"Calculate the derivative of the Sigmoid Linear Unit function.\n\n        The derivative of Sigmoid Linear Unit (dSiLU) function can be\n        viewed as a overshooting version of the sigmoid function.\n\n        Parameters\n        ----------\n        x : ndarray\n            The range of the regressors space.\n        a : float\n            The rate of change.\n        c : int\n            Corresponds to the x value where y = 0.5.\n\n        Returns\n        -------\n        penalty : ndarray of floats\n            The values of the penalty function\n\n        \"\"\"\n        return (\n            1\n            / (1 + np.exp(-a * (x - c)))\n            * (1 + (a * (x - c)) * (1 - 1 / (1 + np.exp(-a * (x - c)))))\n        )\n\n    def predict(\n        self,\n        *,\n        X: Optional[np.ndarray] = None,\n        y: Optional[np.ndarray] = None,\n        steps_ahead: Optional[int] = None,\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        \"\"\"Return the predicted values given an input.\n\n        The predict function allows a friendly usage by the user.\n        Given a previously trained model, predict values given\n        a new set of data.\n\n        This method accept y values mainly for prediction n-steps ahead\n        (to be implemented in the future)\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            The predicted values of the model.\n\n        \"\"\"\n        if self.basis_function.__class__.__name__ == \"Polynomial\":\n            if steps_ahead is None:\n                yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n            if steps_ahead == 1:\n                yhat = self._one_step_ahead_prediction(X, y)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n\n            _check_positive_int(steps_ahead, \"steps_ahead\")\n            yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        raise NotImplementedError(\n            \"MetaMSS doesn't support basis functions other than polynomial yet.\",\n        )\n\n    def _one_step_ahead_prediction(\n        self, X: Optional[np.ndarray], y: Optional[np.ndarray]\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        yhat = super()._one_step_ahead_prediction(X, y)\n        return yhat.reshape(-1, 1)\n\n    def _n_step_ahead_prediction(\n        self,\n        X: Optional[np.ndarray],\n        y: Optional[np.ndarray],\n        steps_ahead: Optional[int],\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        yhat = super()._n_step_ahead_prediction(X, y, steps_ahead)\n        return yhat\n\n    def _model_prediction(\n        self,\n        X: Optional[np.ndarray],\n        y_initial: Optional[np.ndarray],\n        forecast_horizon: int = 1,\n    ):\n        \"\"\"Perform the infinity steps-ahead simulation of a model.\n\n        Parameters\n        ----------\n        y_initial : array-like of shape = max_lag\n            Number of initial conditions values of output\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The predicted values of the model.\n\n        \"\"\"\n        if self.model_type in [\"NARMAX\", \"NAR\"]:\n            return self._narmax_predict(X, y_initial, forecast_horizon)\n        if self.model_type == \"NFIR\":\n            return self._nfir_predict(X, y_initial)\n\n        raise ValueError(\n            f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n        )\n\n    def _narmax_predict(\n        self,\n        X: Optional[np.ndarray],\n        y_initial: Optional[np.ndarray],\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        y_output = super()._narmax_predict(X, y_initial, forecast_horizon)\n        return y_output\n\n    def _nfir_predict(\n        self, X: Optional[np.ndarray], y_initial: Optional[np.ndarray]\n    ) -&gt; np.ndarray:\n        y_output = super()._nfir_predict(X, y_initial)\n        return y_output\n\n    def _basis_function_predict(self, X, y_initial, forecast_horizon=None):\n        \"\"\"Not implemented.\"\"\"\n        raise NotImplementedError(\n            \"You can only use Polynomial Basis Function in MetaMSS for now.\"\n        )\n\n    def _basis_function_n_step_prediction(self, X, y, steps_ahead, forecast_horizon):\n        \"\"\"Not implemented.\"\"\"\n        raise NotImplementedError(\n            \"You can only use Polynomial Basis Function in MetaMSS for now.\"\n        )\n\n    def _basis_function_n_steps_horizon(self, X, y, steps_ahead, forecast_horizon):\n        \"\"\"Not implemented.\"\"\"\n        raise NotImplementedError(\n            \"You can only use Polynomial Basis Function in MetaMSS for now.\"\n        )\n</code></pre>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.aic","title":"<code>aic(y_test, yhat, n_theta)</code>","text":"<p>Calculate the Akaike Information Criterion.</p>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.aic--parameters","title":"Parameters","text":"<p>y_test : ndarray of floats     The output data (initial conditions) to be used in the prediction process. yhat : ndarray of floats     The n-steps-ahead predicted values of the model. n_theta : ndarray of floats     The number of model parameters.</p>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.aic--returns","title":"Returns","text":"<p>aic : float     The Akaike Information Criterion</p> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def aic(self, y_test: np.ndarray, yhat: np.ndarray, n_theta: int) -&gt; float:\n    \"\"\"Calculate the Akaike Information Criterion.\n\n    Parameters\n    ----------\n    y_test : ndarray of floats\n        The output data (initial conditions) to be used in the prediction process.\n    yhat : ndarray of floats\n        The n-steps-ahead predicted values of the model.\n    n_theta : ndarray of floats\n        The number of model parameters.\n\n    Returns\n    -------\n    aic : float\n        The Akaike Information Criterion\n\n    \"\"\"\n    mse = mean_squared_error(y_test, yhat)\n    n = y_test.shape[0]\n    return n * np.log(mse) + 2 * n_theta\n</code></pre>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.bic","title":"<code>bic(y_test, yhat, n_theta)</code>","text":"<p>Calculate the Bayesian Information Criterion.</p>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.bic--parameters","title":"Parameters","text":"<p>y_test : ndarray of floats     The output data (initial conditions) to be used in the prediction process. yhat : ndarray of floats     The n-steps-ahead predicted values of the model. n_theta : ndarray of floats     The number of model parameters.</p>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.bic--returns","title":"Returns","text":"<p>bic : float     The Bayesian Information Criterion</p> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def bic(self, y_test: np.ndarray, yhat: np.ndarray, n_theta: int) -&gt; float:\n    \"\"\"Calculate the Bayesian Information Criterion.\n\n    Parameters\n    ----------\n    y_test : ndarray of floats\n        The output data (initial conditions) to be used in the prediction process.\n    yhat : ndarray of floats\n        The n-steps-ahead predicted values of the model.\n    n_theta : ndarray of floats\n        The number of model parameters.\n\n    Returns\n    -------\n    bic : float\n        The Bayesian Information Criterion\n\n    \"\"\"\n    mse = mean_squared_error(y_test, yhat)\n    n = y_test.shape[0]\n    return n * np.log(mse) + n_theta + np.log(n)\n</code></pre>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.evaluate_objective_function","title":"<code>evaluate_objective_function(X_train, y_train, X_test, y_test, population)</code>","text":"<p>Fit the polynomial NARMAX model.</p>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.evaluate_objective_function--parameters","title":"Parameters","text":"<p>X_train : ndarray of floats     The input data to be used in the training process. y_train : ndarray of floats     The output data to be used in the training process. X_test : ndarray of floats     The input data to be used in the prediction process. y_test : ndarray of floats     The output data (initial conditions) to be used in the prediction process. population : ndarray of zeros and ones     The initial population of agents.</p>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.evaluate_objective_function--returns","title":"Returns","text":"<p>fitness_value : ndarray     The fitness value of each agent.</p> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def evaluate_objective_function(\n    self,\n    X_train: Optional[np.ndarray],\n    y_train: Optional[np.ndarray],\n    X_test: Optional[np.ndarray],\n    y_test: Optional[np.ndarray],\n    population: np.ndarray,\n):\n    \"\"\"Fit the polynomial NARMAX model.\n\n    Parameters\n    ----------\n    X_train : ndarray of floats\n        The input data to be used in the training process.\n    y_train : ndarray of floats\n        The output data to be used in the training process.\n    X_test : ndarray of floats\n        The input data to be used in the prediction process.\n    y_test : ndarray of floats\n        The output data (initial conditions) to be used in the prediction process.\n    population : ndarray of zeros and ones\n        The initial population of agents.\n\n    Returns\n    -------\n    fitness_value : ndarray\n        The fitness value of each agent.\n    \"\"\"\n    fitness = []\n    for agent in population.T:\n        if np.all(agent == 0):\n            fitness.append(30)  # penalty for cases where there is no terms\n            continue\n\n        m = self.regressor_code[agent == 1].copy()\n        yhat = self.simulate(\n            X_train=X_train,\n            y_train=y_train,\n            X_test=X_test,\n            y_test=y_test,\n            model_code=m,\n            steps_ahead=self.steps_ahead,\n        )\n\n        residues = y_test - yhat\n        self.max_lag = self._get_max_lag()\n        lagged_data = self.build_matrix(X_train, y_train)\n\n        psi = self.basis_function.fit(\n            lagged_data, self.max_lag, predefined_regressors=self.pivv\n        )\n\n        pos_insignificant_terms, _, _ = self.perform_t_test(\n            psi, self.theta, residues\n        )\n\n        pos_aux = np.where(agent == 1)[0]\n        pos_aux = pos_aux[pos_insignificant_terms]\n        agent[pos_aux] = 0\n\n        m = self.regressor_code[agent == 1].copy()\n\n        if np.all(agent == 0):\n            fitness.append(1000)  # just a big number as penalty\n            continue\n\n        yhat = self.simulate(\n            X_train=X_train,\n            y_train=y_train,\n            X_test=X_test,\n            y_test=y_test,\n            model_code=m,\n            steps_ahead=self.steps_ahead,\n        )\n\n        self.final_model = m.copy()\n        self.tested_models.append(m)\n        if len(self.theta) == 0:\n            print(m)\n        d = getattr(self, self.loss_func)(y_test, yhat, len(self.theta))\n        fitness.append(d)\n\n    return fitness\n</code></pre>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.fit","title":"<code>fit(*, X=None, y=None)</code>","text":"<p>Fit the polynomial NARMAX model.</p>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.fit--parameters","title":"Parameters","text":"<p>X : ndarray, optional     The input data to be used in the training process. y : ndarray     The output data to be used in the training process.</p>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.fit--returns","title":"Returns","text":"<p>self : returns an instance of self.</p> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def fit(\n    self,\n    *,\n    X: Optional[np.ndarray] = None,\n    y: Optional[np.ndarray] = None,\n):\n    \"\"\"Fit the polynomial NARMAX model.\n\n    Parameters\n    ----------\n    X : ndarray, optional\n        The input data to be used in the training process.\n    y : ndarray\n        The output data to be used in the training process.\n\n    Returns\n    -------\n    self : returns an instance of self.\n\n    \"\"\"\n    if not isinstance(self.basis_function, Polynomial):\n        raise NotImplementedError(\n            \"Currently MetaMSS only supports polynomial models.\"\n        )\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    if X is not None:\n        check_X_y(X, y)\n        self.n_inputs = _num_features(X)\n    else:\n        self.n_inputs = 1  # just to create the regressor space base\n\n    self.max_lag = self._get_max_lag()\n    self.regressor_code = self.regressor_space(self.n_inputs)\n    self.dimension = self.regressor_code.shape[0]\n    velocity = np.zeros([self.dimension, self.n_agents])\n    self.random_state = check_random_state(self.random_state)\n    population = self.generate_random_population(self.random_state)\n    self.best_by_iter = []\n    self.mean_by_iter = []\n    self.optimal_fitness_value = np.inf\n    self.optimal_model = None\n    self.best_model_history = []\n    self.tested_models = []\n\n    X, X_test, y, y_test = train_test_split(X, y, test_size=self.test_size)\n\n    for i in range(self.maxiter):\n        fitness = self.evaluate_objective_function(X, y, X_test, y_test, population)\n        column_of_best_solution = np.nanargmin(fitness)\n        current_best_fitness = fitness[column_of_best_solution]\n\n        if current_best_fitness &lt; self.optimal_fitness_value:\n            self.optimal_fitness_value = current_best_fitness\n            self.optimal_model = population[:, column_of_best_solution].copy()\n            self.best_model_history.append(self.optimal_model)\n\n        self.best_by_iter.append(self.optimal_fitness_value)\n        self.mean_by_iter.append(np.mean(fitness))\n        agent_mass = self.mass_calculation(fitness)\n        gravitational_constant = self.calculate_gravitational_constant(i)\n        acceleration = self.calculate_acceleration(\n            population, agent_mass, gravitational_constant, i\n        )\n        velocity, population = self.update_velocity_position(\n            population,\n            acceleration,\n            velocity,\n            i,\n        )\n\n    self.final_model = self.regressor_code[self.optimal_model == 1].copy()\n    _ = self.simulate(\n        X_train=X,\n        y_train=y,\n        X_test=X_test,\n        y_test=y_test,\n        model_code=self.final_model,\n        steps_ahead=self.steps_ahead,\n    )\n    self.max_lag = self._get_max_lag()\n    return self\n</code></pre>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.metamss_loss","title":"<code>metamss_loss(y_test, yhat, n_terms)</code>","text":"<p>Calculate the MetaMSS loss function.</p>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.metamss_loss--parameters","title":"Parameters","text":"<p>y_test : ndarray of floats     The output data (initial conditions) to be used in the prediction process. yhat : ndarray of floats     The n-steps-ahead predicted values of the model. n_terms : ndarray of floats     The number of model parameters.</p>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.metamss_loss--returns","title":"Returns","text":"<p>metamss_loss : float     The MetaMSS loss function</p> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def metamss_loss(self, y_test: np.ndarray, yhat: np.ndarray, n_terms: int) -&gt; float:\n    \"\"\"Calculate the MetaMSS loss function.\n\n    Parameters\n    ----------\n    y_test : ndarray of floats\n        The output data (initial conditions) to be used in the prediction process.\n    yhat : ndarray of floats\n        The n-steps-ahead predicted values of the model.\n    n_terms : ndarray of floats\n        The number of model parameters.\n\n    Returns\n    -------\n    metamss_loss : float\n        The MetaMSS loss function\n\n    \"\"\"\n    penalty_count = np.arange(0, self.dimension)\n    penalty_distribution = (np.log(n_terms + 1) ** (-1)) / self.dimension\n    penalty = self.sigmoid_linear_unit_derivative(\n        penalty_count, self.dimension / 2, penalty_distribution\n    )\n\n    penalty = penalty - np.min(penalty)\n    rmse = root_relative_squared_error(y_test, yhat)\n    fitness = rmse * penalty[n_terms]\n    if np.isnan(fitness):\n        fitness = 30\n\n    return fitness\n</code></pre>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.perform_t_test","title":"<code>perform_t_test(psi, theta, residues)</code>","text":"<p>Perform the t-test given the p-value defined by the user.</p>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.perform_t_test--parameters","title":"Parameters","text":"<p>psi : array     the data matrix of regressors theta : array     the parameters estimated via least squares algorithm residues : array     the identification residues of the solution</p>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.perform_t_test--returns","title":"Returns","text":"<p>pos_insignificant_terms : array     these regressors in the actual candidate solution are removed     from the population since they are insignificant t_test : array     the values of the p_value of each regressor of the model tail2p: array     The calculated two-tailed p-value.</p> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def perform_t_test(\n    self, psi: np.ndarray, theta: np.ndarray, residues: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Perform the t-test given the p-value defined by the user.\n\n    Parameters\n    ----------\n    psi : array\n        the data matrix of regressors\n    theta : array\n        the parameters estimated via least squares algorithm\n    residues : array\n        the identification residues of the solution\n\n    Returns\n    -------\n    pos_insignificant_terms : array\n        these regressors in the actual candidate solution are removed\n        from the population since they are insignificant\n    t_test : array\n        the values of the p_value of each regressor of the model\n    tail2p: array\n        The calculated two-tailed p-value.\n\n    \"\"\"\n    sum_of_squared_residues = np.sum(residues**2)\n    variance_of_residues = (sum_of_squared_residues) / (\n        len(residues) - psi.shape[1]\n    )\n    if np.isnan(variance_of_residues):\n        variance_of_residues = 4.3645e05\n\n    skk = np.linalg.pinv(psi.T.dot(psi))\n    skk_diag = np.diag(skk)\n    var_e = variance_of_residues * skk_diag\n    se_theta = np.sqrt(var_e)\n    se_theta = se_theta.reshape(-1, 1)\n    t_test = theta / se_theta\n    degree_of_freedom = psi.shape[0] - psi.shape[1]\n\n    tail2p = 2 * t.cdf(-np.abs(t_test), degree_of_freedom)\n\n    pos_insignificant_terms = np.where(tail2p &gt; self.p_value)[0]\n    pos_insignificant_terms = pos_insignificant_terms.reshape(-1, 1).T\n    if pos_insignificant_terms.shape == 0:\n        return np.array([]), t_test, tail2p\n\n    return pos_insignificant_terms, t_test, tail2p\n</code></pre>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.predict","title":"<code>predict(*, X=None, y=None, steps_ahead=None, forecast_horizon=1)</code>","text":"<p>Return the predicted values given an input.</p> <p>The predict function allows a friendly usage by the user. Given a previously trained model, predict values given a new set of data.</p> <p>This method accept y values mainly for prediction n-steps ahead (to be implemented in the future)</p>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.predict--parameters","title":"Parameters","text":"<p>X : ndarray of floats     The input data to be used in the prediction process. y : ndarray of floats     The output data to be used in the prediction process. steps_ahead : int (default = None)     The user can use free run simulation, one-step ahead prediction     and n-step ahead prediction. forecast_horizon : int, default=None     The number of predictions over the time.</p>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.predict--returns","title":"Returns","text":"<p>yhat : ndarray of floats     The predicted values of the model.</p> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def predict(\n    self,\n    *,\n    X: Optional[np.ndarray] = None,\n    y: Optional[np.ndarray] = None,\n    steps_ahead: Optional[int] = None,\n    forecast_horizon: int = 1,\n) -&gt; np.ndarray:\n    \"\"\"Return the predicted values given an input.\n\n    The predict function allows a friendly usage by the user.\n    Given a previously trained model, predict values given\n    a new set of data.\n\n    This method accept y values mainly for prediction n-steps ahead\n    (to be implemented in the future)\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    steps_ahead : int (default = None)\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction.\n    forecast_horizon : int, default=None\n        The number of predictions over the time.\n\n    Returns\n    -------\n    yhat : ndarray of floats\n        The predicted values of the model.\n\n    \"\"\"\n    if self.basis_function.__class__.__name__ == \"Polynomial\":\n        if steps_ahead is None:\n            yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        _check_positive_int(steps_ahead, \"steps_ahead\")\n        yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    raise NotImplementedError(\n        \"MetaMSS doesn't support basis functions other than polynomial yet.\",\n    )\n</code></pre>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.sigmoid_linear_unit_derivative","title":"<code>sigmoid_linear_unit_derivative(x, c, a)</code>","text":"<p>Calculate the derivative of the Sigmoid Linear Unit function.</p> <p>The derivative of Sigmoid Linear Unit (dSiLU) function can be viewed as a overshooting version of the sigmoid function.</p>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.sigmoid_linear_unit_derivative--parameters","title":"Parameters","text":"<p>x : ndarray     The range of the regressors space. a : float     The rate of change. c : int     Corresponds to the x value where y = 0.5.</p>"},{"location":"code/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.sigmoid_linear_unit_derivative--returns","title":"Returns","text":"<p>penalty : ndarray of floats     The values of the penalty function</p> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def sigmoid_linear_unit_derivative(self, x, c, a):\n    \"\"\"Calculate the derivative of the Sigmoid Linear Unit function.\n\n    The derivative of Sigmoid Linear Unit (dSiLU) function can be\n    viewed as a overshooting version of the sigmoid function.\n\n    Parameters\n    ----------\n    x : ndarray\n        The range of the regressors space.\n    a : float\n        The rate of change.\n    c : int\n        Corresponds to the x value where y = 0.5.\n\n    Returns\n    -------\n    penalty : ndarray of floats\n        The values of the penalty function\n\n    \"\"\"\n    return (\n        1\n        / (1 + np.exp(-a * (x - c)))\n        * (1 + (a * (x - c)) * (1 - 1 / (1 + np.exp(-a * (x - c)))))\n    )\n</code></pre>"},{"location":"code/metrics/","title":"Documentation for <code>Metrics</code>","text":"<p>Common metrics to assess performance on NARX models.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.explained_variance_score","title":"<code>explained_variance_score(y, yhat)</code>","text":"<p>Calculate the Explained Variance Score.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.explained_variance_score--parameters","title":"Parameters","text":"<p>y : array-like of shape = number_of_outputs     Represent the target values. yhat : array-like of shape = number_of_outputs     Target values predicted by the model.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.explained_variance_score--returns","title":"Returns","text":"<p>loss : float     EVS output is non-negative values. Becoming 1.0 means your     model outputs are exactly matched by true target values.     Lower values means worse results.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.explained_variance_score--references","title":"References","text":"<ul> <li>Wikipedia entry on the Explained Variance    https://en.wikipedia.org/wiki/Explained_variation</li> </ul>"},{"location":"code/metrics/#sysidentpy.metrics._regression.explained_variance_score--examples","title":"Examples","text":"<p>y = [3, -0.5, 2, 7] yhat = [2.5, 0.0, 2, 8] explained_variance_score(y, yhat) 0.957</p> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def explained_variance_score(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the Explained Variance Score.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        EVS output is non-negative values. Becoming 1.0 means your\n        model outputs are exactly matched by true target values.\n        Lower values means worse results.\n\n    References\n    ----------\n    - Wikipedia entry on the Explained Variance\n       https://en.wikipedia.org/wiki/Explained_variation\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; explained_variance_score(y, yhat)\n    0.957\n\n    \"\"\"\n    y_diff_avg = np.average(y - yhat)\n    numerator = np.average((y - yhat - y_diff_avg) ** 2)\n    y_avg = np.average(y)\n    denominator = np.average((y - y_avg) ** 2)\n    nonzero_numerator = numerator != 0\n    nonzero_denominator = denominator != 0\n    valid_score = nonzero_numerator &amp; nonzero_denominator\n    output_scores = np.ones(y.shape[0])\n    output_scores[valid_score] = 1 - (numerator[valid_score] / denominator[valid_score])\n    output_scores[nonzero_numerator &amp; ~nonzero_denominator] = 0.0\n    return np.average(output_scores)\n</code></pre>"},{"location":"code/metrics/#sysidentpy.metrics._regression.forecast_error","title":"<code>forecast_error(y, yhat)</code>","text":"<p>Calculate the forecast error in a regression model.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.forecast_error--parameters","title":"Parameters","text":"<p>y : array-like of shape = number_of_outputs     Represent the target values. yhat : array-like of shape = number_of_outputs     Target values predicted by the model.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.forecast_error--returns","title":"Returns","text":"<p>loss : ndarray of floats     The difference between the true target values and the predicted     or forecast value in regression or any other phenomenon.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.forecast_error--references","title":"References","text":"<ul> <li>Wikipedia entry on the Forecast error    https://en.wikipedia.org/wiki/Forecast_error</li> </ul>"},{"location":"code/metrics/#sysidentpy.metrics._regression.forecast_error--examples","title":"Examples","text":"<p>y = [3, -0.5, 2, 7] yhat = [2.5, 0.0, 2, 8] forecast_error(y, yhat) [0.5, -0.5, 0, -1]</p> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def forecast_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the forecast error in a regression model.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : ndarray of floats\n        The difference between the true target values and the predicted\n        or forecast value in regression or any other phenomenon.\n\n    References\n    ----------\n    - Wikipedia entry on the Forecast error\n       https://en.wikipedia.org/wiki/Forecast_error\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; forecast_error(y, yhat)\n    [0.5, -0.5, 0, -1]\n\n    \"\"\"\n    return np.array(y - yhat)\n</code></pre>"},{"location":"code/metrics/#sysidentpy.metrics._regression.mean_absolute_error","title":"<code>mean_absolute_error(y, yhat)</code>","text":"<p>Calculate the Mean absolute error.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.mean_absolute_error--parameters","title":"Parameters","text":"<p>y : array-like of shape = number_of_outputs     Represent the target values. yhat : array-like of shape = number_of_outputs     Target values predicted by the model.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.mean_absolute_error--returns","title":"Returns","text":"<p>loss : float or ndarray of floats     MAE output is non-negative values. Becoming 0.0 means your     model outputs are exactly matched by true target values.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.mean_absolute_error--references","title":"References","text":"<ul> <li>Wikipedia entry on the Mean absolute error    https://en.wikipedia.org/wiki/Mean_absolute_error</li> </ul>"},{"location":"code/metrics/#sysidentpy.metrics._regression.mean_absolute_error--examples","title":"Examples","text":"<p>y = [3, -0.5, 2, 7] yhat = [2.5, 0.0, 2, 8] mean_absolute_error(y, yhat) 0.5</p> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def mean_absolute_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the Mean absolute error.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float or ndarray of floats\n        MAE output is non-negative values. Becoming 0.0 means your\n        model outputs are exactly matched by true target values.\n\n    References\n    ----------\n    - Wikipedia entry on the Mean absolute error\n       https://en.wikipedia.org/wiki/Mean_absolute_error\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; mean_absolute_error(y, yhat)\n    0.5\n\n    \"\"\"\n    output_errors = np.average(np.abs(y - yhat))\n    return np.average(output_errors)\n</code></pre>"},{"location":"code/metrics/#sysidentpy.metrics._regression.mean_forecast_error","title":"<code>mean_forecast_error(y, yhat)</code>","text":"<p>Calculate the mean of forecast error of a regression model.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.mean_forecast_error--parameters","title":"Parameters","text":"<p>y : array-like of shape = number_of_outputs     Represent the target values. yhat : array-like of shape = number_of_outputs     Target values predicted by the model.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.mean_forecast_error--returns","title":"Returns","text":"<p>loss : float     The mean  value of the difference between the true target     values and the predicted or forecast value in regression     or any other phenomenon.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.mean_forecast_error--references","title":"References","text":"<ul> <li>Wikipedia entry on the Forecast error    https://en.wikipedia.org/wiki/Forecast_error</li> </ul>"},{"location":"code/metrics/#sysidentpy.metrics._regression.mean_forecast_error--examples","title":"Examples","text":"<p>y = [3, -0.5, 2, 7] yhat = [2.5, 0.0, 2, 8] mean_forecast_error(y, yhat) -0.25</p> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def mean_forecast_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the mean of forecast error of a regression model.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        The mean  value of the difference between the true target\n        values and the predicted or forecast value in regression\n        or any other phenomenon.\n\n    References\n    ----------\n    - Wikipedia entry on the Forecast error\n       https://en.wikipedia.org/wiki/Forecast_error\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; mean_forecast_error(y, yhat)\n    -0.25\n\n    \"\"\"\n    return np.average(y - yhat)\n</code></pre>"},{"location":"code/metrics/#sysidentpy.metrics._regression.mean_squared_error","title":"<code>mean_squared_error(y, yhat)</code>","text":"<p>Calculate the Mean Squared Error.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.mean_squared_error--parameters","title":"Parameters","text":"<p>y : array-like of shape = number_of_outputs     Represent the target values. yhat : array-like of shape = number_of_outputs     Target values predicted by the model.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.mean_squared_error--returns","title":"Returns","text":"<p>loss : float     MSE output is non-negative values. Becoming 0.0 means your     model outputs are exactly matched by true target values.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.mean_squared_error--references","title":"References","text":"<ul> <li>Wikipedia entry on the Mean Squared Error    https://en.wikipedia.org/wiki/Mean_squared_error</li> </ul>"},{"location":"code/metrics/#sysidentpy.metrics._regression.mean_squared_error--examples","title":"Examples","text":"<p>y = [3, -0.5, 2, 7] yhat = [2.5, 0.0, 2, 8] mean_squared_error(y, yhat) 0.375</p> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def mean_squared_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the Mean Squared Error.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        MSE output is non-negative values. Becoming 0.0 means your\n        model outputs are exactly matched by true target values.\n\n    References\n    ----------\n    - Wikipedia entry on the Mean Squared Error\n       https://en.wikipedia.org/wiki/Mean_squared_error\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; mean_squared_error(y, yhat)\n    0.375\n\n    \"\"\"\n    output_error = np.average((y - yhat) ** 2)\n    return np.average(output_error)\n</code></pre>"},{"location":"code/metrics/#sysidentpy.metrics._regression.mean_squared_log_error","title":"<code>mean_squared_log_error(y, yhat)</code>","text":"<p>Calculate the Mean Squared Logarithmic Error.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.mean_squared_log_error--parameters","title":"Parameters","text":"<p>y : array-like of shape = number_of_outputs     Represent the target values. yhat : array-like of shape = number_of_outputs     Target values predicted by the model.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.mean_squared_log_error--returns","title":"Returns","text":"<p>loss : float     MSLE output is non-negative values. Becoming 0.0 means your     model outputs are exactly matched by true target values.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.mean_squared_log_error--examples","title":"Examples","text":"<p>y = [3, 5, 2.5, 7] yhat = [2.5, 5, 4, 8] mean_squared_log_error(y, yhat) 0.039</p> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def mean_squared_log_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the Mean Squared Logarithmic Error.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        MSLE output is non-negative values. Becoming 0.0 means your\n        model outputs are exactly matched by true target values.\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, 5, 2.5, 7]\n    &gt;&gt;&gt; yhat = [2.5, 5, 4, 8]\n    &gt;&gt;&gt; mean_squared_log_error(y, yhat)\n    0.039\n\n    \"\"\"\n    return mean_squared_error(np.log1p(y), np.log1p(yhat))\n</code></pre>"},{"location":"code/metrics/#sysidentpy.metrics._regression.median_absolute_error","title":"<code>median_absolute_error(y, yhat)</code>","text":"<p>Calculate the Median Absolute Error.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.median_absolute_error--parameters","title":"Parameters","text":"<p>y : array-like of shape = number_of_outputs     Represent the target values. yhat : array-like of shape = number_of_outputs     Target values predicted by the model.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.median_absolute_error--returns","title":"Returns","text":"<p>loss : float     MdAE output is non-negative values. Becoming 0.0 means your     model outputs are exactly matched by true target values.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.median_absolute_error--references","title":"References","text":"<ul> <li>Wikipedia entry on the Median absolute deviation    https://en.wikipedia.org/wiki/Median_absolute_deviation</li> </ul>"},{"location":"code/metrics/#sysidentpy.metrics._regression.median_absolute_error--examples","title":"Examples","text":"<p>y = [3, -0.5, 2, 7] yhat = [2.5, 0.0, 2, 8] median_absolute_error(y, yhat) 0.5</p> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def median_absolute_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the Median Absolute Error.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        MdAE output is non-negative values. Becoming 0.0 means your\n        model outputs are exactly matched by true target values.\n\n    References\n    ----------\n    - Wikipedia entry on the Median absolute deviation\n       https://en.wikipedia.org/wiki/Median_absolute_deviation\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; median_absolute_error(y, yhat)\n    0.5\n\n    \"\"\"\n    return np.median(np.abs(y - yhat))\n</code></pre>"},{"location":"code/metrics/#sysidentpy.metrics._regression.normalized_root_mean_squared_error","title":"<code>normalized_root_mean_squared_error(y, yhat)</code>","text":"<p>Calculate the normalized Root Mean Squared Error.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.normalized_root_mean_squared_error--parameters","title":"Parameters","text":"<p>y : array-like of shape = number_of_outputs     Represent the target values. yhat : array-like of shape = number_of_outputs     Target values predicted by the model.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.normalized_root_mean_squared_error--returns","title":"Returns","text":"<p>loss : float     nRMSE output is non-negative values. Becoming 0.0 means your     model outputs are exactly matched by true target values.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.normalized_root_mean_squared_error--references","title":"References","text":"<ul> <li>Wikipedia entry on the normalized Root Mean Squared Error    https://en.wikipedia.org/wiki/Root-mean-square_deviation</li> </ul>"},{"location":"code/metrics/#sysidentpy.metrics._regression.normalized_root_mean_squared_error--examples","title":"Examples","text":"<p>y = [3, -0.5, 2, 7] yhat = [2.5, 0.0, 2, 8] normalized_root_mean_squared_error(y, yhat) 0.081</p> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def normalized_root_mean_squared_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the normalized Root Mean Squared Error.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        nRMSE output is non-negative values. Becoming 0.0 means your\n        model outputs are exactly matched by true target values.\n\n    References\n    ----------\n    - Wikipedia entry on the normalized Root Mean Squared Error\n       https://en.wikipedia.org/wiki/Root-mean-square_deviation\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; normalized_root_mean_squared_error(y, yhat)\n    0.081\n\n    \"\"\"\n    return root_mean_squared_error(y, yhat) / (y.max() - y.min())\n</code></pre>"},{"location":"code/metrics/#sysidentpy.metrics._regression.r2_score","title":"<code>r2_score(y, yhat)</code>","text":"<p>Calculate the R2 score. Based on sklearn solution.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.r2_score--parameters","title":"Parameters","text":"<p>y : array-like of shape = number_of_outputs     Represent the target values. yhat : array-like of shape = number_of_outputs     Target values predicted by the model.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.r2_score--returns","title":"Returns","text":"<p>loss : float     R2 output can be non-negative values or negative value.     Becoming 1.0 means your model outputs are exactly     matched by true target values. Lower values means worse results.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.r2_score--notes","title":"Notes","text":"<p>This is not a symmetric function.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.r2_score--references","title":"References","text":"<ul> <li>Wikipedia entry on the Coefficient of determination    https://en.wikipedia.org/wiki/Coefficient_of_determination</li> </ul>"},{"location":"code/metrics/#sysidentpy.metrics._regression.r2_score--examples","title":"Examples","text":"<p>y = [3, -0.5, 2, 7] yhat = [2.5, 0.0, 2, 8] explained_variance_score(y, yhat) 0.948</p> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def r2_score(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the R2 score. Based on sklearn solution.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        R2 output can be non-negative values or negative value.\n        Becoming 1.0 means your model outputs are exactly\n        matched by true target values. Lower values means worse results.\n\n    Notes\n    -----\n    This is not a symmetric function.\n\n    References\n    ----------\n    - Wikipedia entry on the Coefficient of determination\n       https://en.wikipedia.org/wiki/Coefficient_of_determination\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; explained_variance_score(y, yhat)\n    0.948\n\n    \"\"\"\n    numerator = ((y - yhat) ** 2).sum(axis=0, dtype=np.float64)\n    denominator = ((y - np.average(y, axis=0)) ** 2).sum(axis=0, dtype=np.float64)\n    nonzero_denominator = denominator != 0\n    nonzero_numerator = numerator != 0\n    valid_score = nonzero_denominator &amp; nonzero_numerator\n    output_scores = np.ones([y.shape[1]])\n    output_scores[valid_score] = 1 - (numerator[valid_score] / denominator[valid_score])\n    # arbitrary set to zero to avoid -inf scores, having a constant\n    # y_true is not interesting for scoring a regression anyway\n    output_scores[nonzero_numerator &amp; ~nonzero_denominator] = 0.0\n    return np.average(output_scores)\n</code></pre>"},{"location":"code/metrics/#sysidentpy.metrics._regression.root_mean_squared_error","title":"<code>root_mean_squared_error(y, yhat)</code>","text":"<p>Calculate the Root Mean Squared Error.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.root_mean_squared_error--parameters","title":"Parameters","text":"<p>y : array-like of shape = number_of_outputs     Represent the target values. yhat : array-like of shape = number_of_outputs     Target values predicted by the model.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.root_mean_squared_error--returns","title":"Returns","text":"<p>loss : float     RMSE output is non-negative values. Becoming 0.0 means your     model outputs are exactly matched by true target values.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.root_mean_squared_error--references","title":"References","text":"<ul> <li>Wikipedia entry on the Root Mean Squared Error    https://en.wikipedia.org/wiki/Root-mean-square_deviation</li> </ul>"},{"location":"code/metrics/#sysidentpy.metrics._regression.root_mean_squared_error--examples","title":"Examples","text":"<p>y = [3, -0.5, 2, 7] yhat = [2.5, 0.0, 2, 8] root_mean_squared_error(y, yhat) 0.612</p> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def root_mean_squared_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the Root Mean Squared Error.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        RMSE output is non-negative values. Becoming 0.0 means your\n        model outputs are exactly matched by true target values.\n\n    References\n    ----------\n    - Wikipedia entry on the Root Mean Squared Error\n       https://en.wikipedia.org/wiki/Root-mean-square_deviation\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; root_mean_squared_error(y, yhat)\n    0.612\n\n    \"\"\"\n    return np.sqrt(mean_squared_error(y, yhat))\n</code></pre>"},{"location":"code/metrics/#sysidentpy.metrics._regression.root_relative_squared_error","title":"<code>root_relative_squared_error(y, yhat)</code>","text":"<p>Calculate the Root Relative Mean Squared Error.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.root_relative_squared_error--parameters","title":"Parameters","text":"<p>y : array-like of shape = number_of_outputs     Represent the target values. yhat : array-like of shape = number_of_outputs     Target values predicted by the model.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.root_relative_squared_error--returns","title":"Returns","text":"<p>loss : float     RRSE output is non-negative values. Becoming 0.0 means your     model outputs are exactly matched by true target values.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.root_relative_squared_error--examples","title":"Examples","text":"<p>y = [3, -0.5, 2, 7] yhat = [2.5, 0.0, 2, 8] root_relative_mean_squared_error(y, yhat) 0.206</p> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def root_relative_squared_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the Root Relative Mean Squared Error.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        RRSE output is non-negative values. Becoming 0.0 means your\n        model outputs are exactly matched by true target values.\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; root_relative_mean_squared_error(y, yhat)\n    0.206\n\n    \"\"\"\n    numerator = np.sum(np.square((yhat - y)))\n    denominator = np.sum(np.square((y - np.mean(y, axis=0))))\n    return np.sqrt(np.divide(numerator, denominator))\n</code></pre>"},{"location":"code/metrics/#sysidentpy.metrics._regression.symmetric_mean_absolute_percentage_error","title":"<code>symmetric_mean_absolute_percentage_error(y, yhat)</code>","text":"<p>Calculate the SMAPE score.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.symmetric_mean_absolute_percentage_error--parameters","title":"Parameters","text":"<p>y : array-like of shape = number_of_outputs     Represent the target values. yhat : array-like of shape = number_of_outputs     Target values predicted by the model.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.symmetric_mean_absolute_percentage_error--returns","title":"Returns","text":"<p>loss : float     SMAPE output is a non-negative value.     The results are percentages values.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.symmetric_mean_absolute_percentage_error--notes","title":"Notes","text":"<p>One supposed problem with SMAPE is that it is not symmetric since over-forecasts and under-forecasts are not treated equally.</p>"},{"location":"code/metrics/#sysidentpy.metrics._regression.symmetric_mean_absolute_percentage_error--references","title":"References","text":"<ul> <li>Wikipedia entry on the Symmetric mean absolute percentage error    https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error</li> </ul>"},{"location":"code/metrics/#sysidentpy.metrics._regression.symmetric_mean_absolute_percentage_error--examples","title":"Examples","text":"<p>y = [3, -0.5, 2, 7] yhat = [2.5, 0.0, 2, 8] symmetric_mean_absolute_percentage_error(y, yhat) 57.87</p> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def symmetric_mean_absolute_percentage_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the SMAPE score.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        SMAPE output is a non-negative value.\n        The results are percentages values.\n\n    Notes\n    -----\n    One supposed problem with SMAPE is that it is not symmetric since\n    over-forecasts and under-forecasts are not treated equally.\n\n    References\n    ----------\n    - Wikipedia entry on the Symmetric mean absolute percentage error\n       https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; symmetric_mean_absolute_percentage_error(y, yhat)\n    57.87\n\n    \"\"\"\n    return 100 / len(y) * np.sum(2 * np.abs(yhat - y) / (np.abs(y) + np.abs(yhat)))\n</code></pre>"},{"location":"code/multiobjective-parameter-estimation/","title":"Documentation for <code>Multiobjective Parameter Estimation</code>","text":"<p>Affine Information Least Squares for NARMAX models.</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS","title":"<code>AILS</code>","text":"<p>Affine Information Least Squares (AILS) for NARMAX Parameter Estimation.</p> <p>AILS is a non-iterative multiobjective Least Squares technique used for finding Pareto-set solutions in NARMAX (Nonlinear AutoRegressive Moving Average with eXogenous inputs) model parameter estimation. This method is suitable for linear-in-the-parameter model structures.</p> <p>Two types of auxiliary information can be incorporated: static function and steady-state gain.</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS--parameters","title":"Parameters","text":"<p>static_gain : bool, default=True     Flag indicating the presence of data related to steady-state gain. static_function : bool, default=True     Flag indicating the presence of data concerning static function. final_model : ndarray, default=[[0], [0]]     Model code representation.</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS--references","title":"References","text":"<ol> <li>Nepomuceno, E. G., Takahashi, R. H. C., &amp; Aguirre, L. A. (2007). \"Multiobjective parameter estimation for nonlinear systems: Affine information and least-squares formulation.\" International Journal of Control, 80, 863-871.</li> </ol> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>class AILS:\n    \"\"\"Affine Information Least Squares (AILS) for NARMAX Parameter Estimation.\n\n    AILS is a non-iterative multiobjective Least Squares technique used for finding\n    Pareto-set solutions in NARMAX (Nonlinear AutoRegressive Moving Average with\n    eXogenous inputs) model parameter estimation. This method is suitable for\n    linear-in-the-parameter model structures.\n\n    Two types of auxiliary information can be incorporated: static function and\n    steady-state gain.\n\n    Parameters\n    ----------\n    static_gain : bool, default=True\n        Flag indicating the presence of data related to steady-state gain.\n    static_function : bool, default=True\n        Flag indicating the presence of data concerning static function.\n    final_model : ndarray, default=[[0], [0]]\n        Model code representation.\n\n    References\n    ----------\n    1. Nepomuceno, E. G., Takahashi, R. H. C., &amp; Aguirre, L. A. (2007).\n    \"Multiobjective parameter estimation for nonlinear systems: Affine information and\n    least-squares formulation.\"\n    International Journal of Control, 80, 863-871.\n    \"\"\"\n\n    def __init__(\n        self,\n        static_gain: bool = True,\n        static_function: bool = True,\n        final_model: np.ndarray = np.zeros((1, 1)),\n        normalize: bool = True,\n    ):\n        self.n_inputs = np.max(final_model // 1000) - 1\n        self.degree = np.shape(final_model)[1]\n        self.max_lag = 1\n        self.final_model = final_model\n        self.static_gain = static_gain\n        self.static_function = static_function\n        self.normalize = normalize\n\n    def get_term_clustering(self, qit: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Get the term clustering of the model.\n\n        This function takes a matrix `qit` and compute the term clustering based\n        on their values. It calculates the number of occurrences of each value\n        for each row in the matrix.\n\n        Parameters\n        ----------\n        qit : ndarray\n            Input matrix containing terms clustering to be sorted.\n\n        Returns\n        -------\n        N_aux : ndarray\n            A new matrix with rows representing the number of occurrences of each value\n            for each row in the input matrix `qit`. The columns correspond to different\n            values.\n\n        Examples\n        --------\n        &gt;&gt;&gt; qit = np.array([[1, 2, 2],\n        ...                 [1, 3, 1],\n        ...                 [2, 2, 3]])\n        &gt;&gt;&gt; result = get_term_clustering(qit)\n        &gt;&gt;&gt; print(result)\n        [[1. 2. 0. 0.]\n        [2. 0. 1. 0.]\n        [0. 2. 1. 0.]]\n\n        Notes\n        -----\n        The function calculates the number of occurrences of each value (from 1 to\n        the maximum value in the input matrix `qit`) for each row and returns a matrix\n        where rows represent rows of the input matrix `qit`, and columns represent\n        different values.\n\n        \"\"\"\n        max_value = int(np.max(qit))\n        counts_matrix = np.zeros((qit.shape[0], max_value))\n\n        for k in range(1, max_value + 1):\n            counts_matrix[:, k - 1] = np.sum(qit == k, axis=1)\n\n        return counts_matrix.astype(int)\n\n    def build_linear_mapping(self):\n        \"\"\"Assemble the linear mapping matrix R using the regressor-space method.\n\n        This function constructs the linear mapping matrix R, which plays a key role in\n        mapping the parameter vector to the cluster coefficients. It also generates a\n        row matrix qit that assists in locating terms within the linear mapping matrix.\n        This qit matrix is later used in creating the static regressor matrix (Q).\n\n        Returns\n        -------\n        R : ndarray of int\n            A constant matrix of ones and zeros that maps the parameter vector to\n            cluster coefficients.\n        qit : ndarray of int\n            A row matrix that helps locate terms within the linear mapping matrix R and\n            is used in the creation of the static regressor matrix (Q).\n\n        Notes\n        -----\n        The linear mapping matrix R is constructed using the regressor-space method.\n        It plays a crucial role in the parameter estimation process, facilitating the\n        mapping of parameter values to cluster coefficients. The qit matrix aids in\n        term localization within the linear mapping matrix R and is subsequently used\n        to build the static regressor matrix (Q).\n\n        \"\"\"\n        xlag = [1] * self.n_inputs\n\n        object_qit = RegressorDictionary(xlag=xlag, ylag=[1])\n        # Given xlag and ylag equal to 1, there is no repetition of terms, which is\n        # ideal for building qit.\n        qit = object_qit.regressor_space(n_inputs=self.n_inputs) // 1000\n        model = self.final_model // 1000\n        R = np.all(qit[:, None, :] == model, axis=2).astype(int)\n        # Find rows with all zeros in R (sum of row elements is 0)\n        null_rows = list(np.where(np.sum(R, axis=1) == 0)[0])\n\n        R = np.delete(R, null_rows, axis=0)\n        qit = np.delete(qit, null_rows, axis=0)\n        return R, self.get_term_clustering(qit)\n\n    def build_static_function_information(\n        self, X_static: np.ndarray, y_static: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Construct a matrix of static regressors for a NARMAX model.\n\n        Parameters\n        ----------\n        y_static : array-like, shape (n_samples_static_function,)\n            Output of the static function.\n        X_static : array-like, shape (n_samples_static_function,)\n            Static function input.\n\n        Returns\n        -------\n        Q_dot_R : ndarray of floats, shape (n_samples_static_function, n_parameters)\n            The result of multiplying the matrix of static regressors (Q) with the\n            linear mapping matrix (R), where n_parameters is the number of model\n            parameters.\n        static_covariance: ndarray of floats, shape (n_parameters, n_parameters)\n            The covariance QR'QR\n        static_response: ndarray of floats, shape (n_parameters,)\n            The response QR'y\n\n        Notes\n        -----\n        This function constructs a matrix of static regressors (Q) based on the provided\n        static function outputs (y_static) and inputs (X_static). The linear mapping\n        matrix (R) should be precomputed before calling this function. The result\n        Q_dot_R represents the static regressors for the NARMAX model.\n\n        \"\"\"\n        R, qit = self.build_linear_mapping()\n        Q = y_static ** qit[:, 0]\n        for k in range(self.n_inputs):\n            Q *= X_static ** qit[:, 1 + k]\n\n        Q = Q.reshape(len(y_static), len(qit))\n\n        QR = Q.dot(R)\n        static_covariance = (QR.T).dot(QR)\n        static_response = (QR.T).dot(y_static)\n        return QR, static_covariance, static_response\n\n    def build_static_gain_information(\n        self, X_static: np.ndarray, y_static: np.ndarray, gain: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Construct a matrix of static regressors referring to the derivative (gain).\n\n        Parameters\n        ----------\n        y_static : array-like, shape (n_samples_static_function,)\n            Output of the static function.\n        X_static : array-like, shape (n_samples_static_function,)\n            Static function input.\n        gain : array-like, shape (n_samples_static_gain,)\n            Static gain input.\n\n        Returns\n        -------\n        HR : ndarray of floats, shape (n_samples_static_function, n_parameters)\n            The matrix of static regressors for the derivative (gain) multiplied by the\n            linear mapping matrix R.\n        gain_covariance : ndarray of floats, shape (n_parameters, n_parameters)\n            The covariance matrix (HR'HR) for the gain-related regressors.\n        gain_response : ndarray of floats, shape (n_parameters,)\n            The response vector (HR'y) for the gain-related regressors.\n\n        Notes\n        -----\n        This function constructs a matrix of static regressors (G+H) for the derivative\n        (gain) based on the provided static function outputs (y_static), inputs\n        (X_static), and gain values. The linear mapping matrix (R) should be\n        precomputed before calling this function.\n\n        \"\"\"\n        R, qit = self.build_linear_mapping()\n        H = np.zeros((len(y_static), len(qit)))\n        G = np.zeros((len(y_static), len(qit)))\n        for i in range(len(y_static)):\n            for j in range(1, len(qit)):\n                if y_static[i, 0] == 0:\n                    if (qit[j, 0]) == 1:\n                        H[i, j] = gain[i][0]\n                    else:\n                        H[i, j] = 0\n                else:\n                    H[i, j] = (gain[i] * qit[j, 0] * y_static[i, 0] ** (qit[j, 0] - 1))[\n                        0\n                    ]\n                for k in range(self.n_inputs):\n                    if X_static[i, k] == 0:\n                        if (qit[j, 1 + k]) == 1:\n                            G[i, j] = 1\n                        else:\n                            G[i, j] = 0\n                    else:\n                        G[i, j] = qit[j, 1 + k] * X_static[i, k] ** (qit[j, 1 + k] - 1)\n\n        HR = (G + H).dot(R)\n        gain_covariance = (HR.T).dot(HR)\n        gain_response = (HR.T).dot(gain)\n        return HR, gain_covariance, gain_response\n\n    def get_cost_function(\n        self, y: np.ndarray, psi: np.ndarray, theta: np.ndarray\n    ) -&gt; np.ndarray:\n        \"\"\"Calculate the cost function based on residuals.\n\n        Parameters\n        ----------\n        y : ndarray of floats\n            The target data used in the identification process.\n        psi : ndarray of floats, shape (n_samples, n_parameters)\n            The matrix of regressors.\n        theta : ndarray of floats\n            The parameter vector.\n\n        Returns\n        -------\n        cost_function : float\n            The calculated cost function value.\n\n        Notes\n        -----\n        This method computes the cost function value based on the residuals between\n        the target data (y) and the predicted values using the regressors (dynamic\n        and static) and parameter vector (theta). It quantifies the error in the\n        model's predictions.\n\n        \"\"\"\n        residuals = y - psi.dot(theta)\n        return residuals.T.dot(residuals)\n\n    def build_system_data(\n        self,\n        y: np.ndarray,\n        static_gain: np.ndarray,\n        static_function: np.ndarray,\n    ) -&gt; List[np.ndarray]:\n        \"\"\"Construct a list of output data components for the NARMAX system.\n\n        Parameters\n        ----------\n        y : ndarray of floats\n            The target data used in the identification process.\n        static_gain : ndarray of floats\n            Static gain output data.\n        static_function : ndarray of floats\n            Static function output data.\n\n        Returns\n        -------\n        system_data : list of ndarrays\n            A list containing data components, including the target data (y),\n            static gain data (if present), and static function data (if present).\n\n        Notes\n        -----\n        This method constructs a list of data components that are used in the NARMAX\n        system identification process. The components may include the target data (y),\n        static gain data (if enabled), and static function data (if enabled).\n\n        \"\"\"\n        if not self.static_gain:\n            return [y] + [static_function]\n\n        if not self.static_function:\n            return [y] + [static_gain]\n\n        return [y] + [static_gain] + [static_function]\n\n    def build_affine_data(\n        self, psi: np.ndarray, HR: np.ndarray, QR: np.ndarray\n    ) -&gt; List[np.ndarray]:\n        \"\"\"Construct a list of affine data components for NARMAX modeling.\n\n        Parameters\n        ----------\n        psi : ndarray of floats, shape (n_samples, n_parameters)\n            The matrix of dynamic regressors.\n        HR : ndarray of floats, shape (n_samples_static_gain, n_parameters)\n            The matrix of static gain regressors.\n        QR : ndarray of floats, shape (n_samples_static_function, n_parameters)\n            The matrix of static function regressors.\n\n        Returns\n        -------\n        affine_data : list of ndarrays\n            A list containing affine data components, including the matrix of static\n            regressors (psi), static gain regressors (if present), and static function\n            regressors (if present).\n\n        Notes\n        -----\n        This method constructs a list of affine data components used in the NARMAX\n        modeling process. The components may include the matrix of static regressors\n        (psi), static gain regressors (if enabled), and static function regressors\n        (if enabled).\n\n        \"\"\"\n        if not self.static_gain:\n            return [psi] + [QR]\n\n        if not self.static_function:\n            return [psi] + [HR]\n\n        return [psi] + [HR] + [QR]\n\n    def build_psi(self, X: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Build the matrix of dynamic regressor for NARMAX modeling.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the training process.\n        y : ndarray of floats\n            The output data to be used in the training process.\n\n        Returns\n        -------\n        psi : ndarray of floats, shape (n_samples, n_parameters)\n            The matrix of dynamic regressors.\n\n        \"\"\"\n        psi_builder = RegressorDictionary()\n        xlag_code = psi_builder._list_input_regressor_code(self.final_model)\n        ylag_code = psi_builder._list_output_regressor_code(self.final_model)\n        xlag = psi_builder._get_lag_from_regressor_code(xlag_code)\n        ylag = psi_builder._get_lag_from_regressor_code(ylag_code)\n        self.max_lag = psi_builder._get_max_lag_from_model_code(self.final_model)\n        if self.n_inputs != 1:\n            xlag = self.n_inputs * [list(range(1, self.max_lag + 1))]\n\n        psi_builder.xlag = xlag\n        psi_builder.ylag = ylag\n        regressor_code = psi_builder.regressor_space(self.n_inputs)\n        pivv = psi_builder._get_index_from_regressor_code(\n            regressor_code, self.final_model\n        )\n        self.final_model = regressor_code[pivv]\n\n        lagged_data = InformationMatrix(xlag=xlag, ylag=ylag).build_input_output_matrix(\n            X=X, y=y\n        )\n\n        psi = Polynomial(degree=self.degree).fit(\n            lagged_data, max_lag=self.max_lag, predefined_regressors=pivv\n        )\n        return psi\n\n    def estimate(\n        self,\n        y_static: np.ndarray = np.zeros(1),\n        X_static: np.ndarray = np.zeros(1),\n        gain: np.ndarray = np.zeros(1),\n        y: np.ndarray = np.zeros(1),\n        X: np.ndarray = np.zeros((1, 1)),\n        weighing_matrix: np.ndarray = np.zeros((1, 1)),\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.int64]:\n        \"\"\"Estimate the parameters via multi-objective techniques.\n\n        Parameters\n        ----------\n        y_static : array-like of shape = n_samples_static_function, default = ([0])\n            Output of static function.\n        X_static : array-like of shape = n_samples_static_function, default = ([0])\n            Static function input.\n        gain : array-like of shape = n_samples_static_gain, default = ([0])\n            Static gain input.\n        y : array-like of shape = n_samples, default = ([0])\n            The target data used in the identification process.\n        X : ndarray of floats, default = ([[0],[0]])\n            Matrix of static regressors.\n        weighing_matrix: ndarray\n            Weighing matrix for defining the weight of each objective.\n\n        Returns\n        -------\n        J : ndarray\n            Matrix referring to the objectives.\n        euclidean_norm : ndarray\n            Matrix of the Euclidean norm.\n        theta : ndarray\n            Matrix with parameters for each weight.\n        HR : ndarray\n            H matrix multiplied by R.\n        QR : ndarray\n            Q matrix multiplied by R.\n        position : ndarray, default = ([[0],[0]])\n            Position of the best theta set.\n\n        \"\"\"\n        psi = self.build_psi(X, y)\n        y = y[self.max_lag :]\n        HR, QR = np.zeros((1, 1)), np.zeros((1, 1))\n        n_parameters = weighing_matrix.shape[1]\n        num_objectives = self.static_function + self.static_gain + 1\n        euclidean_norm = np.zeros(n_parameters)\n        theta = np.zeros((n_parameters, self.final_model.shape[0]))\n        dynamic_covariance = psi.T.dot(psi)\n        dynamic_response = psi.T.dot(y)\n\n        if self.static_function:\n            QR, static_covariance, static_response = (\n                self.build_static_function_information(X_static, y_static)\n            )\n        if self.static_gain:\n            HR, gain_covariance, gain_response = self.build_static_gain_information(\n                X_static, y_static, gain\n            )\n        J = np.zeros((num_objectives, n_parameters))\n        system_data = self.build_system_data(y, gain, y_static)\n        affine_information_data = self.build_affine_data(psi, HR, QR)\n        for i in range(n_parameters):\n            theta1 = weighing_matrix[0, i] * dynamic_covariance\n            theta2 = weighing_matrix[0, i] * dynamic_response\n\n            w = 1\n            if self.static_function:\n                theta1 += weighing_matrix[w, i] * static_covariance\n                theta2 += weighing_matrix[w, i] * static_response.reshape(-1, 1)\n                w += 1\n\n            if self.static_gain:\n                theta1 += weighing_matrix[w, i] * gain_covariance\n                theta2 += weighing_matrix[w, i] * gain_response.reshape(-1, 1)\n                w += 1\n\n            tmp_theta = np.linalg.lstsq(theta1, theta2, rcond=None)[0]\n            theta[i, :] = tmp_theta.T\n\n            for j in range(num_objectives):\n                residuals = self.get_cost_function(\n                    system_data[j], affine_information_data[j], tmp_theta\n                )\n                J[j, i] = residuals[0]\n\n            euclidean_norm[i] = np.linalg.norm(J[:, i])\n\n        if self.normalize is True:\n            J /= np.max(J, axis=1)[:, np.newaxis]\n            euclidean_norm /= np.max(euclidean_norm)\n\n            euclidean_norm = euclidean_norm / np.max(euclidean_norm)\n\n        position = np.argmin(euclidean_norm)\n        return (\n            J,\n            euclidean_norm,\n            theta,\n            HR,\n            QR,\n            position,\n        )\n</code></pre>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_affine_data","title":"<code>build_affine_data(psi, HR, QR)</code>","text":"<p>Construct a list of affine data components for NARMAX modeling.</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_affine_data--parameters","title":"Parameters","text":"<p>psi : ndarray of floats, shape (n_samples, n_parameters)     The matrix of dynamic regressors. HR : ndarray of floats, shape (n_samples_static_gain, n_parameters)     The matrix of static gain regressors. QR : ndarray of floats, shape (n_samples_static_function, n_parameters)     The matrix of static function regressors.</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_affine_data--returns","title":"Returns","text":"<p>affine_data : list of ndarrays     A list containing affine data components, including the matrix of static     regressors (psi), static gain regressors (if present), and static function     regressors (if present).</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_affine_data--notes","title":"Notes","text":"<p>This method constructs a list of affine data components used in the NARMAX modeling process. The components may include the matrix of static regressors (psi), static gain regressors (if enabled), and static function regressors (if enabled).</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def build_affine_data(\n    self, psi: np.ndarray, HR: np.ndarray, QR: np.ndarray\n) -&gt; List[np.ndarray]:\n    \"\"\"Construct a list of affine data components for NARMAX modeling.\n\n    Parameters\n    ----------\n    psi : ndarray of floats, shape (n_samples, n_parameters)\n        The matrix of dynamic regressors.\n    HR : ndarray of floats, shape (n_samples_static_gain, n_parameters)\n        The matrix of static gain regressors.\n    QR : ndarray of floats, shape (n_samples_static_function, n_parameters)\n        The matrix of static function regressors.\n\n    Returns\n    -------\n    affine_data : list of ndarrays\n        A list containing affine data components, including the matrix of static\n        regressors (psi), static gain regressors (if present), and static function\n        regressors (if present).\n\n    Notes\n    -----\n    This method constructs a list of affine data components used in the NARMAX\n    modeling process. The components may include the matrix of static regressors\n    (psi), static gain regressors (if enabled), and static function regressors\n    (if enabled).\n\n    \"\"\"\n    if not self.static_gain:\n        return [psi] + [QR]\n\n    if not self.static_function:\n        return [psi] + [HR]\n\n    return [psi] + [HR] + [QR]\n</code></pre>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_linear_mapping","title":"<code>build_linear_mapping()</code>","text":"<p>Assemble the linear mapping matrix R using the regressor-space method.</p> <p>This function constructs the linear mapping matrix R, which plays a key role in mapping the parameter vector to the cluster coefficients. It also generates a row matrix qit that assists in locating terms within the linear mapping matrix. This qit matrix is later used in creating the static regressor matrix (Q).</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_linear_mapping--returns","title":"Returns","text":"<p>R : ndarray of int     A constant matrix of ones and zeros that maps the parameter vector to     cluster coefficients. qit : ndarray of int     A row matrix that helps locate terms within the linear mapping matrix R and     is used in the creation of the static regressor matrix (Q).</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_linear_mapping--notes","title":"Notes","text":"<p>The linear mapping matrix R is constructed using the regressor-space method. It plays a crucial role in the parameter estimation process, facilitating the mapping of parameter values to cluster coefficients. The qit matrix aids in term localization within the linear mapping matrix R and is subsequently used to build the static regressor matrix (Q).</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def build_linear_mapping(self):\n    \"\"\"Assemble the linear mapping matrix R using the regressor-space method.\n\n    This function constructs the linear mapping matrix R, which plays a key role in\n    mapping the parameter vector to the cluster coefficients. It also generates a\n    row matrix qit that assists in locating terms within the linear mapping matrix.\n    This qit matrix is later used in creating the static regressor matrix (Q).\n\n    Returns\n    -------\n    R : ndarray of int\n        A constant matrix of ones and zeros that maps the parameter vector to\n        cluster coefficients.\n    qit : ndarray of int\n        A row matrix that helps locate terms within the linear mapping matrix R and\n        is used in the creation of the static regressor matrix (Q).\n\n    Notes\n    -----\n    The linear mapping matrix R is constructed using the regressor-space method.\n    It plays a crucial role in the parameter estimation process, facilitating the\n    mapping of parameter values to cluster coefficients. The qit matrix aids in\n    term localization within the linear mapping matrix R and is subsequently used\n    to build the static regressor matrix (Q).\n\n    \"\"\"\n    xlag = [1] * self.n_inputs\n\n    object_qit = RegressorDictionary(xlag=xlag, ylag=[1])\n    # Given xlag and ylag equal to 1, there is no repetition of terms, which is\n    # ideal for building qit.\n    qit = object_qit.regressor_space(n_inputs=self.n_inputs) // 1000\n    model = self.final_model // 1000\n    R = np.all(qit[:, None, :] == model, axis=2).astype(int)\n    # Find rows with all zeros in R (sum of row elements is 0)\n    null_rows = list(np.where(np.sum(R, axis=1) == 0)[0])\n\n    R = np.delete(R, null_rows, axis=0)\n    qit = np.delete(qit, null_rows, axis=0)\n    return R, self.get_term_clustering(qit)\n</code></pre>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_psi","title":"<code>build_psi(X, y)</code>","text":"<p>Build the matrix of dynamic regressor for NARMAX modeling.</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_psi--parameters","title":"Parameters","text":"<p>X : ndarray of floats     The input data to be used in the training process. y : ndarray of floats     The output data to be used in the training process.</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_psi--returns","title":"Returns","text":"<p>psi : ndarray of floats, shape (n_samples, n_parameters)     The matrix of dynamic regressors.</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def build_psi(self, X: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Build the matrix of dynamic regressor for NARMAX modeling.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the training process.\n    y : ndarray of floats\n        The output data to be used in the training process.\n\n    Returns\n    -------\n    psi : ndarray of floats, shape (n_samples, n_parameters)\n        The matrix of dynamic regressors.\n\n    \"\"\"\n    psi_builder = RegressorDictionary()\n    xlag_code = psi_builder._list_input_regressor_code(self.final_model)\n    ylag_code = psi_builder._list_output_regressor_code(self.final_model)\n    xlag = psi_builder._get_lag_from_regressor_code(xlag_code)\n    ylag = psi_builder._get_lag_from_regressor_code(ylag_code)\n    self.max_lag = psi_builder._get_max_lag_from_model_code(self.final_model)\n    if self.n_inputs != 1:\n        xlag = self.n_inputs * [list(range(1, self.max_lag + 1))]\n\n    psi_builder.xlag = xlag\n    psi_builder.ylag = ylag\n    regressor_code = psi_builder.regressor_space(self.n_inputs)\n    pivv = psi_builder._get_index_from_regressor_code(\n        regressor_code, self.final_model\n    )\n    self.final_model = regressor_code[pivv]\n\n    lagged_data = InformationMatrix(xlag=xlag, ylag=ylag).build_input_output_matrix(\n        X=X, y=y\n    )\n\n    psi = Polynomial(degree=self.degree).fit(\n        lagged_data, max_lag=self.max_lag, predefined_regressors=pivv\n    )\n    return psi\n</code></pre>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_static_function_information","title":"<code>build_static_function_information(X_static, y_static)</code>","text":"<p>Construct a matrix of static regressors for a NARMAX model.</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_static_function_information--parameters","title":"Parameters","text":"<p>y_static : array-like, shape (n_samples_static_function,)     Output of the static function. X_static : array-like, shape (n_samples_static_function,)     Static function input.</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_static_function_information--returns","title":"Returns","text":"<p>Q_dot_R : ndarray of floats, shape (n_samples_static_function, n_parameters)     The result of multiplying the matrix of static regressors (Q) with the     linear mapping matrix (R), where n_parameters is the number of model     parameters. static_covariance: ndarray of floats, shape (n_parameters, n_parameters)     The covariance QR'QR static_response: ndarray of floats, shape (n_parameters,)     The response QR'y</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_static_function_information--notes","title":"Notes","text":"<p>This function constructs a matrix of static regressors (Q) based on the provided static function outputs (y_static) and inputs (X_static). The linear mapping matrix (R) should be precomputed before calling this function. The result Q_dot_R represents the static regressors for the NARMAX model.</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def build_static_function_information(\n    self, X_static: np.ndarray, y_static: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Construct a matrix of static regressors for a NARMAX model.\n\n    Parameters\n    ----------\n    y_static : array-like, shape (n_samples_static_function,)\n        Output of the static function.\n    X_static : array-like, shape (n_samples_static_function,)\n        Static function input.\n\n    Returns\n    -------\n    Q_dot_R : ndarray of floats, shape (n_samples_static_function, n_parameters)\n        The result of multiplying the matrix of static regressors (Q) with the\n        linear mapping matrix (R), where n_parameters is the number of model\n        parameters.\n    static_covariance: ndarray of floats, shape (n_parameters, n_parameters)\n        The covariance QR'QR\n    static_response: ndarray of floats, shape (n_parameters,)\n        The response QR'y\n\n    Notes\n    -----\n    This function constructs a matrix of static regressors (Q) based on the provided\n    static function outputs (y_static) and inputs (X_static). The linear mapping\n    matrix (R) should be precomputed before calling this function. The result\n    Q_dot_R represents the static regressors for the NARMAX model.\n\n    \"\"\"\n    R, qit = self.build_linear_mapping()\n    Q = y_static ** qit[:, 0]\n    for k in range(self.n_inputs):\n        Q *= X_static ** qit[:, 1 + k]\n\n    Q = Q.reshape(len(y_static), len(qit))\n\n    QR = Q.dot(R)\n    static_covariance = (QR.T).dot(QR)\n    static_response = (QR.T).dot(y_static)\n    return QR, static_covariance, static_response\n</code></pre>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_static_gain_information","title":"<code>build_static_gain_information(X_static, y_static, gain)</code>","text":"<p>Construct a matrix of static regressors referring to the derivative (gain).</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_static_gain_information--parameters","title":"Parameters","text":"<p>y_static : array-like, shape (n_samples_static_function,)     Output of the static function. X_static : array-like, shape (n_samples_static_function,)     Static function input. gain : array-like, shape (n_samples_static_gain,)     Static gain input.</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_static_gain_information--returns","title":"Returns","text":"<p>HR : ndarray of floats, shape (n_samples_static_function, n_parameters)     The matrix of static regressors for the derivative (gain) multiplied by the     linear mapping matrix R. gain_covariance : ndarray of floats, shape (n_parameters, n_parameters)     The covariance matrix (HR'HR) for the gain-related regressors. gain_response : ndarray of floats, shape (n_parameters,)     The response vector (HR'y) for the gain-related regressors.</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_static_gain_information--notes","title":"Notes","text":"<p>This function constructs a matrix of static regressors (G+H) for the derivative (gain) based on the provided static function outputs (y_static), inputs (X_static), and gain values. The linear mapping matrix (R) should be precomputed before calling this function.</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def build_static_gain_information(\n    self, X_static: np.ndarray, y_static: np.ndarray, gain: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Construct a matrix of static regressors referring to the derivative (gain).\n\n    Parameters\n    ----------\n    y_static : array-like, shape (n_samples_static_function,)\n        Output of the static function.\n    X_static : array-like, shape (n_samples_static_function,)\n        Static function input.\n    gain : array-like, shape (n_samples_static_gain,)\n        Static gain input.\n\n    Returns\n    -------\n    HR : ndarray of floats, shape (n_samples_static_function, n_parameters)\n        The matrix of static regressors for the derivative (gain) multiplied by the\n        linear mapping matrix R.\n    gain_covariance : ndarray of floats, shape (n_parameters, n_parameters)\n        The covariance matrix (HR'HR) for the gain-related regressors.\n    gain_response : ndarray of floats, shape (n_parameters,)\n        The response vector (HR'y) for the gain-related regressors.\n\n    Notes\n    -----\n    This function constructs a matrix of static regressors (G+H) for the derivative\n    (gain) based on the provided static function outputs (y_static), inputs\n    (X_static), and gain values. The linear mapping matrix (R) should be\n    precomputed before calling this function.\n\n    \"\"\"\n    R, qit = self.build_linear_mapping()\n    H = np.zeros((len(y_static), len(qit)))\n    G = np.zeros((len(y_static), len(qit)))\n    for i in range(len(y_static)):\n        for j in range(1, len(qit)):\n            if y_static[i, 0] == 0:\n                if (qit[j, 0]) == 1:\n                    H[i, j] = gain[i][0]\n                else:\n                    H[i, j] = 0\n            else:\n                H[i, j] = (gain[i] * qit[j, 0] * y_static[i, 0] ** (qit[j, 0] - 1))[\n                    0\n                ]\n            for k in range(self.n_inputs):\n                if X_static[i, k] == 0:\n                    if (qit[j, 1 + k]) == 1:\n                        G[i, j] = 1\n                    else:\n                        G[i, j] = 0\n                else:\n                    G[i, j] = qit[j, 1 + k] * X_static[i, k] ** (qit[j, 1 + k] - 1)\n\n    HR = (G + H).dot(R)\n    gain_covariance = (HR.T).dot(HR)\n    gain_response = (HR.T).dot(gain)\n    return HR, gain_covariance, gain_response\n</code></pre>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_system_data","title":"<code>build_system_data(y, static_gain, static_function)</code>","text":"<p>Construct a list of output data components for the NARMAX system.</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_system_data--parameters","title":"Parameters","text":"<p>y : ndarray of floats     The target data used in the identification process. static_gain : ndarray of floats     Static gain output data. static_function : ndarray of floats     Static function output data.</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_system_data--returns","title":"Returns","text":"<p>system_data : list of ndarrays     A list containing data components, including the target data (y),     static gain data (if present), and static function data (if present).</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_system_data--notes","title":"Notes","text":"<p>This method constructs a list of data components that are used in the NARMAX system identification process. The components may include the target data (y), static gain data (if enabled), and static function data (if enabled).</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def build_system_data(\n    self,\n    y: np.ndarray,\n    static_gain: np.ndarray,\n    static_function: np.ndarray,\n) -&gt; List[np.ndarray]:\n    \"\"\"Construct a list of output data components for the NARMAX system.\n\n    Parameters\n    ----------\n    y : ndarray of floats\n        The target data used in the identification process.\n    static_gain : ndarray of floats\n        Static gain output data.\n    static_function : ndarray of floats\n        Static function output data.\n\n    Returns\n    -------\n    system_data : list of ndarrays\n        A list containing data components, including the target data (y),\n        static gain data (if present), and static function data (if present).\n\n    Notes\n    -----\n    This method constructs a list of data components that are used in the NARMAX\n    system identification process. The components may include the target data (y),\n    static gain data (if enabled), and static function data (if enabled).\n\n    \"\"\"\n    if not self.static_gain:\n        return [y] + [static_function]\n\n    if not self.static_function:\n        return [y] + [static_gain]\n\n    return [y] + [static_gain] + [static_function]\n</code></pre>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.estimate","title":"<code>estimate(y_static=np.zeros(1), X_static=np.zeros(1), gain=np.zeros(1), y=np.zeros(1), X=np.zeros((1, 1)), weighing_matrix=np.zeros((1, 1)))</code>","text":"<p>Estimate the parameters via multi-objective techniques.</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.estimate--parameters","title":"Parameters","text":"<p>y_static : array-like of shape = n_samples_static_function, default = ([0])     Output of static function. X_static : array-like of shape = n_samples_static_function, default = ([0])     Static function input. gain : array-like of shape = n_samples_static_gain, default = ([0])     Static gain input. y : array-like of shape = n_samples, default = ([0])     The target data used in the identification process. X : ndarray of floats, default = ([[0],[0]])     Matrix of static regressors. weighing_matrix: ndarray     Weighing matrix for defining the weight of each objective.</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.estimate--returns","title":"Returns","text":"<p>J : ndarray     Matrix referring to the objectives. euclidean_norm : ndarray     Matrix of the Euclidean norm. theta : ndarray     Matrix with parameters for each weight. HR : ndarray     H matrix multiplied by R. QR : ndarray     Q matrix multiplied by R. position : ndarray, default = ([[0],[0]])     Position of the best theta set.</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def estimate(\n    self,\n    y_static: np.ndarray = np.zeros(1),\n    X_static: np.ndarray = np.zeros(1),\n    gain: np.ndarray = np.zeros(1),\n    y: np.ndarray = np.zeros(1),\n    X: np.ndarray = np.zeros((1, 1)),\n    weighing_matrix: np.ndarray = np.zeros((1, 1)),\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.int64]:\n    \"\"\"Estimate the parameters via multi-objective techniques.\n\n    Parameters\n    ----------\n    y_static : array-like of shape = n_samples_static_function, default = ([0])\n        Output of static function.\n    X_static : array-like of shape = n_samples_static_function, default = ([0])\n        Static function input.\n    gain : array-like of shape = n_samples_static_gain, default = ([0])\n        Static gain input.\n    y : array-like of shape = n_samples, default = ([0])\n        The target data used in the identification process.\n    X : ndarray of floats, default = ([[0],[0]])\n        Matrix of static regressors.\n    weighing_matrix: ndarray\n        Weighing matrix for defining the weight of each objective.\n\n    Returns\n    -------\n    J : ndarray\n        Matrix referring to the objectives.\n    euclidean_norm : ndarray\n        Matrix of the Euclidean norm.\n    theta : ndarray\n        Matrix with parameters for each weight.\n    HR : ndarray\n        H matrix multiplied by R.\n    QR : ndarray\n        Q matrix multiplied by R.\n    position : ndarray, default = ([[0],[0]])\n        Position of the best theta set.\n\n    \"\"\"\n    psi = self.build_psi(X, y)\n    y = y[self.max_lag :]\n    HR, QR = np.zeros((1, 1)), np.zeros((1, 1))\n    n_parameters = weighing_matrix.shape[1]\n    num_objectives = self.static_function + self.static_gain + 1\n    euclidean_norm = np.zeros(n_parameters)\n    theta = np.zeros((n_parameters, self.final_model.shape[0]))\n    dynamic_covariance = psi.T.dot(psi)\n    dynamic_response = psi.T.dot(y)\n\n    if self.static_function:\n        QR, static_covariance, static_response = (\n            self.build_static_function_information(X_static, y_static)\n        )\n    if self.static_gain:\n        HR, gain_covariance, gain_response = self.build_static_gain_information(\n            X_static, y_static, gain\n        )\n    J = np.zeros((num_objectives, n_parameters))\n    system_data = self.build_system_data(y, gain, y_static)\n    affine_information_data = self.build_affine_data(psi, HR, QR)\n    for i in range(n_parameters):\n        theta1 = weighing_matrix[0, i] * dynamic_covariance\n        theta2 = weighing_matrix[0, i] * dynamic_response\n\n        w = 1\n        if self.static_function:\n            theta1 += weighing_matrix[w, i] * static_covariance\n            theta2 += weighing_matrix[w, i] * static_response.reshape(-1, 1)\n            w += 1\n\n        if self.static_gain:\n            theta1 += weighing_matrix[w, i] * gain_covariance\n            theta2 += weighing_matrix[w, i] * gain_response.reshape(-1, 1)\n            w += 1\n\n        tmp_theta = np.linalg.lstsq(theta1, theta2, rcond=None)[0]\n        theta[i, :] = tmp_theta.T\n\n        for j in range(num_objectives):\n            residuals = self.get_cost_function(\n                system_data[j], affine_information_data[j], tmp_theta\n            )\n            J[j, i] = residuals[0]\n\n        euclidean_norm[i] = np.linalg.norm(J[:, i])\n\n    if self.normalize is True:\n        J /= np.max(J, axis=1)[:, np.newaxis]\n        euclidean_norm /= np.max(euclidean_norm)\n\n        euclidean_norm = euclidean_norm / np.max(euclidean_norm)\n\n    position = np.argmin(euclidean_norm)\n    return (\n        J,\n        euclidean_norm,\n        theta,\n        HR,\n        QR,\n        position,\n    )\n</code></pre>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.get_cost_function","title":"<code>get_cost_function(y, psi, theta)</code>","text":"<p>Calculate the cost function based on residuals.</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.get_cost_function--parameters","title":"Parameters","text":"<p>y : ndarray of floats     The target data used in the identification process. psi : ndarray of floats, shape (n_samples, n_parameters)     The matrix of regressors. theta : ndarray of floats     The parameter vector.</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.get_cost_function--returns","title":"Returns","text":"<p>cost_function : float     The calculated cost function value.</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.get_cost_function--notes","title":"Notes","text":"<p>This method computes the cost function value based on the residuals between the target data (y) and the predicted values using the regressors (dynamic and static) and parameter vector (theta). It quantifies the error in the model's predictions.</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def get_cost_function(\n    self, y: np.ndarray, psi: np.ndarray, theta: np.ndarray\n) -&gt; np.ndarray:\n    \"\"\"Calculate the cost function based on residuals.\n\n    Parameters\n    ----------\n    y : ndarray of floats\n        The target data used in the identification process.\n    psi : ndarray of floats, shape (n_samples, n_parameters)\n        The matrix of regressors.\n    theta : ndarray of floats\n        The parameter vector.\n\n    Returns\n    -------\n    cost_function : float\n        The calculated cost function value.\n\n    Notes\n    -----\n    This method computes the cost function value based on the residuals between\n    the target data (y) and the predicted values using the regressors (dynamic\n    and static) and parameter vector (theta). It quantifies the error in the\n    model's predictions.\n\n    \"\"\"\n    residuals = y - psi.dot(theta)\n    return residuals.T.dot(residuals)\n</code></pre>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.get_term_clustering","title":"<code>get_term_clustering(qit)</code>","text":"<p>Get the term clustering of the model.</p> <p>This function takes a matrix <code>qit</code> and compute the term clustering based on their values. It calculates the number of occurrences of each value for each row in the matrix.</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.get_term_clustering--parameters","title":"Parameters","text":"<p>qit : ndarray     Input matrix containing terms clustering to be sorted.</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.get_term_clustering--returns","title":"Returns","text":"<p>N_aux : ndarray     A new matrix with rows representing the number of occurrences of each value     for each row in the input matrix <code>qit</code>. The columns correspond to different     values.</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.get_term_clustering--examples","title":"Examples","text":"<p>qit = np.array([[1, 2, 2], ...                 [1, 3, 1], ...                 [2, 2, 3]]) result = get_term_clustering(qit) print(result) [[1. 2. 0. 0.] [2. 0. 1. 0.] [0. 2. 1. 0.]]</p>"},{"location":"code/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.get_term_clustering--notes","title":"Notes","text":"<p>The function calculates the number of occurrences of each value (from 1 to the maximum value in the input matrix <code>qit</code>) for each row and returns a matrix where rows represent rows of the input matrix <code>qit</code>, and columns represent different values.</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def get_term_clustering(self, qit: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Get the term clustering of the model.\n\n    This function takes a matrix `qit` and compute the term clustering based\n    on their values. It calculates the number of occurrences of each value\n    for each row in the matrix.\n\n    Parameters\n    ----------\n    qit : ndarray\n        Input matrix containing terms clustering to be sorted.\n\n    Returns\n    -------\n    N_aux : ndarray\n        A new matrix with rows representing the number of occurrences of each value\n        for each row in the input matrix `qit`. The columns correspond to different\n        values.\n\n    Examples\n    --------\n    &gt;&gt;&gt; qit = np.array([[1, 2, 2],\n    ...                 [1, 3, 1],\n    ...                 [2, 2, 3]])\n    &gt;&gt;&gt; result = get_term_clustering(qit)\n    &gt;&gt;&gt; print(result)\n    [[1. 2. 0. 0.]\n    [2. 0. 1. 0.]\n    [0. 2. 1. 0.]]\n\n    Notes\n    -----\n    The function calculates the number of occurrences of each value (from 1 to\n    the maximum value in the input matrix `qit`) for each row and returns a matrix\n    where rows represent rows of the input matrix `qit`, and columns represent\n    different values.\n\n    \"\"\"\n    max_value = int(np.max(qit))\n    counts_matrix = np.zeros((qit.shape[0], max_value))\n\n    for k in range(1, max_value + 1):\n        counts_matrix[:, k - 1] = np.sum(qit == k, axis=1)\n\n    return counts_matrix.astype(int)\n</code></pre>"},{"location":"code/narmax-base/","title":"Documentation for <code>narmax-base</code>","text":"<p>Base classes for NARMAX estimator.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.BaseMSS","title":"<code>BaseMSS</code>","text":"<p>               Bases: <code>RegressorDictionary</code></p> <p>Base class for Model Structure Selection.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>class BaseMSS(RegressorDictionary, metaclass=ABCMeta):\n    \"\"\"Base class for Model Structure Selection.\"\"\"\n\n    @abstractmethod\n    def __init__(self):\n        super().__init__(self)\n        self.max_lag = None\n        self.n_inputs = None\n        self.theta = None\n        self.final_model = None\n        self.pivv = None\n\n    @abstractmethod\n    def fit(self, *, X, y):\n        \"\"\"Abstract method.\"\"\"\n\n    @abstractmethod\n    def predict(\n        self,\n        *,\n        X: Optional[np.ndarray] = None,\n        y: np.ndarray,\n        steps_ahead: Optional[int] = None,\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        \"\"\"Abstract method.\"\"\"\n\n    def _code2exponents(self, *, code: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Convert regressor code to exponents array.\n\n        Parameters\n        ----------\n        code : 1D-array of int\n            Codification of one regressor.\n\n        Returns\n        -------\n        exponents = ndarray of ints\n        \"\"\"\n        regressors = np.array(list(set(code)))\n        regressors_count = Counter(code)\n\n        if np.all(regressors == 0):\n            return np.zeros(self.max_lag * (1 + self.n_inputs))\n\n        exponents = np.array([], dtype=float)\n        elements = np.round(np.divide(regressors, 1000), 0)[(regressors &gt; 0)].astype(\n            int\n        )\n\n        for j in range(1, self.n_inputs + 2):\n            base_exponents = np.zeros(self.max_lag, dtype=float)\n            if j in elements:\n                for i in range(1, self.max_lag + 1):\n                    regressor_code = int(j * 1000 + i)\n                    base_exponents[-i] = regressors_count[regressor_code]\n                exponents = np.append(exponents, base_exponents)\n\n            else:\n                exponents = np.append(exponents, base_exponents)\n\n        return exponents\n\n    def _one_step_ahead_prediction(self, X_base: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        X_base : ndarray of floats of shape = n_samples\n            Regressor matrix with input-output arrays.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        yhat = np.dot(X_base, self.theta.flatten())\n        return yhat.reshape(-1, 1)\n\n    @abstractmethod\n    def _model_prediction(\n        self,\n        X: Optional[np.ndarray],\n        y_initial: np.ndarray,\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        \"\"\"Model prediction wrapper.\"\"\"\n\n    def _narmax_predict(\n        self,\n        X: np.ndarray,\n        y_initial: np.ndarray,\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        \"\"\"narmax_predict method.\"\"\"\n        y_output = np.zeros(forecast_horizon, dtype=float)\n        y_output.fill(np.nan)\n        y_output[: self.max_lag] = y_initial[: self.max_lag, 0]\n\n        model_exponents = [\n            self._code2exponents(code=model) for model in self.final_model\n        ]\n        raw_regressor = np.zeros(len(model_exponents[0]), dtype=float)\n        for i in range(self.max_lag, forecast_horizon):\n            init = 0\n            final = self.max_lag\n            k = int(i - self.max_lag)\n            raw_regressor[:final] = y_output[k:i]\n            for j in range(self.n_inputs):\n                init += self.max_lag\n                final += self.max_lag\n                raw_regressor[init:final] = X[k:i, j]\n\n            regressor_value = np.zeros(len(model_exponents))\n            for j, model_exponent in enumerate(model_exponents):\n                regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\n\n            y_output[i] = np.dot(regressor_value, self.theta.flatten())\n        return y_output[self.max_lag : :].reshape(-1, 1)\n\n    @abstractmethod\n    def _nfir_predict(self, X: np.ndarray, y_initial: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Nfir predict method.\"\"\"\n        y_output = np.zeros(X.shape[0], dtype=float)\n        y_output.fill(np.nan)\n        y_output[: self.max_lag] = y_initial[: self.max_lag, 0]\n        X = X.reshape(-1, self.n_inputs)\n        model_exponents = [\n            self._code2exponents(code=model) for model in self.final_model\n        ]\n        raw_regressor = np.zeros(len(model_exponents[0]), dtype=float)\n        for i in range(self.max_lag, X.shape[0]):\n            init = 0\n            final = self.max_lag\n            k = int(i - self.max_lag)\n            raw_regressor[:final] = y_output[k:i]\n            for j in range(self.n_inputs):\n                init += self.max_lag\n                final += self.max_lag\n                raw_regressor[init:final] = X[k:i, j]\n\n            regressor_value = np.zeros(len(model_exponents))\n            for j, model_exponent in enumerate(model_exponents):\n                regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\n\n            y_output[i] = np.dot(regressor_value, self.theta.flatten())\n        return y_output[self.max_lag : :].reshape(-1, 1)\n\n    def _nar_step_ahead(self, y: np.ndarray, steps_ahead: int) -&gt; np.ndarray:\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        to_remove = int(np.ceil((len(y) - self.max_lag) / steps_ahead))\n        yhat_length = len(y) + steps_ahead\n        yhat = np.zeros(yhat_length, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n        i = self.max_lag\n\n        steps = [step for step in range(0, to_remove * steps_ahead, steps_ahead)]\n        if len(steps) &gt; 1:\n            for step in steps[:-1]:\n                yhat[i : i + steps_ahead] = self._model_prediction(\n                    X=None, y_initial=y[step:i], forecast_horizon=steps_ahead\n                )[-steps_ahead:].ravel()\n                i += steps_ahead\n\n            steps_ahead = np.sum(np.isnan(yhat))\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                X=None, y_initial=y[steps[-1] : i]\n            )[-steps_ahead:].ravel()\n        else:\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                X=None, y_initial=y[0:i], forecast_horizon=steps_ahead\n            )[-steps_ahead:].ravel()\n\n        yhat = yhat.ravel()[self.max_lag : :]\n        return yhat.reshape(-1, 1)\n\n    def narmax_n_step_ahead(\n        self,\n        X: np.ndarray,\n        y: np.ndarray,\n        steps_ahead: Optional[int],\n    ) -&gt; np.ndarray:\n        \"\"\"n_steps ahead prediction method for NARMAX model.\"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        to_remove = int(np.ceil((len(y) - self.max_lag) / steps_ahead))\n        X = X.reshape(-1, self.n_inputs)\n        yhat = np.zeros(X.shape[0], dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n        i = self.max_lag\n        steps = [step for step in range(0, to_remove * steps_ahead, steps_ahead)]\n        if len(steps) &gt; 1:\n            for step in steps[:-1]:\n                yhat[i : i + steps_ahead] = self._model_prediction(\n                    X=X[step : i + steps_ahead],\n                    y_initial=y[step:i],\n                )[-steps_ahead:].ravel()\n                i += steps_ahead\n\n            steps_ahead = np.sum(np.isnan(yhat))\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                X=X[steps[-1] : i + steps_ahead],\n                y_initial=y[steps[-1] : i],\n            )[-steps_ahead:].ravel()\n        else:\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                X=X[0 : i + steps_ahead],\n                y_initial=y[0:i],\n            )[-steps_ahead:].ravel()\n\n        yhat = yhat.ravel()[self.max_lag : :]\n        return yhat.reshape(-1, 1)\n\n    @abstractmethod\n    def _n_step_ahead_prediction(\n        self,\n        X: Optional[np.ndarray],\n        y: Optional[np.ndarray],\n        steps_ahead: Optional[int],\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            Predicted values for NARMAX and NAR models.\n        \"\"\"\n        if self.model_type == \"NARMAX\":\n            return self.narmax_n_step_ahead(X, y, steps_ahead)\n\n        if self.model_type == \"NAR\":\n            return self._nar_step_ahead(y, steps_ahead)\n\n        raise ValueError(\n            \"n_steps_ahead prediction will be implemented for NFIR models in v0.4.*\"\n        )\n\n    @abstractmethod\n    def _basis_function_predict(\n        self,\n        X: Optional[np.ndarray],\n        y_initial: np.ndarray,\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        \"\"\"Basis function prediction.\"\"\"\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y_initial[: self.max_lag, 0]\n\n        # Discard unnecessary initial values\n        analyzed_elements_number = self.max_lag + 1\n\n        for i in range(forecast_horizon - self.max_lag):\n            if self.model_type == \"NARMAX\":\n                lagged_data = self.build_input_output_matrix(\n                    X[i : i + analyzed_elements_number],\n                    yhat[i : i + analyzed_elements_number].reshape(-1, 1),\n                )\n            elif self.model_type == \"NAR\":\n                lagged_data = self.build_output_matrix(\n                    None, yhat[i : i + analyzed_elements_number].reshape(-1, 1)\n                )\n            elif self.model_type == \"NFIR\":\n                lagged_data = self.build_input_matrix(\n                    X[i : i + analyzed_elements_number], None\n                )\n            else:\n                raise ValueError(\n                    f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n                )\n\n            X_tmp = self.basis_function.transform(\n                lagged_data,\n                self.max_lag,\n                predefined_regressors=self.pivv[: len(self.final_model)],\n            )\n\n            a = X_tmp @ self.theta\n            yhat[i + self.max_lag] = a.item()\n\n        return yhat[self.max_lag :].reshape(-1, 1)\n\n    @abstractmethod\n    def _basis_function_n_step_prediction(\n        self,\n        X: Optional[np.ndarray],\n        y: np.ndarray,\n        steps_ahead: int,\n        forecast_horizon: int,\n    ) -&gt; np.ndarray:\n        \"\"\"Basis function n step ahead.\"\"\"\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n\n        # Discard unnecessary initial values\n        i = self.max_lag\n\n        while i &lt; len(y):\n            k = int(i - self.max_lag)\n            if i + steps_ahead &gt; len(y):\n                steps_ahead = len(y) - i  # predicts the remaining values\n\n            if self.model_type == \"NARMAX\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    X[k : i + steps_ahead],\n                    y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-steps_ahead:].ravel()\n            elif self.model_type == \"NAR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    X=None,\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NFIR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    X=X[k : i + steps_ahead],\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-steps_ahead:].ravel()\n            else:\n                raise ValueError(\n                    f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n                )\n\n            i += steps_ahead\n\n        return yhat[self.max_lag :].reshape(-1, 1)\n\n    @abstractmethod\n    def _basis_function_n_steps_horizon(\n        self,\n        X: Optional[np.ndarray],\n        y: np.ndarray,\n        steps_ahead: int,\n        forecast_horizon: int,\n    ) -&gt; np.ndarray:\n        \"\"\"Basis n steps horizon.\"\"\"\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n\n        # Discard unnecessary initial values\n        i = self.max_lag\n\n        while i &lt; len(y):\n            k = int(i - self.max_lag)\n            if i + steps_ahead &gt; len(y):\n                steps_ahead = len(y) - i  # predicts the remaining values\n\n            if self.model_type == \"NARMAX\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    X[k : i + steps_ahead], y[k : i + steps_ahead], forecast_horizon\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NAR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    X=None,\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NFIR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    X=X[k : i + steps_ahead],\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            else:\n                raise ValueError(\n                    f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n                )\n\n            i += steps_ahead\n\n        yhat = yhat.ravel()\n        return yhat[self.max_lag :].reshape(-1, 1)\n</code></pre>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.BaseMSS.fit","title":"<code>fit(*, X, y)</code>  <code>abstractmethod</code>","text":"<p>Abstract method.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>@abstractmethod\ndef fit(self, *, X, y):\n    \"\"\"Abstract method.\"\"\"\n</code></pre>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.BaseMSS.narmax_n_step_ahead","title":"<code>narmax_n_step_ahead(X, y, steps_ahead)</code>","text":"<p>n_steps ahead prediction method for NARMAX model.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def narmax_n_step_ahead(\n    self,\n    X: np.ndarray,\n    y: np.ndarray,\n    steps_ahead: Optional[int],\n) -&gt; np.ndarray:\n    \"\"\"n_steps ahead prediction method for NARMAX model.\"\"\"\n    if len(y) &lt; self.max_lag:\n        raise ValueError(\n            \"Insufficient initial condition elements! Expected at least\"\n            f\" {self.max_lag} elements.\"\n        )\n\n    to_remove = int(np.ceil((len(y) - self.max_lag) / steps_ahead))\n    X = X.reshape(-1, self.n_inputs)\n    yhat = np.zeros(X.shape[0], dtype=float)\n    yhat.fill(np.nan)\n    yhat[: self.max_lag] = y[: self.max_lag, 0]\n    i = self.max_lag\n    steps = [step for step in range(0, to_remove * steps_ahead, steps_ahead)]\n    if len(steps) &gt; 1:\n        for step in steps[:-1]:\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                X=X[step : i + steps_ahead],\n                y_initial=y[step:i],\n            )[-steps_ahead:].ravel()\n            i += steps_ahead\n\n        steps_ahead = np.sum(np.isnan(yhat))\n        yhat[i : i + steps_ahead] = self._model_prediction(\n            X=X[steps[-1] : i + steps_ahead],\n            y_initial=y[steps[-1] : i],\n        )[-steps_ahead:].ravel()\n    else:\n        yhat[i : i + steps_ahead] = self._model_prediction(\n            X=X[0 : i + steps_ahead],\n            y_initial=y[0:i],\n        )[-steps_ahead:].ravel()\n\n    yhat = yhat.ravel()[self.max_lag : :]\n    return yhat.reshape(-1, 1)\n</code></pre>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.BaseMSS.predict","title":"<code>predict(*, X=None, y, steps_ahead=None, forecast_horizon=1)</code>  <code>abstractmethod</code>","text":"<p>Abstract method.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>@abstractmethod\ndef predict(\n    self,\n    *,\n    X: Optional[np.ndarray] = None,\n    y: np.ndarray,\n    steps_ahead: Optional[int] = None,\n    forecast_horizon: int = 1,\n) -&gt; np.ndarray:\n    \"\"\"Abstract method.\"\"\"\n</code></pre>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.InformationMatrix","title":"<code>InformationMatrix</code>","text":"<p>Class for methods regarding preprocessing of columns.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>class InformationMatrix:\n    \"\"\"Class for methods regarding preprocessing of columns.\"\"\"\n\n    def __init__(\n        self,\n        xlag: Union[List[Any], Any] = 1,\n        ylag: Union[List[Any], Any] = 1,\n    ) -&gt; None:\n        self.xlag = xlag\n        self.ylag = ylag\n\n    def shift_column(self, col_to_shift: np.ndarray, lag: int) -&gt; np.ndarray:\n        \"\"\"Shift values based on a lag.\n\n        Parameters\n        ----------\n        col_to_shift : array-like of shape = n_samples\n            The samples of the input or output.\n        lag : int\n            The respective lag of the regressor.\n\n        Returns\n        -------\n        tmp_column : array-like of shape = n_samples\n            The shifted array of the input or output.\n\n        Examples\n        --------\n        &gt;&gt;&gt; y = [1, 2, 3, 4, 5]\n        &gt;&gt;&gt; shift_column(y, 1)\n        [0, 1, 2, 3, 4]\n\n        \"\"\"\n        n_samples = col_to_shift.shape[0]\n        tmp_column = np.zeros((n_samples, 1))\n        aux = col_to_shift[0 : n_samples - lag].reshape(-1, 1)\n        tmp_column[lag:, 0] = aux[:, 0]\n        return tmp_column\n\n    def _process_xlag(self, X: np.ndarray) -&gt; Tuple[int, List[int]]:\n        \"\"\"Create the list of lags to be used for the inputs.\n\n        Parameters\n        ----------\n        X : array-like\n            Input data used on training phase.\n\n        Returns\n        -------\n        x_lag : ndarray of int\n            The range of lags according to user definition.\n        n_inputs : int\n            Number of input variables.\n\n        \"\"\"\n        n_inputs = _num_features(X)\n        if isinstance(self.xlag, int) and n_inputs &gt; 1:\n            raise ValueError(\n                f\"If n_inputs &gt; 1, xlag must be a nested list. Got {self.xlag}\"\n            )\n\n        if isinstance(self.xlag, int):\n            xlag = list(range(1, self.xlag + 1))\n        else:\n            xlag = self.xlag\n\n        return n_inputs, xlag\n\n    def _process_ylag(self) -&gt; List[int]:\n        \"\"\"Create the list of lags to be used for the outputs.\n\n        Returns\n        -------\n        y_lag : ndarray of int\n            The range of lags according to user definition.\n\n        \"\"\"\n        if isinstance(self.ylag, int):\n            ylag = list(range(1, self.ylag + 1))\n        else:\n            ylag = self.ylag\n\n        return ylag\n\n    def _create_lagged_X(self, X: np.ndarray, n_inputs: int) -&gt; np.ndarray:\n        \"\"\"Create a lagged matrix of inputs without combinations.\n\n        Parameters\n        ----------\n        X : array-like\n            Input data used on training phase.\n        n_inputs : int\n            Number of input variables.\n\n        Returns\n        -------\n        x_lagged : ndarray of floats\n            A lagged input matrix formed by the input regressors\n            without combinations.\n\n        \"\"\"\n        if n_inputs == 1:\n            x_lagged = np.column_stack(\n                [self.shift_column(X[:, 0], lag) for lag in self.xlag]\n            )\n        else:\n            x_lagged = np.zeros([len(X), 1])  # just to stack other columns\n            # if user input a nested list like [[1, 2], 4], the following\n            # line convert it to [[1, 2], [4]].\n            # Remember, for multiple inputs all lags must be entered explicitly\n            xlag = [[i] if isinstance(i, int) else i for i in self.xlag]\n            for col in range(n_inputs):\n                x_lagged_col = np.column_stack(\n                    [self.shift_column(X[:, col], lag) for lag in xlag[col]]\n                )\n                x_lagged = np.column_stack([x_lagged, x_lagged_col])\n\n            x_lagged = x_lagged[:, 1:]  # remove the column of 0 created above\n\n        return x_lagged\n\n    def _create_lagged_y(self, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Create a lagged matrix of the output without combinations.\n\n        Parameters\n        ----------\n        y : array-like\n            Output data used on training phase.\n\n        Returns\n        -------\n        y_lagged : ndarray of floats\n            A lagged input matrix formed by the output regressors\n            without combinations.\n\n        \"\"\"\n        y_lagged = np.column_stack(\n            [self.shift_column(y[:, 0], lag) for lag in self.ylag]\n        )\n        return y_lagged\n\n    def initial_lagged_matrix(self, X: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Build a lagged matrix concerning each lag for each column.\n\n        Parameters\n        ----------\n        y : array-like\n            Target data used on training phase.\n        X : array-like\n            Input data used on training phase.\n\n        Returns\n        -------\n        lagged_data : ndarray of floats\n            The lagged matrix built in respect with each lag and column.\n\n        Examples\n        --------\n        Let X and y be the input and output values of shape Nx1.\n        If the chosen lags are 2 for both input and output\n        the initial lagged matrix will be formed by Y[k-1], Y[k-2],\n        X[k-1], and X[k-2].\n\n        \"\"\"\n        n_inputs, self.xlag = self._process_xlag(X)\n        self.ylag = self._process_ylag()\n        x_lagged = self._create_lagged_X(X, n_inputs)\n        y_lagged = self._create_lagged_y(y)\n        lagged_data = np.concatenate([y_lagged, x_lagged], axis=1)\n        return lagged_data\n\n    def build_output_matrix(self, *args: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Build the information matrix of output values.\n\n        Each columns of the information matrix represents a candidate\n        regressor. The set of candidate regressors are based on xlag,\n        ylag, and degree entered by the user.\n\n        Parameters\n        ----------\n        args : array-like\n            Target data used on training phase.\n            args[0] is X=None in NAR scenario\n\n        Returns\n        -------\n        data = ndarray of floats\n            The lagged matrix built in respect with each lag and column.\n\n        \"\"\"\n        # Generate a lagged data which each column is a input or output\n        # related to its respective lags. With this approach we can create\n        # the information matrix by using all possible combination of\n        # the columns as a product in the iterations\n        y = args[1]  # args[0] is X=None in NAR scenario\n        self.ylag = self._process_ylag()\n        y_lagged = self._create_lagged_y(y)\n        constant = np.ones([y_lagged.shape[0], 1])\n        data = np.concatenate([constant, y_lagged], axis=1)\n        return data\n\n    def build_input_matrix(self, *args: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Build the information matrix of input values.\n\n        Each columns of the information matrix represents a candidate\n        regressor. The set of candidate regressors are based on xlag,\n        ylag, and degree entered by the user.\n\n        Parameters\n        ----------\n        *args : array-like\n            Input data (X) used on training phase.\n            args[0] is X=None in NAR scenario\n\n        Returns\n        -------\n        data = ndarray of floats\n            The lagged matrix built in respect with each lag and column.\n\n        \"\"\"\n        # Generate a lagged data which each column is a input or output\n        # related to its respective lags. With this approach we can create\n        # the information matrix by using all possible combination of\n        # the columns as a product in the iterations\n\n        X = args[0]  # args[1] is y=None in NFIR scenario\n        n_inputs, self.xlag = self._process_xlag(X)\n        x_lagged = self._create_lagged_X(X, n_inputs)\n        constant = np.ones([x_lagged.shape[0], 1])\n        data = np.concatenate([constant, x_lagged], axis=1)\n        return data\n\n    def build_input_output_matrix(self, X: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Build the information matrix.\n\n        Each columns of the information matrix represents a candidate\n        regressor. The set of candidate regressors are based on xlag,\n        ylag, and degree entered by the user.\n\n        Parameters\n        ----------\n        y : array-like\n            Target data used on training phase.\n        X : array-like\n            Input data used on training phase.\n\n        Returns\n        -------\n        data = ndarray of floats\n            The lagged matrix built in respect with each lag and column.\n\n        \"\"\"\n        # Generate a lagged data which each column is a input or output\n        # related to its respective lags. With this approach we can create\n        # the information matrix by using all possible combination of\n        # the columns as a product in the iterations\n        lagged_data = self.initial_lagged_matrix(X, y)\n        constant = np.ones([lagged_data.shape[0], 1])\n        data = np.concatenate([constant, lagged_data], axis=1)\n        return data\n</code></pre>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.InformationMatrix.build_input_matrix","title":"<code>build_input_matrix(*args)</code>","text":"<p>Build the information matrix of input values.</p> <p>Each columns of the information matrix represents a candidate regressor. The set of candidate regressors are based on xlag, ylag, and degree entered by the user.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.InformationMatrix.build_input_matrix--parameters","title":"Parameters","text":"<p>*args : array-like     Input data (X) used on training phase.     args[0] is X=None in NAR scenario</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.InformationMatrix.build_input_matrix--returns","title":"Returns","text":"<p>data = ndarray of floats     The lagged matrix built in respect with each lag and column.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def build_input_matrix(self, *args: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Build the information matrix of input values.\n\n    Each columns of the information matrix represents a candidate\n    regressor. The set of candidate regressors are based on xlag,\n    ylag, and degree entered by the user.\n\n    Parameters\n    ----------\n    *args : array-like\n        Input data (X) used on training phase.\n        args[0] is X=None in NAR scenario\n\n    Returns\n    -------\n    data = ndarray of floats\n        The lagged matrix built in respect with each lag and column.\n\n    \"\"\"\n    # Generate a lagged data which each column is a input or output\n    # related to its respective lags. With this approach we can create\n    # the information matrix by using all possible combination of\n    # the columns as a product in the iterations\n\n    X = args[0]  # args[1] is y=None in NFIR scenario\n    n_inputs, self.xlag = self._process_xlag(X)\n    x_lagged = self._create_lagged_X(X, n_inputs)\n    constant = np.ones([x_lagged.shape[0], 1])\n    data = np.concatenate([constant, x_lagged], axis=1)\n    return data\n</code></pre>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.InformationMatrix.build_input_output_matrix","title":"<code>build_input_output_matrix(X, y)</code>","text":"<p>Build the information matrix.</p> <p>Each columns of the information matrix represents a candidate regressor. The set of candidate regressors are based on xlag, ylag, and degree entered by the user.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.InformationMatrix.build_input_output_matrix--parameters","title":"Parameters","text":"<p>y : array-like     Target data used on training phase. X : array-like     Input data used on training phase.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.InformationMatrix.build_input_output_matrix--returns","title":"Returns","text":"<p>data = ndarray of floats     The lagged matrix built in respect with each lag and column.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def build_input_output_matrix(self, X: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Build the information matrix.\n\n    Each columns of the information matrix represents a candidate\n    regressor. The set of candidate regressors are based on xlag,\n    ylag, and degree entered by the user.\n\n    Parameters\n    ----------\n    y : array-like\n        Target data used on training phase.\n    X : array-like\n        Input data used on training phase.\n\n    Returns\n    -------\n    data = ndarray of floats\n        The lagged matrix built in respect with each lag and column.\n\n    \"\"\"\n    # Generate a lagged data which each column is a input or output\n    # related to its respective lags. With this approach we can create\n    # the information matrix by using all possible combination of\n    # the columns as a product in the iterations\n    lagged_data = self.initial_lagged_matrix(X, y)\n    constant = np.ones([lagged_data.shape[0], 1])\n    data = np.concatenate([constant, lagged_data], axis=1)\n    return data\n</code></pre>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.InformationMatrix.build_output_matrix","title":"<code>build_output_matrix(*args)</code>","text":"<p>Build the information matrix of output values.</p> <p>Each columns of the information matrix represents a candidate regressor. The set of candidate regressors are based on xlag, ylag, and degree entered by the user.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.InformationMatrix.build_output_matrix--parameters","title":"Parameters","text":"<p>args : array-like     Target data used on training phase.     args[0] is X=None in NAR scenario</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.InformationMatrix.build_output_matrix--returns","title":"Returns","text":"<p>data = ndarray of floats     The lagged matrix built in respect with each lag and column.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def build_output_matrix(self, *args: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Build the information matrix of output values.\n\n    Each columns of the information matrix represents a candidate\n    regressor. The set of candidate regressors are based on xlag,\n    ylag, and degree entered by the user.\n\n    Parameters\n    ----------\n    args : array-like\n        Target data used on training phase.\n        args[0] is X=None in NAR scenario\n\n    Returns\n    -------\n    data = ndarray of floats\n        The lagged matrix built in respect with each lag and column.\n\n    \"\"\"\n    # Generate a lagged data which each column is a input or output\n    # related to its respective lags. With this approach we can create\n    # the information matrix by using all possible combination of\n    # the columns as a product in the iterations\n    y = args[1]  # args[0] is X=None in NAR scenario\n    self.ylag = self._process_ylag()\n    y_lagged = self._create_lagged_y(y)\n    constant = np.ones([y_lagged.shape[0], 1])\n    data = np.concatenate([constant, y_lagged], axis=1)\n    return data\n</code></pre>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.InformationMatrix.initial_lagged_matrix","title":"<code>initial_lagged_matrix(X, y)</code>","text":"<p>Build a lagged matrix concerning each lag for each column.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.InformationMatrix.initial_lagged_matrix--parameters","title":"Parameters","text":"<p>y : array-like     Target data used on training phase. X : array-like     Input data used on training phase.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.InformationMatrix.initial_lagged_matrix--returns","title":"Returns","text":"<p>lagged_data : ndarray of floats     The lagged matrix built in respect with each lag and column.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.InformationMatrix.initial_lagged_matrix--examples","title":"Examples","text":"<p>Let X and y be the input and output values of shape Nx1. If the chosen lags are 2 for both input and output the initial lagged matrix will be formed by Y[k-1], Y[k-2], X[k-1], and X[k-2].</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def initial_lagged_matrix(self, X: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Build a lagged matrix concerning each lag for each column.\n\n    Parameters\n    ----------\n    y : array-like\n        Target data used on training phase.\n    X : array-like\n        Input data used on training phase.\n\n    Returns\n    -------\n    lagged_data : ndarray of floats\n        The lagged matrix built in respect with each lag and column.\n\n    Examples\n    --------\n    Let X and y be the input and output values of shape Nx1.\n    If the chosen lags are 2 for both input and output\n    the initial lagged matrix will be formed by Y[k-1], Y[k-2],\n    X[k-1], and X[k-2].\n\n    \"\"\"\n    n_inputs, self.xlag = self._process_xlag(X)\n    self.ylag = self._process_ylag()\n    x_lagged = self._create_lagged_X(X, n_inputs)\n    y_lagged = self._create_lagged_y(y)\n    lagged_data = np.concatenate([y_lagged, x_lagged], axis=1)\n    return lagged_data\n</code></pre>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.InformationMatrix.shift_column","title":"<code>shift_column(col_to_shift, lag)</code>","text":"<p>Shift values based on a lag.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.InformationMatrix.shift_column--parameters","title":"Parameters","text":"<p>col_to_shift : array-like of shape = n_samples     The samples of the input or output. lag : int     The respective lag of the regressor.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.InformationMatrix.shift_column--returns","title":"Returns","text":"<p>tmp_column : array-like of shape = n_samples     The shifted array of the input or output.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.InformationMatrix.shift_column--examples","title":"Examples","text":"<p>y = [1, 2, 3, 4, 5] shift_column(y, 1) [0, 1, 2, 3, 4]</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def shift_column(self, col_to_shift: np.ndarray, lag: int) -&gt; np.ndarray:\n    \"\"\"Shift values based on a lag.\n\n    Parameters\n    ----------\n    col_to_shift : array-like of shape = n_samples\n        The samples of the input or output.\n    lag : int\n        The respective lag of the regressor.\n\n    Returns\n    -------\n    tmp_column : array-like of shape = n_samples\n        The shifted array of the input or output.\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [1, 2, 3, 4, 5]\n    &gt;&gt;&gt; shift_column(y, 1)\n    [0, 1, 2, 3, 4]\n\n    \"\"\"\n    n_samples = col_to_shift.shape[0]\n    tmp_column = np.zeros((n_samples, 1))\n    aux = col_to_shift[0 : n_samples - lag].reshape(-1, 1)\n    tmp_column[lag:, 0] = aux[:, 0]\n    return tmp_column\n</code></pre>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.Orthogonalization","title":"<code>Orthogonalization</code>","text":"<p>Householder reflection and transformation.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>class Orthogonalization:\n    \"\"\"Householder reflection and transformation.\"\"\"\n\n    def house(self, x: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Perform a Householder reflection of vector.\n\n        Parameters\n        ----------\n        x : array-like of shape = number_of_training_samples\n            The respective column of the matrix of regressors in each\n            iteration of ERR function.\n\n        Returns\n        -------\n        v : array-like of shape = number_of_training_samples\n            The reflection of the array x.\n\n        References\n        ----------\n        - Manuscript: Chen, S., Billings, S. A., &amp; Luo, W. (1989).\n            Orthogonal least squares methods and their application to non-linear\n            system identification.\n\n        \"\"\"\n        u = np.linalg.norm(x, 2)\n        if u != 0:\n            aux_b = x[0] + np.sign(x[0]) * u\n            x = x[1:] / (aux_b + np.finfo(np.float64).eps)\n            x = np.concatenate((np.array([1]), x))\n        return x\n\n    def rowhouse(self, RA: np.ndarray, v: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Perform a row Householder transformation.\n\n        Parameters\n        ----------\n        RA : array-like of shape = number_of_training_samples\n            The respective column of the matrix of regressors in each\n            iteration of ERR function.\n        v : array-like of shape = number_of_training_samples\n            The reflected vector obtained by using the householder reflection.\n\n        Returns\n        -------\n        B : array-like of shape = number_of_training_samples\n\n        References\n        ----------\n        - Manuscript: Chen, S., Billings, S. A., &amp; Luo, W. (1989).\n            Orthogonal least squares methods and their application to\n            non-linear system identification. International Journal of\n            control, 50(5), 1873-1896.\n\n        \"\"\"\n        b = -2 / np.dot(v.T, v)\n        w = b * np.dot(RA.T, v)\n        w = w.reshape(1, -1)\n        v = v.reshape(-1, 1)\n        RA = RA + v * w\n        B = RA\n        return B\n</code></pre>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.Orthogonalization.house","title":"<code>house(x)</code>","text":"<p>Perform a Householder reflection of vector.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.Orthogonalization.house--parameters","title":"Parameters","text":"<p>x : array-like of shape = number_of_training_samples     The respective column of the matrix of regressors in each     iteration of ERR function.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.Orthogonalization.house--returns","title":"Returns","text":"<p>v : array-like of shape = number_of_training_samples     The reflection of the array x.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.Orthogonalization.house--references","title":"References","text":"<ul> <li>Manuscript: Chen, S., Billings, S. A., &amp; Luo, W. (1989).     Orthogonal least squares methods and their application to non-linear     system identification.</li> </ul> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def house(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Perform a Householder reflection of vector.\n\n    Parameters\n    ----------\n    x : array-like of shape = number_of_training_samples\n        The respective column of the matrix of regressors in each\n        iteration of ERR function.\n\n    Returns\n    -------\n    v : array-like of shape = number_of_training_samples\n        The reflection of the array x.\n\n    References\n    ----------\n    - Manuscript: Chen, S., Billings, S. A., &amp; Luo, W. (1989).\n        Orthogonal least squares methods and their application to non-linear\n        system identification.\n\n    \"\"\"\n    u = np.linalg.norm(x, 2)\n    if u != 0:\n        aux_b = x[0] + np.sign(x[0]) * u\n        x = x[1:] / (aux_b + np.finfo(np.float64).eps)\n        x = np.concatenate((np.array([1]), x))\n    return x\n</code></pre>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.Orthogonalization.rowhouse","title":"<code>rowhouse(RA, v)</code>","text":"<p>Perform a row Householder transformation.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.Orthogonalization.rowhouse--parameters","title":"Parameters","text":"<p>RA : array-like of shape = number_of_training_samples     The respective column of the matrix of regressors in each     iteration of ERR function. v : array-like of shape = number_of_training_samples     The reflected vector obtained by using the householder reflection.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.Orthogonalization.rowhouse--returns","title":"Returns","text":"<p>B : array-like of shape = number_of_training_samples</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.Orthogonalization.rowhouse--references","title":"References","text":"<ul> <li>Manuscript: Chen, S., Billings, S. A., &amp; Luo, W. (1989).     Orthogonal least squares methods and their application to     non-linear system identification. International Journal of     control, 50(5), 1873-1896.</li> </ul> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def rowhouse(self, RA: np.ndarray, v: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Perform a row Householder transformation.\n\n    Parameters\n    ----------\n    RA : array-like of shape = number_of_training_samples\n        The respective column of the matrix of regressors in each\n        iteration of ERR function.\n    v : array-like of shape = number_of_training_samples\n        The reflected vector obtained by using the householder reflection.\n\n    Returns\n    -------\n    B : array-like of shape = number_of_training_samples\n\n    References\n    ----------\n    - Manuscript: Chen, S., Billings, S. A., &amp; Luo, W. (1989).\n        Orthogonal least squares methods and their application to\n        non-linear system identification. International Journal of\n        control, 50(5), 1873-1896.\n\n    \"\"\"\n    b = -2 / np.dot(v.T, v)\n    w = b * np.dot(RA.T, v)\n    w = w.reshape(1, -1)\n    v = v.reshape(-1, 1)\n    RA = RA + v * w\n    B = RA\n    return B\n</code></pre>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.RegressorDictionary","title":"<code>RegressorDictionary</code>","text":"<p>               Bases: <code>InformationMatrix</code></p> <p>Base class for Model Structure Selection.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>class RegressorDictionary(InformationMatrix):\n    \"\"\"Base class for Model Structure Selection.\"\"\"\n\n    def __init__(\n        self,\n        xlag: Union[List[Any], Any] = 1,\n        ylag: Union[List[Any], Any] = 1,\n        basis_function: Union[Polynomial, Fourier] = Polynomial(),\n        model_type: str = \"NARMAX\",\n    ):\n        super().__init__(xlag, ylag)\n        self.basis_function = basis_function\n        self.model_type = model_type\n\n    def create_narmax_code(self, n_inputs: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Create the code representation of the regressors.\n\n        This function generates a codification from all possibles\n        regressors given the maximum lag of the input and output.\n        This is used to write the final terms of the model in a\n        readable form. [1001] -&gt; y(k-1).\n        This code format was based on a dissertation from UFMG. See\n        reference below.\n\n        Parameters\n        ----------\n        n_inputs : int\n            Number of input variables.\n\n        Returns\n        -------\n        x_vec : ndarray of int\n            List of the input lags.\n        y_vec : ndarray of int\n            List of the output lags.\n\n        Examples\n        --------\n        The codification is defined as:\n\n        &gt;&gt;&gt; 100n = y(k-n)\n        &gt;&gt;&gt; 200n = u(k-n)\n        &gt;&gt;&gt; [100n 100n] = y(k-n)y(k-n)\n        &gt;&gt;&gt; [200n 200n] = u(k-n)u(k-n)\n\n        References\n        ----------\n        - Master Thesis: Barbosa, Al\u00edpio Monteiro.\n            T\u00e9cnicas de otimiza\u00e7\u00e3o bi-objetivo para a determina\u00e7\u00e3o\n            da estrutura de modelos NARX (2010).\n\n        \"\"\"\n        if self.basis_function.degree &lt; 1:\n            raise ValueError(\n                f\"degree must be integer and &gt; zero. Got {self.basis_function.degree}\"\n            )\n\n        if np.min(np.minimum(self.ylag, 1)) &lt; 1:\n            raise ValueError(\n                f\"ylag must be integer or list and &gt; zero. Got {self.ylag}\"\n            )\n\n        if (\n            np.min(\n                np.min(\n                    np.array(list(chain.from_iterable([[self.xlag]])), dtype=\"object\")\n                )\n            )\n            &lt; 1\n        ):\n            raise ValueError(\n                f\"xlag must be integer or list and &gt; zero. Got {self.xlag}\"\n            )\n\n        y_vec = self.get_y_lag_list()\n\n        if n_inputs == 1:\n            x_vec = self.get_siso_x_lag_list()\n        else:\n            x_vec = self.get_miso_x_lag_list(n_inputs)\n\n        return x_vec, y_vec\n\n    def get_y_lag_list(self) -&gt; np.ndarray:\n        \"\"\"Return y regressor code list.\n\n        Returns\n        -------\n        y_vec = ndarray of ints\n            The y regressor code list given the ylag.\n\n        \"\"\"\n        if isinstance(self.ylag, list):\n            # create only the lags passed from list\n            y_vec = []\n            y_vec.extend([lag + 1000 for lag in self.ylag])\n            return np.array(y_vec)\n\n        # create a range of lags if passed a int value\n        return np.arange(1001, 1001 + self.ylag)\n\n    def get_siso_x_lag_list(self) -&gt; np.ndarray:\n        \"\"\"Return x regressor code list for SISO models.\n\n        Returns\n        -------\n        x_vec_tmp = ndarray of ints\n            The x regressor code list given the xlag for a SISO model.\n\n        \"\"\"\n        if isinstance(self.xlag, list):\n            # create only the lags passed from list\n            x_vec_tmp = []\n            x_vec_tmp.extend([lag + 2000 for lag in self.xlag])\n            return np.array(x_vec_tmp)\n\n        # create a range of lags if passed a int value\n        return np.arange(2001, 2001 + self.xlag)\n\n    def get_miso_x_lag_list(self, n_inputs: int) -&gt; np.ndarray:\n        \"\"\"Return x regressor code list for MISO models.\n\n        Parameters\n        ----------\n        n_inputs : int\n            Number of input variables.\n\n        Returns\n        -------\n        x_vec = ndarray of ints\n            The x regressor code list given the xlag for a MISO model.\n\n        \"\"\"\n        # only list are allowed if n_inputs &gt; 1\n        # the user must entered list of the desired lags explicitly\n        x_vec_tmp = []\n        for i in range(n_inputs):\n            if isinstance(self.xlag[i], list):\n                # create 200n, 300n,..., 400n to describe each input\n                x_vec_tmp.extend([lag + 2000 + i * 1000 for lag in self.xlag[i]])\n            elif isinstance(self.xlag[i], int) and n_inputs &gt; 1:\n                x_vec_tmp.extend(\n                    [np.arange(2001 + i * 1000, 2001 + i * 1000 + self.xlag[i])]\n                )\n\n        # if x_vec is a nested list, ensure all elements are arrays\n        all_arrays = [np.array([i]) if isinstance(i, int) else i for i in x_vec_tmp]\n        return np.concatenate([i for i in all_arrays])\n\n    def regressor_space(self, n_inputs: int) -&gt; np.ndarray:\n        \"\"\"Create regressor code based on model type.\n\n        Parameters\n        ----------\n        n_inputs : int\n            Number of input variables.\n\n        Returns\n        -------\n        regressor_code = ndarray of ints\n            The regressor code list given the xlag and ylag for a MISO model.\n\n        \"\"\"\n        x_vec, y_vec = self.create_narmax_code(n_inputs)\n        reg_aux = np.array([0])\n        if self.model_type == \"NARMAX\":\n            reg_aux = np.concatenate([reg_aux, y_vec, x_vec])\n        elif self.model_type == \"NAR\":\n            reg_aux = np.concatenate([reg_aux, y_vec])\n        elif self.model_type == \"NFIR\":\n            reg_aux = np.concatenate([reg_aux, x_vec])\n        else:\n            raise ValueError(\n                \"Unrecognized model type. Model type should be NARMAX, NAR or NFIR\"\n            )\n\n        regressor_code = list(\n            combinations_with_replacement(reg_aux, self.basis_function.degree)\n        )\n\n        regressor_code = np.array(regressor_code)\n        regressor_code = regressor_code[:, regressor_code.shape[1] :: -1]\n        return regressor_code\n\n    def _get_index_from_regressor_code(\n        self, regressor_code: np.ndarray, model_code: List[int]\n    ):\n        \"\"\"Get the index of user regressor in regressor space.\n\n        Took from: https://stackoverflow.com/questions/38674027/find-the-row-indexes-of-several-values-in-a-numpy-array/38674038#38674038\n\n        Parameters\n        ----------\n        regressor_code : ndarray of int\n            Matrix codification of all possible regressors.\n        model_code : ndarray of int\n            Model defined by the user to simulate.\n\n        Returns\n        -------\n        model_index : ndarray of int\n            Index of model code in the regressor space.\n\n        \"\"\"\n        dims = regressor_code.max(0) + 1\n        model_index = np.where(\n            np.in1d(\n                np.ravel_multi_index(regressor_code.T, dims),\n                np.ravel_multi_index(model_code.T, dims),\n            )\n        )[0]\n        return model_index\n\n    def _list_output_regressor_code(self, model_code: List[int]) -&gt; np.ndarray:\n        \"\"\"Create a flattened array of output regressors.\n\n        Parameters\n        ----------\n        model_code : ndarray of int\n            Model defined by the user to simulate.\n\n        Returns\n        -------\n        regressor_code : ndarray of int\n            Flattened list of output regressors.\n\n        \"\"\"\n        regressor_code = [\n            code for code in model_code.ravel() if (code != 0) and (str(code)[0] == \"1\")\n        ]\n\n        return np.asarray(regressor_code)\n\n    def _list_input_regressor_code(self, model_code: List[int]) -&gt; np.ndarray:\n        \"\"\"Create a flattened array of input regressors.\n\n        Parameters\n        ----------\n        model_code : ndarray of int\n            Model defined by the user to simulate.\n\n        Returns\n        -------\n        regressor_code : ndarray of int\n            Flattened list of output regressors.\n\n        \"\"\"\n        regressor_code = [\n            code for code in model_code.ravel() if (code != 0) and (str(code)[0] != \"1\")\n        ]\n        return np.asarray(regressor_code)\n\n    def _get_lag_from_regressor_code(self, regressors):\n        \"\"\"Get the maximum lag from array of regressors.\n\n        Parameters\n        ----------\n        regressors : ndarray of int\n            Flattened list of input or output regressors.\n\n        Returns\n        -------\n        max_lag : int\n            Maximum lag of list of regressors.\n\n        \"\"\"\n        lag_list = [\n            int(i) for i in regressors.astype(\"str\") for i in [np.sum(int(i[2:]))]\n        ]\n        if len(lag_list) != 0:\n            return max(lag_list)\n\n        return 1\n\n    def _get_max_lag_from_model_code(self, model_code: List[int]) -&gt; int:\n        \"\"\"Create a flattened array of input regressors.\n\n        Parameters\n        ----------\n        model_code : ndarray of int\n            Model defined by the user to simulate.\n\n        Returns\n        -------\n        max_lag : int\n            Maximum lag of list of regressors.\n\n        \"\"\"\n        xlag_code = self._list_input_regressor_code(model_code)\n        ylag_code = self._list_output_regressor_code(model_code)\n        xlag = self._get_lag_from_regressor_code(xlag_code)\n        ylag = self._get_lag_from_regressor_code(ylag_code)\n        return max(xlag, ylag)\n\n    def _get_max_lag(self):\n        \"\"\"Get the max lag defined by the user.\n\n        Returns\n        -------\n        max_lag = int\n            The max lag value defined by the user.\n        \"\"\"\n        ny = np.max(list(chain.from_iterable([[self.ylag]])))\n        nx = np.max(list(chain.from_iterable([[np.array(self.xlag, dtype=object)]])))\n        return np.max([ny, np.max(nx)])\n\n    def get_build_io_method(self, model_type):\n        \"\"\"Get info criteria method.\n\n        Parameters\n        ----------\n        model_type = str\n            The type of the model (NARMAX, NAR or NFIR)\n\n        Returns\n        -------\n        build_method = Self\n            Method to build the input-output matrix\n        \"\"\"\n        build_matrix_options = {\n            \"NARMAX\": self.build_input_output_matrix,\n            \"NFIR\": self.build_input_matrix,\n            \"NAR\": self.build_output_matrix,\n        }\n        return build_matrix_options.get(model_type, None)\n</code></pre>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.create_narmax_code","title":"<code>create_narmax_code(n_inputs)</code>","text":"<p>Create the code representation of the regressors.</p> <p>This function generates a codification from all possibles regressors given the maximum lag of the input and output. This is used to write the final terms of the model in a readable form. [1001] -&gt; y(k-1). This code format was based on a dissertation from UFMG. See reference below.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.create_narmax_code--parameters","title":"Parameters","text":"<p>n_inputs : int     Number of input variables.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.create_narmax_code--returns","title":"Returns","text":"<p>x_vec : ndarray of int     List of the input lags. y_vec : ndarray of int     List of the output lags.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.create_narmax_code--examples","title":"Examples","text":"<p>The codification is defined as:</p> <p>100n = y(k-n) 200n = u(k-n) [100n 100n] = y(k-n)y(k-n) [200n 200n] = u(k-n)u(k-n)</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.create_narmax_code--references","title":"References","text":"<ul> <li>Master Thesis: Barbosa, Al\u00edpio Monteiro.     T\u00e9cnicas de otimiza\u00e7\u00e3o bi-objetivo para a determina\u00e7\u00e3o     da estrutura de modelos NARX (2010).</li> </ul> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def create_narmax_code(self, n_inputs: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Create the code representation of the regressors.\n\n    This function generates a codification from all possibles\n    regressors given the maximum lag of the input and output.\n    This is used to write the final terms of the model in a\n    readable form. [1001] -&gt; y(k-1).\n    This code format was based on a dissertation from UFMG. See\n    reference below.\n\n    Parameters\n    ----------\n    n_inputs : int\n        Number of input variables.\n\n    Returns\n    -------\n    x_vec : ndarray of int\n        List of the input lags.\n    y_vec : ndarray of int\n        List of the output lags.\n\n    Examples\n    --------\n    The codification is defined as:\n\n    &gt;&gt;&gt; 100n = y(k-n)\n    &gt;&gt;&gt; 200n = u(k-n)\n    &gt;&gt;&gt; [100n 100n] = y(k-n)y(k-n)\n    &gt;&gt;&gt; [200n 200n] = u(k-n)u(k-n)\n\n    References\n    ----------\n    - Master Thesis: Barbosa, Al\u00edpio Monteiro.\n        T\u00e9cnicas de otimiza\u00e7\u00e3o bi-objetivo para a determina\u00e7\u00e3o\n        da estrutura de modelos NARX (2010).\n\n    \"\"\"\n    if self.basis_function.degree &lt; 1:\n        raise ValueError(\n            f\"degree must be integer and &gt; zero. Got {self.basis_function.degree}\"\n        )\n\n    if np.min(np.minimum(self.ylag, 1)) &lt; 1:\n        raise ValueError(\n            f\"ylag must be integer or list and &gt; zero. Got {self.ylag}\"\n        )\n\n    if (\n        np.min(\n            np.min(\n                np.array(list(chain.from_iterable([[self.xlag]])), dtype=\"object\")\n            )\n        )\n        &lt; 1\n    ):\n        raise ValueError(\n            f\"xlag must be integer or list and &gt; zero. Got {self.xlag}\"\n        )\n\n    y_vec = self.get_y_lag_list()\n\n    if n_inputs == 1:\n        x_vec = self.get_siso_x_lag_list()\n    else:\n        x_vec = self.get_miso_x_lag_list(n_inputs)\n\n    return x_vec, y_vec\n</code></pre>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.get_build_io_method","title":"<code>get_build_io_method(model_type)</code>","text":"<p>Get info criteria method.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.get_build_io_method--parameters","title":"Parameters","text":"<p>model_type = str     The type of the model (NARMAX, NAR or NFIR)</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.get_build_io_method--returns","title":"Returns","text":"<p>build_method = Self     Method to build the input-output matrix</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def get_build_io_method(self, model_type):\n    \"\"\"Get info criteria method.\n\n    Parameters\n    ----------\n    model_type = str\n        The type of the model (NARMAX, NAR or NFIR)\n\n    Returns\n    -------\n    build_method = Self\n        Method to build the input-output matrix\n    \"\"\"\n    build_matrix_options = {\n        \"NARMAX\": self.build_input_output_matrix,\n        \"NFIR\": self.build_input_matrix,\n        \"NAR\": self.build_output_matrix,\n    }\n    return build_matrix_options.get(model_type, None)\n</code></pre>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.get_miso_x_lag_list","title":"<code>get_miso_x_lag_list(n_inputs)</code>","text":"<p>Return x regressor code list for MISO models.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.get_miso_x_lag_list--parameters","title":"Parameters","text":"<p>n_inputs : int     Number of input variables.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.get_miso_x_lag_list--returns","title":"Returns","text":"<p>x_vec = ndarray of ints     The x regressor code list given the xlag for a MISO model.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def get_miso_x_lag_list(self, n_inputs: int) -&gt; np.ndarray:\n    \"\"\"Return x regressor code list for MISO models.\n\n    Parameters\n    ----------\n    n_inputs : int\n        Number of input variables.\n\n    Returns\n    -------\n    x_vec = ndarray of ints\n        The x regressor code list given the xlag for a MISO model.\n\n    \"\"\"\n    # only list are allowed if n_inputs &gt; 1\n    # the user must entered list of the desired lags explicitly\n    x_vec_tmp = []\n    for i in range(n_inputs):\n        if isinstance(self.xlag[i], list):\n            # create 200n, 300n,..., 400n to describe each input\n            x_vec_tmp.extend([lag + 2000 + i * 1000 for lag in self.xlag[i]])\n        elif isinstance(self.xlag[i], int) and n_inputs &gt; 1:\n            x_vec_tmp.extend(\n                [np.arange(2001 + i * 1000, 2001 + i * 1000 + self.xlag[i])]\n            )\n\n    # if x_vec is a nested list, ensure all elements are arrays\n    all_arrays = [np.array([i]) if isinstance(i, int) else i for i in x_vec_tmp]\n    return np.concatenate([i for i in all_arrays])\n</code></pre>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.get_siso_x_lag_list","title":"<code>get_siso_x_lag_list()</code>","text":"<p>Return x regressor code list for SISO models.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.get_siso_x_lag_list--returns","title":"Returns","text":"<p>x_vec_tmp = ndarray of ints     The x regressor code list given the xlag for a SISO model.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def get_siso_x_lag_list(self) -&gt; np.ndarray:\n    \"\"\"Return x regressor code list for SISO models.\n\n    Returns\n    -------\n    x_vec_tmp = ndarray of ints\n        The x regressor code list given the xlag for a SISO model.\n\n    \"\"\"\n    if isinstance(self.xlag, list):\n        # create only the lags passed from list\n        x_vec_tmp = []\n        x_vec_tmp.extend([lag + 2000 for lag in self.xlag])\n        return np.array(x_vec_tmp)\n\n    # create a range of lags if passed a int value\n    return np.arange(2001, 2001 + self.xlag)\n</code></pre>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.get_y_lag_list","title":"<code>get_y_lag_list()</code>","text":"<p>Return y regressor code list.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.get_y_lag_list--returns","title":"Returns","text":"<p>y_vec = ndarray of ints     The y regressor code list given the ylag.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def get_y_lag_list(self) -&gt; np.ndarray:\n    \"\"\"Return y regressor code list.\n\n    Returns\n    -------\n    y_vec = ndarray of ints\n        The y regressor code list given the ylag.\n\n    \"\"\"\n    if isinstance(self.ylag, list):\n        # create only the lags passed from list\n        y_vec = []\n        y_vec.extend([lag + 1000 for lag in self.ylag])\n        return np.array(y_vec)\n\n    # create a range of lags if passed a int value\n    return np.arange(1001, 1001 + self.ylag)\n</code></pre>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.regressor_space","title":"<code>regressor_space(n_inputs)</code>","text":"<p>Create regressor code based on model type.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.regressor_space--parameters","title":"Parameters","text":"<p>n_inputs : int     Number of input variables.</p>"},{"location":"code/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.regressor_space--returns","title":"Returns","text":"<p>regressor_code = ndarray of ints     The regressor code list given the xlag and ylag for a MISO model.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def regressor_space(self, n_inputs: int) -&gt; np.ndarray:\n    \"\"\"Create regressor code based on model type.\n\n    Parameters\n    ----------\n    n_inputs : int\n        Number of input variables.\n\n    Returns\n    -------\n    regressor_code = ndarray of ints\n        The regressor code list given the xlag and ylag for a MISO model.\n\n    \"\"\"\n    x_vec, y_vec = self.create_narmax_code(n_inputs)\n    reg_aux = np.array([0])\n    if self.model_type == \"NARMAX\":\n        reg_aux = np.concatenate([reg_aux, y_vec, x_vec])\n    elif self.model_type == \"NAR\":\n        reg_aux = np.concatenate([reg_aux, y_vec])\n    elif self.model_type == \"NFIR\":\n        reg_aux = np.concatenate([reg_aux, x_vec])\n    else:\n        raise ValueError(\n            \"Unrecognized model type. Model type should be NARMAX, NAR or NFIR\"\n        )\n\n    regressor_code = list(\n        combinations_with_replacement(reg_aux, self.basis_function.degree)\n    )\n\n    regressor_code = np.array(regressor_code)\n    regressor_code = regressor_code[:, regressor_code.shape[1] :: -1]\n    return regressor_code\n</code></pre>"},{"location":"code/neural-narx/","title":"Documentation for <code>Neural NARX</code>","text":"<p>Build Polynomial NARMAX Models.</p>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN","title":"<code>NARXNN</code>","text":"<p>               Bases: <code>BaseMSS</code></p> <p>NARX Neural Network model build on top of Pytorch.</p> <p>Currently we support a Series-Parallel (open-loop) Feedforward Network training process, which make the training process easier, and we convert the NARX network from Series-Parallel to the Parallel (closed-loop) configuration for prediction.</p>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN--parameters","title":"Parameters","text":"<p>ylag : int, default=2     The maximum lag of the output. xlag : int, default=2     The maximum lag of the input. basis_function: Polynomial or Fourier basis functions     Defines which basis function will be used in the model. model_type: str, default=\"NARMAX\"     The user can choose \"NARMAX\", \"NAR\" and \"NFIR\" models batch_size : int, default=100     Size of mini-batches of data for stochastic optimizers learning_rate : float, default=0.01     Learning rate schedule for weight updates epochs : int, default=100     Number of training epochs loss_func : str, default='mse_loss'     Select the loss function available in torch.nn.functional optimizer : str, default='SGD'     The solver for weight optimization optim_params : dict, default=None     Optional parameters for the optimizer net : default=None     The defined network using nn.Module verbose : bool, default=False     Show the training and validation loss at each iteration</p>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN--examples","title":"Examples","text":"<p>from torch import nn import numpy as np import pandas as pd import matplotlib.pyplot as plt from sysidentpy.metrics import mean_squared_error from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.neural_network import NARXNN from sysidentpy.utils.generate_data import get_siso_data x_train, x_valid, y_train, y_valid = get_siso_data( ...     n=1000, ...     colored_noise=False, ...     sigma=0.01, ...     train_percentage=80 ... ) narx_nn = NARXNN( ...     ylag=2, ...     xlag=2, ...     basis_function=basis_function, ...     model_type=\"NARMAX\", ...     loss_func='mse_loss', ...     optimizer='Adam', ...     epochs=200, ...     verbose=False, ...     optim_params={'betas': (0.9, 0.999), 'eps': 1e-05} # for the optimizer ... ) class Net(nn.Module): ...     def init(self): ...         super().init() ...         self.lin = nn.Linear(4, 10) ...         self.lin2 = nn.Linear(10, 10) ...         self.lin3 = nn.Linear(10, 1) ...         self.tanh = nn.Tanh()</p> <p>...     def forward(self, xb): ...         z = self.lin(xb) ...         z = self.tanh(z) ...         z = self.lin2(z) ...         z = self.tanh(z) ...         z = self.lin3(z) ...         return z</p> <p>narx_nn.net = Net() neural_narx.fit(X=x_train, y=y_train) yhat = neural_narx.predict(X=x_valid, y=y_valid) print(mean_squared_error(y_valid, yhat)) 0.000131</p>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN--references","title":"References","text":"<ul> <li>Manuscript: Orthogonal least squares methods and their application    to non-linear system identification    https://eprints.soton.ac.uk/251147/1/778742007_content.pdf`_</li> </ul> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>class NARXNN(BaseMSS):\n    \"\"\"NARX Neural Network model build on top of Pytorch.\n\n    Currently we support a Series-Parallel (open-loop) Feedforward Network training\n    process, which make the training process easier, and we convert the\n    NARX network from Series-Parallel to the Parallel (closed-loop) configuration for\n    prediction.\n\n    Parameters\n    ----------\n    ylag : int, default=2\n        The maximum lag of the output.\n    xlag : int, default=2\n        The maximum lag of the input.\n    basis_function: Polynomial or Fourier basis functions\n        Defines which basis function will be used in the model.\n    model_type: str, default=\"NARMAX\"\n        The user can choose \"NARMAX\", \"NAR\" and \"NFIR\" models\n    batch_size : int, default=100\n        Size of mini-batches of data for stochastic optimizers\n    learning_rate : float, default=0.01\n        Learning rate schedule for weight updates\n    epochs : int, default=100\n        Number of training epochs\n    loss_func : str, default='mse_loss'\n        Select the loss function available in torch.nn.functional\n    optimizer : str, default='SGD'\n        The solver for weight optimization\n    optim_params : dict, default=None\n        Optional parameters for the optimizer\n    net : default=None\n        The defined network using nn.Module\n    verbose : bool, default=False\n        Show the training and validation loss at each iteration\n\n    Examples\n    --------\n    &gt;&gt;&gt; from torch import nn\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from sysidentpy.metrics import mean_squared_error\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_siso_data\n    &gt;&gt;&gt; from sysidentpy.neural_network import NARXNN\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_siso_data\n    &gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(\n    ...     n=1000,\n    ...     colored_noise=False,\n    ...     sigma=0.01,\n    ...     train_percentage=80\n    ... )\n    &gt;&gt;&gt; narx_nn = NARXNN(\n    ...     ylag=2,\n    ...     xlag=2,\n    ...     basis_function=basis_function,\n    ...     model_type=\"NARMAX\",\n    ...     loss_func='mse_loss',\n    ...     optimizer='Adam',\n    ...     epochs=200,\n    ...     verbose=False,\n    ...     optim_params={'betas': (0.9, 0.999), 'eps': 1e-05} # for the optimizer\n    ... )\n    &gt;&gt;&gt; class Net(nn.Module):\n    ...     def __init__(self):\n    ...         super().__init__()\n    ...         self.lin = nn.Linear(4, 10)\n    ...         self.lin2 = nn.Linear(10, 10)\n    ...         self.lin3 = nn.Linear(10, 1)\n    ...         self.tanh = nn.Tanh()\n    &gt;&gt;&gt;\n    ...     def forward(self, xb):\n    ...         z = self.lin(xb)\n    ...         z = self.tanh(z)\n    ...         z = self.lin2(z)\n    ...         z = self.tanh(z)\n    ...         z = self.lin3(z)\n    ...         return z\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; narx_nn.net = Net()\n    &gt;&gt;&gt; neural_narx.fit(X=x_train, y=y_train)\n    &gt;&gt;&gt; yhat = neural_narx.predict(X=x_valid, y=y_valid)\n    &gt;&gt;&gt; print(mean_squared_error(y_valid, yhat))\n    0.000131\n\n    References\n    ----------\n    - Manuscript: Orthogonal least squares methods and their application\n       to non-linear system identification\n       &lt;https://eprints.soton.ac.uk/251147/1/778742007_content.pdf&gt;`_\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        ylag=1,\n        xlag=1,\n        model_type=\"NARMAX\",\n        basis_function=Polynomial(),\n        batch_size=100,\n        learning_rate=0.01,\n        epochs=200,\n        loss_func=\"mse_loss\",\n        optimizer=\"Adam\",\n        net=None,\n        train_percentage=80,\n        verbose=False,\n        optim_params=None,\n        device=\"cpu\",\n    ):\n        self.ylag = ylag\n        self.xlag = xlag\n        self.basis_function = basis_function\n        self.model_type = model_type\n        self.build_matrix = self.get_build_io_method(model_type)\n        self.non_degree = basis_function.degree\n        self.max_lag = self._get_max_lag()\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.loss_func = getattr(F, loss_func)\n        self.optimizer = optimizer\n        self.net = net\n        self.train_percentage = train_percentage\n        self.verbose = verbose\n        self.optim_params = optim_params\n        self.device = self._check_cuda(device)\n        self.regressor_code = None\n        self.train_loss = None\n        self.val_loss = None\n        self.ensemble = None\n        self.n_inputs = None\n        self.final_model = None\n        self._validate_params()\n\n    def _validate_params(self):\n        \"\"\"Validate input params.\"\"\"\n        if not isinstance(self.batch_size, int) or self.batch_size &lt; 1:\n            raise ValueError(\n                f\"bacth_size must be integer and &gt; zero. Got {self.batch_size}\"\n            )\n\n        if not isinstance(self.epochs, int) or self.epochs &lt; 1:\n            raise ValueError(f\"epochs must be integer and &gt; zero. Got {self.epochs}\")\n\n        if not isinstance(self.train_percentage, int) or self.train_percentage &lt; 0:\n            raise ValueError(\n                f\"bacth_size must be integer and &gt; zero. Got {self.train_percentage}\"\n            )\n\n        if not isinstance(self.verbose, bool):\n            raise TypeError(f\"verbose must be False or True. Got {self.verbose}\")\n\n        if isinstance(self.ylag, int) and self.ylag &lt; 1:\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if isinstance(self.xlag, int) and self.xlag &lt; 1:\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.xlag, (int, list)):\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.ylag, (int, list)):\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if self.model_type not in [\"NARMAX\", \"NAR\", \"NFIR\"]:\n            raise ValueError(\n                f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n            )\n\n    def _check_cuda(self, device):\n        if device not in [\"cpu\", \"cuda\"]:\n            raise ValueError(f\"device must be 'cpu' or 'cuda'. Got {device}\")\n\n        if device == \"cpu\":\n            return torch.device(\"cpu\")\n\n        if device == \"cuda\":\n            if torch.cuda.is_available():\n                return torch.device(\"cuda\")\n\n            warnings.warn(\n                \"No CUDA available. We set the device as CPU\",\n                stacklevel=2,\n            )\n\n        return torch.device(\"cpu\")\n\n    def define_opt(self):\n        \"\"\"Define the optimizer using the user parameters.\"\"\"\n        opt = getattr(optim, self.optimizer)\n        return opt(self.net.parameters(), lr=self.learning_rate, **self.optim_params)\n\n    def loss_batch(self, X, y, opt=None):\n        \"\"\"Compute the loss for one batch.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The regressor matrix.\n        y : ndarray of floats\n            The output data.\n        opt: Torch optimizer\n            Torch optimizer chosen by the user.\n\n        Returns\n        -------\n        loss : float\n            The loss of one batch.\n\n        \"\"\"\n        loss = self.loss_func(self.net(X), y)\n\n        if opt is not None:\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n\n        return loss.item(), len(X)\n\n    def split_data(self, X, y):\n        \"\"\"Return the lagged matrix and the y values given the maximum lags.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data.\n        y : ndarray of floats\n            The output data.\n\n        Returns\n        -------\n        y : ndarray of floats\n            The y values considering the lags.\n        reg_matrix : ndarray of floats\n            The information matrix of the model.\n\n        \"\"\"\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        self.max_lag = self._get_max_lag()\n        lagged_data = self.build_matrix(X, y)\n\n        basis_name = self.basis_function.__class__.__name__\n        if basis_name == \"Polynomial\":\n            reg_matrix = self.basis_function.fit(\n                lagged_data, self.max_lag, predefined_regressors=None\n            )\n            reg_matrix = reg_matrix[:, 1:]\n        else:\n            reg_matrix = self.basis_function.fit(\n                lagged_data, self.max_lag, predefined_regressors=None\n            )\n\n        if X is not None:\n            self.n_inputs = _num_features(X)\n        else:\n            self.n_inputs = 1  # only used to create the regressor space base\n\n        self.regressor_code = self.regressor_space(self.n_inputs)\n        repetition = len(reg_matrix)\n        if not isinstance(self.basis_function, Polynomial):\n            tmp_code = np.sort(\n                np.tile(self.regressor_code[1:, :], (repetition, 1)),\n                axis=0,\n            )\n            self.regressor_code = tmp_code[list(range(len(reg_matrix))), :].copy()\n        else:\n            self.regressor_code = self.regressor_code[\n                1:\n            ]  # removes the column of the constant\n\n        self.final_model = self.regressor_code.copy()\n        reg_matrix = np.atleast_1d(reg_matrix).astype(np.float32)\n\n        y = np.atleast_1d(y[self.max_lag :]).astype(np.float32)\n        return reg_matrix, y\n\n    def convert_to_tensor(self, reg_matrix, y):\n        \"\"\"Return the lagged matrix and the y values given the maximum lags.\n\n        Based on Pytorch official docs:\n        https://pytorch.org/tutorials/beginner/nn_tutorial.html\n\n        Parameters\n        ----------\n        reg_matrix : ndarray of floats\n            The information matrix of the model.\n        y : ndarray of floats\n            The output data\n\n        Returns\n        -------\n        Tensor: tensor\n            tensors that have the same size of the first dimension.\n\n        \"\"\"\n        reg_matrix, y = map(torch.tensor, (reg_matrix, y))\n        return TensorDataset(reg_matrix, y)\n\n    def get_data(self, train_ds):\n        \"\"\"Return the lagged matrix and the y values given the maximum lags.\n\n        Based on Pytorch official docs:\n        https://pytorch.org/tutorials/beginner/nn_tutorial.html\n\n        Parameters\n        ----------\n        train_ds: tensor\n            Tensors that have the same size of the first dimension.\n\n        Returns\n        -------\n        Dataloader: dataloader\n            tensors that have the same size of the first dimension.\n\n        \"\"\"\n        pin_memory = False if self.device.type == \"cpu\" else True\n        return DataLoader(\n            train_ds, batch_size=self.batch_size, pin_memory=pin_memory, shuffle=False\n        )\n\n    def data_transform(self, X, y):\n        \"\"\"Return the data transformed in tensors using Dataloader.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data.\n        y : ndarray of floats\n            The output data.\n\n        Returns\n        -------\n        Tensors : Dataloader\n\n        \"\"\"\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        x_train, y_train = self.split_data(X, y)\n        train_ds = self.convert_to_tensor(x_train, y_train)\n        train_dl = self.get_data(train_ds)\n        return train_dl\n\n    def fit(self, *, X=None, y=None, X_test=None, y_test=None):\n        \"\"\"Train a NARX Neural Network model.\n\n        This is an training pipeline that allows a friendly usage\n        by the user. The training pipeline was based on\n        https://pytorch.org/tutorials/beginner/nn_tutorial.html\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the training process.\n        y : ndarray of floats\n            The output data to be used in the training process.\n        X_test : ndarray of floats\n            The input data to be used in the prediction process.\n        y_test : ndarray of floats\n            The output data (initial conditions) to be used in the prediction process.\n\n        Returns\n        -------\n        net : nn.Module\n            The model fitted.\n        train_loss: ndarrays of floats\n            The training loss of each batch\n        val_loss: ndarrays of floats\n            The validation loss of each batch\n\n        \"\"\"\n        train_dl = self.data_transform(X, y)\n        if self.verbose:\n            if X_test is None or y_test is None:\n                raise ValueError(\n                    \"X_test and y_test cannot be None if you set verbose=True\"\n                )\n            valid_dl = self.data_transform(X_test, y_test)\n\n        opt = self.define_opt()\n        self.val_loss = []\n        self.train_loss = []\n        for epoch in range(self.epochs):\n            self.net.train()\n            for input_data, output_data in train_dl:\n                X, y = input_data.to(self.device), output_data.to(self.device)\n                self.loss_batch(X, y, opt=opt)\n\n            if self.verbose:\n                train_losses, train_nums = zip(\n                    *[\n                        self.loss_batch(X.to(self.device), y.to(self.device))\n                        for X, y in train_dl\n                    ]\n                )\n                self.train_loss.append(\n                    np.sum(np.multiply(train_losses, train_nums)) / np.sum(train_nums)\n                )\n\n                self.net.eval()\n                with torch.no_grad():\n                    losses, nums = zip(\n                        *[\n                            self.loss_batch(X.to(self.device), y.to(self.device))\n                            for X, y in valid_dl\n                        ]\n                    )\n                self.val_loss.append(np.sum(np.multiply(losses, nums)) / np.sum(nums))\n                logging.info(\n                    \"Train metrics: %s | Validation metrics: %s\",\n                    self.train_loss[epoch],\n                    self.val_loss[epoch],\n                )\n        return self\n\n    def predict(self, *, X=None, y=None, steps_ahead=None, forecast_horizon=None):\n        \"\"\"Return the predicted given an input and initial values.\n\n        The predict function allows a friendly usage by the user.\n        Given a trained model, predict values given\n        a new set of data.\n\n        This method accept y values mainly for prediction n-steps ahead\n        (to be implemented in the future).\n\n        Currently we only support infinity-steps-ahead prediction,\n        but run 1-step-ahead prediction manually is straightforward.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            The predicted values of the model.\n\n        \"\"\"\n        if self.basis_function.__class__.__name__ == \"Polynomial\":\n            if steps_ahead is None:\n                return self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n            if steps_ahead == 1:\n                return self._one_step_ahead_prediction(X, y)\n\n            _check_positive_int(steps_ahead, \"steps_ahead\")\n            return self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n\n        if steps_ahead is None:\n            return self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n        if steps_ahead == 1:\n            return self._one_step_ahead_prediction(X, y)\n\n        return self._basis_function_n_step_prediction(\n            X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n        )\n\n    def _one_step_ahead_prediction(self, X, y):\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        lagged_data = self.build_matrix(X, y)\n\n        basis_name = self.basis_function.__class__.__name__\n        if basis_name == \"Polynomial\":\n            X_base = self.basis_function.transform(\n                lagged_data,\n                self.max_lag,\n            )\n            X_base = X_base[:, 1:]\n        else:\n            X_base = self.basis_function.transform(\n                lagged_data,\n                self.max_lag,\n            )\n\n        yhat = np.zeros(X.shape[0], dtype=float)\n        X_base = np.atleast_1d(X_base).astype(np.float32)\n        yhat = yhat.astype(np.float32)\n        x_valid, _ = map(torch.tensor, (X_base, yhat))\n        yhat = self.net(x_valid.to(self.device)).detach().cpu().numpy()\n        yhat = np.concatenate([y.ravel()[: self.max_lag].flatten(), yhat.ravel()])\n        return yhat.reshape(-1, 1)\n\n    def _n_step_ahead_prediction(self, X, y, steps_ahead):\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        yhat = np.zeros(X.shape[0], dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n        i = self.max_lag\n        X = X.reshape(-1, self.n_inputs)\n        while i &lt; len(y):\n            k = int(i - self.max_lag)\n            if i + steps_ahead &gt; len(y):\n                steps_ahead = len(y) - i  # predicts the remaining values\n\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                X[k : i + steps_ahead], y[k : i + steps_ahead]\n            )[-steps_ahead:].ravel()\n\n            i += steps_ahead\n\n        yhat = yhat.ravel()\n        return yhat.reshape(-1, 1)\n\n    def _model_prediction(self, X, y_initial, forecast_horizon=None):\n        \"\"\"Perform the infinity steps-ahead simulation of a model.\n\n        Parameters\n        ----------\n        y_initial : array-like of shape = max_lag\n            Number of initial conditions values of output\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The predicted values of the model.\n\n        \"\"\"\n        if self.model_type in [\"NARMAX\", \"NAR\"]:\n            return self._narmax_predict(X, y_initial, forecast_horizon)\n\n        if self.model_type == \"NFIR\":\n            return self._nfir_predict(X, y_initial)\n\n        raise ValueError(\n            f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n        )\n\n    def _narmax_predict(self, X, y_initial, forecast_horizon):\n        if len(y_initial) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if X is not None:\n            forecast_horizon = X.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        y_output = np.zeros(forecast_horizon, dtype=float)\n        y_output.fill(np.nan)\n        y_output[: self.max_lag] = y_initial[: self.max_lag, 0]\n\n        model_exponents = [\n            self._code2exponents(code=model) for model in self.final_model\n        ]\n        raw_regressor = np.zeros(len(model_exponents[0]), dtype=float)\n        for i in range(self.max_lag, forecast_horizon):\n            init = 0\n            final = self.max_lag\n            k = int(i - self.max_lag)\n            raw_regressor[:final] = y_output[k:i]\n            for j in range(self.n_inputs):\n                init += self.max_lag\n                final += self.max_lag\n                raw_regressor[init:final] = X[k:i, j]\n\n            regressor_value = np.zeros(len(model_exponents))\n            for j, model_exponent in enumerate(model_exponents):\n                regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\n\n            regressor_value = np.atleast_1d(regressor_value).astype(np.float32)\n            y_output = y_output.astype(np.float32)\n            x_valid, _ = map(torch.tensor, (regressor_value, y_output))\n            y_output[i] = self.net(x_valid.to(self.device))[0].detach().cpu().numpy()\n        return y_output.reshape(-1, 1)\n\n    def _nfir_predict(self, X, y_initial):\n        y_output = np.zeros(X.shape[0], dtype=float)\n        y_output.fill(np.nan)\n        y_output[: self.max_lag] = y_initial[: self.max_lag, 0]\n        X = X.reshape(-1, self.n_inputs)\n        model_exponents = [\n            self._code2exponents(code=model) for model in self.final_model\n        ]\n        raw_regressor = np.zeros(len(model_exponents[0]), dtype=float)\n        for i in range(self.max_lag, X.shape[0]):\n            init = 0\n            final = self.max_lag\n            k = int(i - self.max_lag)\n            for j in range(self.n_inputs):\n                raw_regressor[init:final] = X[k:i, j]\n                init += self.max_lag\n                final += self.max_lag\n\n            regressor_value = np.zeros(len(model_exponents))\n            for j, model_exponent in enumerate(model_exponents):\n                regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\n\n            regressor_value = np.atleast_1d(regressor_value).astype(np.float32)\n            y_output = y_output.astype(np.float32)\n            x_valid, _ = map(torch.tensor, (regressor_value, y_output))\n            y_output[i] = self.net(x_valid.to(self.device))[0].detach().cpu().numpy()\n        return y_output.reshape(-1, 1)\n\n    def _basis_function_predict(self, X, y_initial, forecast_horizon=None):\n        if X is not None:\n            forecast_horizon = X.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y_initial[: self.max_lag, 0]\n\n        analyzed_elements_number = self.max_lag + 1\n\n        for i in range(forecast_horizon - self.max_lag):\n            if self.model_type == \"NARMAX\":\n                lagged_data = self.build_input_output_matrix(\n                    X[i : i + analyzed_elements_number],\n                    yhat[i : i + analyzed_elements_number].reshape(-1, 1),\n                )\n            elif self.model_type == \"NAR\":\n                lagged_data = self.build_output_matrix(\n                    yhat[i : i + analyzed_elements_number].reshape(-1, 1)\n                )\n            elif self.model_type == \"NFIR\":\n                lagged_data = self.build_input_matrix(\n                    X[i : i + analyzed_elements_number]\n                )\n            else:\n                raise ValueError(\n                    \"Unrecognized model type. The model_type should be NARMAX, NAR or\"\n                    \" NFIR.\"\n                )\n\n            X_tmp = self.basis_function.transform(\n                lagged_data,\n                self.max_lag,\n            )\n            X_tmp = np.atleast_1d(X_tmp).astype(np.float32)\n            yhat = yhat.astype(np.float32)\n            x_valid, _ = map(torch.tensor, (X_tmp, yhat))\n            yhat[i + self.max_lag] = (\n                self.net(x_valid.to(self.device))[0].detach().cpu().numpy()\n            )[0]\n        return yhat.reshape(-1, 1)\n\n    def _basis_function_n_step_prediction(self, X, y, steps_ahead, forecast_horizon):\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        X : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if X is not None:\n            forecast_horizon = X.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n\n        i = self.max_lag\n\n        while i &lt; len(y):\n            k = int(i - self.max_lag)\n            if i + steps_ahead &gt; len(y):\n                steps_ahead = len(y) - i  # predicts the remaining values\n\n            if self.model_type == \"NARMAX\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    X[k : i + steps_ahead], y[k : i + steps_ahead]\n                )[-steps_ahead:].ravel()\n            elif self.model_type == \"NAR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    X=None,\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NFIR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    X=X[k : i + steps_ahead],\n                    y_initial=y[k : i + steps_ahead],\n                )[-steps_ahead:].ravel()\n            else:\n                raise ValueError(\n                    f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n                )\n\n            i += steps_ahead\n\n        return yhat.reshape(-1, 1)\n\n    def _basis_function_n_steps_horizon(self, X, y, steps_ahead, forecast_horizon):\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n\n        i = self.max_lag\n\n        while i &lt; len(y):\n            k = int(i - self.max_lag)\n            if i + steps_ahead &gt; len(y):\n                steps_ahead = len(y) - i  # predicts the remaining values\n\n            if self.model_type == \"NARMAX\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    X[k : i + steps_ahead], y[k : i + steps_ahead]\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NAR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    X=None,\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NFIR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    X=X[k : i + steps_ahead],\n                    y_initial=y[k : i + steps_ahead],\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            else:\n                raise ValueError(\n                    f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n                )\n\n            i += steps_ahead\n\n        yhat = yhat.ravel()\n        return yhat.reshape(-1, 1)\n</code></pre>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.convert_to_tensor","title":"<code>convert_to_tensor(reg_matrix, y)</code>","text":"<p>Return the lagged matrix and the y values given the maximum lags.</p> <p>Based on Pytorch official docs: https://pytorch.org/tutorials/beginner/nn_tutorial.html</p>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.convert_to_tensor--parameters","title":"Parameters","text":"<p>reg_matrix : ndarray of floats     The information matrix of the model. y : ndarray of floats     The output data</p>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.convert_to_tensor--returns","title":"Returns","text":"<p>Tensor: tensor     tensors that have the same size of the first dimension.</p> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def convert_to_tensor(self, reg_matrix, y):\n    \"\"\"Return the lagged matrix and the y values given the maximum lags.\n\n    Based on Pytorch official docs:\n    https://pytorch.org/tutorials/beginner/nn_tutorial.html\n\n    Parameters\n    ----------\n    reg_matrix : ndarray of floats\n        The information matrix of the model.\n    y : ndarray of floats\n        The output data\n\n    Returns\n    -------\n    Tensor: tensor\n        tensors that have the same size of the first dimension.\n\n    \"\"\"\n    reg_matrix, y = map(torch.tensor, (reg_matrix, y))\n    return TensorDataset(reg_matrix, y)\n</code></pre>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.data_transform","title":"<code>data_transform(X, y)</code>","text":"<p>Return the data transformed in tensors using Dataloader.</p>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.data_transform--parameters","title":"Parameters","text":"<p>X : ndarray of floats     The input data. y : ndarray of floats     The output data.</p>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.data_transform--returns","title":"Returns","text":"<p>Tensors : Dataloader</p> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def data_transform(self, X, y):\n    \"\"\"Return the data transformed in tensors using Dataloader.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data.\n    y : ndarray of floats\n        The output data.\n\n    Returns\n    -------\n    Tensors : Dataloader\n\n    \"\"\"\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    x_train, y_train = self.split_data(X, y)\n    train_ds = self.convert_to_tensor(x_train, y_train)\n    train_dl = self.get_data(train_ds)\n    return train_dl\n</code></pre>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.define_opt","title":"<code>define_opt()</code>","text":"<p>Define the optimizer using the user parameters.</p> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def define_opt(self):\n    \"\"\"Define the optimizer using the user parameters.\"\"\"\n    opt = getattr(optim, self.optimizer)\n    return opt(self.net.parameters(), lr=self.learning_rate, **self.optim_params)\n</code></pre>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.fit","title":"<code>fit(*, X=None, y=None, X_test=None, y_test=None)</code>","text":"<p>Train a NARX Neural Network model.</p> <p>This is an training pipeline that allows a friendly usage by the user. The training pipeline was based on https://pytorch.org/tutorials/beginner/nn_tutorial.html</p>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.fit--parameters","title":"Parameters","text":"<p>X : ndarray of floats     The input data to be used in the training process. y : ndarray of floats     The output data to be used in the training process. X_test : ndarray of floats     The input data to be used in the prediction process. y_test : ndarray of floats     The output data (initial conditions) to be used in the prediction process.</p>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.fit--returns","title":"Returns","text":"<p>net : nn.Module     The model fitted. train_loss: ndarrays of floats     The training loss of each batch val_loss: ndarrays of floats     The validation loss of each batch</p> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def fit(self, *, X=None, y=None, X_test=None, y_test=None):\n    \"\"\"Train a NARX Neural Network model.\n\n    This is an training pipeline that allows a friendly usage\n    by the user. The training pipeline was based on\n    https://pytorch.org/tutorials/beginner/nn_tutorial.html\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the training process.\n    y : ndarray of floats\n        The output data to be used in the training process.\n    X_test : ndarray of floats\n        The input data to be used in the prediction process.\n    y_test : ndarray of floats\n        The output data (initial conditions) to be used in the prediction process.\n\n    Returns\n    -------\n    net : nn.Module\n        The model fitted.\n    train_loss: ndarrays of floats\n        The training loss of each batch\n    val_loss: ndarrays of floats\n        The validation loss of each batch\n\n    \"\"\"\n    train_dl = self.data_transform(X, y)\n    if self.verbose:\n        if X_test is None or y_test is None:\n            raise ValueError(\n                \"X_test and y_test cannot be None if you set verbose=True\"\n            )\n        valid_dl = self.data_transform(X_test, y_test)\n\n    opt = self.define_opt()\n    self.val_loss = []\n    self.train_loss = []\n    for epoch in range(self.epochs):\n        self.net.train()\n        for input_data, output_data in train_dl:\n            X, y = input_data.to(self.device), output_data.to(self.device)\n            self.loss_batch(X, y, opt=opt)\n\n        if self.verbose:\n            train_losses, train_nums = zip(\n                *[\n                    self.loss_batch(X.to(self.device), y.to(self.device))\n                    for X, y in train_dl\n                ]\n            )\n            self.train_loss.append(\n                np.sum(np.multiply(train_losses, train_nums)) / np.sum(train_nums)\n            )\n\n            self.net.eval()\n            with torch.no_grad():\n                losses, nums = zip(\n                    *[\n                        self.loss_batch(X.to(self.device), y.to(self.device))\n                        for X, y in valid_dl\n                    ]\n                )\n            self.val_loss.append(np.sum(np.multiply(losses, nums)) / np.sum(nums))\n            logging.info(\n                \"Train metrics: %s | Validation metrics: %s\",\n                self.train_loss[epoch],\n                self.val_loss[epoch],\n            )\n    return self\n</code></pre>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.get_data","title":"<code>get_data(train_ds)</code>","text":"<p>Return the lagged matrix and the y values given the maximum lags.</p> <p>Based on Pytorch official docs: https://pytorch.org/tutorials/beginner/nn_tutorial.html</p>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.get_data--parameters","title":"Parameters","text":"<p>train_ds: tensor     Tensors that have the same size of the first dimension.</p>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.get_data--returns","title":"Returns","text":"<p>Dataloader: dataloader     tensors that have the same size of the first dimension.</p> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def get_data(self, train_ds):\n    \"\"\"Return the lagged matrix and the y values given the maximum lags.\n\n    Based on Pytorch official docs:\n    https://pytorch.org/tutorials/beginner/nn_tutorial.html\n\n    Parameters\n    ----------\n    train_ds: tensor\n        Tensors that have the same size of the first dimension.\n\n    Returns\n    -------\n    Dataloader: dataloader\n        tensors that have the same size of the first dimension.\n\n    \"\"\"\n    pin_memory = False if self.device.type == \"cpu\" else True\n    return DataLoader(\n        train_ds, batch_size=self.batch_size, pin_memory=pin_memory, shuffle=False\n    )\n</code></pre>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.loss_batch","title":"<code>loss_batch(X, y, opt=None)</code>","text":"<p>Compute the loss for one batch.</p>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.loss_batch--parameters","title":"Parameters","text":"<p>X : ndarray of floats     The regressor matrix. y : ndarray of floats     The output data. opt: Torch optimizer     Torch optimizer chosen by the user.</p>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.loss_batch--returns","title":"Returns","text":"<p>loss : float     The loss of one batch.</p> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def loss_batch(self, X, y, opt=None):\n    \"\"\"Compute the loss for one batch.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The regressor matrix.\n    y : ndarray of floats\n        The output data.\n    opt: Torch optimizer\n        Torch optimizer chosen by the user.\n\n    Returns\n    -------\n    loss : float\n        The loss of one batch.\n\n    \"\"\"\n    loss = self.loss_func(self.net(X), y)\n\n    if opt is not None:\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n\n    return loss.item(), len(X)\n</code></pre>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.predict","title":"<code>predict(*, X=None, y=None, steps_ahead=None, forecast_horizon=None)</code>","text":"<p>Return the predicted given an input and initial values.</p> <p>The predict function allows a friendly usage by the user. Given a trained model, predict values given a new set of data.</p> <p>This method accept y values mainly for prediction n-steps ahead (to be implemented in the future).</p> <p>Currently we only support infinity-steps-ahead prediction, but run 1-step-ahead prediction manually is straightforward.</p>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.predict--parameters","title":"Parameters","text":"<p>X : ndarray of floats     The input data to be used in the prediction process. y : ndarray of floats     The output data to be used in the prediction process. steps_ahead : int (default = None)     The user can use free run simulation, one-step ahead prediction     and n-step ahead prediction. forecast_horizon : int, default=None     The number of predictions over the time.</p>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.predict--returns","title":"Returns","text":"<p>yhat : ndarray of floats     The predicted values of the model.</p> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def predict(self, *, X=None, y=None, steps_ahead=None, forecast_horizon=None):\n    \"\"\"Return the predicted given an input and initial values.\n\n    The predict function allows a friendly usage by the user.\n    Given a trained model, predict values given\n    a new set of data.\n\n    This method accept y values mainly for prediction n-steps ahead\n    (to be implemented in the future).\n\n    Currently we only support infinity-steps-ahead prediction,\n    but run 1-step-ahead prediction manually is straightforward.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    steps_ahead : int (default = None)\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction.\n    forecast_horizon : int, default=None\n        The number of predictions over the time.\n\n    Returns\n    -------\n    yhat : ndarray of floats\n        The predicted values of the model.\n\n    \"\"\"\n    if self.basis_function.__class__.__name__ == \"Polynomial\":\n        if steps_ahead is None:\n            return self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n        if steps_ahead == 1:\n            return self._one_step_ahead_prediction(X, y)\n\n        _check_positive_int(steps_ahead, \"steps_ahead\")\n        return self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n\n    if steps_ahead is None:\n        return self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n    if steps_ahead == 1:\n        return self._one_step_ahead_prediction(X, y)\n\n    return self._basis_function_n_step_prediction(\n        X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n    )\n</code></pre>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.split_data","title":"<code>split_data(X, y)</code>","text":"<p>Return the lagged matrix and the y values given the maximum lags.</p>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.split_data--parameters","title":"Parameters","text":"<p>X : ndarray of floats     The input data. y : ndarray of floats     The output data.</p>"},{"location":"code/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.split_data--returns","title":"Returns","text":"<p>y : ndarray of floats     The y values considering the lags. reg_matrix : ndarray of floats     The information matrix of the model.</p> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def split_data(self, X, y):\n    \"\"\"Return the lagged matrix and the y values given the maximum lags.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data.\n    y : ndarray of floats\n        The output data.\n\n    Returns\n    -------\n    y : ndarray of floats\n        The y values considering the lags.\n    reg_matrix : ndarray of floats\n        The information matrix of the model.\n\n    \"\"\"\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    self.max_lag = self._get_max_lag()\n    lagged_data = self.build_matrix(X, y)\n\n    basis_name = self.basis_function.__class__.__name__\n    if basis_name == \"Polynomial\":\n        reg_matrix = self.basis_function.fit(\n            lagged_data, self.max_lag, predefined_regressors=None\n        )\n        reg_matrix = reg_matrix[:, 1:]\n    else:\n        reg_matrix = self.basis_function.fit(\n            lagged_data, self.max_lag, predefined_regressors=None\n        )\n\n    if X is not None:\n        self.n_inputs = _num_features(X)\n    else:\n        self.n_inputs = 1  # only used to create the regressor space base\n\n    self.regressor_code = self.regressor_space(self.n_inputs)\n    repetition = len(reg_matrix)\n    if not isinstance(self.basis_function, Polynomial):\n        tmp_code = np.sort(\n            np.tile(self.regressor_code[1:, :], (repetition, 1)),\n            axis=0,\n        )\n        self.regressor_code = tmp_code[list(range(len(reg_matrix))), :].copy()\n    else:\n        self.regressor_code = self.regressor_code[\n            1:\n        ]  # removes the column of the constant\n\n    self.final_model = self.regressor_code.copy()\n    reg_matrix = np.atleast_1d(reg_matrix).astype(np.float32)\n\n    y = np.atleast_1d(y[self.max_lag :]).astype(np.float32)\n    return reg_matrix, y\n</code></pre>"},{"location":"code/parameter-estimation/","title":"Documentation for <code>Parameters Estimation</code>","text":"<p>Methods for parameter estimation.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.AffineLeastMeanSquares","title":"<code>AffineLeastMeanSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Affine Least Mean Squares (ALMS) filter for parameter estimation.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.AffineLeastMeanSquares--parameters","title":"Parameters","text":"<p>mu : float, default=0.01     The learning rate or step size for the LMS algorithm. offset_covariance : float, default=0.2     The offset covariance factor of the affine least mean squares     filter.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.AffineLeastMeanSquares--attributes","title":"Attributes","text":"<p>mu : float     The learning rate or step size for the LMS algorithm. offset_covariance : float, default=0.2     The offset covariance factor of the affine least mean squares     filter. xi : np.ndarray or None     The estimation error at each iteration. Initialized as None and updated during     optimization.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.AffineLeastMeanSquares--methods","title":"Methods","text":"<p>optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray     Estimate the model parameters using the LMS filter.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class AffineLeastMeanSquares(BaseEstimator):\n    \"\"\"Affine Least Mean Squares (ALMS) filter for parameter estimation.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    offset_covariance : float, default=0.2\n        The offset covariance factor of the affine least mean squares\n        filter.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    offset_covariance : float, default=0.2\n        The offset covariance factor of the affine least mean squares\n        filter.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMS filter.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        offset_covariance: float = 0.2,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.offset_covariance = offset_covariance\n        self.uiter = uiter\n        self.unbiased = unbiased\n        self._validate_params(vars(self))\n        self.xi: np.ndarray\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Estimate the model parameters using the Affine Least Mean Squares.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        A more in-depth documentation of all methods for parameters estimation\n        will be available soon. For now, please refer to the mentioned\n        references.\n\n        References\n        ----------\n        - Book: Poularikas, A. D. (2017). Adaptive filtering: Fundamentals\n           of least mean squares with MATLAB\u00ae. CRC Press.\n\n        \"\"\"\n        n_theta, n, theta, self.xi = self._initial_values(psi)\n\n        for i in range(n_theta, n):\n            self.xi = y - psi.dot(theta[:, i - 1].reshape(-1, 1))\n            aux = (\n                self.mu\n                * psi\n                @ np.linalg.pinv(psi.T @ psi + self.offset_covariance * np.eye(n_theta))\n            )\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + aux.T.dot(self.xi)\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.AffineLeastMeanSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Estimate the model parameters using the Affine Least Mean Squares.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.AffineLeastMeanSquares.optimize--parameters","title":"Parameters","text":"<p>psi : ndarray of floats     The information matrix of the model. y : array-like of shape = y_training     The data used to training the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.AffineLeastMeanSquares.optimize--returns","title":"Returns","text":"<p>theta : array-like of shape = number_of_model_elements     The estimated parameters of the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.AffineLeastMeanSquares.optimize--notes","title":"Notes","text":"<p>A more in-depth documentation of all methods for parameters estimation will be available soon. For now, please refer to the mentioned references.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.AffineLeastMeanSquares.optimize--references","title":"References","text":"<ul> <li>Book: Poularikas, A. D. (2017). Adaptive filtering: Fundamentals    of least mean squares with MATLAB\u00ae. CRC Press.</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Estimate the model parameters using the Affine Least Mean Squares.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    A more in-depth documentation of all methods for parameters estimation\n    will be available soon. For now, please refer to the mentioned\n    references.\n\n    References\n    ----------\n    - Book: Poularikas, A. D. (2017). Adaptive filtering: Fundamentals\n       of least mean squares with MATLAB\u00ae. CRC Press.\n\n    \"\"\"\n    n_theta, n, theta, self.xi = self._initial_values(psi)\n\n    for i in range(n_theta, n):\n        self.xi = y - psi.dot(theta[:, i - 1].reshape(-1, 1))\n        aux = (\n            self.mu\n            * psi\n            @ np.linalg.pinv(psi.T @ psi + self.offset_covariance * np.eye(n_theta))\n        )\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + aux.T.dot(self.xi)\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.BoundedVariableLeastSquares","title":"<code>BoundedVariableLeastSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Solve a linear least-squares problem with bounds on the variables.</p> <p>This is a wrapper class for the <code>scipy.optimize.lsq_linear</code> method.</p> <p>Given a m-by-n design matrix A and a target vector b with m elements, <code>lsq_linear</code> solves the following optimization problem::</p> <pre><code>minimize 0.5 * ||A x - b||**2\nsubject to lb &lt;= x &lt;= ub\n</code></pre> <p>This optimization problem is convex, hence a found minimum (if iterations have converged) is guaranteed to be global.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.BoundedVariableLeastSquares--attributes","title":"Attributes","text":"<p>unbiased : bool     Indicates whether an unbiased estimator is applied. uiter : int     Number of iterations for the unbiased estimator. method : 'trf' or 'bvls', optional     Method to perform minimization.</p> <pre><code>    * 'trf' : Trust Region Reflective algorithm adapted for a linear\n      least-squares problem. This is an interior-point-like method\n      and the required number of iterations is weakly correlated with\n      the number of variables.\n    * 'bvls' : Bounded-variable least-squares algorithm. This is\n      an active set method, which requires the number of iterations\n      comparable to the number of variables. Can't be used when `A` is\n      sparse or LinearOperator.\n\nDefault is 'trf'.\n</code></pre> <p>tol : float, optional     Tolerance parameter. The algorithm terminates if a relative change     of the cost function is less than <code>tol</code> on the last iteration.     Additionally, the first-order optimality measure is considered:</p> <pre><code>    * ``method='trf'`` terminates if the uniform norm of the gradient,\n      scaled to account for the presence of the bounds, is less than\n      `tol`.\n    * ``method='bvls'`` terminates if Karush-Kuhn-Tucker conditions\n      are satisfied within `tol` tolerance.\n</code></pre> {None, 'exact', 'lsmr'}, optional <p>Method of solving unbounded least-squares problems throughout iterations:</p> <pre><code>* 'exact' : Use dense QR or SVD decomposition approach. Can't be\n  used when `A` is sparse or LinearOperator.\n* 'lsmr' : Use `scipy.sparse.linalg.lsmr` iterative procedure\n  which requires only matrix-vector product evaluations. Can't\n  be used with ``method='bvls'``.\n</code></pre> <p>If None (default), the solver is chosen based on type of <code>A</code>.</p> <p>lsmr_tol : None, float or 'auto', optional     Tolerance parameters 'atol' and 'btol' for <code>scipy.sparse.linalg.lsmr</code>     If None (default), it is set to <code>1e-2 * tol</code>. If 'auto', the     tolerance will be adjusted based on the optimality of the current     iterate, which can speed up the optimization process, but is not always     reliable. max_iter : None or int, optional     Maximum number of iterations before termination. If None (default), it     is set to 100 for <code>method='trf'</code> or to the number of variables for     <code>method='bvls'</code> (not counting iterations for 'bvls' initialization). verbose : {0, 1, 2}, optional     Level of algorithm's verbosity:</p> <pre><code>    * 0 : work silently (default).\n    * 1 : display a termination report.\n    * 2 : display progress during iterations.\n</code></pre> <p>lsmr_maxiter : None or int, optional     Maximum number of iterations for the lsmr least squares solver,     if it is used (by setting <code>lsq_solver='lsmr'</code>). If None (default), it     uses lsmr's default of <code>min(m, n)</code> where <code>m</code> and <code>n</code> are the     number of rows and columns of <code>A</code>, respectively. Has no effect if     <code>lsq_solver='exact'</code>.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.BoundedVariableLeastSquares--references","title":"References","text":"<p>.. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,           and Conjugate Gradient Method for Large-Scale Bound-Constrained           Minimization Problems,\" SIAM Journal on Scientific Computing,           Vol. 21, Number 1, pp 1-23, 1999. .. [BVLS] P. B. Start and R. L. Parker, \"Bounded-Variable Least-Squares:           an Algorithm and Applications\", Computational Statistics, 10,           129-141, 1995.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.BoundedVariableLeastSquares--notes","title":"Notes","text":"<p>This docstring is adapted from the <code>scipy.optimize.lsq_linear</code> method.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.BoundedVariableLeastSquares--examples","title":"Examples","text":"<p>In this example, a problem with a large sparse matrix and bounds on the variables is solved.</p> <p>import numpy as np from scipy.sparse import rand from sysidentpy.parameter_estimation import BoundedVariableLeastSquares rng = np.random.default_rng() ... m = 20000 n = 10000 ... A = rand(m, n, density=1e-4, random_state=rng) b = rng.standard_normal(m) ... lb = rng.standard_normal(n) ub = lb + 1 ... res = BoundedVariableLeastSquares(A, b, bounds=(lb, ub), lsmr_tol='auto', verbose=1) The relative change of the cost function is less than <code>tol</code>. Number of iterations 16, initial cost 1.5039e+04, final cost 1.1112e+04, first-order optimality 4.66e-08.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class BoundedVariableLeastSquares(BaseEstimator):\n    \"\"\"Solve a linear least-squares problem with bounds on the variables.\n\n    This is a wrapper class for the `scipy.optimize.lsq_linear` method.\n\n    Given a m-by-n design matrix A and a target vector b with m elements,\n    `lsq_linear` solves the following optimization problem::\n\n        minimize 0.5 * ||A x - b||**2\n        subject to lb &lt;= x &lt;= ub\n\n    This optimization problem is convex, hence a found minimum (if iterations\n    have converged) is guaranteed to be global.\n\n    Attributes\n    ----------\n    unbiased : bool\n        Indicates whether an unbiased estimator is applied.\n    uiter : int\n        Number of iterations for the unbiased estimator.\n    method : 'trf' or 'bvls', optional\n        Method to perform minimization.\n\n            * 'trf' : Trust Region Reflective algorithm adapted for a linear\n              least-squares problem. This is an interior-point-like method\n              and the required number of iterations is weakly correlated with\n              the number of variables.\n            * 'bvls' : Bounded-variable least-squares algorithm. This is\n              an active set method, which requires the number of iterations\n              comparable to the number of variables. Can't be used when `A` is\n              sparse or LinearOperator.\n\n        Default is 'trf'.\n    tol : float, optional\n        Tolerance parameter. The algorithm terminates if a relative change\n        of the cost function is less than `tol` on the last iteration.\n        Additionally, the first-order optimality measure is considered:\n\n            * ``method='trf'`` terminates if the uniform norm of the gradient,\n              scaled to account for the presence of the bounds, is less than\n              `tol`.\n            * ``method='bvls'`` terminates if Karush-Kuhn-Tucker conditions\n              are satisfied within `tol` tolerance.\n\n    lsq_solver : {None, 'exact', 'lsmr'}, optional\n        Method of solving unbounded least-squares problems throughout\n        iterations:\n\n            * 'exact' : Use dense QR or SVD decomposition approach. Can't be\n              used when `A` is sparse or LinearOperator.\n            * 'lsmr' : Use `scipy.sparse.linalg.lsmr` iterative procedure\n              which requires only matrix-vector product evaluations. Can't\n              be used with ``method='bvls'``.\n\n        If None (default), the solver is chosen based on type of `A`.\n    lsmr_tol : None, float or 'auto', optional\n        Tolerance parameters 'atol' and 'btol' for `scipy.sparse.linalg.lsmr`\n        If None (default), it is set to ``1e-2 * tol``. If 'auto', the\n        tolerance will be adjusted based on the optimality of the current\n        iterate, which can speed up the optimization process, but is not always\n        reliable.\n    max_iter : None or int, optional\n        Maximum number of iterations before termination. If None (default), it\n        is set to 100 for ``method='trf'`` or to the number of variables for\n        ``method='bvls'`` (not counting iterations for 'bvls' initialization).\n    verbose : {0, 1, 2}, optional\n        Level of algorithm's verbosity:\n\n            * 0 : work silently (default).\n            * 1 : display a termination report.\n            * 2 : display progress during iterations.\n    lsmr_maxiter : None or int, optional\n        Maximum number of iterations for the lsmr least squares solver,\n        if it is used (by setting ``lsq_solver='lsmr'``). If None (default), it\n        uses lsmr's default of ``min(m, n)`` where ``m`` and ``n`` are the\n        number of rows and columns of `A`, respectively. Has no effect if\n        ``lsq_solver='exact'``.\n\n    References\n    ----------\n    .. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,\n              and Conjugate Gradient Method for Large-Scale Bound-Constrained\n              Minimization Problems,\" SIAM Journal on Scientific Computing,\n              Vol. 21, Number 1, pp 1-23, 1999.\n    .. [BVLS] P. B. Start and R. L. Parker, \"Bounded-Variable Least-Squares:\n              an Algorithm and Applications\", Computational Statistics, 10,\n              129-141, 1995.\n\n\n    Notes\n    -----\n    This docstring is adapted from the `scipy.optimize.lsq_linear` method.\n\n    Examples\n    --------\n    In this example, a problem with a large sparse matrix and bounds on the\n    variables is solved.\n\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from scipy.sparse import rand\n    &gt;&gt;&gt; from sysidentpy.parameter_estimation import BoundedVariableLeastSquares\n    &gt;&gt;&gt; rng = np.random.default_rng()\n    ...\n    &gt;&gt;&gt; m = 20000\n    &gt;&gt;&gt; n = 10000\n    ...\n    &gt;&gt;&gt; A = rand(m, n, density=1e-4, random_state=rng)\n    &gt;&gt;&gt; b = rng.standard_normal(m)\n    ...\n    &gt;&gt;&gt; lb = rng.standard_normal(n)\n    &gt;&gt;&gt; ub = lb + 1\n    ...\n    &gt;&gt;&gt; res = BoundedVariableLeastSquares(A, b, bounds=(lb, ub), lsmr_tol='auto',\n    verbose=1)\n    The relative change of the cost function is less than `tol`.\n    Number of iterations 16, initial cost 1.5039e+04, final cost 1.1112e+04,\n    first-order optimality 4.66e-08.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        unbiased: bool = False,\n        uiter: int = 30,\n        bounds=(-np.inf, np.inf),\n        method=\"trf\",\n        tol=1e-10,\n        lsq_solver=None,\n        lsmr_tol=None,\n        max_iter=None,\n        verbose=0,\n        lsmr_maxiter=None,\n    ):\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self.max_iter = max_iter\n        self.bounds = bounds\n        self.method = method\n        self.tol = tol\n        self.lsq_solver = lsq_solver\n        self.lsmr_tol = lsmr_tol\n        self.verbose = verbose\n        self.lsmr_maxiter = lsmr_maxiter\n\n    def optimize(self, psi, y):\n        \"\"\"Parameter estimation using the BoundedVariableLeastSquares algorithm.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        This is a wrapper class for the `scipy.optimize.lsq_linear` method.\n\n        References\n        ----------\n        .. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.lsq_linear.html\n        \"\"\"\n        theta = lsq_linear(\n            psi,\n            y.ravel(),\n            bounds=self.bounds,\n            method=self.method,\n            tol=self.tol,\n            lsq_solver=self.lsq_solver,\n            lsmr_tol=self.lsmr_tol,\n            max_iter=self.max_iter,\n            verbose=self.verbose,\n            lsmr_maxiter=self.lsmr_maxiter,\n        )\n        return theta.x.reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.BoundedVariableLeastSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the BoundedVariableLeastSquares algorithm.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.BoundedVariableLeastSquares.optimize--parameters","title":"Parameters","text":"<p>psi : ndarray of floats     The information matrix of the model. y : array-like     The data used to training the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.BoundedVariableLeastSquares.optimize--returns","title":"Returns","text":"<p>theta : array-like of shape = number_of_model_elements     The estimated parameters of the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.BoundedVariableLeastSquares.optimize--notes","title":"Notes","text":"<p>This is a wrapper class for the <code>scipy.optimize.lsq_linear</code> method.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.BoundedVariableLeastSquares.optimize--references","title":"References","text":"<p>.. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.lsq_linear.html</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi, y):\n    \"\"\"Parameter estimation using the BoundedVariableLeastSquares algorithm.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    This is a wrapper class for the `scipy.optimize.lsq_linear` method.\n\n    References\n    ----------\n    .. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.lsq_linear.html\n    \"\"\"\n    theta = lsq_linear(\n        psi,\n        y.ravel(),\n        bounds=self.bounds,\n        method=self.method,\n        tol=self.tol,\n        lsq_solver=self.lsq_solver,\n        lsmr_tol=self.lsmr_tol,\n        max_iter=self.max_iter,\n        verbose=self.verbose,\n        lsmr_maxiter=self.lsmr_maxiter,\n    )\n    return theta.x.reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.EstimatorError","title":"<code>EstimatorError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Generic Python-exception-derived object raised by estimator functions.</p> <p>General purpose exception class, derived from Python's ValueError class, programmatically raised in estimators functions when a Estimator-related condition would prevent further correct execution of the function.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.EstimatorError--parameters","title":"Parameters","text":"<p>None</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class EstimatorError(Exception):\n    \"\"\"Generic Python-exception-derived object raised by estimator functions.\n\n    General purpose exception class, derived from Python's ValueError\n    class, programmatically raised in estimators functions when a Estimator-related\n    condition would prevent further correct execution of the function.\n\n    Parameters\n    ----------\n    None\n\n    \"\"\"\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquareMixedNorm","title":"<code>LeastMeanSquareMixedNorm</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Least Mean Square Mixed Norm (LMS-MN) Adaptive Filter.</p> <p>This class implements the Mixed-norm Least Mean Square (LMS) adaptive filter algorithm, which incorporates an additional weight factor to control the proportions of the error norms, thus providing an extra degree of freedom in the adaptation process.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquareMixedNorm--parameters","title":"Parameters","text":"<p>mu : float, optional     The adaptation step size. Default is 0.01. weight : float, optional     The weight factor for mixed-norm control. Weight factor to control the     proportions of the error norms and offers an extra degree of freedom within     the adaptation of the LMS mixed norm method.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquareMixedNorm--attributes","title":"Attributes","text":"<p>mu : float     The adaptation step size. weight : float     The weight factor for mixed-norm control. Weight factor to control the     proportions of the error norms and offers an extra degree of freedom within     the adaptation of the LMS mixed norm method. xi : ndarray or None     The error signal, initialized to None.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquareMixedNorm(BaseEstimator):\n    \"\"\"Least Mean Square Mixed Norm (LMS-MN) Adaptive Filter.\n\n    This class implements the Mixed-norm Least Mean Square (LMS) adaptive filter\n    algorithm, which incorporates an additional weight factor to control the\n    proportions of the error norms, thus providing an extra degree of freedom\n    in the adaptation process.\n\n    Parameters\n    ----------\n    mu : float, optional\n        The adaptation step size. Default is 0.01.\n    weight : float, optional\n        The weight factor for mixed-norm control. Weight factor to control the\n        proportions of the error norms and offers an extra degree of freedom within\n        the adaptation of the LMS mixed norm method.\n\n    Attributes\n    ----------\n    mu : float\n        The adaptation step size.\n    weight : float\n        The weight factor for mixed-norm control. Weight factor to control the\n        proportions of the error norms and offers an extra degree of freedom within\n        the adaptation of the LMS mixed norm method.\n    xi : ndarray or None\n        The error signal, initialized to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        weight: float = 0.02,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.weight = weight\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self._validate_params(vars(self))\n        self.xi: np.ndarray\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Parameter estimation using the Mixed-norm LMS filter.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        A more in-depth documentation of all methods for parameters estimation\n        will be available soon. For now, please refer to the mentioned\n        references.\n\n        References\n        ----------\n        - Chambers, J. A., Tanrikulu, O., &amp; Constantinides, A. G. (1994).\n           Least mean mixed-norm adaptive filtering.\n           Electronics letters, 30(19), 1574-1575.\n           https://ieeexplore.ieee.org/document/326382\n        - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n           an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n           vari\u00e1vel.\n        - Wikipedia entry on Least Mean Squares\n           https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n        \"\"\"\n        n_theta, n, theta, self.xi = self._initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + self.mu * psi_tmp * self.xi[\n                i, 0\n            ] * (self.weight + (1 - self.weight) * self.xi[i, 0] ** 2)\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquareMixedNorm.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Mixed-norm LMS filter.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquareMixedNorm.optimize--parameters","title":"Parameters","text":"<p>psi : ndarray of floats     The information matrix of the model. y : array-like of shape = y_training     The data used to training the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquareMixedNorm.optimize--returns","title":"Returns","text":"<p>theta : array-like of shape = number_of_model_elements     The estimated parameters of the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquareMixedNorm.optimize--notes","title":"Notes","text":"<p>A more in-depth documentation of all methods for parameters estimation will be available soon. For now, please refer to the mentioned references.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquareMixedNorm.optimize--references","title":"References","text":"<ul> <li>Chambers, J. A., Tanrikulu, O., &amp; Constantinides, A. G. (1994).    Least mean mixed-norm adaptive filtering.    Electronics letters, 30(19), 1574-1575.    https://ieeexplore.ieee.org/document/326382</li> <li>Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,    an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo    vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares    https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Parameter estimation using the Mixed-norm LMS filter.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    A more in-depth documentation of all methods for parameters estimation\n    will be available soon. For now, please refer to the mentioned\n    references.\n\n    References\n    ----------\n    - Chambers, J. A., Tanrikulu, O., &amp; Constantinides, A. G. (1994).\n       Least mean mixed-norm adaptive filtering.\n       Electronics letters, 30(19), 1574-1575.\n       https://ieeexplore.ieee.org/document/326382\n    - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n       an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n       vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares\n       https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n    \"\"\"\n    n_theta, n, theta, self.xi = self._initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + self.mu * psi_tmp * self.xi[\n            i, 0\n        ] * (self.weight + (1 - self.weight) * self.xi[i, 0] ** 2)\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquares","title":"<code>LeastMeanSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Least Mean Squares (LMS) filter for parameter estimation in adaptive filtering.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquares--parameters","title":"Parameters","text":"<p>mu : float, default=0.01     The learning rate or step size for the LMS algorithm.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquares--attributes","title":"Attributes","text":"<p>mu : float     The learning rate or step size for the LMS algorithm. xi : np.ndarray or None     The estimation error at each iteration. Initialized as None and updated during     optimization.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquares--methods","title":"Methods","text":"<p>optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray     Estimate the model parameters using the LMS filter.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquares(BaseEstimator):\n    \"\"\"Least Mean Squares (LMS) filter for parameter estimation in adaptive filtering.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMS filter.\n    \"\"\"\n\n    def __init__(self, *, mu: float = 0.01, unbiased: bool = False, uiter: int = 30):\n        self.mu = mu\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self._validate_params(vars(self))\n        self.xi: np.ndarray\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Estimate the model parameters using the Least Mean Squares filter.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        A more in-depth documentation of all methods for parameters estimation\n        will be available soon. For now, please refer to the mentioned\n        references.\n\n        References\n        ----------\n        - Book: Haykin, S., &amp; Widrow, B. (Eds.). (2003). Least-mean-square\n           adaptive filters (Vol. 31). John Wiley &amp; Sons.\n        - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n           an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n           vari\u00e1vel.\n        - Wikipedia entry on Least Mean Squares\n           https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n        \"\"\"\n        n_theta, n, theta, self.xi = self._initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = (\n                theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * self.xi[i, 0] * psi_tmp\n            )\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Estimate the model parameters using the Least Mean Squares filter.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquares.optimize--parameters","title":"Parameters","text":"<p>psi : ndarray of floats     The information matrix of the model. y : array-like of shape = y_training     The data used to training the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquares.optimize--returns","title":"Returns","text":"<p>theta : array-like of shape = number_of_model_elements     The estimated parameters of the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquares.optimize--notes","title":"Notes","text":"<p>A more in-depth documentation of all methods for parameters estimation will be available soon. For now, please refer to the mentioned references.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquares.optimize--references","title":"References","text":"<ul> <li>Book: Haykin, S., &amp; Widrow, B. (Eds.). (2003). Least-mean-square    adaptive filters (Vol. 31). John Wiley &amp; Sons.</li> <li>Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,    an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo    vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares    https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Estimate the model parameters using the Least Mean Squares filter.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    A more in-depth documentation of all methods for parameters estimation\n    will be available soon. For now, please refer to the mentioned\n    references.\n\n    References\n    ----------\n    - Book: Haykin, S., &amp; Widrow, B. (Eds.). (2003). Least-mean-square\n       adaptive filters (Vol. 31). John Wiley &amp; Sons.\n    - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n       an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n       vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares\n       https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n    \"\"\"\n    n_theta, n, theta, self.xi = self._initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = (\n            theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * self.xi[i, 0] * psi_tmp\n        )\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresFourth","title":"<code>LeastMeanSquaresFourth</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Least Mean Squares Fourth(LMSF) filter for parameter estimation.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresFourth--parameters","title":"Parameters","text":"<p>mu : float, default=0.01     The learning rate or step size for the LMS algorithm.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresFourth--attributes","title":"Attributes","text":"<p>mu : float     The learning rate or step size for the LMS algorithm. xi : np.ndarray or None     The estimation error at each iteration. Initialized as None and updated during     optimization.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresFourth--methods","title":"Methods","text":"<p>optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray     Estimate the model parameters using the LMS filter.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresFourth(BaseEstimator):\n    \"\"\"Least Mean Squares Fourth(LMSF) filter for parameter estimation.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMS filter.\n    \"\"\"\n\n    def __init__(self, *, mu: float = 0.5, unbiased: bool = False, uiter: int = 30):\n        self.mu = mu\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self._validate_params(vars(self))\n        self.xi: np.ndarray\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Parameter estimation using the  LMS Fourth filter.\n\n        When the leakage factor, gama, is set to 0 then there is no\n        leakage in the estimation process.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : ndarray of floats of shape = y_training\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : ndarray of floats of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        A more in-depth documentation of all methods for parameters estimation\n        will be available soon. For now, please refer to the mentioned\n        references.\n\n        References\n        ----------\n        - Book: Hayes, M. H. (2009). Statistical digital signal processing\n           and modeling. John Wiley &amp; Sons.\n        - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n           an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n           vari\u00e1vel.\n        - Manuscript:Gui, G., Mehbodniya, A., &amp; Adachi, F. (2013).\n           Least mean square/fourth algorithm with application to sparse\n           channel estimation. arXiv preprint arXiv:1304.3911.\n           https://arxiv.org/pdf/1304.3911.pdf\n        - Manuscript: Nascimento, V. H., &amp; Bermudez, J. C. M. (2005, March).\n           When is the least-mean fourth algorithm mean-square stable?\n           In Proceedings.(ICASSP'05). IEEE International Conference on\n           Acoustics, Speech, and Signal Processing, 2005.\n           (Vol. 4, pp. iv-341). IEEE.\n           http://www.lps.usp.br/vitor/artigos/icassp05.pdf\n        - Wikipedia entry on Least Mean Squares\n           https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n        \"\"\"\n        n_theta, n, theta, self.xi = self._initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = (\n                theta[:, i - 1].reshape(-1, 1) + self.mu * psi_tmp * self.xi[i, 0] ** 3\n            )\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresFourth.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the  LMS Fourth filter.</p> <p>When the leakage factor, gama, is set to 0 then there is no leakage in the estimation process.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresFourth.optimize--parameters","title":"Parameters","text":"<p>psi : ndarray of floats     The information matrix of the model. y : ndarray of floats of shape = y_training     The data used to training the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresFourth.optimize--returns","title":"Returns","text":"<p>theta : ndarray of floats of shape = number_of_model_elements     The estimated parameters of the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresFourth.optimize--notes","title":"Notes","text":"<p>A more in-depth documentation of all methods for parameters estimation will be available soon. For now, please refer to the mentioned references.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresFourth.optimize--references","title":"References","text":"<ul> <li>Book: Hayes, M. H. (2009). Statistical digital signal processing    and modeling. John Wiley &amp; Sons.</li> <li>Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,    an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo    vari\u00e1vel.</li> <li>Manuscript:Gui, G., Mehbodniya, A., &amp; Adachi, F. (2013).    Least mean square/fourth algorithm with application to sparse    channel estimation. arXiv preprint arXiv:1304.3911.    https://arxiv.org/pdf/1304.3911.pdf</li> <li>Manuscript: Nascimento, V. H., &amp; Bermudez, J. C. M. (2005, March).    When is the least-mean fourth algorithm mean-square stable?    In Proceedings.(ICASSP'05). IEEE International Conference on    Acoustics, Speech, and Signal Processing, 2005.    (Vol. 4, pp. iv-341). IEEE.    http://www.lps.usp.br/vitor/artigos/icassp05.pdf</li> <li>Wikipedia entry on Least Mean Squares    https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Parameter estimation using the  LMS Fourth filter.\n\n    When the leakage factor, gama, is set to 0 then there is no\n    leakage in the estimation process.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : ndarray of floats of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : ndarray of floats of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    A more in-depth documentation of all methods for parameters estimation\n    will be available soon. For now, please refer to the mentioned\n    references.\n\n    References\n    ----------\n    - Book: Hayes, M. H. (2009). Statistical digital signal processing\n       and modeling. John Wiley &amp; Sons.\n    - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n       an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n       vari\u00e1vel.\n    - Manuscript:Gui, G., Mehbodniya, A., &amp; Adachi, F. (2013).\n       Least mean square/fourth algorithm with application to sparse\n       channel estimation. arXiv preprint arXiv:1304.3911.\n       https://arxiv.org/pdf/1304.3911.pdf\n    - Manuscript: Nascimento, V. H., &amp; Bermudez, J. C. M. (2005, March).\n       When is the least-mean fourth algorithm mean-square stable?\n       In Proceedings.(ICASSP'05). IEEE International Conference on\n       Acoustics, Speech, and Signal Processing, 2005.\n       (Vol. 4, pp. iv-341). IEEE.\n       http://www.lps.usp.br/vitor/artigos/icassp05.pdf\n    - Wikipedia entry on Least Mean Squares\n       https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n    \"\"\"\n    n_theta, n, theta, self.xi = self._initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = (\n            theta[:, i - 1].reshape(-1, 1) + self.mu * psi_tmp * self.xi[i, 0] ** 3\n        )\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresLeaky","title":"<code>LeastMeanSquaresLeaky</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Least Mean Squares Leaky(LMSL) filter for parameter estimation.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresLeaky--parameters","title":"Parameters","text":"<p>mu : float, default=0.01     The learning rate or step size for the LMS algorithm. gama : float, default=0.2     The leakage factor of the Leaky LMS method.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresLeaky--attributes","title":"Attributes","text":"<p>mu : float     The learning rate or step size for the LMS algorithm. gama : float, default=0.2     The leakage factor of the Leaky LMS method. xi : np.ndarray or None     The estimation error at each iteration. Initialized as None and updated during     optimization.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresLeaky--methods","title":"Methods","text":"<p>optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray     Estimate the model parameters using the LMS filter.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresLeaky(BaseEstimator):\n    \"\"\"Least Mean Squares Leaky(LMSL) filter for parameter estimation.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    gama : float, default=0.2\n        The leakage factor of the Leaky LMS method.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    gama : float, default=0.2\n        The leakage factor of the Leaky LMS method.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMS filter.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        gama: float = 0.001,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.gama = gama\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self._validate_params(vars(self))\n        self.xi: np.ndarray\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Parameter estimation using the  Leaky LMS filter.\n\n        When the leakage factor, gama, is set to 0 then there is no\n        leakage in the estimation process.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        A more in-depth documentation of all methods for parameters estimation\n        will be available soon. For now, please refer to the mentioned\n        references.\n\n        References\n        ----------\n        - Book: Hayes, M. H. (2009). Statistical digital signal processing\n           and modeling. John Wiley &amp; Sons.\n        - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n           an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n           vari\u00e1vel.\n        - Wikipedia entry on Least Mean Squares\n           https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n        \"\"\"\n        n_theta, n, theta, self.xi = self._initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = (\n                theta[:, i - 1].reshape(-1, 1) * (1 - self.mu * self.gama)\n                + self.mu * self.xi[i, 0] * psi_tmp\n            )\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresLeaky.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the  Leaky LMS filter.</p> <p>When the leakage factor, gama, is set to 0 then there is no leakage in the estimation process.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresLeaky.optimize--parameters","title":"Parameters","text":"<p>psi : ndarray of floats     The information matrix of the model. y : array-like of shape = y_training     The data used to training the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresLeaky.optimize--returns","title":"Returns","text":"<p>theta : array-like of shape = number_of_model_elements     The estimated parameters of the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresLeaky.optimize--notes","title":"Notes","text":"<p>A more in-depth documentation of all methods for parameters estimation will be available soon. For now, please refer to the mentioned references.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresLeaky.optimize--references","title":"References","text":"<ul> <li>Book: Hayes, M. H. (2009). Statistical digital signal processing    and modeling. John Wiley &amp; Sons.</li> <li>Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,    an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo    vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares    https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Parameter estimation using the  Leaky LMS filter.\n\n    When the leakage factor, gama, is set to 0 then there is no\n    leakage in the estimation process.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    A more in-depth documentation of all methods for parameters estimation\n    will be available soon. For now, please refer to the mentioned\n    references.\n\n    References\n    ----------\n    - Book: Hayes, M. H. (2009). Statistical digital signal processing\n       and modeling. John Wiley &amp; Sons.\n    - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n       an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n       vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares\n       https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n    \"\"\"\n    n_theta, n, theta, self.xi = self._initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = (\n            theta[:, i - 1].reshape(-1, 1) * (1 - self.mu * self.gama)\n            + self.mu * self.xi[i, 0] * psi_tmp\n        )\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedLeaky","title":"<code>LeastMeanSquaresNormalizedLeaky</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Normalized Least Mean Squares Leaky(NLMSL) filter for parameter estimation.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedLeaky--parameters","title":"Parameters","text":"<p>mu : float, default=0.01     The learning rate or step size for the LMS algorithm. eps : float, default=np.finfo(np.float64).eps     Normalization factor of the normalized filters. gama : float, default=0.2     The leakage factor of the Leaky LMS method.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedLeaky--attributes","title":"Attributes","text":"<p>mu : float     The learning rate or step size for the LMS algorithm. eps : float, default=np.finfo(np.float64).eps     Normalization factor of the normalized filters. gama : float, default=0.2     The leakage factor of the Leaky LMS method. xi : np.ndarray or None     The estimation error at each iteration. Initialized as None and updated during     optimization.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedLeaky--methods","title":"Methods","text":"<p>optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray     Estimate the model parameters using the LMS filter.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresNormalizedLeaky(BaseEstimator):\n    \"\"\"Normalized Least Mean Squares Leaky(NLMSL) filter for parameter estimation.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n    gama : float, default=0.2\n        The leakage factor of the Leaky LMS method.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n    gama : float, default=0.2\n        The leakage factor of the Leaky LMS method.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMS filter.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        gama: float = 0.2,\n        eps: np.float64 = np.finfo(np.float64).eps,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.eps = eps\n        self.gama = gama\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self._validate_params(vars(self))\n        self.xi: np.ndarray\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Parameter estimation using the  Normalized Leaky LMS filter.\n\n        When the leakage factor, gama, is set to 0 then there is no\n        leakage in the estimation process.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        A more in-depth documentation of all methods for parameters estimation\n        will be available soon. For now, please refer to the mentioned\n        references.\n\n        References\n        ----------\n        - Book: Hayes, M. H. (2009). Statistical digital signal processing\n           and modeling. John Wiley &amp; Sons.\n        - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n           an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n           vari\u00e1vel.\n        - Wikipedia entry on Least Mean Squares\n           https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n        \"\"\"\n        n_theta, n, theta, self.xi = self._initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) * (\n                1 - self.mu * self.gama\n            ) + self.mu * self.xi[i, 0] * psi_tmp / (\n                self.eps + np.dot(psi_tmp.T, psi_tmp)\n            )\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedLeaky.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the  Normalized Leaky LMS filter.</p> <p>When the leakage factor, gama, is set to 0 then there is no leakage in the estimation process.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedLeaky.optimize--parameters","title":"Parameters","text":"<p>psi : ndarray of floats     The information matrix of the model. y : array-like of shape = y_training     The data used to training the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedLeaky.optimize--returns","title":"Returns","text":"<p>theta : array-like of shape = number_of_model_elements     The estimated parameters of the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedLeaky.optimize--notes","title":"Notes","text":"<p>A more in-depth documentation of all methods for parameters estimation will be available soon. For now, please refer to the mentioned references.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedLeaky.optimize--references","title":"References","text":"<ul> <li>Book: Hayes, M. H. (2009). Statistical digital signal processing    and modeling. John Wiley &amp; Sons.</li> <li>Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,    an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo    vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares    https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Parameter estimation using the  Normalized Leaky LMS filter.\n\n    When the leakage factor, gama, is set to 0 then there is no\n    leakage in the estimation process.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    A more in-depth documentation of all methods for parameters estimation\n    will be available soon. For now, please refer to the mentioned\n    references.\n\n    References\n    ----------\n    - Book: Hayes, M. H. (2009). Statistical digital signal processing\n       and modeling. John Wiley &amp; Sons.\n    - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n       an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n       vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares\n       https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n    \"\"\"\n    n_theta, n, theta, self.xi = self._initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) * (\n            1 - self.mu * self.gama\n        ) + self.mu * self.xi[i, 0] * psi_tmp / (\n            self.eps + np.dot(psi_tmp.T, psi_tmp)\n        )\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignRegressor","title":"<code>LeastMeanSquaresNormalizedSignRegressor</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Normalized Least Mean Squares SignRegressor filter for parameter estimation.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignRegressor--parameters","title":"Parameters","text":"<p>mu : float, default=0.01     The learning rate or step size for the LMS algorithm. eps : float, default=np.finfo(np.float64).eps     Normalization factor of the normalized filters.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignRegressor--attributes","title":"Attributes","text":"<p>mu : float     The learning rate or step size for the LMS algorithm. eps : float, default=np.finfo(np.float64).eps     Normalization factor of the normalized filters. xi : np.ndarray or None     The estimation error at each iteration. Initialized as None and updated during     optimization.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignRegressor--methods","title":"Methods","text":"<p>optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray     Estimate the model parameters using the LMS filter.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresNormalizedSignRegressor(BaseEstimator):\n    \"\"\"Normalized Least Mean Squares SignRegressor filter for parameter estimation.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMS filter.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        eps: np.float64 = np.finfo(np.float64).eps,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.eps = eps\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self._validate_params(vars(self))\n        self.xi: np.ndarray\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Parameter estimation using the Normalized Sign-Regressor LMS filter.\n\n        The normalization is used to avoid numerical instability when updating\n        the estimated parameters and the sign of the information matrix is\n        used to change the filter coefficients.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        A more in-depth documentation of all methods for parameters estimation\n        will be available soon. For now, please refer to the mentioned\n        references.\n\n        References\n        ----------\n        .. [1] Book: Hayes, M. H. (2009). Statistical digital signal processing\n           and modeling. John Wiley &amp; Sons.\n        .. [2] Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n           an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n           vari\u00e1vel.\n        .. [3] Wikipedia entry on Least Mean Squares\n           https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n        \"\"\"\n        n_theta, n, theta, self.xi = self._initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + self.mu * self.xi[i, 0] * (\n                np.sign(psi_tmp) / (self.eps + np.dot(psi_tmp.T, psi_tmp))\n            )\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignRegressor.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Normalized Sign-Regressor LMS filter.</p> <p>The normalization is used to avoid numerical instability when updating the estimated parameters and the sign of the information matrix is used to change the filter coefficients.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignRegressor.optimize--parameters","title":"Parameters","text":"<p>psi : ndarray of floats     The information matrix of the model. y : array-like of shape = y_training     The data used to training the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignRegressor.optimize--returns","title":"Returns","text":"<p>theta : array-like of shape = number_of_model_elements     The estimated parameters of the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignRegressor.optimize--notes","title":"Notes","text":"<p>A more in-depth documentation of all methods for parameters estimation will be available soon. For now, please refer to the mentioned references.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignRegressor.optimize--references","title":"References","text":"<p>.. [1] Book: Hayes, M. H. (2009). Statistical digital signal processing    and modeling. John Wiley &amp; Sons. .. [2] Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,    an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo    vari\u00e1vel. .. [3] Wikipedia entry on Least Mean Squares    https://en.wikipedia.org/wiki/Least_mean_squares_filter</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Parameter estimation using the Normalized Sign-Regressor LMS filter.\n\n    The normalization is used to avoid numerical instability when updating\n    the estimated parameters and the sign of the information matrix is\n    used to change the filter coefficients.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    A more in-depth documentation of all methods for parameters estimation\n    will be available soon. For now, please refer to the mentioned\n    references.\n\n    References\n    ----------\n    .. [1] Book: Hayes, M. H. (2009). Statistical digital signal processing\n       and modeling. John Wiley &amp; Sons.\n    .. [2] Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n       an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n       vari\u00e1vel.\n    .. [3] Wikipedia entry on Least Mean Squares\n       https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n    \"\"\"\n    n_theta, n, theta, self.xi = self._initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + self.mu * self.xi[i, 0] * (\n            np.sign(psi_tmp) / (self.eps + np.dot(psi_tmp.T, psi_tmp))\n        )\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignSign","title":"<code>LeastMeanSquaresNormalizedSignSign</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Normalized Least Mean Squares SignSign(NLMSSS) filter for parameter estimation.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignSign--parameters","title":"Parameters","text":"<p>mu : float, default=0.01     The learning rate or step size for the LMS algorithm. eps : float, default=np.finfo(np.float64).eps     Normalization factor of the normalized filters.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignSign--attributes","title":"Attributes","text":"<p>mu : float     The learning rate or step size for the LMS algorithm. eps : float, default=np.finfo(np.float64).eps     Normalization factor of the normalized filters. xi : np.ndarray or None     The estimation error at each iteration. Initialized as None and updated during     optimization.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignSign--methods","title":"Methods","text":"<p>optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray     Estimate the model parameters using the LMS filter.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresNormalizedSignSign(BaseEstimator):\n    \"\"\"Normalized Least Mean Squares SignSign(NLMSSS) filter for parameter estimation.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMS filter.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        eps: np.float64 = np.finfo(np.float64).eps,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.eps = eps\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self._validate_params(vars(self))\n        self.xi: np.ndarray\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Parameter estimation using the Normalized Sign-Sign LMS filter.\n\n        The normalization is used to avoid numerical instability when updating\n        the estimated parameters and both the sign of the information matrix\n        and the sign of the error vector are used to change the filter\n        coefficients.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        A more in-depth documentation of all methods for parameters estimation\n        will be available soon. For now, please refer to the mentioned\n        references.\n\n        References\n        ----------\n        - Book: Hayes, M. H. (2009). Statistical digital signal processing\n           and modeling. John Wiley &amp; Sons.\n        - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n           an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n           vari\u00e1vel.\n        - Wikipedia entry on Least Mean Squares\n           https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n        \"\"\"\n        n_theta, n, theta, self.xi = self._initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * np.sign(\n                self.xi[i, 0]\n            ) * (np.sign(psi_tmp) / (self.eps + np.dot(psi_tmp.T, psi_tmp)))\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignSign.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Normalized Sign-Sign LMS filter.</p> <p>The normalization is used to avoid numerical instability when updating the estimated parameters and both the sign of the information matrix and the sign of the error vector are used to change the filter coefficients.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignSign.optimize--parameters","title":"Parameters","text":"<p>psi : ndarray of floats     The information matrix of the model. y : array-like of shape = y_training     The data used to training the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignSign.optimize--returns","title":"Returns","text":"<p>theta : array-like of shape = number_of_model_elements     The estimated parameters of the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignSign.optimize--notes","title":"Notes","text":"<p>A more in-depth documentation of all methods for parameters estimation will be available soon. For now, please refer to the mentioned references.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignSign.optimize--references","title":"References","text":"<ul> <li>Book: Hayes, M. H. (2009). Statistical digital signal processing    and modeling. John Wiley &amp; Sons.</li> <li>Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,    an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo    vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares    https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Parameter estimation using the Normalized Sign-Sign LMS filter.\n\n    The normalization is used to avoid numerical instability when updating\n    the estimated parameters and both the sign of the information matrix\n    and the sign of the error vector are used to change the filter\n    coefficients.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    A more in-depth documentation of all methods for parameters estimation\n    will be available soon. For now, please refer to the mentioned\n    references.\n\n    References\n    ----------\n    - Book: Hayes, M. H. (2009). Statistical digital signal processing\n       and modeling. John Wiley &amp; Sons.\n    - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n       an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n       vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares\n       https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n    \"\"\"\n    n_theta, n, theta, self.xi = self._initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * np.sign(\n            self.xi[i, 0]\n        ) * (np.sign(psi_tmp) / (self.eps + np.dot(psi_tmp.T, psi_tmp)))\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignError","title":"<code>LeastMeanSquaresSignError</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Least Mean Squares (LMS) filter for parameter estimation.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignError--parameters","title":"Parameters","text":"<p>mu : float, default=0.01     The learning rate or step size for the LMS algorithm.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignError--attributes","title":"Attributes","text":"<p>mu : float     The learning rate or step size for the LMS algorithm. xi : np.ndarray or None     The estimation error at each iteration. Initialized as None and updated during     optimization.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignError--methods","title":"Methods","text":"<p>optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray     Estimate the model parameters using the LMS filter.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresSignError(BaseEstimator):\n    \"\"\"Least Mean Squares (LMS) filter for parameter estimation.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMS filter.\n    \"\"\"\n\n    def __init__(self, *, mu: float = 0.01, unbiased: bool = False, uiter: int = 30):\n        self.mu = mu\n        self.uiter = uiter\n        self.unbiased = unbiased\n        self._validate_params(vars(self))\n        self.xi: np.ndarray\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Parameter estimation using the Sign-Error  Least Mean Squares filter.\n\n        The sign-error LMS algorithm uses the sign of the error vector\n        to change the filter coefficients.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        A more in-depth documentation of all methods for parameters estimation\n        will be available soon. For now, please refer to the mentioned\n        references.\n\n        References\n        ----------\n        - Book: Hayes, M. H. (2009). Statistical digital signal processing\n           and modeling. John Wiley &amp; Sons.\n        - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n           an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n           vari\u00e1vel.\n        - Wikipedia entry on Least Mean Squares\n           https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n        \"\"\"\n        n_theta, n, theta, self.xi = self._initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = (\n                theta[:, i - 1].reshape(-1, 1)\n                + self.mu * np.sign(self.xi[i, 0]) * psi_tmp\n            )\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignError.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Sign-Error  Least Mean Squares filter.</p> <p>The sign-error LMS algorithm uses the sign of the error vector to change the filter coefficients.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignError.optimize--parameters","title":"Parameters","text":"<p>psi : ndarray of floats     The information matrix of the model. y : array-like of shape = y_training     The data used to training the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignError.optimize--returns","title":"Returns","text":"<p>theta : array-like of shape = number_of_model_elements     The estimated parameters of the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignError.optimize--notes","title":"Notes","text":"<p>A more in-depth documentation of all methods for parameters estimation will be available soon. For now, please refer to the mentioned references.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignError.optimize--references","title":"References","text":"<ul> <li>Book: Hayes, M. H. (2009). Statistical digital signal processing    and modeling. John Wiley &amp; Sons.</li> <li>Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,    an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo    vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares    https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Parameter estimation using the Sign-Error  Least Mean Squares filter.\n\n    The sign-error LMS algorithm uses the sign of the error vector\n    to change the filter coefficients.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    A more in-depth documentation of all methods for parameters estimation\n    will be available soon. For now, please refer to the mentioned\n    references.\n\n    References\n    ----------\n    - Book: Hayes, M. H. (2009). Statistical digital signal processing\n       and modeling. John Wiley &amp; Sons.\n    - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n       an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n       vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares\n       https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n    \"\"\"\n    n_theta, n, theta, self.xi = self._initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = (\n            theta[:, i - 1].reshape(-1, 1)\n            + self.mu * np.sign(self.xi[i, 0]) * psi_tmp\n        )\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignRegressor","title":"<code>LeastMeanSquaresSignRegressor</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Least Mean Squares (LMSSR) filter for parameter estimation.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignRegressor--parameters","title":"Parameters","text":"<p>mu : float, default=0.01     The learning rate or step size for the LMS algorithm.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignRegressor--attributes","title":"Attributes","text":"<p>mu : float     The learning rate or step size for the LMS algorithm. xi : np.ndarray or None     The estimation error at each iteration. Initialized as None and updated during     optimization.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignRegressor--methods","title":"Methods","text":"<p>optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray     Estimate the model parameters using the LMS filter.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresSignRegressor(BaseEstimator):\n    \"\"\"Least Mean Squares (LMSSR) filter for parameter estimation.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMS filter.\n    \"\"\"\n\n    def __init__(self, *, mu: float = 0.01, unbiased: bool = False, uiter: int = 30):\n        self.mu = mu\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self._validate_params(vars(self))\n        self.xi: np.ndarray\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Parameter estimation using the  Sign-Regressor LMS filter.\n\n        The sign-regressor LMS algorithm uses the sign of the matrix\n        information to change the filter coefficients.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        A more in-depth documentation of all methods for parameters estimation\n        will be available soon. For now, please refer to the mentioned\n        references.\n\n        References\n        ----------\n        - Book: Hayes, M. H. (2009). Statistical digital signal processing\n           and modeling. John Wiley &amp; Sons.\n        - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n           an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n           vari\u00e1vel.\n        - Wikipedia entry on Least Mean Squares\n           https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n        \"\"\"\n        n_theta, n, theta, self.xi = self._initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + self.mu * self.xi[\n                i, 0\n            ] * np.sign(psi_tmp)\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignRegressor.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the  Sign-Regressor LMS filter.</p> <p>The sign-regressor LMS algorithm uses the sign of the matrix information to change the filter coefficients.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignRegressor.optimize--parameters","title":"Parameters","text":"<p>psi : ndarray of floats     The information matrix of the model. y : array-like of shape = y_training     The data used to training the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignRegressor.optimize--returns","title":"Returns","text":"<p>theta : array-like of shape = number_of_model_elements     The estimated parameters of the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignRegressor.optimize--notes","title":"Notes","text":"<p>A more in-depth documentation of all methods for parameters estimation will be available soon. For now, please refer to the mentioned references.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignRegressor.optimize--references","title":"References","text":"<ul> <li>Book: Hayes, M. H. (2009). Statistical digital signal processing    and modeling. John Wiley &amp; Sons.</li> <li>Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,    an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo    vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares    https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Parameter estimation using the  Sign-Regressor LMS filter.\n\n    The sign-regressor LMS algorithm uses the sign of the matrix\n    information to change the filter coefficients.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    A more in-depth documentation of all methods for parameters estimation\n    will be available soon. For now, please refer to the mentioned\n    references.\n\n    References\n    ----------\n    - Book: Hayes, M. H. (2009). Statistical digital signal processing\n       and modeling. John Wiley &amp; Sons.\n    - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n       an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n       vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares\n       https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n    \"\"\"\n    n_theta, n, theta, self.xi = self._initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + self.mu * self.xi[\n            i, 0\n        ] * np.sign(psi_tmp)\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignSign","title":"<code>LeastMeanSquaresSignSign</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Least Mean Squares SignSign(LMSSS) filter for parameter estimation.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignSign--parameters","title":"Parameters","text":"<p>mu : float, default=0.01     The learning rate or step size for the LMS algorithm.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignSign--attributes","title":"Attributes","text":"<p>mu : float     The learning rate or step size for the LMS algorithm. xi : np.ndarray or None     The estimation error at each iteration. Initialized as None and updated during     optimization.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignSign--methods","title":"Methods","text":"<p>optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray     Estimate the model parameters using the LMS filter.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresSignSign(BaseEstimator):\n    \"\"\"Least Mean Squares SignSign(LMSSS) filter for parameter estimation.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMS filter.\n    \"\"\"\n\n    def __init__(self, *, mu: float = 0.01, unbiased: bool = False, uiter: int = 30):\n        self.mu = mu\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self._validate_params(vars(self))\n        self.xi: np.ndarray\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Parameter estimation using the  Sign-Sign LMS filter.\n\n        The sign-regressor LMS algorithm uses both the sign of the matrix\n        information and the sign of the error vector to change the filter\n        coefficients.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        A more in-depth documentation of all methods for parameters estimation\n        will be available soon. For now, please refer to the mentioned\n        references.\n\n        References\n        ----------\n        - Book: Hayes, M. H. (2009). Statistical digital signal processing\n           and modeling. John Wiley &amp; Sons.\n        - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n           an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n           vari\u00e1vel.\n        - Wikipedia entry on Least Mean Squares\n           https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n        \"\"\"\n        n_theta, n, theta, self.xi = self._initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * np.sign(\n                self.xi[i, 0]\n            ) * np.sign(psi_tmp)\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignSign.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the  Sign-Sign LMS filter.</p> <p>The sign-regressor LMS algorithm uses both the sign of the matrix information and the sign of the error vector to change the filter coefficients.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignSign.optimize--parameters","title":"Parameters","text":"<p>psi : ndarray of floats     The information matrix of the model. y : array-like of shape = y_training     The data used to training the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignSign.optimize--returns","title":"Returns","text":"<p>theta : array-like of shape = number_of_model_elements     The estimated parameters of the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignSign.optimize--notes","title":"Notes","text":"<p>A more in-depth documentation of all methods for parameters estimation will be available soon. For now, please refer to the mentioned references.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignSign.optimize--references","title":"References","text":"<ul> <li>Book: Hayes, M. H. (2009). Statistical digital signal processing    and modeling. John Wiley &amp; Sons.</li> <li>Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,    an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo    vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares    https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Parameter estimation using the  Sign-Sign LMS filter.\n\n    The sign-regressor LMS algorithm uses both the sign of the matrix\n    information and the sign of the error vector to change the filter\n    coefficients.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    A more in-depth documentation of all methods for parameters estimation\n    will be available soon. For now, please refer to the mentioned\n    references.\n\n    References\n    ----------\n    - Book: Hayes, M. H. (2009). Statistical digital signal processing\n       and modeling. John Wiley &amp; Sons.\n    - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n       an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n       vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares\n       https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n    \"\"\"\n    n_theta, n, theta, self.xi = self._initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * np.sign(\n            self.xi[i, 0]\n        ) * np.sign(psi_tmp)\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquares","title":"<code>LeastSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Ordinary Least Squares for linear parameter estimation.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquares--references","title":"References","text":"<ul> <li>Manuscript: Sorenson, H. W. (1970). Least-squares estimation:     from Gauss to Kalman. IEEE spectrum, 7(7), 63-68.     http://pzs.dstu.dp.ua/DataMining/mls/bibl/Gauss2Kalman.pdf</li> <li>Book (Portuguese): Aguirre, L. A. (2007). Introdu\u00e7\u00e3o identifica\u00e7\u00e3o     de sistemas: t\u00e9cnicas lineares e n\u00e3o-lineares aplicadas a sistemas     reais. Editora da UFMG. 3a edi\u00e7\u00e3o.</li> <li>Manuscript: Markovsky, I., &amp; Van Huffel, S. (2007).     Overview of total least-squares methods.     Signal processing, 87(10), 2283-2302.     https://eprints.soton.ac.uk/263855/1/tls_overview.pdf</li> <li>Wikipedia entry on Least Squares     https://en.wikipedia.org/wiki/Least_squares</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastSquares(BaseEstimator):\n    \"\"\"Ordinary Least Squares for linear parameter estimation.\n\n    References\n    ----------\n    - Manuscript: Sorenson, H. W. (1970). Least-squares estimation:\n        from Gauss to Kalman. IEEE spectrum, 7(7), 63-68.\n        http://pzs.dstu.dp.ua/DataMining/mls/bibl/Gauss2Kalman.pdf\n    - Book (Portuguese): Aguirre, L. A. (2007). Introdu\u00e7\u00e3o identifica\u00e7\u00e3o\n        de sistemas: t\u00e9cnicas lineares e n\u00e3o-lineares aplicadas a sistemas\n        reais. Editora da UFMG. 3a edi\u00e7\u00e3o.\n    - Manuscript: Markovsky, I., &amp; Van Huffel, S. (2007).\n        Overview of total least-squares methods.\n        Signal processing, 87(10), 2283-2302.\n        https://eprints.soton.ac.uk/263855/1/tls_overview.pdf\n    - Wikipedia entry on Least Squares\n        https://en.wikipedia.org/wiki/Least_squares\n    \"\"\"\n\n    def __init__(self, *, unbiased: bool = False, uiter: int = 20):\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self._validate_params(vars(self))\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Estimate the model parameters using Least Squares method.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        \"\"\"\n        self._check_linear_dependence_rows(psi)\n        theta = np.linalg.lstsq(psi, y, rcond=None)[0]\n        return theta\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Estimate the model parameters using Least Squares method.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquares.optimize--parameters","title":"Parameters","text":"<p>psi : ndarray of floats     The information matrix of the model. y : array-like of shape = y_training     The data used to training the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquares.optimize--returns","title":"Returns","text":"<p>theta : array-like of shape = number_of_model_elements     The estimated parameters of the model.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Estimate the model parameters using Least Squares method.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    \"\"\"\n    self._check_linear_dependence_rows(psi)\n    theta = np.linalg.lstsq(psi, y, rcond=None)[0]\n    return theta\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquaresMinimalResidual","title":"<code>LeastSquaresMinimalResidual</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Iterative solver for least-squares minimal residual problems.</p> <p>This is a wrapper class for the <code>scipy.sparse.linalg.lsmr</code> method.</p> <p>lsmr solves the system of linear equations <code>Ax = b</code>. If the system is inconsistent, it solves the least-squares problem <code>min ||b - Ax||_2</code>. <code>A</code> is a rectangular matrix of dimension m-by-n, where all cases are allowed: m = n, m &gt; n, or m &lt; n. <code>b</code> is a vector of length m. The matrix A may be dense or sparse (usually sparse).</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquaresMinimalResidual--parameters","title":"Parameters","text":"<p>unbiased : bool, optional     If True, applies an unbiased estimator. Default is False. uiter : int, optional     Number of iterations for the unbiased estimator. Default is 30.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquaresMinimalResidual--attributes","title":"Attributes","text":"<p>unbiased : bool     Indicates whether an unbiased estimator is applied. uiter : int     Number of iterations for the unbiased estimator. damp : float     Damping factor for regularized least-squares. <code>lsmr</code> solves     the regularized least-squares problem::</p> <pre><code> min ||(b) - (  A   )x||\n     ||(0)   (damp*I) ||_2\n\nwhere damp is a scalar.  If damp is None or 0, the system\nis solved without regularization. Default is 0.\n</code></pre> <p>atol, btol : float, optional     Stopping tolerances. <code>lsmr</code> continues iterations until a     certain backward error estimate is smaller than some quantity     depending on atol and btol.  Let <code>r = b - Ax</code> be the     residual vector for the current approximate solution <code>x</code>.     If <code>Ax = b</code> seems to be consistent, <code>lsmr</code> terminates     when <code>norm(r) &lt;= atol * norm(A) * norm(x) + btol * norm(b)</code>.     Otherwise, <code>lsmr</code> terminates when <code>norm(A^H r) &lt;=     atol * norm(A) * norm(r)</code>.  If both tolerances are 1.0e-6 (default),     the final <code>norm(r)</code> should be accurate to about 6     digits. (The final <code>x</code> will usually have fewer correct digits,     depending on <code>cond(A)</code> and the size of LAMBDA.)  If <code>atol</code>     or <code>btol</code> is None, a default value of 1.0e-6 will be used.     Ideally, they should be estimates of the relative error in the     entries of <code>A</code> and <code>b</code> respectively.  For example, if the entries     of <code>A</code> have 7 correct digits, set <code>atol = 1e-7</code>. This prevents     the algorithm from doing unnecessary work beyond the     uncertainty of the input data. conlim : float, optional     <code>lsmr</code> terminates if an estimate of <code>cond(A)</code> exceeds     <code>conlim</code>.  For compatible systems <code>Ax = b</code>, conlim could be     as large as 1.0e+12 (say).  For least-squares problems,     <code>conlim</code> should be less than 1.0e+8. If <code>conlim</code> is None, the     default value is 1e+8.  Maximum precision can be obtained by     setting <code>atol = btol = conlim = 0</code>, but the number of     iterations may then be excessive. Default is 1e8. maxiter : int, optional     <code>lsmr</code> terminates if the number of iterations reaches     <code>maxiter</code>.  The default is <code>maxiter = min(m, n)</code>.  For     ill-conditioned systems, a larger value of <code>maxiter</code> may be     needed. Default is False. show : bool, optional     Print iterations logs if <code>show=True</code>. Default is False. x0 : array_like, shape (n,), optional     Initial guess of <code>x</code>, if None zeros are used. Default is None.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquaresMinimalResidual--references","title":"References","text":"<p>.. [1] D. C.-L. Fong and M. A. Saunders,        \"LSMR: An iterative algorithm for sparse least-squares problems\",        SIAM J. Sci. Comput., vol. 33, pp. 2950-2971, 2011.        :arxiv:<code>1006.0758</code> .. [2] LSMR Software, https://web.stanford.edu/group/SOL/software/lsmr/</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquaresMinimalResidual--notes","title":"Notes","text":"<p>This docstring is adapted from the <code>scipy.sparse.linalg.lsmr</code> method.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastSquaresMinimalResidual(BaseEstimator):\n    \"\"\"Iterative solver for least-squares minimal residual problems.\n\n    This is a wrapper class for the `scipy.sparse.linalg.lsmr` method.\n\n    lsmr solves the system of linear equations ``Ax = b``. If the system\n    is inconsistent, it solves the least-squares problem ``min ||b - Ax||_2``.\n    ``A`` is a rectangular matrix of dimension m-by-n, where all cases are\n    allowed: m = n, m &gt; n, or m &lt; n. ``b`` is a vector of length m.\n    The matrix A may be dense or sparse (usually sparse).\n\n    Parameters\n    ----------\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    Attributes\n    ----------\n    unbiased : bool\n        Indicates whether an unbiased estimator is applied.\n    uiter : int\n        Number of iterations for the unbiased estimator.\n    damp : float\n        Damping factor for regularized least-squares. `lsmr` solves\n        the regularized least-squares problem::\n\n         min ||(b) - (  A   )x||\n             ||(0)   (damp*I) ||_2\n\n        where damp is a scalar.  If damp is None or 0, the system\n        is solved without regularization. Default is 0.\n    atol, btol : float, optional\n        Stopping tolerances. `lsmr` continues iterations until a\n        certain backward error estimate is smaller than some quantity\n        depending on atol and btol.  Let ``r = b - Ax`` be the\n        residual vector for the current approximate solution ``x``.\n        If ``Ax = b`` seems to be consistent, `lsmr` terminates\n        when ``norm(r) &lt;= atol * norm(A) * norm(x) + btol * norm(b)``.\n        Otherwise, `lsmr` terminates when ``norm(A^H r) &lt;=\n        atol * norm(A) * norm(r)``.  If both tolerances are 1.0e-6 (default),\n        the final ``norm(r)`` should be accurate to about 6\n        digits. (The final ``x`` will usually have fewer correct digits,\n        depending on ``cond(A)`` and the size of LAMBDA.)  If `atol`\n        or `btol` is None, a default value of 1.0e-6 will be used.\n        Ideally, they should be estimates of the relative error in the\n        entries of ``A`` and ``b`` respectively.  For example, if the entries\n        of ``A`` have 7 correct digits, set ``atol = 1e-7``. This prevents\n        the algorithm from doing unnecessary work beyond the\n        uncertainty of the input data.\n    conlim : float, optional\n        `lsmr` terminates if an estimate of ``cond(A)`` exceeds\n        `conlim`.  For compatible systems ``Ax = b``, conlim could be\n        as large as 1.0e+12 (say).  For least-squares problems,\n        `conlim` should be less than 1.0e+8. If `conlim` is None, the\n        default value is 1e+8.  Maximum precision can be obtained by\n        setting ``atol = btol = conlim = 0``, but the number of\n        iterations may then be excessive. Default is 1e8.\n    maxiter : int, optional\n        `lsmr` terminates if the number of iterations reaches\n        `maxiter`.  The default is ``maxiter = min(m, n)``.  For\n        ill-conditioned systems, a larger value of `maxiter` may be\n        needed. Default is False.\n    show : bool, optional\n        Print iterations logs if ``show=True``. Default is False.\n    x0 : array_like, shape (n,), optional\n        Initial guess of ``x``, if None zeros are used. Default is None.\n\n    References\n    ----------\n    .. [1] D. C.-L. Fong and M. A. Saunders,\n           \"LSMR: An iterative algorithm for sparse least-squares problems\",\n           SIAM J. Sci. Comput., vol. 33, pp. 2950-2971, 2011.\n           :arxiv:`1006.0758`\n    .. [2] LSMR Software, https://web.stanford.edu/group/SOL/software/lsmr/\n\n    Notes\n    -----\n    This docstring is adapted from the `scipy.sparse.linalg.lsmr` method.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        unbiased: bool = False,\n        uiter: int = 30,\n        damp=0.0,\n        atol=1e-6,\n        btol=1e-6,\n        conlim=1e8,\n        maxiter=None,\n        show=False,\n        x0=None,\n    ):\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self.damp = damp\n        self.atol = atol\n        self.btol = btol\n        self.conlim = conlim\n        self.maxiter = maxiter\n        self.show = show\n        self.x0 = x0\n\n    def optimize(self, psi, y):\n        \"\"\"Parameter estimation using the Mixed-norm LMS filter.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        This is a wrapper class for the `scipy.sparse.linalg.lsmr` method.\n\n        References\n        ----------\n        .. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsmr.html\n        \"\"\"\n        theta = lsmr(\n            psi,\n            y.ravel(),\n            damp=self.damp,\n            atol=self.atol,\n            btol=self.btol,\n            conlim=self.conlim,\n            maxiter=self.maxiter,\n            show=self.show,\n            x0=self.x0,\n        )[0]\n        return theta.reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquaresMinimalResidual.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Mixed-norm LMS filter.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquaresMinimalResidual.optimize--parameters","title":"Parameters","text":"<p>psi : ndarray of floats     The information matrix of the model. y : array-like     The data used to training the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquaresMinimalResidual.optimize--returns","title":"Returns","text":"<p>theta : array-like of shape = number_of_model_elements     The estimated parameters of the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquaresMinimalResidual.optimize--notes","title":"Notes","text":"<p>This is a wrapper class for the <code>scipy.sparse.linalg.lsmr</code> method.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquaresMinimalResidual.optimize--references","title":"References","text":"<p>.. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsmr.html</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi, y):\n    \"\"\"Parameter estimation using the Mixed-norm LMS filter.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    This is a wrapper class for the `scipy.sparse.linalg.lsmr` method.\n\n    References\n    ----------\n    .. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsmr.html\n    \"\"\"\n    theta = lsmr(\n        psi,\n        y.ravel(),\n        damp=self.damp,\n        atol=self.atol,\n        btol=self.btol,\n        conlim=self.conlim,\n        maxiter=self.maxiter,\n        show=self.show,\n        x0=self.x0,\n    )[0]\n    return theta.reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NonNegativeLeastSquares","title":"<code>NonNegativeLeastSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Solve <code>argmin_x || Ax - b ||_2</code> for <code>x &gt;= 0</code>.</p> <p>This is a wrapper class for the <code>scipy.optimize.nnls</code> method.</p> <p>This problem, often called NonNegative Least Squares (NNLS), is a convex optimization problem with convex constraints. It typically arises when the <code>x</code> models quantities for which only nonnegative values are attainable; such as weights of ingredients, component costs, and so on.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NonNegativeLeastSquares--parameters","title":"Parameters","text":"<p>unbiased : bool, optional     If True, applies an unbiased estimator. Default is False. uiter : int, optional     Number of iterations for the unbiased estimator. Default is 30. maxiter : int, optional     Maximum number of iterations. Default value is <code>3 * n</code> where <code>n</code>     is the number of features. atol : float, optional     Tolerance value used in the algorithm to assess closeness to zero in     the projected residual <code>(A.T @ (A x - b))</code> entries. Increasing this     value relaxes the solution constraints. A typical relaxation value can     be selected as <code>max(m, n) * np.linalg.norm(A, 1) * np.spacing(1.)</code>.     Default is None.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NonNegativeLeastSquares--attributes","title":"Attributes","text":"<p>unbiased : bool     Indicates whether an unbiased estimator is applied. uiter : int     Number of iterations for the unbiased estimator. maxiter : int     Maximum number of iterations. atol : float     Tolerance value for the algorithm.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NonNegativeLeastSquares--references","title":"References","text":"<p>.. [1] Lawson C., Hanson R.J., \"Solving Least Squares Problems\", SIAM,    1995, :doi:<code>10.1137/1.9781611971217</code> .. [2] Bro, Rasmus and de Jong, Sijmen, \"A Fast Non-Negativity-Constrained Least Squares Algorithm\", Journal Of Chemometrics, 1997, :doi:<code>10.1002/(SICI)1099-128X(199709/10)11:5&lt;393::AID-CEM483&gt;3.0.CO;2-L</code></p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NonNegativeLeastSquares--examples","title":"Examples","text":"<p>import numpy as np from sysidentpy.parameter_estimation import NonNegativeLeastSquares ... A = np.array([[1, 0], [1, 0], [0, 1]]) b = np.array([2, 1, 1]) nnls_solver = NonNegativeLeastSquares() x = nnls_solver.optimize(A, b) print(x) [[1.5]  [1. ]]</p> <p>b = np.array([-1, -1, -1]) x = nnls_solver.optimize(A, b) print(x) [[0.]  [0.]]</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class NonNegativeLeastSquares(BaseEstimator):\n    \"\"\"Solve ``argmin_x || Ax - b ||_2`` for ``x &gt;= 0``.\n\n    This is a wrapper class for the `scipy.optimize.nnls` method.\n\n    This problem, often called NonNegative Least Squares (NNLS), is a convex\n    optimization problem with convex constraints. It typically arises when\n    the ``x`` models quantities for which only nonnegative values are\n    attainable; such as weights of ingredients, component costs, and so on.\n\n    Parameters\n    ----------\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n    maxiter : int, optional\n        Maximum number of iterations. Default value is ``3 * n`` where ``n``\n        is the number of features.\n    atol : float, optional\n        Tolerance value used in the algorithm to assess closeness to zero in\n        the projected residual ``(A.T @ (A x - b))`` entries. Increasing this\n        value relaxes the solution constraints. A typical relaxation value can\n        be selected as ``max(m, n) * np.linalg.norm(A, 1) * np.spacing(1.)``.\n        Default is None.\n\n    Attributes\n    ----------\n    unbiased : bool\n        Indicates whether an unbiased estimator is applied.\n    uiter : int\n        Number of iterations for the unbiased estimator.\n    maxiter : int\n        Maximum number of iterations.\n    atol : float\n        Tolerance value for the algorithm.\n\n    References\n    ----------\n    .. [1] Lawson C., Hanson R.J., \"Solving Least Squares Problems\", SIAM,\n       1995, :doi:`10.1137/1.9781611971217`\n    .. [2] Bro, Rasmus and de Jong, Sijmen, \"A Fast Non-Negativity-Constrained Least\n    Squares Algorithm\", Journal Of Chemometrics, 1997,\n    :doi:`10.1002/(SICI)1099-128X(199709/10)11:5&lt;393::AID-CEM483&gt;3.0.CO;2-L`\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from sysidentpy.parameter_estimation import NonNegativeLeastSquares\n    ...\n    &gt;&gt;&gt; A = np.array([[1, 0], [1, 0], [0, 1]])\n    &gt;&gt;&gt; b = np.array([2, 1, 1])\n    &gt;&gt;&gt; nnls_solver = NonNegativeLeastSquares()\n    &gt;&gt;&gt; x = nnls_solver.optimize(A, b)\n    &gt;&gt;&gt; print(x)\n    [[1.5]\n     [1. ]]\n\n    &gt;&gt;&gt; b = np.array([-1, -1, -1])\n    &gt;&gt;&gt; x = nnls_solver.optimize(A, b)\n    &gt;&gt;&gt; print(x)\n    [[0.]\n     [0.]]\n    \"\"\"\n\n    def __init__(\n        self, unbiased: bool = False, uiter: int = 30, maxiter=None, atol=None\n    ):\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self.maxiter = maxiter\n        self.atol = atol\n\n    def optimize(self, psi, y):\n        \"\"\"Parameter estimation using the NonNegativeLeastSquares algorithm.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        This is a wrapper class for the `scipy.optimize.nnls` method.\n\n        References\n        ----------\n        .. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.nnls.html\n        \"\"\"\n        theta, _ = nnls(psi, y.ravel(), maxiter=self.maxiter, atol=self.atol)\n        return theta.reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NonNegativeLeastSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the NonNegativeLeastSquares algorithm.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NonNegativeLeastSquares.optimize--parameters","title":"Parameters","text":"<p>psi : ndarray of floats     The information matrix of the model. y : array-like     The data used to training the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NonNegativeLeastSquares.optimize--returns","title":"Returns","text":"<p>theta : array-like of shape = number_of_model_elements     The estimated parameters of the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NonNegativeLeastSquares.optimize--notes","title":"Notes","text":"<p>This is a wrapper class for the <code>scipy.optimize.nnls</code> method.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NonNegativeLeastSquares.optimize--references","title":"References","text":"<p>.. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.nnls.html</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi, y):\n    \"\"\"Parameter estimation using the NonNegativeLeastSquares algorithm.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    This is a wrapper class for the `scipy.optimize.nnls` method.\n\n    References\n    ----------\n    .. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.nnls.html\n    \"\"\"\n    theta, _ = nnls(psi, y.ravel(), maxiter=self.maxiter, atol=self.atol)\n    return theta.reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquares","title":"<code>NormalizedLeastMeanSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Normalized Least Mean Squares (ALMS) filter for parameter estimation.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquares--parameters","title":"Parameters","text":"<p>mu : float, default=0.01     The learning rate or step size for the LMS algorithm. eps : float, default=np.finfo(np.float64).eps     Normalization factor of the normalized filters.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquares--attributes","title":"Attributes","text":"<p>mu : float     The learning rate or step size for the LMS algorithm. eps : float, default=np.finfo(np.float64).eps     Normalization factor of the normalized filters. xi : np.ndarray or None     The estimation error at each iteration. Initialized as None and updated during     optimization.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquares--methods","title":"Methods","text":"<p>optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray     Estimate the model parameters using the LMS filter.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class NormalizedLeastMeanSquares(BaseEstimator):\n    \"\"\"Normalized Least Mean Squares (ALMS) filter for parameter estimation.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMS filter.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        eps: np.float64 = np.finfo(np.float64).eps,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.eps = eps\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self._validate_params(vars(self))\n        self.xi: np.ndarray\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Parameter estimation using the Normalized Least Mean Squares filter.\n\n        The normalization is used to avoid numerical instability when updating\n        the estimated parameters.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        A more in-depth documentation of all methods for parameters estimation\n        will be available soon. For now, please refer to the mentioned\n        references.\n\n        References\n        ----------\n        - Book: Hayes, M. H. (2009). Statistical digital signal processing\n           and modeling. John Wiley &amp; Sons.\n        - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n           an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n           vari\u00e1vel.\n        - Wikipedia entry on Least Mean Squares\n           https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n        \"\"\"\n        n_theta, n, theta, self.xi = self._initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * self.xi[i, 0] * (\n                psi_tmp / (self.eps + np.dot(psi_tmp.T, psi_tmp))\n            )\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Normalized Least Mean Squares filter.</p> <p>The normalization is used to avoid numerical instability when updating the estimated parameters.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquares.optimize--parameters","title":"Parameters","text":"<p>psi : ndarray of floats     The information matrix of the model. y : array-like of shape = y_training     The data used to training the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquares.optimize--returns","title":"Returns","text":"<p>theta : array-like of shape = number_of_model_elements     The estimated parameters of the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquares.optimize--notes","title":"Notes","text":"<p>A more in-depth documentation of all methods for parameters estimation will be available soon. For now, please refer to the mentioned references.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquares.optimize--references","title":"References","text":"<ul> <li>Book: Hayes, M. H. (2009). Statistical digital signal processing    and modeling. John Wiley &amp; Sons.</li> <li>Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,    an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo    vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares    https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Parameter estimation using the Normalized Least Mean Squares filter.\n\n    The normalization is used to avoid numerical instability when updating\n    the estimated parameters.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    A more in-depth documentation of all methods for parameters estimation\n    will be available soon. For now, please refer to the mentioned\n    references.\n\n    References\n    ----------\n    - Book: Hayes, M. H. (2009). Statistical digital signal processing\n       and modeling. John Wiley &amp; Sons.\n    - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n       an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n       vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares\n       https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n    \"\"\"\n    n_theta, n, theta, self.xi = self._initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * self.xi[i, 0] * (\n            psi_tmp / (self.eps + np.dot(psi_tmp.T, psi_tmp))\n        )\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquaresSignError","title":"<code>NormalizedLeastMeanSquaresSignError</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Normalized Least Mean Squares SignError(NLMSSE) filter for parameter estimation.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquaresSignError--parameters","title":"Parameters","text":"<p>mu : float, default=0.01     The learning rate or step size for the LMS algorithm. eps : float, default=np.finfo(np.float64).eps     Normalization factor of the normalized filters.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquaresSignError--attributes","title":"Attributes","text":"<p>mu : float     The learning rate or step size for the LMS algorithm. eps : float, default=np.finfo(np.float64).eps     Normalization factor of the normalized filters. xi : np.ndarray or None     The estimation error at each iteration. Initialized as None and updated during     optimization.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquaresSignError--methods","title":"Methods","text":"<p>optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray     Estimate the model parameters using the LMS filter.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class NormalizedLeastMeanSquaresSignError(BaseEstimator):\n    \"\"\"Normalized Least Mean Squares SignError(NLMSSE) filter for parameter estimation.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMS filter.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        eps: np.float64 = np.finfo(np.float64).eps,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.eps = eps\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self._validate_params(vars(self))\n        self.xi: np.ndarray\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Parameter estimation using the Normalized Sign-Error LMS filter.\n\n        The normalization is used to avoid numerical instability when updating\n        the estimated parameters and the sign of the error vector is used to\n        to change the filter coefficients.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        A more in-depth documentation of all methods for parameters estimation\n        will be available soon. For now, please refer to the mentioned\n        references.\n\n        References\n        ----------\n        - Book: Hayes, M. H. (2009). Statistical digital signal processing\n           and modeling. John Wiley &amp; Sons.\n        - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n           an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n           vari\u00e1vel.\n        - Wikipedia entry on Least Mean Squares\n           https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n        \"\"\"\n        n_theta, n, theta, self.xi = self._initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * np.sign(\n                self.xi[i, 0]\n            ) * (psi_tmp / (self.eps + np.dot(psi_tmp.T, psi_tmp)))\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquaresSignError.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Normalized Sign-Error LMS filter.</p> <p>The normalization is used to avoid numerical instability when updating the estimated parameters and the sign of the error vector is used to to change the filter coefficients.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquaresSignError.optimize--parameters","title":"Parameters","text":"<p>psi : ndarray of floats     The information matrix of the model. y : array-like of shape = y_training     The data used to training the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquaresSignError.optimize--returns","title":"Returns","text":"<p>theta : array-like of shape = number_of_model_elements     The estimated parameters of the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquaresSignError.optimize--notes","title":"Notes","text":"<p>A more in-depth documentation of all methods for parameters estimation will be available soon. For now, please refer to the mentioned references.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquaresSignError.optimize--references","title":"References","text":"<ul> <li>Book: Hayes, M. H. (2009). Statistical digital signal processing    and modeling. John Wiley &amp; Sons.</li> <li>Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,    an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo    vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares    https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Parameter estimation using the Normalized Sign-Error LMS filter.\n\n    The normalization is used to avoid numerical instability when updating\n    the estimated parameters and the sign of the error vector is used to\n    to change the filter coefficients.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    A more in-depth documentation of all methods for parameters estimation\n    will be available soon. For now, please refer to the mentioned\n    references.\n\n    References\n    ----------\n    - Book: Hayes, M. H. (2009). Statistical digital signal processing\n       and modeling. John Wiley &amp; Sons.\n    - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n       an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n       vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares\n       https://en.wikipedia.org/wiki/Least_mean_squares_filter\n\n    \"\"\"\n    n_theta, n, theta, self.xi = self._initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * np.sign(\n            self.xi[i, 0]\n        ) * (psi_tmp / (self.eps + np.dot(psi_tmp.T, psi_tmp)))\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RecursiveLeastSquares","title":"<code>RecursiveLeastSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>summary.</p> <p>extended_summary</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RecursiveLeastSquares--parameters","title":"Parameters","text":"<p>lam : float, default=0.98     Forgetting factor of the Recursive Least Squares method. delta : float, default=0.01     Normalization factor of the P matrix.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class RecursiveLeastSquares(BaseEstimator):\n    \"\"\"_summary_.\n\n    _extended_summary_\n\n    Parameters\n    ----------\n    lam : float, default=0.98\n        Forgetting factor of the Recursive Least Squares method.\n    delta : float, default=0.01\n        Normalization factor of the P matrix.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        delta: float = 0.01,\n        lam: float = 0.98,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.delta = delta\n        self.lam = lam\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self._validate_params(vars(self))\n        self.xi: np.ndarray\n        self.theta_evolution: np.ndarray\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Estimate the model parameters using the Recursive Least Squares method.\n\n        The implementation consider the forgetting factor.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        A more in-depth documentation of all methods for parameters estimation\n        will be available soon. For now, please refer to the mentioned\n        references.\n\n        References\n        ----------\n        - Book (Portuguese): Aguirre, L. A. (2007). Introdu\u00e7\u00e3o identifica\u00e7\u00e3o\n           de sistemas: t\u00e9cnicas lineares e n\u00e3o-lineares aplicadas a sistemas\n           reais. Editora da UFMG. 3a edi\u00e7\u00e3o.\n\n        \"\"\"\n        n_theta, n, theta, self.xi = self._initial_values(psi)\n        p = np.eye(n_theta) / self.delta\n\n        for i in range(2, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            k_numerator = self.lam ** (-1) * p.dot(psi_tmp)\n            k_denominator = 1 + self.lam ** (-1) * psi_tmp.T.dot(p).dot(psi_tmp)\n            k = np.divide(k_numerator, k_denominator)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + k.dot(self.xi[i, 0])\n            theta[:, i] = tmp_list.flatten()\n\n            p1 = p.dot(psi[i, :].reshape(-1, 1)).dot(psi[i, :].reshape(-1, 1).T).dot(p)\n            p2 = (\n                psi[i, :].reshape(-1, 1).T.dot(p).dot(psi[i, :].reshape(-1, 1))\n                + self.lam\n            )\n\n            p_numerator = p - np.divide(p1, p2)\n            p = np.divide(p_numerator, self.lam)\n\n        self.theta_evolution = theta.copy()\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RecursiveLeastSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Estimate the model parameters using the Recursive Least Squares method.</p> <p>The implementation consider the forgetting factor.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RecursiveLeastSquares.optimize--parameters","title":"Parameters","text":"<p>psi : ndarray of floats     The information matrix of the model. y : array-like of shape = y_training     The data used to training the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RecursiveLeastSquares.optimize--returns","title":"Returns","text":"<p>theta : array-like of shape = number_of_model_elements     The estimated parameters of the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RecursiveLeastSquares.optimize--notes","title":"Notes","text":"<p>A more in-depth documentation of all methods for parameters estimation will be available soon. For now, please refer to the mentioned references.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RecursiveLeastSquares.optimize--references","title":"References","text":"<ul> <li>Book (Portuguese): Aguirre, L. A. (2007). Introdu\u00e7\u00e3o identifica\u00e7\u00e3o    de sistemas: t\u00e9cnicas lineares e n\u00e3o-lineares aplicadas a sistemas    reais. Editora da UFMG. 3a edi\u00e7\u00e3o.</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Estimate the model parameters using the Recursive Least Squares method.\n\n    The implementation consider the forgetting factor.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    A more in-depth documentation of all methods for parameters estimation\n    will be available soon. For now, please refer to the mentioned\n    references.\n\n    References\n    ----------\n    - Book (Portuguese): Aguirre, L. A. (2007). Introdu\u00e7\u00e3o identifica\u00e7\u00e3o\n       de sistemas: t\u00e9cnicas lineares e n\u00e3o-lineares aplicadas a sistemas\n       reais. Editora da UFMG. 3a edi\u00e7\u00e3o.\n\n    \"\"\"\n    n_theta, n, theta, self.xi = self._initial_values(psi)\n    p = np.eye(n_theta) / self.delta\n\n    for i in range(2, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        k_numerator = self.lam ** (-1) * p.dot(psi_tmp)\n        k_denominator = 1 + self.lam ** (-1) * psi_tmp.T.dot(p).dot(psi_tmp)\n        k = np.divide(k_numerator, k_denominator)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + k.dot(self.xi[i, 0])\n        theta[:, i] = tmp_list.flatten()\n\n        p1 = p.dot(psi[i, :].reshape(-1, 1)).dot(psi[i, :].reshape(-1, 1).T).dot(p)\n        p2 = (\n            psi[i, :].reshape(-1, 1).T.dot(p).dot(psi[i, :].reshape(-1, 1))\n            + self.lam\n        )\n\n        p_numerator = p - np.divide(p1, p2)\n        p = np.divide(p_numerator, self.lam)\n\n    self.theta_evolution = theta.copy()\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RidgeRegression","title":"<code>RidgeRegression</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Ridge Regression estimator using classic and SVD methods.</p> <p>This class implements Ridge Regression, a type of linear regression that includes an L2 penalty to prevent overfitting. The implementation offers two methods for parameter estimation: a classic approach and an approach based on Singular Value Decomposition (SVD).</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RidgeRegression--parameters","title":"Parameters","text":"<p>alpha : np.float64, optional (default=np.finfo(np.float64).eps)     Regularization strength; must be a positive float. Regularization improves the     conditioning of the problem and reduces the variance of the estimates. Larger     values specify stronger regularization. If the input is a noisy signal,     the ridge parameter is likely to be set close to the noise level, at least as     a starting point. Entered through the self data structure. solver : str, optional (default=\"svd\")     Solver to use in the parameter estimation procedure.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RidgeRegression--methods","title":"Methods","text":"<p>ridge_regression_classic(psi, y)     Estimate the model parameters using the classic ridge regression method. ridge_regression(psi, y)     Estimate the model parameters using the SVD-based ridge regression method. optimize(psi, y)     Optimize the model parameters using the chosen method (SVD or classic).</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RidgeRegression--references","title":"References","text":"<ul> <li>Wikipedia entry on ridge regression   https://en.wikipedia.org/wiki/Ridge_regression</li> <li>D. J. Gauthier, E. Bollt, A. Griffith, W. A. S. Barbosa, 'Next generation   reservoir computing,' Nat. Commun. 12, 5564 (2021).   https://www.nature.com/articles/s41467-021-25801-2</li> <li>Hoerl, A. E.; Kennard, R. W. Ridge regression: applications to nonorthogonal   problems. Technometrics, Taylor &amp; Francis, v. 12, n. 1, p. 69-82, 1970.</li> <li>StackExchange: whuber. The proof of shrinking coefficients using ridge regression   through \"spectral decomposition\".   Cross Validated, accessed 21 September 2023,   https://stats.stackexchange.com/q/220324</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class RidgeRegression(BaseEstimator):\n    \"\"\"Ridge Regression estimator using classic and SVD methods.\n\n    This class implements Ridge Regression, a type of linear regression that includes\n    an L2 penalty to prevent overfitting. The implementation offers two methods for\n    parameter estimation: a classic approach and an approach based on Singular Value\n    Decomposition (SVD).\n\n    Parameters\n    ----------\n    alpha : np.float64, optional (default=np.finfo(np.float64).eps)\n        Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger\n        values specify stronger regularization. If the input is a noisy signal,\n        the ridge parameter is likely to be set close to the noise level, at least as\n        a starting point. Entered through the self data structure.\n    solver : str, optional (default=\"svd\")\n        Solver to use in the parameter estimation procedure.\n\n    Methods\n    -------\n    ridge_regression_classic(psi, y)\n        Estimate the model parameters using the classic ridge regression method.\n    ridge_regression(psi, y)\n        Estimate the model parameters using the SVD-based ridge regression method.\n    optimize(psi, y)\n        Optimize the model parameters using the chosen method (SVD or classic).\n\n    References\n    ----------\n    - Wikipedia entry on ridge regression\n      https://en.wikipedia.org/wiki/Ridge_regression\n    - D. J. Gauthier, E. Bollt, A. Griffith, W. A. S. Barbosa, 'Next generation\n      reservoir computing,' Nat. Commun. 12, 5564 (2021).\n      https://www.nature.com/articles/s41467-021-25801-2\n    - Hoerl, A. E.; Kennard, R. W. Ridge regression: applications to nonorthogonal\n      problems. Technometrics, Taylor &amp; Francis, v. 12, n. 1, p. 69-82, 1970.\n    - StackExchange: whuber. The proof of shrinking coefficients using ridge regression\n      through \"spectral decomposition\".\n      Cross Validated, accessed 21 September 2023,\n      https://stats.stackexchange.com/q/220324\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        alpha: np.float64 = np.finfo(np.float64).eps,\n        solver: str = \"svd\",\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.alpha = alpha\n        self.solver = solver\n        self.uiter = uiter\n        self.unbiased = unbiased\n        self._validate_params(vars(self))\n\n    def ridge_regression_classic(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Estimate the model parameters using ridge regression.\n\n           Based on the least_squares module and uses the same data format but you need\n           to pass alpha in the call to FROLS.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        References\n        ----------\n        - Wikipedia entry on ridge regression\n          https://en.wikipedia.org/wiki/Ridge_regression\n\n        alpha multiplied by the identity matrix (np.eye) favors models (theta) that\n        have small size using an L2 norm.  This prevents over fitting of the model.\n        For applications where preventing overfitting is important, see, for example,\n        D. J. Gauthier, E. Bollt, A. Griffith, W. A. S. Barbosa, 'Next generation\n        reservoir computing,' Nat. Commun. 12, 5564 (2021).\n        https://www.nature.com/articles/s41467-021-25801-2\n\n        \"\"\"\n        self._check_linear_dependence_rows(psi)\n\n        theta = (\n            np.linalg.pinv(psi.T @ psi + self.alpha * np.eye(psi.shape[1])) @ psi.T @ y\n        )\n        return theta\n\n    def ridge_regression(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Estimate the model parameters using SVD and Ridge Regression method.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        References\n        ----------\n        - Manuscript: Hoerl, A. E.; Kennard, R. W. Ridge regression:\n                      applications to nonorthogonal problems. Technometrics,\n                      Taylor &amp; Francis, v. 12, n. 1, p. 69-82, 1970.\n\n        - StackExchange: whuber. The proof of shrinking coefficients using ridge\n                         regression through \"spectral decomposition\".\n                         Cross Validated, accessed 21 September 2023,\n                         https://stats.stackexchange.com/q/220324\n        \"\"\"\n        self._check_linear_dependence_rows(psi)\n        try:\n            U, S, Vh = np.linalg.svd(psi, full_matrices=False)\n            S = np.diag(S)\n            i = np.identity(len(S))\n            theta = Vh.T @ np.linalg.inv(S**2 + self.alpha * i) @ S @ U.T @ y\n        except EstimatorError:\n            warnings.warn(\n                \"The SVD computation did not converge.\"\n                \"Theta values will be calculated with the classic algorithm.\",\n                stacklevel=2,\n            )\n\n            theta = self.ridge_regression_classic(psi, y)\n\n        return theta\n\n    def optimize(self, psi: np.ndarray, y):\n        if self.solver == \"svd\":\n            return self.ridge_regression(psi, y)\n\n        return self.ridge_regression_classic(psi, y)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RidgeRegression.ridge_regression","title":"<code>ridge_regression(psi, y)</code>","text":"<p>Estimate the model parameters using SVD and Ridge Regression method.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RidgeRegression.ridge_regression--parameters","title":"Parameters","text":"<p>psi : ndarray of floats     The information matrix of the model. y : array-like of shape = y_training     The data used to training the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RidgeRegression.ridge_regression--returns","title":"Returns","text":"<p>theta : array-like of shape = number_of_model_elements     The estimated parameters of the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RidgeRegression.ridge_regression--references","title":"References","text":"<ul> <li> <p>Manuscript: Hoerl, A. E.; Kennard, R. W. Ridge regression:               applications to nonorthogonal problems. Technometrics,               Taylor &amp; Francis, v. 12, n. 1, p. 69-82, 1970.</p> </li> <li> <p>StackExchange: whuber. The proof of shrinking coefficients using ridge                  regression through \"spectral decomposition\".                  Cross Validated, accessed 21 September 2023,                  https://stats.stackexchange.com/q/220324</p> </li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def ridge_regression(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Estimate the model parameters using SVD and Ridge Regression method.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    References\n    ----------\n    - Manuscript: Hoerl, A. E.; Kennard, R. W. Ridge regression:\n                  applications to nonorthogonal problems. Technometrics,\n                  Taylor &amp; Francis, v. 12, n. 1, p. 69-82, 1970.\n\n    - StackExchange: whuber. The proof of shrinking coefficients using ridge\n                     regression through \"spectral decomposition\".\n                     Cross Validated, accessed 21 September 2023,\n                     https://stats.stackexchange.com/q/220324\n    \"\"\"\n    self._check_linear_dependence_rows(psi)\n    try:\n        U, S, Vh = np.linalg.svd(psi, full_matrices=False)\n        S = np.diag(S)\n        i = np.identity(len(S))\n        theta = Vh.T @ np.linalg.inv(S**2 + self.alpha * i) @ S @ U.T @ y\n    except EstimatorError:\n        warnings.warn(\n            \"The SVD computation did not converge.\"\n            \"Theta values will be calculated with the classic algorithm.\",\n            stacklevel=2,\n        )\n\n        theta = self.ridge_regression_classic(psi, y)\n\n    return theta\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RidgeRegression.ridge_regression_classic","title":"<code>ridge_regression_classic(psi, y)</code>","text":"<p>Estimate the model parameters using ridge regression.</p> <p>Based on the least_squares module and uses the same data format but you need    to pass alpha in the call to FROLS.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RidgeRegression.ridge_regression_classic--parameters","title":"Parameters","text":"<p>psi : ndarray of floats     The information matrix of the model. y : array-like of shape = y_training     The data used to training the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RidgeRegression.ridge_regression_classic--returns","title":"Returns","text":"<p>theta : array-like of shape = number_of_model_elements     The estimated parameters of the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RidgeRegression.ridge_regression_classic--references","title":"References","text":"<ul> <li>Wikipedia entry on ridge regression   https://en.wikipedia.org/wiki/Ridge_regression</li> </ul> <p>alpha multiplied by the identity matrix (np.eye) favors models (theta) that have small size using an L2 norm.  This prevents over fitting of the model. For applications where preventing overfitting is important, see, for example, D. J. Gauthier, E. Bollt, A. Griffith, W. A. S. Barbosa, 'Next generation reservoir computing,' Nat. Commun. 12, 5564 (2021). https://www.nature.com/articles/s41467-021-25801-2</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def ridge_regression_classic(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Estimate the model parameters using ridge regression.\n\n       Based on the least_squares module and uses the same data format but you need\n       to pass alpha in the call to FROLS.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    References\n    ----------\n    - Wikipedia entry on ridge regression\n      https://en.wikipedia.org/wiki/Ridge_regression\n\n    alpha multiplied by the identity matrix (np.eye) favors models (theta) that\n    have small size using an L2 norm.  This prevents over fitting of the model.\n    For applications where preventing overfitting is important, see, for example,\n    D. J. Gauthier, E. Bollt, A. Griffith, W. A. S. Barbosa, 'Next generation\n    reservoir computing,' Nat. Commun. 12, 5564 (2021).\n    https://www.nature.com/articles/s41467-021-25801-2\n\n    \"\"\"\n    self._check_linear_dependence_rows(psi)\n\n    theta = (\n        np.linalg.pinv(psi.T @ psi + self.alpha * np.eye(psi.shape[1])) @ psi.T @ y\n    )\n    return theta\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.TotalLeastSquares","title":"<code>TotalLeastSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Estimate the model parameters using Total Least Squares method.</p> <p>extended_summary</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.TotalLeastSquares--parameters","title":"Parameters","text":"<p>BaseEstimator : type description</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.TotalLeastSquares--references","title":"References","text":"<ul> <li>Manuscript: Golub, G. H., &amp; Van Loan, C. F. (1980).     An analysis of the total least squares problem.     SIAM journal on numerical analysis, 17(6), 883-893.</li> <li>Manuscript: Markovsky, I., &amp; Van Huffel, S. (2007).     Overview of total least-squares methods.     Signal processing, 87(10), 2283-2302.     https://eprints.soton.ac.uk/263855/1/tls_overview.pdf</li> <li>Wikipedia entry on Total Least Squares     https://en.wikipedia.org/wiki/Total_least_squares</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class TotalLeastSquares(BaseEstimator):\n    \"\"\"Estimate the model parameters using Total Least Squares method.\n\n    _extended_summary_\n\n    Parameters\n    ----------\n    BaseEstimator : _type_\n        _description_\n\n    References\n    ----------\n    - Manuscript: Golub, G. H., &amp; Van Loan, C. F. (1980).\n        An analysis of the total least squares problem.\n        SIAM journal on numerical analysis, 17(6), 883-893.\n    - Manuscript: Markovsky, I., &amp; Van Huffel, S. (2007).\n        Overview of total least-squares methods.\n        Signal processing, 87(10), 2283-2302.\n        https://eprints.soton.ac.uk/263855/1/tls_overview.pdf\n    - Wikipedia entry on Total Least Squares\n        https://en.wikipedia.org/wiki/Total_least_squares\n    \"\"\"\n\n    def __init__(self, *, unbiased: bool = False, uiter: int = 30):\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self._validate_params(vars(self))\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Estimate the model parameters using Total Least Squares method.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        \"\"\"\n        self._check_linear_dependence_rows(psi)\n        full = np.hstack((psi, y))\n        n = psi.shape[1]\n        _, _, v = np.linalg.svd(full, full_matrices=True)\n        theta = -v.T[:n, n:] / v.T[n:, n:]\n        return theta.reshape(-1, 1)\n</code></pre>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.TotalLeastSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Estimate the model parameters using Total Least Squares method.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.TotalLeastSquares.optimize--parameters","title":"Parameters","text":"<p>psi : ndarray of floats     The information matrix of the model. y : array-like of shape = y_training     The data used to training the model.</p>"},{"location":"code/parameter-estimation/#sysidentpy.parameter_estimation.estimators.TotalLeastSquares.optimize--returns","title":"Returns","text":"<p>theta : array-like of shape = number_of_model_elements     The estimated parameters of the model.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Estimate the model parameters using Total Least Squares method.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    \"\"\"\n    self._check_linear_dependence_rows(psi)\n    full = np.hstack((psi, y))\n    n = psi.shape[1]\n    _, _, v = np.linalg.svd(full, full_matrices=True)\n    theta = -v.T[:n, n:] / v.T[n:, n:]\n    return theta.reshape(-1, 1)\n</code></pre>"},{"location":"code/residues/","title":"Documentation for <code>Residual Analysis</code>","text":""},{"location":"code/simulation/","title":"Documentation for <code>Simulation</code>","text":"<p>Simulation methods for NARMAX models.</p>"},{"location":"code/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX","title":"<code>SimulateNARMAX</code>","text":"<p>               Bases: <code>BaseMSS</code></p> <p>Simulation of Polynomial NARMAX model.</p> <p>The NARMAX model is described as:</p> \\[     y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1},     \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>where \\(n_y\\in \\mathbb{N}^*\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\), are the maximum lags for the system output and input respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}^\\ell\\) is some nonlinear function of the input and output regressors with nonlinearity degree \\(\\ell \\in \\mathbb{N}\\) and \\(d\\) is a time delay typically set to \\(d=1\\).</p>"},{"location":"code/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX--parameters","title":"Parameters","text":"<p>estimator : str, default=\"least_squares\"     The parameter estimation method. estimate_parameter : bool, default=False     Whether to use a method for parameter estimation.     Must be True if the user do not enter the pre-estimated parameters.     Note that we define a specific set of noise regressors. calculate_err : bool, default=False     Whether to use a ERR algorithm to the pre-defined regressors. eps : float     Normalization factor of the normalized filters.</p>"},{"location":"code/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX--examples","title":"Examples","text":"<p>import numpy as np import matplotlib.pyplot as plt from sysidentpy.simulation import SimulateNARMAX from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.generate_data import get_miso_data, get_siso_data x_train, x_valid, y_train, y_valid = get_siso_data(n=1000, ...                                                    colored_noise=True, ...                                                    sigma=0.2, ...                                                    train_percentage=90) basis_function = Polynomial(degree=2) s = SimulateNARMAX(basis_function=basis_function) model = np.array( ...     [ ...     [1001,    0], # y(k-1) ...     [2001, 1001], # x1(k-1)y(k-1) ...     [2002,    0], # x1(k-2) ...     ] ...                 )</p> Source code in <code>sysidentpy/simulation/_simulation.py</code> <pre><code>class SimulateNARMAX(BaseMSS):\n    r\"\"\"Simulation of Polynomial NARMAX model.\n\n    The NARMAX model is described as:\n\n    $$\n        y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1},\n        \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k\n    $$\n\n    where $n_y\\in \\mathbb{N}^*$, $n_x \\in \\mathbb{N}$, $n_e \\in \\mathbb{N}$,\n    are the maximum lags for the system output and input respectively;\n    $x_k \\in \\mathbb{R}^{n_x}$ is the system input and $y_k \\in \\mathbb{R}^{n_y}$\n    is the system output at discrete time $k \\in \\mathbb{N}^n$;\n    $e_k \\in \\mathbb{R}^{n_e}$ stands for uncertainties and possible noise\n    at discrete time $k$. In this case, $\\mathcal{F}^\\ell$ is some nonlinear function\n    of the input and output regressors with nonlinearity degree $\\ell \\in \\mathbb{N}$\n    and $d$ is a time delay typically set to $d=1$.\n\n    Parameters\n    ----------\n    estimator : str, default=\"least_squares\"\n        The parameter estimation method.\n    estimate_parameter : bool, default=False\n        Whether to use a method for parameter estimation.\n        Must be True if the user do not enter the pre-estimated parameters.\n        Note that we define a specific set of noise regressors.\n    calculate_err : bool, default=False\n        Whether to use a ERR algorithm to the pre-defined regressors.\n    eps : float\n        Normalization factor of the normalized filters.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from sysidentpy.simulation import SimulateNARMAX\n    &gt;&gt;&gt; from sysidentpy.basis_function._basis_function import Polynomial\n    &gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_miso_data, get_siso_data\n    &gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000,\n    ...                                                    colored_noise=True,\n    ...                                                    sigma=0.2,\n    ...                                                    train_percentage=90)\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; s = SimulateNARMAX(basis_function=basis_function)\n    &gt;&gt;&gt; model = np.array(\n    ...     [\n    ...     [1001,    0], # y(k-1)\n    ...     [2001, 1001], # x1(k-1)y(k-1)\n    ...     [2002,    0], # x1(k-2)\n    ...     ]\n    ...                 )\n    &gt;&gt;&gt; # theta must be a numpy array of shape (n, 1) where n\n    ... is the number of regressors\n    &gt;&gt;&gt; theta = np.array([[0.2, 0.9, 0.1]]).T\n    &gt;&gt;&gt; yhat = s.simulate(\n    ...     X_test=x_test,\n    ...     y_test=y_test,\n    ...     model_code=model,\n    ...     theta=theta,\n    ...     )\n    &gt;&gt;&gt; r = pd.DataFrame(\n    ...     results(\n    ...         model.final_model, model.theta, model.err,\n    ...         model.n_terms, err_precision=8, dtype='sci'\n    ...         ),\n    ...     columns=['Regressors', 'Parameters', 'ERR'])\n    &gt;&gt;&gt; print(r)\n        Regressors Parameters         ERR\n    0        x1(k-2)     0.9000       0.0\n    1         y(k-1)     0.1999       0.0\n    2  x1(k-1)y(k-1)     0.1000       0.0\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        estimator: Estimators = RecursiveLeastSquares(),\n        elag: Union[int, list] = 2,\n        estimate_parameter: bool = True,\n        calculate_err: bool = False,\n        model_type: str = \"NARMAX\",\n        basis_function: Union[Polynomial, Fourier] = Polynomial(),\n        eps: np.float64 = np.finfo(np.float64).eps,\n    ):\n        self.elag = elag\n        self.model_type = model_type\n        self.build_matrix = self.get_build_io_method(model_type)\n        self.basis_function = basis_function\n        self.estimator = estimator\n        self.estimate_parameter = estimate_parameter\n        self.calculate_err = calculate_err\n        self.eps = eps\n        self.n_inputs = None\n        self.xlag = None\n        self.ylag = None\n        self.n_terms = None\n        self.err = None\n        self.final_model = None\n        self.theta = None\n        self.pivv = None\n        self.non_degree = None\n        self._validate_simulate_params()\n\n    def _validate_simulate_params(self):\n        if not isinstance(self.estimate_parameter, bool):\n            raise TypeError(\n                \"estimate_parameter must be False or True. Got\"\n                f\" {self.estimate_parameter}\"\n            )\n\n        if not isinstance(self.calculate_err, bool):\n            raise TypeError(\n                f\"calculate_err must be False or True. Got {self.calculate_err}\"\n            )\n\n        if self.basis_function is None:\n            raise TypeError(f\"basis_function can't be. Got {self.basis_function}\")\n\n        if self.model_type not in [\"NARMAX\", \"NAR\", \"NFIR\"]:\n            raise ValueError(\n                f\"model_type must be NARMAX, NAR, or NFIR. Got {self.model_type}\"\n            )\n\n    def _check_simulate_params(self, y_train, y_test, model_code, steps_ahead, theta):\n        if self.basis_function.__class__.__name__ != \"Polynomial\":\n            raise NotImplementedError(\n                \"Currently, SimulateNARMAX only works for polynomial models.\"\n            )\n\n        if y_test is None:\n            raise ValueError(\"y_test cannot be None\")\n\n        if not isinstance(model_code, np.ndarray):\n            raise TypeError(f\"model_code must be an np.np.ndarray. Got {model_code}\")\n\n        if not isinstance(steps_ahead, (int, type(None))):\n            raise ValueError(\n                f\"steps_ahead must be None or integer &gt; zero. Got {steps_ahead}\"\n            )\n\n        if not isinstance(theta, np.ndarray) and not self.estimate_parameter:\n            raise TypeError(\n                \"If estimate_parameter is False, theta must be an np.ndarray. Got\"\n                f\" {theta}\"\n            )\n\n        if self.estimate_parameter:\n            if not all(isinstance(i, np.ndarray) for i in [y_train]):\n                raise TypeError(\n                    \"If estimate_parameter is True, X_train and y_train must be an\"\n                    f\" np.ndarray. Got {type(y_train)}\"\n                )\n\n    def simulate(\n        self,\n        *,\n        X_train=None,\n        y_train=None,\n        X_test=None,\n        y_test=None,\n        model_code=None,\n        steps_ahead=None,\n        theta=None,\n        forecast_horizon=None,\n    ):\n        \"\"\"Simulate a model defined by the user.\n\n        Parameters\n        ----------\n        X_train : array_like\n            The input data to be used in the training process.\n        y_train : array_like\n            The output data to be used in the training process.\n        X_test : array_like\n            The input data to be used in the prediction process.\n        y_test : array_like\n            The output data (initial conditions) to be used in the prediction process.\n        model_code : array_like\n            Flattened list of input or output regressors.\n        steps_ahead : int or None, optional\n            The forecast horizon. Default is None\n        theta : array-like\n            The parameters of the model.\n        forecast_horizon : int or None, optional\n            The forecast horizon used in NARMA models and variants.\n\n        Returns\n        -------\n        yhat : array_like\n            The predicted values of the model.\n\n        \"\"\"\n        self._check_simulate_params(y_train, y_test, model_code, steps_ahead, theta)\n\n        if X_test is not None:\n            self.n_inputs = _num_features(X_test)\n        else:\n            self.n_inputs = 1  # just to create the regressor space base\n\n        xlag_code = self._list_input_regressor_code(model_code)\n        ylag_code = self._list_output_regressor_code(model_code)\n        self.xlag = self._get_lag_from_regressor_code(xlag_code)\n        self.ylag = self._get_lag_from_regressor_code(ylag_code)\n        self.max_lag = max(self.xlag, self.ylag)\n        if self.n_inputs != 1:\n            self.xlag = self.n_inputs * [list(range(1, self.max_lag + 1))]\n\n        # for MetaMSS NAR modelling\n        if self.model_type == \"NAR\" and forecast_horizon is None:\n            forecast_horizon = y_test.shape[0] - self.max_lag\n\n        self.non_degree = model_code.shape[1]\n        regressor_code = self.regressor_space(self.n_inputs)\n\n        self.pivv = self._get_index_from_regressor_code(regressor_code, model_code)\n        self.final_model = regressor_code[self.pivv]\n        # to use in the predict function\n        self.n_terms = self.final_model.shape[0]\n        if self.estimate_parameter and not self.calculate_err:\n            self.max_lag = self._get_max_lag()\n            lagged_data = self.build_matrix(X_train, y_train)\n            psi = self.basis_function.fit(\n                lagged_data, self.max_lag, predefined_regressors=self.pivv\n            )\n\n            self.theta = self.estimator.optimize(\n                psi, y_train[self.max_lag :, 0].reshape(-1, 1)\n            )\n            if self.estimator.unbiased is True:\n                self.theta = self.estimator.unbiased_estimator(\n                    psi,\n                    y_train[self.max_lag :, 0].reshape(-1, 1),\n                    self.theta,\n                    self.elag,\n                    self.max_lag,\n                    self.estimator,\n                    self.basis_function,\n                    self.estimator.uiter,\n                )\n\n            self.err = self.n_terms * [0]\n        elif not self.estimate_parameter:\n            self.theta = theta\n            self.err = self.n_terms * [0]\n        else:\n            self.max_lag = self._get_max_lag()\n            lagged_data = self.build_matrix(X_train, y_train)\n            psi = self.basis_function.fit(\n                lagged_data, self.max_lag, predefined_regressors=self.pivv\n            )\n\n            _, self.err, _, _ = self.error_reduction_ratio(\n                psi, y_train, self.n_terms, self.final_model\n            )\n            self.theta = self.estimator.optimize(\n                psi, y_train[self.max_lag :, 0].reshape(-1, 1)\n            )\n            if self.estimator.unbiased is True:\n                self.theta = self.estimator.unbiased_estimator(\n                    psi,\n                    y_train[self.max_lag :, 0].reshape(-1, 1),\n                    self.theta,\n                    self.elag,\n                    self.max_lag,\n                    self.estimator,\n                    self.basis_function,\n                    self.estimator.uiter,\n                )\n\n        return self.predict(\n            X=X_test,\n            y=y_test,\n            steps_ahead=steps_ahead,\n            forecast_horizon=forecast_horizon,\n        )\n\n    def error_reduction_ratio(self, psi, y, process_term_number, regressor_code):\n        \"\"\"Perform the Error Reduction Ration algorithm.\n\n        Parameters\n        ----------\n        psi : array_like\n            The information matrix of the model.\n        y : array-like\n            The target data used in the identification process.\n        process_term_number : int\n            Number of Process Terms defined by the user.\n        regressor_code : array_like\n            The regressor code list given the xlag and ylag for a MISO model.\n\n        Returns\n        -------\n        model_code : array_like\n            Model defined by the user to simulate.\n        err : array-like\n            The respective ERR calculated for each regressor.\n        piv : array-like\n            Contains the index to put the regressors in the correct order\n            based on err values.\n        psi_orthogonal : array_like\n            The updated and orthogonal information matrix.\n\n        References\n        ----------\n        - Manuscript: Orthogonal least squares methods and their application\n           to non-linear system identification\n           https://eprints.soton.ac.uk/251147/1/778742007_content.pdf\n        - Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares\n           Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o\n           e Novos Resultados\n\n        \"\"\"\n        squared_y = np.dot(y[self.max_lag :].T, y[self.max_lag :])\n        tmp_psi = psi.copy()\n        y = y[self.max_lag :, 0].reshape(-1, 1)\n        tmp_y = y.copy()\n        dimension = tmp_psi.shape[1]\n        piv = np.arange(dimension)\n        tmp_err = np.zeros(dimension)\n        err = np.zeros(dimension)\n\n        for i in np.arange(0, dimension):\n            for j in np.arange(i, dimension):\n                # Add `eps` in the denominator to omit division by zero if\n                # denominator is zero\n                tmp_err[j] = (\n                    (np.dot(tmp_psi[i:, j].T, tmp_y[i:]) ** 2)\n                    / (np.dot(tmp_psi[i:, j].T, tmp_psi[i:, j]) * squared_y + self.eps)\n                )[0, 0]\n\n            if i == process_term_number:\n                break\n\n            piv_index = np.argmax(tmp_err[i:]) + i\n            err[i] = tmp_err[piv_index]\n            tmp_psi[:, [piv_index, i]] = tmp_psi[:, [i, piv_index]]\n            piv[[piv_index, i]] = piv[[i, piv_index]]\n\n            v = Orthogonalization().house(tmp_psi[i:, i])\n\n            row_result = Orthogonalization().rowhouse(tmp_psi[i:, i:], v)\n\n            tmp_y[i:] = Orthogonalization().rowhouse(tmp_y[i:], v)\n\n            tmp_psi[i:, i:] = np.copy(row_result)\n\n        tmp_piv = piv[0:process_term_number]\n        psi_orthogonal = psi[:, tmp_piv]\n        model_code = regressor_code[tmp_piv, :].copy()\n        return model_code, err, piv, psi_orthogonal\n\n    def predict(self, *, X=None, y=None, steps_ahead=None, forecast_horizon=None):\n        \"\"\"Return the predicted values given an input.\n\n        The predict function allows a friendly usage by the user.\n        Given a previously trained model, predict values given\n        a new set of data.\n\n        This method accept y values mainly for prediction n-steps ahead\n        (to be implemented in the future)\n\n        Parameters\n        ----------\n        X : array_like\n            The input data to be used in the prediction process.\n        y : array_like\n            The output data to be used in the prediction process.\n        steps_ahead : int\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction. The default is None\n        forecast_horizon : int\n            The number of predictions over the time. The default is None\n\n        Returns\n        -------\n        yhat : array_like\n            The predicted values of the model.\n\n        \"\"\"\n        if self.basis_function.__class__.__name__ == \"Polynomial\":\n            if steps_ahead is None:\n                yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n            if steps_ahead == 1:\n                yhat = self._one_step_ahead_prediction(X, y)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n\n            _check_positive_int(steps_ahead, \"steps_ahead\")\n            yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        if steps_ahead is None:\n            yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        yhat = self._basis_function_n_step_prediction(\n            X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n        )\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    def _one_step_ahead_prediction(self, X, y):\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        X : array_like of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : array_like\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        lagged_data = self.build_matrix(X, y)\n        X_base = self.basis_function.transform(\n            lagged_data,\n            self.max_lag,\n            predefined_regressors=self.pivv[: len(self.final_model)],\n        )\n\n        yhat = super()._one_step_ahead_prediction(X_base)\n        return yhat.reshape(-1, 1)\n\n    def _n_step_ahead_prediction(self, X, y, steps_ahead):\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        X : array_like of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : array_like\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        yhat = super()._n_step_ahead_prediction(X, y, steps_ahead)\n        return yhat\n\n    def _model_prediction(self, X, y_initial, forecast_horizon=None):\n        \"\"\"Perform the infinity steps-ahead simulation of a model.\n\n        Parameters\n        ----------\n        y_initial : array-like of shape = max_lag\n            Number of initial conditions values of output\n            to start recursive process.\n        X : array_like of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : array_like\n               The predicted values of the model.\n\n        \"\"\"\n        if self.model_type in [\"NARMAX\", \"NAR\"]:\n            return self._narmax_predict(X, y_initial, forecast_horizon)\n        if self.model_type == \"NFIR\":\n            return self._nfir_predict(X, y_initial)\n\n        raise ValueError(\n            f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n        )\n\n    def _narmax_predict(self, X, y_initial, forecast_horizon):\n        if len(y_initial) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if X is not None:\n            forecast_horizon = X.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        y_output = super()._narmax_predict(X, y_initial, forecast_horizon)\n        return y_output\n\n    def _nfir_predict(self, X, y_initial):\n        y_output = super()._nfir_predict(X, y_initial)\n        return y_output\n\n    def _basis_function_predict(self, X, y_initial, forecast_horizon=None):\n        \"\"\"Not implemented.\"\"\"\n        raise NotImplementedError(\n            \"You can only use Polynomial Basis Function in SimulateNARMAX for now.\"\n        )\n\n    def _basis_function_n_step_prediction(self, X, y, steps_ahead, forecast_horizon):\n        \"\"\"Not implemented.\"\"\"\n        raise NotImplementedError(\n            \"You can only use Polynomial Basis Function in SimulateNARMAX for now.\"\n        )\n\n    def _basis_function_n_steps_horizon(self, X, y, steps_ahead, forecast_horizon):\n        \"\"\"Not implemented.\"\"\"\n        raise NotImplementedError(\n            \"You can only use Polynomial Basis Function in SimulateNARMAX for now.\"\n        )\n\n    def fit(self, *, X=None, y=None):\n        \"\"\"Not implemented.\"\"\"\n        raise NotImplementedError(\n            \"There is no fit method in Simulate because the model is predefined.\"\n        )\n</code></pre>"},{"location":"code/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX--theta-must-be-a-numpy-array-of-shape-n-1-where-n","title":"theta must be a numpy array of shape (n, 1) where n","text":"<p>... is the number of regressors theta = np.array([[0.2, 0.9, 0.1]]).T yhat = s.simulate( ...     X_test=x_test, ...     y_test=y_test, ...     model_code=model, ...     theta=theta, ...     ) r = pd.DataFrame( ...     results( ...         model.final_model, model.theta, model.err, ...         model.n_terms, err_precision=8, dtype='sci' ...         ), ...     columns=['Regressors', 'Parameters', 'ERR']) print\u00ae     Regressors Parameters         ERR 0        x1(k-2)     0.9000       0.0 1         y(k-1)     0.1999       0.0 2  x1(k-1)y(k-1)     0.1000       0.0</p>"},{"location":"code/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX.error_reduction_ratio","title":"<code>error_reduction_ratio(psi, y, process_term_number, regressor_code)</code>","text":"<p>Perform the Error Reduction Ration algorithm.</p>"},{"location":"code/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX.error_reduction_ratio--parameters","title":"Parameters","text":"<p>psi : array_like     The information matrix of the model. y : array-like     The target data used in the identification process. process_term_number : int     Number of Process Terms defined by the user. regressor_code : array_like     The regressor code list given the xlag and ylag for a MISO model.</p>"},{"location":"code/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX.error_reduction_ratio--returns","title":"Returns","text":"<p>model_code : array_like     Model defined by the user to simulate. err : array-like     The respective ERR calculated for each regressor. piv : array-like     Contains the index to put the regressors in the correct order     based on err values. psi_orthogonal : array_like     The updated and orthogonal information matrix.</p>"},{"location":"code/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX.error_reduction_ratio--references","title":"References","text":"<ul> <li>Manuscript: Orthogonal least squares methods and their application    to non-linear system identification    https://eprints.soton.ac.uk/251147/1/778742007_content.pdf</li> <li>Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares    Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o    e Novos Resultados</li> </ul> Source code in <code>sysidentpy/simulation/_simulation.py</code> <pre><code>def error_reduction_ratio(self, psi, y, process_term_number, regressor_code):\n    \"\"\"Perform the Error Reduction Ration algorithm.\n\n    Parameters\n    ----------\n    psi : array_like\n        The information matrix of the model.\n    y : array-like\n        The target data used in the identification process.\n    process_term_number : int\n        Number of Process Terms defined by the user.\n    regressor_code : array_like\n        The regressor code list given the xlag and ylag for a MISO model.\n\n    Returns\n    -------\n    model_code : array_like\n        Model defined by the user to simulate.\n    err : array-like\n        The respective ERR calculated for each regressor.\n    piv : array-like\n        Contains the index to put the regressors in the correct order\n        based on err values.\n    psi_orthogonal : array_like\n        The updated and orthogonal information matrix.\n\n    References\n    ----------\n    - Manuscript: Orthogonal least squares methods and their application\n       to non-linear system identification\n       https://eprints.soton.ac.uk/251147/1/778742007_content.pdf\n    - Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares\n       Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o\n       e Novos Resultados\n\n    \"\"\"\n    squared_y = np.dot(y[self.max_lag :].T, y[self.max_lag :])\n    tmp_psi = psi.copy()\n    y = y[self.max_lag :, 0].reshape(-1, 1)\n    tmp_y = y.copy()\n    dimension = tmp_psi.shape[1]\n    piv = np.arange(dimension)\n    tmp_err = np.zeros(dimension)\n    err = np.zeros(dimension)\n\n    for i in np.arange(0, dimension):\n        for j in np.arange(i, dimension):\n            # Add `eps` in the denominator to omit division by zero if\n            # denominator is zero\n            tmp_err[j] = (\n                (np.dot(tmp_psi[i:, j].T, tmp_y[i:]) ** 2)\n                / (np.dot(tmp_psi[i:, j].T, tmp_psi[i:, j]) * squared_y + self.eps)\n            )[0, 0]\n\n        if i == process_term_number:\n            break\n\n        piv_index = np.argmax(tmp_err[i:]) + i\n        err[i] = tmp_err[piv_index]\n        tmp_psi[:, [piv_index, i]] = tmp_psi[:, [i, piv_index]]\n        piv[[piv_index, i]] = piv[[i, piv_index]]\n\n        v = Orthogonalization().house(tmp_psi[i:, i])\n\n        row_result = Orthogonalization().rowhouse(tmp_psi[i:, i:], v)\n\n        tmp_y[i:] = Orthogonalization().rowhouse(tmp_y[i:], v)\n\n        tmp_psi[i:, i:] = np.copy(row_result)\n\n    tmp_piv = piv[0:process_term_number]\n    psi_orthogonal = psi[:, tmp_piv]\n    model_code = regressor_code[tmp_piv, :].copy()\n    return model_code, err, piv, psi_orthogonal\n</code></pre>"},{"location":"code/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX.fit","title":"<code>fit(*, X=None, y=None)</code>","text":"<p>Not implemented.</p> Source code in <code>sysidentpy/simulation/_simulation.py</code> <pre><code>def fit(self, *, X=None, y=None):\n    \"\"\"Not implemented.\"\"\"\n    raise NotImplementedError(\n        \"There is no fit method in Simulate because the model is predefined.\"\n    )\n</code></pre>"},{"location":"code/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX.predict","title":"<code>predict(*, X=None, y=None, steps_ahead=None, forecast_horizon=None)</code>","text":"<p>Return the predicted values given an input.</p> <p>The predict function allows a friendly usage by the user. Given a previously trained model, predict values given a new set of data.</p> <p>This method accept y values mainly for prediction n-steps ahead (to be implemented in the future)</p>"},{"location":"code/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX.predict--parameters","title":"Parameters","text":"<p>X : array_like     The input data to be used in the prediction process. y : array_like     The output data to be used in the prediction process. steps_ahead : int     The user can use free run simulation, one-step ahead prediction     and n-step ahead prediction. The default is None forecast_horizon : int     The number of predictions over the time. The default is None</p>"},{"location":"code/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX.predict--returns","title":"Returns","text":"<p>yhat : array_like     The predicted values of the model.</p> Source code in <code>sysidentpy/simulation/_simulation.py</code> <pre><code>def predict(self, *, X=None, y=None, steps_ahead=None, forecast_horizon=None):\n    \"\"\"Return the predicted values given an input.\n\n    The predict function allows a friendly usage by the user.\n    Given a previously trained model, predict values given\n    a new set of data.\n\n    This method accept y values mainly for prediction n-steps ahead\n    (to be implemented in the future)\n\n    Parameters\n    ----------\n    X : array_like\n        The input data to be used in the prediction process.\n    y : array_like\n        The output data to be used in the prediction process.\n    steps_ahead : int\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction. The default is None\n    forecast_horizon : int\n        The number of predictions over the time. The default is None\n\n    Returns\n    -------\n    yhat : array_like\n        The predicted values of the model.\n\n    \"\"\"\n    if self.basis_function.__class__.__name__ == \"Polynomial\":\n        if steps_ahead is None:\n            yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        _check_positive_int(steps_ahead, \"steps_ahead\")\n        yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    if steps_ahead is None:\n        yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n    if steps_ahead == 1:\n        yhat = self._one_step_ahead_prediction(X, y)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    yhat = self._basis_function_n_step_prediction(\n        X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n    )\n    yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n    return yhat\n</code></pre>"},{"location":"code/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX.simulate","title":"<code>simulate(*, X_train=None, y_train=None, X_test=None, y_test=None, model_code=None, steps_ahead=None, theta=None, forecast_horizon=None)</code>","text":"<p>Simulate a model defined by the user.</p>"},{"location":"code/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX.simulate--parameters","title":"Parameters","text":"<p>X_train : array_like     The input data to be used in the training process. y_train : array_like     The output data to be used in the training process. X_test : array_like     The input data to be used in the prediction process. y_test : array_like     The output data (initial conditions) to be used in the prediction process. model_code : array_like     Flattened list of input or output regressors. steps_ahead : int or None, optional     The forecast horizon. Default is None theta : array-like     The parameters of the model. forecast_horizon : int or None, optional     The forecast horizon used in NARMA models and variants.</p>"},{"location":"code/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX.simulate--returns","title":"Returns","text":"<p>yhat : array_like     The predicted values of the model.</p> Source code in <code>sysidentpy/simulation/_simulation.py</code> <pre><code>def simulate(\n    self,\n    *,\n    X_train=None,\n    y_train=None,\n    X_test=None,\n    y_test=None,\n    model_code=None,\n    steps_ahead=None,\n    theta=None,\n    forecast_horizon=None,\n):\n    \"\"\"Simulate a model defined by the user.\n\n    Parameters\n    ----------\n    X_train : array_like\n        The input data to be used in the training process.\n    y_train : array_like\n        The output data to be used in the training process.\n    X_test : array_like\n        The input data to be used in the prediction process.\n    y_test : array_like\n        The output data (initial conditions) to be used in the prediction process.\n    model_code : array_like\n        Flattened list of input or output regressors.\n    steps_ahead : int or None, optional\n        The forecast horizon. Default is None\n    theta : array-like\n        The parameters of the model.\n    forecast_horizon : int or None, optional\n        The forecast horizon used in NARMA models and variants.\n\n    Returns\n    -------\n    yhat : array_like\n        The predicted values of the model.\n\n    \"\"\"\n    self._check_simulate_params(y_train, y_test, model_code, steps_ahead, theta)\n\n    if X_test is not None:\n        self.n_inputs = _num_features(X_test)\n    else:\n        self.n_inputs = 1  # just to create the regressor space base\n\n    xlag_code = self._list_input_regressor_code(model_code)\n    ylag_code = self._list_output_regressor_code(model_code)\n    self.xlag = self._get_lag_from_regressor_code(xlag_code)\n    self.ylag = self._get_lag_from_regressor_code(ylag_code)\n    self.max_lag = max(self.xlag, self.ylag)\n    if self.n_inputs != 1:\n        self.xlag = self.n_inputs * [list(range(1, self.max_lag + 1))]\n\n    # for MetaMSS NAR modelling\n    if self.model_type == \"NAR\" and forecast_horizon is None:\n        forecast_horizon = y_test.shape[0] - self.max_lag\n\n    self.non_degree = model_code.shape[1]\n    regressor_code = self.regressor_space(self.n_inputs)\n\n    self.pivv = self._get_index_from_regressor_code(regressor_code, model_code)\n    self.final_model = regressor_code[self.pivv]\n    # to use in the predict function\n    self.n_terms = self.final_model.shape[0]\n    if self.estimate_parameter and not self.calculate_err:\n        self.max_lag = self._get_max_lag()\n        lagged_data = self.build_matrix(X_train, y_train)\n        psi = self.basis_function.fit(\n            lagged_data, self.max_lag, predefined_regressors=self.pivv\n        )\n\n        self.theta = self.estimator.optimize(\n            psi, y_train[self.max_lag :, 0].reshape(-1, 1)\n        )\n        if self.estimator.unbiased is True:\n            self.theta = self.estimator.unbiased_estimator(\n                psi,\n                y_train[self.max_lag :, 0].reshape(-1, 1),\n                self.theta,\n                self.elag,\n                self.max_lag,\n                self.estimator,\n                self.basis_function,\n                self.estimator.uiter,\n            )\n\n        self.err = self.n_terms * [0]\n    elif not self.estimate_parameter:\n        self.theta = theta\n        self.err = self.n_terms * [0]\n    else:\n        self.max_lag = self._get_max_lag()\n        lagged_data = self.build_matrix(X_train, y_train)\n        psi = self.basis_function.fit(\n            lagged_data, self.max_lag, predefined_regressors=self.pivv\n        )\n\n        _, self.err, _, _ = self.error_reduction_ratio(\n            psi, y_train, self.n_terms, self.final_model\n        )\n        self.theta = self.estimator.optimize(\n            psi, y_train[self.max_lag :, 0].reshape(-1, 1)\n        )\n        if self.estimator.unbiased is True:\n            self.theta = self.estimator.unbiased_estimator(\n                psi,\n                y_train[self.max_lag :, 0].reshape(-1, 1),\n                self.theta,\n                self.elag,\n                self.max_lag,\n                self.estimator,\n                self.basis_function,\n                self.estimator.uiter,\n            )\n\n    return self.predict(\n        X=X_test,\n        y=y_test,\n        steps_ahead=steps_ahead,\n        forecast_horizon=forecast_horizon,\n    )\n</code></pre>"},{"location":"code/utils/","title":"Documentation for <code>Neural NARX</code>","text":"<p>Utilities fo data validation.</p> <p>Display results formatted for the user.</p> <p>Utilities for data generation.</p> <p>Utils methods for NARMAX modeling.</p> <p>Plotting methods.</p>"},{"location":"code/utils/#sysidentpy.utils._check_arrays.check_X_y","title":"<code>check_X_y(X, y)</code>","text":"<p>Validate input and output data using some crucial tests.</p>"},{"location":"code/utils/#sysidentpy.utils._check_arrays.check_X_y--parameters","title":"Parameters","text":"<p>X : ndarray of floats     The input data. y : ndarray of floats     The output data.</p> Source code in <code>sysidentpy/utils/_check_arrays.py</code> <pre><code>def check_X_y(X, y):\n    \"\"\"Validate input and output data using some crucial tests.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data.\n    y : ndarray of floats\n        The output data.\n\n    \"\"\"\n    check_length(X, y)\n    check_dimension(X, y)\n    check_infinity(X, y)\n    check_nan(X, y)\n</code></pre>"},{"location":"code/utils/#sysidentpy.utils._check_arrays.check_dimension","title":"<code>check_dimension(X, y)</code>","text":"<p>Check if X and y have only real values.</p> <p>If there is any string or object samples a ValueError is raised.</p>"},{"location":"code/utils/#sysidentpy.utils._check_arrays.check_dimension--parameters","title":"Parameters","text":"<p>X : ndarray of floats     The input data. y : ndarray of floats     The output data.</p> Source code in <code>sysidentpy/utils/_check_arrays.py</code> <pre><code>def check_dimension(X, y):\n    \"\"\"Check if X and y have only real values.\n\n    If there is any string or object samples a ValueError is raised.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data.\n    y : ndarray of floats\n        The output data.\n\n    \"\"\"\n    if X.ndim == 0:\n        raise ValueError(\n            \"Input must be a 2d array, got scalar instead. Reshape your data using\"\n            \" array.reshape(-1, 1)\"\n        )\n\n    if X.ndim == 1:\n        raise ValueError(\n            \"Input must be a 2d array, got 1d array instead. \"\n            \"Reshape your data using array.reshape(-1, 1)\"\n        )\n\n    if y.ndim == 0:\n        raise ValueError(\n            \"Output must be a 2d array, got scalar instead. \"\n            \"Reshape your data using array.reshape(-1, 1)\"\n        )\n\n    if y.ndim == 1:\n        raise ValueError(\n            \"Output must be a 2d array, got 1d array instead. \"\n            \"Reshape your data using array.reshape(-1, 1)\"\n        )\n</code></pre>"},{"location":"code/utils/#sysidentpy.utils._check_arrays.check_infinity","title":"<code>check_infinity(X, y)</code>","text":"<p>Check that X and y have no NaN or Inf samples.</p> <p>If there is any NaN or Inf samples a ValueError is raised.</p>"},{"location":"code/utils/#sysidentpy.utils._check_arrays.check_infinity--parameters","title":"Parameters","text":"<p>X : ndarray of floats     The input data. y : ndarray of floats     The output data.</p> Source code in <code>sysidentpy/utils/_check_arrays.py</code> <pre><code>def check_infinity(X, y):\n    \"\"\"Check that X and y have no NaN or Inf samples.\n\n    If there is any NaN or Inf samples a ValueError is raised.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data.\n    y : ndarray of floats\n        The output data.\n\n    \"\"\"\n    if np.isinf(X).any():\n        msg_error = (\n            \"Input contains invalid values (e.g. NaN, Inf) on \"\n            f\"index {np.argwhere(np.isinf(X))}\"\n        )\n        raise ValueError(msg_error)\n\n    if np.isinf(y).any():\n        msg_error = (\n            \"Output contains invalid values (e.g Inf) on \"\n            f\"index {np.argwhere(np.isinf(y))}\"\n        )\n        raise ValueError(msg_error)\n</code></pre>"},{"location":"code/utils/#sysidentpy.utils._check_arrays.check_length","title":"<code>check_length(X, y)</code>","text":"<p>Check that X and y have the same number of samples.</p> <p>If the length of X and y are different a ValueError is raised.</p>"},{"location":"code/utils/#sysidentpy.utils._check_arrays.check_length--parameters","title":"Parameters","text":"<p>X : ndarray of floats     The input data. y : ndarray of floats     The output data.</p> Source code in <code>sysidentpy/utils/_check_arrays.py</code> <pre><code>def check_length(X, y):\n    \"\"\"Check that X and y have the same number of samples.\n\n    If the length of X and y are different a ValueError is raised.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data.\n    y : ndarray of floats\n        The output data.\n\n    \"\"\"\n    if X.shape[0] != y.shape[0]:\n        msg_error = (\n            \"Input and output data must have the same number of \"\n            f\"samples. X has dimension {X.shape} and \"\n            f\"y has dimension {y.shape}\"\n        )\n        raise ValueError(msg_error)\n</code></pre>"},{"location":"code/utils/#sysidentpy.utils._check_arrays.check_nan","title":"<code>check_nan(X, y)</code>","text":"<p>Check that X and y have no NaN or Inf samples.</p> <p>If there is any NaN or Inf samples a ValueError is raised.</p>"},{"location":"code/utils/#sysidentpy.utils._check_arrays.check_nan--parameters","title":"Parameters","text":"<p>X : ndarray of floats     The input data. y : ndarray of floats     The output data.</p> Source code in <code>sysidentpy/utils/_check_arrays.py</code> <pre><code>def check_nan(X, y):\n    \"\"\"Check that X and y have no NaN or Inf samples.\n\n    If there is any NaN or Inf samples a ValueError is raised.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data.\n    y : ndarray of floats\n        The output data.\n\n    \"\"\"\n    if np.isnan(X).any():\n        msg_error = (\n            \"Input contains invalid values (e.g. NaN, Inf) on \"\n            f\"index {np.argwhere(np.isnan(X))}\"\n        )\n        raise ValueError(msg_error)\n\n    if not ~np.isnan(y).any():\n        msg_error = (\n            \"Output contains invalid values (e.g. NaN, Inf) on \"\n            f\"index {np.argwhere(np.isnan(y))}\"\n        )\n        raise ValueError(msg_error)\n</code></pre>"},{"location":"code/utils/#sysidentpy.utils._check_arrays.check_random_state","title":"<code>check_random_state(seed)</code>","text":"<p>Turn <code>seed</code> into a <code>np.random.RandomState</code> instance.</p>"},{"location":"code/utils/#sysidentpy.utils._check_arrays.check_random_state--parameters","title":"Parameters","text":"<p>seed : {None, int, <code>numpy.random.Generator</code>,         <code>numpy.random.RandomState</code>}, optional     If <code>seed</code> is None (or <code>np.random</code>), the <code>numpy.random.RandomState</code>     singleton is used.     If <code>seed</code> is an int, a new <code>RandomState</code> instance is used,     seeded with <code>seed</code>.     If <code>seed</code> is already a <code>Generator</code> or <code>RandomState</code> instance then     that instance is used.</p>"},{"location":"code/utils/#sysidentpy.utils._check_arrays.check_random_state--returns","title":"Returns","text":"<p>seed : {<code>numpy.random.Generator</code>, <code>numpy.random.RandomState</code>}     Random number generator.</p> Source code in <code>sysidentpy/utils/_check_arrays.py</code> <pre><code>def check_random_state(seed):\n    \"\"\"Turn `seed` into a `np.random.RandomState` instance.\n\n    Parameters\n    ----------\n    seed : {None, int, `numpy.random.Generator`,\n            `numpy.random.RandomState`}, optional\n        If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n        singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used,\n        seeded with `seed`.\n        If `seed` is already a ``Generator`` or ``RandomState`` instance then\n        that instance is used.\n\n    Returns\n    -------\n    seed : {`numpy.random.Generator`, `numpy.random.RandomState`}\n        Random number generator.\n\n    \"\"\"\n    if seed is None or seed is np.random:\n        return np.random.mtrand._rand\n    if isinstance(seed, (numbers.Integral, np.integer)):\n        return np.random.default_rng(seed)\n    if isinstance(seed, (np.random.RandomState, np.random.Generator)):\n        return seed\n\n    raise ValueError(\n        \"%r cannot be used to seed a numpy.random.RandomState instance\" % seed\n    )\n</code></pre>"},{"location":"code/utils/#sysidentpy.utils.display_results.results","title":"<code>results(final_model=None, theta=None, err=None, n_terms=None, theta_precision=4, err_precision=8, dtype='dec')</code>","text":"<p>Write the model regressors, parameters and ERR values.</p> <p>This function returns the model regressors, its respective parameter and ERR value on a string matrix.</p>"},{"location":"code/utils/#sysidentpy.utils.display_results.results--parameters","title":"Parameters","text":"<p>theta_precision : int (default: 4)     Precision of shown parameters values. err_precision : int (default: 8)     Precision of shown ERR values. dtype : string (default: 'dec')     Type of representation:     sci - Scientific notation;     dec - Decimal notation.</p>"},{"location":"code/utils/#sysidentpy.utils.display_results.results--returns","title":"Returns","text":"<p>output_matrix : string     Where:         First column represents each regressor element;         Second column represents associated parameter;         Third column represents the error reduction ratio associated         to each regressor.</p> Source code in <code>sysidentpy/utils/display_results.py</code> <pre><code>def results(\n    final_model=None,\n    theta=None,\n    err=None,\n    n_terms=None,\n    theta_precision=4,\n    err_precision=8,\n    dtype=\"dec\",\n):\n    \"\"\"Write the model regressors, parameters and ERR values.\n\n    This function returns the model regressors, its respective parameter\n    and ERR value on a string matrix.\n\n    Parameters\n    ----------\n    theta_precision : int (default: 4)\n        Precision of shown parameters values.\n    err_precision : int (default: 8)\n        Precision of shown ERR values.\n    dtype : string (default: 'dec')\n        Type of representation:\n        sci - Scientific notation;\n        dec - Decimal notation.\n\n    Returns\n    -------\n    output_matrix : string\n        Where:\n            First column represents each regressor element;\n            Second column represents associated parameter;\n            Third column represents the error reduction ratio associated\n            to each regressor.\n\n    \"\"\"\n    if not isinstance(theta_precision, int) or theta_precision &lt; 1:\n        raise ValueError(\n            f\"theta_precision must be integer and &gt; zero. Got {theta_precision}.\"\n        )\n\n    if not isinstance(err_precision, int) or err_precision &lt; 1:\n        raise ValueError(\n            f\"err_precision must be integer and &gt; zero. Got {err_precision}.\"\n        )\n\n    if dtype not in (\"dec\", \"sci\"):\n        raise ValueError(f\"dtype must be dec or sci. Got {dtype}.\")\n\n    output_matrix = []\n    theta_output_format = \"{:.\" + str(theta_precision)\n    err_output_format = \"{:.\" + str(err_precision)\n\n    if dtype == \"dec\":\n        theta_output_format = theta_output_format + \"f}\"\n        err_output_format = err_output_format + \"f}\"\n    else:\n        theta_output_format = theta_output_format + \"E}\"\n        err_output_format = err_output_format + \"E}\"\n\n    for i in range(n_terms):\n        if np.max(final_model[i]) &lt; 1:\n            tmp_regressor = str(1)\n        else:\n            regressor_dic = Counter(final_model[i])\n            regressor_string = []\n            for j in range(len(list(regressor_dic.keys()))):\n                regressor_key = list(regressor_dic.keys())[j]\n                if regressor_key &lt; 1:\n                    translated_key = \"\"\n                    translated_exponent = \"\"\n                else:\n                    delay_string = str(\n                        int(regressor_key - np.floor(regressor_key / 1000) * 1000)\n                    )\n                    if int(regressor_key / 1000) &lt; 2:\n                        translated_key = \"y(k-\" + delay_string + \")\"\n                    else:\n                        translated_key = (\n                            \"x\"\n                            + str(int(regressor_key / 1000) - 1)\n                            + \"(k-\"\n                            + delay_string\n                            + \")\"\n                        )\n                    if regressor_dic[regressor_key] &lt; 2:\n                        translated_exponent = \"\"\n                    else:\n                        translated_exponent = \"^\" + str(regressor_dic[regressor_key])\n                regressor_string.append(translated_key + translated_exponent)\n            tmp_regressor = \"\".join(regressor_string)\n\n        current_parameter = theta_output_format.format(theta[i, 0])\n        current_err = err_output_format.format(err[i])\n        current_output = [tmp_regressor, current_parameter, current_err]\n        output_matrix.append(current_output)\n\n    return output_matrix\n</code></pre>"},{"location":"code/utils/#sysidentpy.utils.generate_data.get_miso_data","title":"<code>get_miso_data(n=5000, colored_noise=False, sigma=0.05, train_percentage=90)</code>","text":"<p>Perform the Error Reduction Ration algorithm.</p>"},{"location":"code/utils/#sysidentpy.utils.generate_data.get_miso_data--parameters","title":"Parameters","text":"<p>n : int     The number of samples. colored_noise : bool     Select white noise or colored noise (autoregressive noise). sigma : float     The standard deviation of the random distribution to generate     the noise. train_percentage : int     The percentage of the data to be used as train data.</p>"},{"location":"code/utils/#sysidentpy.utils.generate_data.get_miso_data--returns","title":"Returns","text":"<p>x_train, x_valid : array-like     The input data to be used in identification and validation,     respectively. y_train, y_valid : array-like     The output data to be used in identification and validation,     respectively.</p> Source code in <code>sysidentpy/utils/generate_data.py</code> <pre><code>def get_miso_data(n=5000, colored_noise=False, sigma=0.05, train_percentage=90):\n    \"\"\"Perform the Error Reduction Ration algorithm.\n\n    Parameters\n    ----------\n    n : int\n        The number of samples.\n    colored_noise : bool\n        Select white noise or colored noise (autoregressive noise).\n    sigma : float\n        The standard deviation of the random distribution to generate\n        the noise.\n    train_percentage : int\n        The percentage of the data to be used as train data.\n\n    Returns\n    -------\n    x_train, x_valid : array-like\n        The input data to be used in identification and validation,\n        respectively.\n    y_train, y_valid : array-like\n        The output data to be used in identification and validation,\n        respectively.\n\n    \"\"\"\n    mu = 0  # mean of the distribution\n    nu = np.random.normal(mu, sigma, n).T\n    e = np.zeros((n, 1))\n\n    lag = 2\n    if colored_noise is True:\n        for k in range(lag, len(e)):\n            e[k] = 0.8 * nu[k - 1] + nu[k]\n    else:\n        e = nu\n\n    x1 = np.random.uniform(-1, 1, n).T\n    x2 = np.random.uniform(-1, 1, n).T\n    y = np.zeros((n, 1))\n    theta = np.array([[0.4], [0.1], [0.6], [-0.3]])\n\n    lag = 2\n    for k in range(lag, len(e)):\n        y[k] = (\n            theta[0] * y[k - 1] ** 2\n            + theta[1] * y[k - 1] * x1[k - 1]\n            + theta[2] * x2[k - 1]\n            + theta[3] * x1[k - 1] * x2[k - 2]\n            + e[k]\n        )\n\n    split_data = int(len(x1) * (train_percentage / 100))\n    x1_train = x1[0:split_data].reshape(-1, 1)\n    x2_train = x2[0:split_data].reshape(-1, 1)\n    x1_valid = x1[split_data::].reshape(-1, 1)\n    x2_valid = x2[split_data::].reshape(-1, 1)\n\n    x_train = np.hstack([x1_train, x2_train])\n    x_valid = np.hstack([x1_valid, x2_valid])\n\n    y_train = y[0:split_data].reshape(-1, 1)\n    y_valid = y[split_data::].reshape(-1, 1)\n\n    return x_train, x_valid, y_train, y_valid\n</code></pre>"},{"location":"code/utils/#sysidentpy.utils.generate_data.get_siso_data","title":"<code>get_siso_data(n=5000, colored_noise=False, sigma=0.05, train_percentage=90)</code>","text":"<p>Perform the Error Reduction Ration algorithm.</p>"},{"location":"code/utils/#sysidentpy.utils.generate_data.get_siso_data--parameters","title":"Parameters","text":"<p>n : int     The number of samples. colored_noise : bool     Select white noise or colored noise (autoregressive noise). sigma : float     The standard deviation of the random distribution to generate     the noise. train_percentage : int     The percentage of the data to be used as train data.</p>"},{"location":"code/utils/#sysidentpy.utils.generate_data.get_siso_data--returns","title":"Returns","text":"<p>x_train, x_valid : array-like     The input data to be used in identification and validation,     respectively. y_train, y_valid : array-like     The output data to be used in identification and validation,     respectively.</p> Source code in <code>sysidentpy/utils/generate_data.py</code> <pre><code>def get_siso_data(n=5000, colored_noise=False, sigma=0.05, train_percentage=90):\n    \"\"\"Perform the Error Reduction Ration algorithm.\n\n    Parameters\n    ----------\n    n : int\n        The number of samples.\n    colored_noise : bool\n        Select white noise or colored noise (autoregressive noise).\n    sigma : float\n        The standard deviation of the random distribution to generate\n        the noise.\n    train_percentage : int\n        The percentage of the data to be used as train data.\n\n    Returns\n    -------\n    x_train, x_valid : array-like\n        The input data to be used in identification and validation,\n        respectively.\n    y_train, y_valid : array-like\n        The output data to be used in identification and validation,\n        respectively.\n\n    \"\"\"\n    mu = 0  # mean of the distribution\n    nu = np.random.normal(mu, sigma, n).T\n    e = np.zeros((n, 1))\n\n    lag = 2\n    if colored_noise is True:\n        for k in range(lag, len(e)):\n            e[k] = 0.8 * nu[k - 1] + nu[k]\n    else:\n        e = nu\n\n    x = np.random.uniform(-1, 1, n).T\n    y = np.zeros((n, 1))\n    theta = np.array([[0.2], [0.1], [0.9]])\n    lag = 2\n    for k in range(lag, len(x)):\n        y[k] = (\n            theta[0] * y[k - 1]\n            + theta[1] * y[k - 1] * x[k - 1]\n            + theta[2] * x[k - 2]\n            + e[k]\n        )\n\n    split_data = int(len(x) * (train_percentage / 100))\n\n    x_train = x[0:split_data].reshape(-1, 1)\n    x_valid = x[split_data::].reshape(-1, 1)\n\n    y_train = y[0:split_data].reshape(-1, 1)\n    y_valid = y[split_data::].reshape(-1, 1)\n\n    return x_train, x_valid, y_train, y_valid\n</code></pre>"},{"location":"code/utils/#sysidentpy.utils.narmax_tools.regressor_code","title":"<code>regressor_code(*, X=None, xlag=2, ylag=2, model_type='NARMAX', model_representation=None, basis_function=None)</code>","text":"<p>Generate a regressor code based on the provided parameters.</p>"},{"location":"code/utils/#sysidentpy.utils.narmax_tools.regressor_code--parameters","title":"Parameters","text":"<p>X : np.ndarray, optional     The input feature matrix. xlag : int, optional     The number of lags for the input features. ylag : int, optional     The number of lags for the target variable. model_type : str, optional     The type of model to be used. Default is \"NARMAX\". model_representation : str, optional     The model representation to be used. basis_function : object, optional     The basis function object used to transform the regressor space.</p>"},{"location":"code/utils/#sysidentpy.utils.narmax_tools.regressor_code--returns","title":"Returns","text":"<p>encoding : np.ndarray     The generated regressor encoding.</p> Source code in <code>sysidentpy/utils/narmax_tools.py</code> <pre><code>def regressor_code(\n    *,\n    X: Optional[np.ndarray] = None,\n    xlag: int = 2,\n    ylag: int = 2,\n    model_type: str = \"NARMAX\",\n    model_representation: Optional[str] = None,\n    basis_function: Optional[Any] = None,\n) -&gt; np.ndarray:\n    \"\"\"Generate a regressor code based on the provided parameters.\n\n    Parameters\n    ----------\n    X : np.ndarray, optional\n        The input feature matrix.\n    xlag : int, optional\n        The number of lags for the input features.\n    ylag : int, optional\n        The number of lags for the target variable.\n    model_type : str, optional\n        The type of model to be used. Default is \"NARMAX\".\n    model_representation : str, optional\n        The model representation to be used.\n    basis_function : object, optional\n        The basis function object used to transform the regressor space.\n\n    Returns\n    -------\n    encoding : np.ndarray\n        The generated regressor encoding.\n    \"\"\"\n    if X is not None:\n        n_inputs = _num_features(X)\n    else:\n        n_inputs = 1  # only used to create the regressor space base\n\n    encoding = RegressorDictionary(\n        xlag=xlag, ylag=ylag, model_type=model_type, basis_function=basis_function\n    ).regressor_space(n_inputs)\n\n    basis_name = basis_function.__class__.__name__\n    if basis_name != \"Polynomial\" and basis_function.ensemble:\n        repetition = basis_function.n * 2\n        basis_code = np.sort(\n            np.tile(encoding[1:, :], (repetition, 1)),\n            axis=0,\n        )\n        encoding = np.concatenate([encoding[1:], basis_code])\n    elif basis_name != \"Polynomial\" and basis_function.ensemble is False:\n        repetition = basis_function.n * 2\n        encoding = np.sort(\n            np.tile(encoding[1:, :], (repetition, 1)),\n            axis=0,\n        )\n\n    if basis_name == \"Polynomial\" and model_representation == \"neural_network\":\n        return encoding[1:]\n    if basis_name == \"Polynomial\" and model_representation is None:\n        return encoding\n\n    return encoding\n</code></pre>"},{"location":"code/utils/#sysidentpy.utils.narmax_tools.set_weights","title":"<code>set_weights(*, static_function=True, static_gain=True, start=-0.01, stop=-5, num=50, base=2.71)</code>","text":"<p>Set log-spaced weights assigned to each objective in the MO optimization.</p>"},{"location":"code/utils/#sysidentpy.utils.narmax_tools.set_weights--parameters","title":"Parameters","text":"<p>static_function : bool, optional     Indicator for the presence of static function data. Default is True. static_gain : bool, optional     Indicator for the presence of static gain data. Default is True. start : float, optional     The starting exponent for the log-spaced weights. Default is -0.01. stop : float, optional     The stopping exponent for the log-spaced weights. Default is -5. num : int, optional     The number of weights to generate. Default is 50. base : float, optional     The base of the logarithm used to generate weights. Default is 2.71.</p>"},{"location":"code/utils/#sysidentpy.utils.narmax_tools.set_weights--returns","title":"Returns","text":"<p>weights : ndarray of floats     An array containing the weights for each objective.</p>"},{"location":"code/utils/#sysidentpy.utils.narmax_tools.set_weights--notes","title":"Notes","text":"<p>This method calculates the weights to be assigned to different objectives in multi-objective optimization. The choice of weights depends on the presence of static function and static gain data. If both are present, a set of weights for dynamic, gain, and static objectives is computed. If either static function or static gain is absent, a simplified set of weights is generated.</p> Source code in <code>sysidentpy/utils/narmax_tools.py</code> <pre><code>def set_weights(\n    *,\n    static_function: bool = True,\n    static_gain: bool = True,\n    start: float = -0.01,\n    stop: float = -5,\n    num: int = 50,\n    base: float = 2.71,\n) -&gt; np.ndarray:\n    \"\"\"Set log-spaced weights assigned to each objective in the MO optimization.\n\n    Parameters\n    ----------\n    static_function : bool, optional\n        Indicator for the presence of static function data. Default is True.\n    static_gain : bool, optional\n        Indicator for the presence of static gain data. Default is True.\n    start : float, optional\n        The starting exponent for the log-spaced weights. Default is -0.01.\n    stop : float, optional\n        The stopping exponent for the log-spaced weights. Default is -5.\n    num : int, optional\n        The number of weights to generate. Default is 50.\n    base : float, optional\n        The base of the logarithm used to generate weights. Default is 2.71.\n\n    Returns\n    -------\n    weights : ndarray of floats\n        An array containing the weights for each objective.\n\n    Notes\n    -----\n    This method calculates the weights to be assigned to different objectives in\n    multi-objective optimization. The choice of weights depends on the presence\n    of static function and static gain data. If both are present, a set of weights\n    for dynamic, gain, and static objectives is computed. If either static function\n    or static gain is absent, a simplified set of weights is generated.\n\n    \"\"\"\n    w1 = np.logspace(start=start, stop=stop, num=num, base=base)\n    if static_function is False or static_gain is False:\n        w2 = 1 - w1\n        return np.vstack([w1, w2])\n\n    w2 = w1[::-1]\n    w1_grid, w2_grid = np.meshgrid(w1, w2)\n    w3_grid = 1 - (w1_grid + w2_grid)\n    mask = w1_grid + w2_grid &lt;= 1\n    dynamic_weight = np.flip(w1_grid[mask])\n    gain_weight = np.flip(w2_grid[mask])\n    static_weight = np.flip(w3_grid[mask])\n    return np.vstack([dynamic_weight, gain_weight, static_weight])\n</code></pre>"},{"location":"code/utils/#sysidentpy.utils.narmax_tools.train_test_split","title":"<code>train_test_split(X, y, test_size=0.25)</code>","text":"<p>Split the time series dataset into training and testing sets.</p>"},{"location":"code/utils/#sysidentpy.utils.narmax_tools.train_test_split--parameters","title":"Parameters","text":"<p>X : np.ndarray, optional     The feature matrix. Can be None if there are no features. y : np.ndarray     The target vector. test_size : float, optional     The proportion of the dataset to include in the test split. Default is 0.25.</p>"},{"location":"code/utils/#sysidentpy.utils.narmax_tools.train_test_split--returns","title":"Returns","text":"<p>X_train : np.ndarray or None     The training set feature matrix, or None if X is None. X_test : np.ndarray or None     The testing set feature matrix, or None if X is None. y_train : np.ndarray     The training set target vector. y_test : np.ndarray     The testing set target vector.</p> Source code in <code>sysidentpy/utils/narmax_tools.py</code> <pre><code>def train_test_split(\n    X: Optional[np.ndarray], y: np.ndarray, test_size: float = 0.25\n) -&gt; Tuple[Optional[np.ndarray], Optional[np.ndarray], np.ndarray, np.ndarray]:\n    \"\"\"Split the time series dataset into training and testing sets.\n\n    Parameters\n    ----------\n    X : np.ndarray, optional\n        The feature matrix. Can be None if there are no features.\n    y : np.ndarray\n        The target vector.\n    test_size : float, optional\n        The proportion of the dataset to include in the test split. Default is 0.25.\n\n    Returns\n    -------\n    X_train : np.ndarray or None\n        The training set feature matrix, or None if X is None.\n    X_test : np.ndarray or None\n        The testing set feature matrix, or None if X is None.\n    y_train : np.ndarray\n        The training set target vector.\n    y_test : np.ndarray\n        The testing set target vector.\n    \"\"\"\n    if not 0 &lt; test_size &lt; 1:\n        raise ValueError(\"test_size should be between 0 and 1\")\n\n    # Determine the split index\n    split_index = int(len(y) * (1 - test_size))\n\n    y_train, y_test = y[:split_index], y[split_index:]\n\n    if X is None:\n        return None, None, y_train, y_test\n\n    X_train, X_test = X[:split_index], X[split_index:]\n\n    return X_train, X_test, y_train, y_test\n</code></pre>"},{"location":"code/utils/#sysidentpy.utils.plotting.plot_residues_correlation","title":"<code>plot_residues_correlation(data=None, *, figsize=(10, 6), n=100, style='default', facecolor='white', title='Residual Analysis', ylabel='Correlation')</code>","text":"<p>Plot the residual validation.</p> Source code in <code>sysidentpy/utils/plotting.py</code> <pre><code>def plot_residues_correlation(\n    data=None,\n    *,\n    figsize: Tuple[int, int] = (10, 6),\n    n: int = 100,\n    style: str = \"default\",\n    facecolor: str = \"white\",\n    title: str = \"Residual Analysis\",\n    ylabel: str = \"Correlation\",\n) -&gt; None:\n    \"\"\"Plot the residual validation.\"\"\"\n    plt.style.use(style)\n    plt.rcParams[\"axes.facecolor\"] = facecolor\n    _, ax = plt.subplots(figsize=figsize, facecolor=facecolor)\n    ax.plot(data[0][:n], color=\"#1f77b4\")\n    ax.axhspan(data[1], data[2], color=\"#ccd9ff\", alpha=0.5, lw=0)\n    ax.set_xlabel(\"Lag\", fontsize=14)\n    ax.set_ylabel(ylabel, fontsize=14)\n    ax.tick_params(labelsize=14)\n    ax.set_ylim([-1, 1])\n    ax.set_title(title, fontsize=18)\n    plt.show()\n</code></pre>"},{"location":"code/utils/#sysidentpy.utils.plotting.plot_results","title":"<code>plot_results(y, *, yhat, n=100, title='Free run simulation', xlabel='Samples', ylabel='y, $\\\\hat{y}$', data_color='#1f77b4', model_color='#ff7f0e', marker='o', model_marker='*', linewidth=1.5, figsize=(10, 6), style='default', facecolor='white')</code>","text":"<p>Plot the results of a simulation.</p>"},{"location":"code/utils/#sysidentpy.utils.plotting.plot_results--parameters","title":"Parameters","text":"<p>y : np.ndarray     True data values. yhat : np.ndarray     Model predictions. n : int     Number of samples to plot. title : str     Plot title. xlabel : str     Label for the x-axis. ylabel : str     Label for the y-axis. data_color : str     Color for the data line. model_color : str     Color for the model line. marker : str     Marker style for the data line. model_marker : str     Marker style for the model line. linewidth : float     Line width for both lines. figsize : Tuple[int, int]     Figure size (width, height). style : str     Matplotlib style. facecolor : str     Figure facecolor.</p> Source code in <code>sysidentpy/utils/plotting.py</code> <pre><code>def plot_results(\n    y: np.ndarray,\n    *,\n    yhat: np.ndarray,\n    n: int = 100,\n    title: str = \"Free run simulation\",\n    xlabel: str = \"Samples\",\n    ylabel: str = r\"y, $\\hat{y}$\",\n    data_color: str = \"#1f77b4\",\n    model_color: str = \"#ff7f0e\",\n    marker: str = \"o\",\n    model_marker: str = \"*\",\n    linewidth: float = 1.5,\n    figsize: Tuple[int, int] = (10, 6),\n    style: str = \"default\",\n    facecolor: str = \"white\",\n) -&gt; None:\n    \"\"\"Plot the results of a simulation.\n\n    Parameters\n    ----------\n    y : np.ndarray\n        True data values.\n    yhat : np.ndarray\n        Model predictions.\n    n : int\n        Number of samples to plot.\n    title : str\n        Plot title.\n    xlabel : str\n        Label for the x-axis.\n    ylabel : str\n        Label for the y-axis.\n    data_color : str\n        Color for the data line.\n    model_color : str\n        Color for the model line.\n    marker : str\n        Marker style for the data line.\n    model_marker : str\n        Marker style for the model line.\n    linewidth : float\n        Line width for both lines.\n    figsize : Tuple[int, int]\n        Figure size (width, height).\n    style : str\n        Matplotlib style.\n    facecolor : str\n        Figure facecolor.\n\n    \"\"\"\n    if len(y) == 0 or len(yhat) == 0:\n        raise ValueError(\"Arrays must have at least 1 samples.\")\n\n    # Set Matplotlib style and figure properties\n    plt.style.use(style)\n    plt.rcParams[\"axes.facecolor\"] = facecolor\n\n    _, ax = plt.subplots(figsize=figsize, facecolor=facecolor)\n    ax.plot(\n        y[:n], c=data_color, alpha=1, marker=marker, label=\"Data\", linewidth=linewidth\n    )\n    ax.plot(\n        yhat[:n], c=model_color, marker=model_marker, label=\"Model\", linewidth=linewidth\n    )\n\n    # Customize plot properties\n    ax.set_title(title, fontsize=18)\n    ax.legend()\n    ax.tick_params(labelsize=14)\n    ax.set_xlabel(xlabel, fontsize=14)\n    ax.set_ylabel(ylabel, fontsize=14)\n    plt.show()\n</code></pre>"},{"location":"code/utils/#sysidentpy.utils.save_load.load_model","title":"<code>load_model(*, file_name='model', path=None)</code>","text":"<p>Load the model from file \"file_name.syspy\" located at path \"path\".</p>"},{"location":"code/utils/#sysidentpy.utils.save_load.load_model--parameters","title":"Parameters","text":"<p>file_name: file name (str), along with .syspy extension of the file containing     model to be loaded path: location where \"file_name.syspy\" is (optional).</p>"},{"location":"code/utils/#sysidentpy.utils.save_load.load_model--returns","title":"Returns","text":"<p>model_loaded: model loaded, as a variable, containing model and its attributes</p> Source code in <code>sysidentpy/utils/save_load.py</code> <pre><code>def load_model(\n    *,\n    file_name=\"model\",\n    path=None,\n):\n    \"\"\"Load the model from file \"file_name.syspy\" located at path \"path\".\n\n    Parameters\n    ----------\n    file_name: file name (str), along with .syspy extension of the file containing\n        model to be loaded\n    path: location where \"file_name.syspy\" is (optional).\n\n    Returns\n    -------\n    model_loaded: model loaded, as a variable, containing model and its attributes\n\n    \"\"\"\n    # Checking if path is provided\n    if path is not None:\n\n        # Composing file_name with path\n        file_name = os.path.join(path, file_name)\n\n    # Loading the model\n    with open(file_name, \"rb\") as fp:\n        model_loaded = pk.load(fp)\n\n    return model_loaded\n</code></pre>"},{"location":"code/utils/#sysidentpy.utils.save_load.save_model","title":"<code>save_model(*, model=None, file_name='model', path=None)</code>","text":"<p>Save the model \"model\" in folder \"folder\" using an extension .syspy.</p>"},{"location":"code/utils/#sysidentpy.utils.save_load.save_model--parameters","title":"Parameters","text":"<p>model: the model variable to be saved file_name: file name, along with .syspy extension path: location where the model will be saved (optional)</p>"},{"location":"code/utils/#sysidentpy.utils.save_load.save_model--returns","title":"Returns","text":"<p>file file_name.syspy located at \"path\", containing the estimated model.</p> Source code in <code>sysidentpy/utils/save_load.py</code> <pre><code>def save_model(\n    *,\n    model=None,\n    file_name=\"model\",\n    path=None,\n):\n    \"\"\"Save the model \"model\" in folder \"folder\" using an extension .syspy.\n\n    Parameters\n    ----------\n    model: the model variable to be saved\n    file_name: file name, along with .syspy extension\n    path: location where the model will be saved (optional)\n\n    Returns\n    -------\n    file file_name.syspy located at \"path\", containing the estimated model.\n\n    \"\"\"\n    if model is None:\n        raise TypeError(\"model cannot be None.\")\n\n    # Checking if path is provided\n    if path is not None:\n\n        # Composing file_name with path\n        file_name = os.path.join(path, file_name)\n\n    # Saving model\n    with open(file_name, \"wb\") as fp:\n        pk.dump(model, fp)\n</code></pre>"},{"location":"events/ai-networks-meetup/","title":"AI Networks Meetup","text":""},{"location":"events/ai-networks-meetup/#about-ai-networks","title":"About AI Networks","text":"<p>Founded on August 5, 2019, AI Networks has quickly established itself as one of the most engaged and focused communities in the realm of artificial intelligence. Our community thrives on a shared passion for knowledge and a commitment to exploring the latest advancements in AI and machine learning.</p> <p>As we celebrate our 5<sup>th</sup> anniversary in August 2024, we reflect on five years of enriching knowledge exchange and vibrant discussions.</p> <p>Our community currently hosts four specialized groups:</p> <ul> <li>AI/ML Brasil - Principal 01 &amp; 02: Our main groups dedicated to a wide range of AI and machine learning topics.</li> <li>AI/ML Brasil - Prompt Engineering: Focused on the art and science of designing effective prompts for AI systems.</li> <li>AI/ML Brasil - Neuroci\u00eancia: Exploring the intersection of artificial intelligence and neuroscience.</li> <li>AI/ML Brasil - Divulga\u00e7\u00e3o Servi\u00e7os IA: Dedicated to the dissemination and promotion of AI services.</li> </ul> <p>Join us to connect with like-minded individuals, engage in thought-provoking discussions, and stay at the forefront of AI innovation.</p> <p>In Portuguese</p> <p>Talk: Modelando Sistemas Din\u00e2micos | AI Networks Meetup</p> <p>Just click in the link above to watch the video.</p>"},{"location":"events/estatidados/","title":"Nubank Meetup","text":"<p>Estatidados is a big statistic and data science community in Brazil. They host an online meetup that brings together leading researchers and developers from the Statistics and Data science community to join a multiple set of talks covering current trends in the Data Science field.</p> <p>Wilson Rocha, the maintainer of SysIdentPy, gave a meetup talk about SysIdentPy and the video is available bellow:</p> <p>In Portuguese</p> <p>Talk: SysIdentPy: Uma Biblioteca Para Cria\u00e7\u00e3o de Modelos N\u00e3o Lineares para S\u00e9ries Temporais</p> <p>Just click in the link above to watch the video.</p>"},{"location":"events/events/","title":"By the Community, For the Community","text":"<p>Events that gave the opportunity for SysIdentPy community members to share their expertise, new methods/concepts, cool projects, and, more generally, their experiences and inspirations.</p>"},{"location":"events/gcom-meetup/","title":"GCoM Meetup","text":"<p>GCoM (Control and Modelling Group) is a research group of the Department of Electrical Engineering Federal University of S\u00e3o Jo\u00e3o del-Rei. GCoM works on two main areas: Analysis and Modelling Systems and Control Systems. They are devoted to undertake knowledge from Electrical Engineering, Mathematics, Computer Science and Physics to build and analyze models that mimics and control real dynamical systems. Some of applications are on robust control of uncertainty systems, Assistive Technology, Hysteresis system identification, Chaotic Dynamics. Recently, a great effort has been undertaken to better understand the play that numerical computation plays at modelling and control of nonlinear dynamical systems.</p> <p>Wilson Rocha, the maintainer of SysIdentPy, gave a meetup talk about SysIdentPy and the video is available bellow:</p> <p>In Portuguese</p> <p>Talk: Aplica\u00e7\u00f5es em Data Science e Identifica\u00e7\u00e3o de Sistemas com o SysIdentPy</p> <p>Just click in the link above to watch the video.</p>"},{"location":"events/nubank-meetup-open-source%20-%20Copia/","title":"Nubank Meetup","text":"<p>Nubank is a leading financial technology company in Latin America with more than 54 million clients in Brazil, Mexico and Colombia. They host an online meetup that brings together leading researchers and developers from the Machine Learning (ML) community to join a multiple set of talks covering current trends in ML development.</p> <p>Wilson Rocha, the maintainer of SysIdentPy, joined Bruno Rocha (software engineer at Red Hat) and Tatyana Zabanova (Data Scientist at Nubank )in a talk about the experience of making a open source package. The video is available bellow:</p> <p>In Portuguese</p> <p>Talk: Painel sobre desenvolvimento de bibliotecas em Data Science | Nubank DS &amp; ML Meetup</p> <p>Just click in the link above to watch the video.</p>"},{"location":"events/nubank-meetup/","title":"Nubank Meetup","text":"<p>Nubank is a leading financial technology company in Latin America with more than 54 million clients in Brazil, Mexico and Colombia. They host an online meetup that brings together leading researchers and developers from the Machine Learning (ML) community to join a multiple set of talks covering current trends in ML development.</p> <p>Wilson Rocha, the maintainer of SysIdentPy, gave a meetup talk about SysIdentPy and the video is available bellow:</p> <p>In Portuguese</p> <p>Talk: SysIdentPy: Uma Biblioteca Para Cria\u00e7\u00e3o de Modelos N\u00e3o Lineares para S\u00e9ries Temporais</p> <p>Just click in the link above to watch the video.</p>"},{"location":"examples/NFIR/","title":"NFIR Example","text":"<p>This example shows how to use SysIdentPy to build NFIR models. NFIR models are models with no output feedback. In other words, there are no $y(k-n_y)$ regressors, only $x(k-n_x)$.</p> <p>The NFIR model can be described as:</p> <p>$$     y_k= F^\\ell[x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k $$</p> <p>where $n_x \\in \\mathbb{N}$ is the maximum lag for the system input; $x_k \\in \\mathbb{R}^{n_x}$ is the system input at discrete time $k \\in \\mathbb{N}^n$; $e_k \\in \\mathbb{R}^{n_e}$ stands for uncertainties and possible noise at discrete time $k$. In this case, $\\mathcal{F}^\\ell$ is some nonlinear function of the input regressors with nonlinearity degree $\\ell \\in \\mathbb{N}$ and $d$ is a time delay typically set to $d=1$.</p> <p>It is important to note that NFIR model size is generally significantly higher compared to their counterpart NARMAX model size. This drawback can be noted in linear models dimensionality and it leads to even more complex scenarios in the nonlinear case.</p> <p>So, if you are looking for parsimonious and compact models, consider using NARMAX models. However, when comparing NFIR and NARMAX models, it's generally more challenging to establish stability, particularly in a control-oriented context, with NARMAX models than with NFIR models.</p> In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[9]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares, RecursiveLeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.model_structure_selection import AOLS, FROLS\n</pre> import pandas as pd import numpy as np from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.metrics import root_relative_squared_error from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.parameter_estimation import LeastSquares, RecursiveLeastSquares from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_results from sysidentpy.model_structure_selection import AOLS, FROLS <p>NFIR x NARMAX</p> <p>We will reproduce the same example provided in the \"presenting main functionality section\". In that example, we use a NARX model with xlag and ylag equal to 2 and a nonlinearity degree equal to 2. That resulted in a model with 3 regressors and a RRSE (validation metric) equal to $0.000184$</p> Regressors Parameters ERR x1(k-2) 9.0001E-01 9.57505011E-01 y(k-1) 2.0001E-01 3.89117583E-02 x1(k-1)y(k-1) 9.9992E-02 3.58319976E-03 In\u00a0[3]: Copied! <pre>np.random.seed(seed=42)\n# generating simulated data\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    model_type=\"NFIR\",\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\n</pre> np.random.seed(seed=42) # generating simulated data x_train, x_test, y_train, y_test = get_siso_data(     n=1000, colored_noise=False, sigma=0.001, train_percentage=90 )  basis_function = Polynomial(degree=2) estimator = LeastSquares() model = FROLS(     order_selection=True,     xlag=2,     info_criteria=\"aic\",     estimator=estimator,     basis_function=basis_function,     model_type=\"NFIR\", )  model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_test, y=y_test) rrse = root_relative_squared_error(y_test, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  plot_results(y=y_test, yhat=yhat, n=1000) <pre>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:618: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 6\n  self.info_values = self.information_criterion(reg_matrix, y)\n</pre> <pre>0.2129700627690414\n  Regressors  Parameters             ERR\n0    x1(k-2)  8.9017E-01  9.55432286E-01\n</pre> <p>In the NFIR case, we got a model with 1 regressor, but with significantly worse RRSE ($0.21$)</p> Regressors Parameters ERR x1(k-2) 8.9017E-01 9.55432286E-01 <p>So, to get a better NFIR model, we have to set a higher order model. In other words, we have to set a higher maximum lag to build the model.</p> <p>Lets set xlag=3.</p> In\u00a0[4]: Copied! <pre>np.random.seed(seed=42)\n# generating simulated data\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    xlag=3,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    model_type=\"NFIR\",\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\n</pre> np.random.seed(seed=42) # generating simulated data x_train, x_test, y_train, y_test = get_siso_data(     n=1000, colored_noise=False, sigma=0.001, train_percentage=90 )  basis_function = Polynomial(degree=2) estimator = LeastSquares() model = FROLS(     order_selection=True,     xlag=3,     info_criteria=\"aic\",     estimator=estimator,     basis_function=basis_function,     model_type=\"NFIR\", )  model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_test, y=y_test) rrse = root_relative_squared_error(y_test, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  plot_results(y=y_test, yhat=yhat, n=1000) <pre>0.04314951932710626\n       Regressors  Parameters             ERR\n0         x1(k-2)  8.9980E-01  9.55367779E-01\n1         x1(k-3)  1.7832E-01  3.94348076E-02\n2  x1(k-3)x1(k-1)  9.1104E-02  3.33315478E-03\n</pre> <p>Now, the model have 3 regressors, but the RRSE is still worse ($0.04$).</p> Regressors Parameters ERR x1(k-2) 8.9980E-01 9.55367779E-01 x1(k-3) 1.7832E-01 3.94348076E-02 x1(k-3)x1(k-1) 9.1104E-02 3.33315478E-03 <p>Lets set xlag=5.</p> In\u00a0[6]: Copied! <pre>np.random.seed(seed=42)\n# generating simulated data\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    xlag=5,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    model_type=\"NFIR\",\n    err_tol=None,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\n</pre> np.random.seed(seed=42) # generating simulated data x_train, x_test, y_train, y_test = get_siso_data(     n=1000, colored_noise=False, sigma=0.001, train_percentage=90 )  basis_function = Polynomial(degree=2) estimator = LeastSquares() model = FROLS(     order_selection=True,     xlag=5,     info_criteria=\"aic\",     estimator=estimator,     basis_function=basis_function,     model_type=\"NFIR\",     err_tol=None, )  model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_test, y=y_test) rrse = root_relative_squared_error(y_test, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  plot_results(y=y_test, yhat=yhat, n=1000) <pre>0.004209451216121233\n       Regressors  Parameters             ERR\n0         x1(k-2)  8.9978E-01  9.55485306E-01\n1         x1(k-3)  1.7979E-01  3.93181813E-02\n2  x1(k-3)x1(k-1)  8.9706E-02  3.33141271E-03\n3         x1(k-4)  3.5772E-02  1.54789285E-03\n4  x1(k-4)x1(k-2)  1.7615E-02  1.09675506E-04\n5  x1(k-4)x1(k-1)  1.7871E-02  1.13215338E-04\n6         x1(k-5)  6.9594E-03  6.23773643E-05\n7  x1(k-5)x1(k-1)  4.1353E-03  6.10794551E-06\n8  x1(k-5)x1(k-3)  3.4007E-03  3.98364615E-06\n9  x1(k-5)x1(k-2)  2.9798E-03  3.42693984E-06\n</pre> <p>Now the RRSE is closer to the NARMAX model, but the NFIR model have 10 regressors. So, as mentioned before, the order of NFIR models is generally higher than NARMAX model to get comparable results.</p> Regressors Parameters ERR x1(k-2) 8.9978E-01 9.55485306E-01 x1(k-3) 1.7979E-01 3.93181813E-02 x1(k-3)x1(k-1) 8.9706E-02 3.33141271E-03 x1(k-4) 3.5772E-02 1.54789285E-03 x1(k-4)x1(k-2) 1.7615E-02 1.09675506E-04 x1(k-4)x1(k-1) 1.7871E-02 1.13215338E-04 x1(k-5) 6.9594E-03 6.23773643E-05 x1(k-5)x1(k-1) 4.1353E-03 6.10794551E-06 x1(k-5)x1(k-3) 3.4007E-03 3.98364615E-06 x1(k-5)x1(k-2) 2.9798E-03 3.42693984E-06 <p>xlag = 35</p> In\u00a0[10]: Copied! <pre>np.random.seed(seed=42)\n# generating simulated data\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = RecursiveLeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    xlag=35,\n    n_info_values=200,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    model_type=\"NFIR\",\n    err_tol=None,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\n</pre> np.random.seed(seed=42) # generating simulated data x_train, x_test, y_train, y_test = get_siso_data(     n=1000, colored_noise=False, sigma=0.001, train_percentage=90 )  basis_function = Polynomial(degree=2) estimator = RecursiveLeastSquares() model = FROLS(     order_selection=True,     xlag=35,     n_info_values=200,     info_criteria=\"aic\",     estimator=estimator,     basis_function=basis_function,     model_type=\"NFIR\",     err_tol=None, )  model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_test, y=y_test) rrse = root_relative_squared_error(y_test, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  plot_results(y=y_test, yhat=yhat, n=1000) <pre>0.0033427508754120074\n        Regressors  Parameters             ERR\n0          x1(k-2)  9.0009E-01  9.55386378E-01\n1          x1(k-3)  1.8001E-01  3.94178379E-02\n2   x1(k-3)x1(k-1)  9.0886E-02  3.32874170E-03\n3          x1(k-4)  3.5412E-02  1.54540871E-03\n4   x1(k-4)x1(k-2)  1.8743E-02  1.12751104E-04\n5   x1(k-4)x1(k-1)  1.8378E-02  1.13878189E-04\n6          x1(k-5)  6.7236E-03  6.27406151E-05\n7   x1(k-5)x1(k-1)  4.4974E-03  6.32909200E-06\n8   x1(k-5)x1(k-3)  3.5420E-03  3.95779051E-06\n9   x1(k-5)x1(k-2)  5.5656E-03  3.39231220E-06\n10         x1(k-6)  1.5079E-03  2.37202762E-06\n11  x1(k-6)x1(k-2)  1.8768E-03  3.65196792E-07\n12  x1(k-6)x1(k-3)  1.0685E-03  2.92529290E-07\n13  x1(k-6)x1(k-1)  6.3191E-04  2.55676107E-07\n</pre> <p>Now the RRSE is closer to the NARMAX model, but the NFIR model have 14 regressors, with <code>RRSE=0.0033</code>. So, as you can check in these examples, the order of NFIR models is generally higher than NARMAX model to get comparable results, even trying different parameter estimation algorithms</p> Regressors Parameters ERR x1(k-2) 9.0009E-01 9.55386378E-01 x1(k-3) 1.8001E-01 3.94178379E-02 x1(k-3)x1(k-1) 9.0886E-02 3.32874170E-03 x1(k-4) 3.5412E-02 1.54540871E-03 x1(k-4)x1(k-2) 1.8743E-02 1.12751104E-04 x1(k-4)x1(k-1) 1.8378E-02 1.13878189E-04 x1(k-5) 6.7236E-03 6.27406151E-05 x1(k-5)x1(k-1) 4.4974E-03 6.32909200E-06 x1(k-5)x1(k-3) 3.5420E-03 3.95779051E-06 x1(k-5)x1(k-2) 5.5656E-03 3.39231220E-06 x1(k-6) 1.5079E-03 2.37202762E-06 x1(k-6)x1(k-2) 1.8768E-03 3.65196792E-07 x1(k-6)x1(k-3) 1.0685E-03 2.92529290E-07 x1(k-6)x1(k-1) 6.3191E-04 2.55676107E-07"},{"location":"examples/NFIR/#nfir-example","title":"NFIR Example\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"examples/NFIR/#so-what-happens-if-i-use-a-nfir-model-with-the-same-configuration","title":"So, what happens if I use a NFIR model with the same configuration?\u00b6","text":""},{"location":"examples/PV_forecasting_benchmark/","title":"PV forecasting benchmark","text":"<p>We will compare a 1-step ahead forecaster on solar irradiance data (that can be a proxy for solar PV production). The config of the neuralprophet model was taken from the neuralprophet documentation (https://neuralprophet.com/html/example_links/energy_data_example.html)</p> <p>The training will occur on 80% of the data, reserving the last 20% for the validation.</p> <p>Note: the data used in this example can be found in neuralprophet github.</p> In\u00a0[1]: Copied! <pre>from warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS, AOLS, MetaMSS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.neural_network import NARXNN\nfrom sysidentpy.metrics import mean_squared_error\n\nfrom neuralprophet import NeuralProphet\nfrom neuralprophet import set_random_seed\n\n\nsimplefilter(\"ignore\", FutureWarning)\nnp.seterr(all=\"ignore\")\n\n%matplotlib inline\n\nloss = mean_squared_error\n\ndata_location = r\".\\datasets\"\n</pre> from warnings import simplefilter import matplotlib.pyplot as plt import numpy as np import pandas as pd  from sysidentpy.model_structure_selection import FROLS, AOLS, MetaMSS from sysidentpy.basis_function import Polynomial from sysidentpy.parameter_estimation import LeastSquares from sysidentpy.utils.plotting import plot_results from sysidentpy.neural_network import NARXNN from sysidentpy.metrics import mean_squared_error  from neuralprophet import NeuralProphet from neuralprophet import set_random_seed   simplefilter(\"ignore\", FutureWarning) np.seterr(all=\"ignore\")  %matplotlib inline  loss = mean_squared_error  data_location = r\".\\datasets\" In\u00a0[2]: Copied! <pre>files = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760))\ndf[\"y\"] = raw.iloc[:, 0].values\n\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\n\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\n\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy = FROLS(\n    order_selection=True,\n    ylag=24,\n    xlag=24,\n    info_criteria=\"bic\",\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    estimator=LeastSquares(),\n)\n\nsysidentpy.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])\n\nyhat = sysidentpy.predict(X=x_test, y=y_test, steps_ahead=1)\nsysidentpy_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy.max_lag :]),\n)\nprint(sysidentpy_loss)\n\n\nplot_results(y=y_test[-104:], yhat=yhat[-104:])\n</pre> files = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"] raw = pd.read_csv(data_location + files[0]) df = pd.DataFrame() df[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760)) df[\"y\"] = raw.iloc[:, 0].values  df_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]  y = df[\"y\"].values.reshape(-1, 1) y_train = df_train[\"y\"].values.reshape(-1, 1) y_test = df_val[\"y\"].values.reshape(-1, 1)  x_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1) x_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)  basis_function = Polynomial(degree=1) sysidentpy = FROLS(     order_selection=True,     ylag=24,     xlag=24,     info_criteria=\"bic\",     basis_function=basis_function,     model_type=\"NARMAX\",     estimator=LeastSquares(), )  sysidentpy.fit(X=x_train, y=y_train) x_test = np.concatenate([x_train[-sysidentpy.max_lag :], x_test]) y_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])  yhat = sysidentpy.predict(X=x_test, y=y_test, steps_ahead=1) sysidentpy_loss = loss(     pd.Series(y_test.flatten()[sysidentpy.max_lag :]),     pd.Series(yhat.flatten()[sysidentpy.max_lag :]), ) print(sysidentpy_loss)   plot_results(y=y_test[-104:], yhat=yhat[-104:]) <pre>3869.3472912979273\n</pre> In\u00a0[4]: Copied! <pre>set_random_seed(42)\nfiles = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760))\ndf[\"y\"] = raw.iloc[:, 0].values\n\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\n\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\n\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nestimator = LeastSquares()\nsysidentpy_metamss = MetaMSS(\n    basis_function=basis_function,\n    xlag=24,\n    ylag=24,\n    estimator=estimator,\n    maxiter=10,\n    steps_ahead=1,\n    n_agents=15,\n    loss_func=\"metamss_loss\",\n    model_type=\"NARMAX\",\n    random_state=42,\n)\nsysidentpy_metamss.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy_metamss.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy_metamss.max_lag :], y_test])\n\nyhat = sysidentpy_metamss.predict(X=x_test, y=y_test, steps_ahead=1)\nmetamss_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy_metamss.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]),\n)\nprint(metamss_loss)\n\n\nplot_results(y=y_test[-104:], yhat=yhat[-104:])\n</pre> set_random_seed(42) files = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"] raw = pd.read_csv(data_location + files[0]) df = pd.DataFrame() df[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760)) df[\"y\"] = raw.iloc[:, 0].values  df_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]  y = df[\"y\"].values.reshape(-1, 1) y_train = df_train[\"y\"].values.reshape(-1, 1) y_test = df_val[\"y\"].values.reshape(-1, 1)  x_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1) x_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)  basis_function = Polynomial(degree=1) estimator = LeastSquares() sysidentpy_metamss = MetaMSS(     basis_function=basis_function,     xlag=24,     ylag=24,     estimator=estimator,     maxiter=10,     steps_ahead=1,     n_agents=15,     loss_func=\"metamss_loss\",     model_type=\"NARMAX\",     random_state=42, ) sysidentpy_metamss.fit(X=x_train, y=y_train) x_test = np.concatenate([x_train[-sysidentpy_metamss.max_lag :], x_test]) y_test = np.concatenate([y_train[-sysidentpy_metamss.max_lag :], y_test])  yhat = sysidentpy_metamss.predict(X=x_test, y=y_test, steps_ahead=1) metamss_loss = loss(     pd.Series(y_test.flatten()[sysidentpy_metamss.max_lag :]),     pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]), ) print(metamss_loss)   plot_results(y=y_test[-104:], yhat=yhat[-104:]) <pre>2157.7700127350877\n</pre> In\u00a0[7]: Copied! <pre>set_random_seed(42)\nfiles = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760))\ndf[\"y\"] = raw.iloc[:, 0].values\n\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\n\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\n\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\nbasis_function = Polynomial(degree=1)\nsysidentpy_AOLS = AOLS(\n    ylag=24, xlag=24, k=2, L=1, model_type=\"NARMAX\", basis_function=basis_function\n)\nsysidentpy_AOLS.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy_AOLS.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])\n\nyhat = sysidentpy_AOLS.predict(X=x_test, y=y_test, steps_ahead=1)\naols_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]),\n)\nprint(aols_loss)\n\n\nplot_results(y=y_test[-104:], yhat=yhat[-104:])\n</pre> set_random_seed(42) files = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"] raw = pd.read_csv(data_location + files[0]) df = pd.DataFrame() df[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760)) df[\"y\"] = raw.iloc[:, 0].values  df_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]  y = df[\"y\"].values.reshape(-1, 1) y_train = df_train[\"y\"].values.reshape(-1, 1) y_test = df_val[\"y\"].values.reshape(-1, 1)  x_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1) x_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1) basis_function = Polynomial(degree=1) sysidentpy_AOLS = AOLS(     ylag=24, xlag=24, k=2, L=1, model_type=\"NARMAX\", basis_function=basis_function ) sysidentpy_AOLS.fit(X=x_train, y=y_train) x_test = np.concatenate([x_train[-sysidentpy_AOLS.max_lag :], x_test]) y_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])  yhat = sysidentpy_AOLS.predict(X=x_test, y=y_test, steps_ahead=1) aols_loss = loss(     pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),     pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]), ) print(aols_loss)   plot_results(y=y_test[-104:], yhat=yhat[-104:]) <pre>2361.561682547365\n</pre> In\u00a0[6]: Copied! <pre>set_random_seed(42)\n\n# set_log_level(\"ERROR\")\nfiles = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760))\ndf[\"y\"] = raw.iloc[:, 0].values\n\nm = NeuralProphet(\n    n_lags=24,\n    ar_sparsity=0.5,\n    # num_hidden_layers = 2,\n    # d_hidden=20,\n)\nmetrics = m.fit(df, freq=\"H\", valid_p=0.2)\n\ndf_train, df_val = m.split_df(df, valid_p=0.2)\nm.test(df_val)\n\nfuture = m.make_future_dataframe(df_val, n_historic_predictions=True)\nforecast = m.predict(future)\n# fig = m.plot(forecast)\nprint(loss(forecast[\"y\"][24:-1], forecast[\"yhat1\"][24:-1]))\n</pre> set_random_seed(42)  # set_log_level(\"ERROR\") files = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"] raw = pd.read_csv(data_location + files[0]) df = pd.DataFrame() df[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760)) df[\"y\"] = raw.iloc[:, 0].values  m = NeuralProphet(     n_lags=24,     ar_sparsity=0.5,     # num_hidden_layers = 2,     # d_hidden=20, ) metrics = m.fit(df, freq=\"H\", valid_p=0.2)  df_train, df_val = m.split_df(df, valid_p=0.2) m.test(df_val)  future = m.make_future_dataframe(df_val, n_historic_predictions=True) forecast = m.predict(future) # fig = m.plot(forecast) print(loss(forecast[\"y\"][24:-1], forecast[\"yhat1\"][24:-1])) <pre>WARNING: nprophet - fit: Parts of code may break if using other than daily data.\n</pre> <pre>04-26 20:16:52 - WARNING - Parts of code may break if using other than daily data.\n</pre> <pre>INFO: nprophet.utils - set_auto_seasonalities: Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n</pre> <pre>04-26 20:16:52 - INFO - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n</pre> <pre>INFO: nprophet.config - set_auto_batch_epoch: Auto-set batch_size to 32\n</pre> <pre>04-26 20:16:52 - INFO - Auto-set batch_size to 32\n</pre> <pre>INFO: nprophet.config - set_auto_batch_epoch: Auto-set epochs to 7\n</pre> <pre>04-26 20:16:52 - INFO - Auto-set epochs to 7\n</pre> <pre> 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 87/100 [00:00&lt;00:00, 617.37it/s]\nINFO: nprophet - _lr_range_test: learning rate range test found optimal lr: 1.23E-01\n</pre> <pre>04-26 20:16:52 - INFO - learning rate range test found optimal lr: 1.23E-01\n</pre> <pre>Epoch[7/7]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:02&lt;00:00,  2.61it/s, SmoothL1Loss=0.00415, MAE=58.8, RegLoss=0.0112]\nINFO: nprophet - _evaluate: Validation metrics:    SmoothL1Loss    MAE\n1         0.003 48.746\n</pre> <pre>04-26 20:16:55 - INFO - Validation metrics:    SmoothL1Loss    MAE\n1         0.003 48.746\n4642.234763049609\n</pre> In\u00a0[7]: Copied! <pre>plt.plot(forecast[\"y\"][-104:], \"ro-\")\nplt.plot(forecast[\"yhat1\"][-104:], \"k*-\")\n</pre> plt.plot(forecast[\"y\"][-104:], \"ro-\") plt.plot(forecast[\"yhat1\"][-104:], \"k*-\") Out[7]: <pre>[&lt;matplotlib.lines.Line2D at 0x2618e76ebe0&gt;]</pre>"},{"location":"examples/PV_forecasting_benchmark/#pv-forecasting-benchmark","title":"PV forecasting benchmark\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"examples/PV_forecasting_benchmark/#note","title":"Note\u00b6","text":"<p>The following example is not intended to say that one library is better than another. The main focus of these examples is to show that SysIdentPy can be a good alternative for people looking to model time series.</p> <p>We will compare the results obtained against neural prophet library.</p> <p>For the sake of brevity, from SysIdentPy only the MetaMSS, AOLS and FROLS (with polynomial base function) methods will be used. See the SysIdentPy documentation to learn other ways of modeling with the library.</p>"},{"location":"examples/PV_forecasting_benchmark/#frols","title":"FROLS\u00b6","text":""},{"location":"examples/PV_forecasting_benchmark/#metamss","title":"MetaMSS\u00b6","text":""},{"location":"examples/PV_forecasting_benchmark/#aols","title":"AOLS\u00b6","text":""},{"location":"examples/PV_forecasting_benchmark/#neural-prophet","title":"Neural Prophet\u00b6","text":""},{"location":"examples/air_passenger_benchmark/","title":"Air Passenger benchmark","text":"In\u00a0[1]: Copied! <pre>from warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport scipy.signal.signaltools\n\n\ndef _centered(arr, newsize):\n    # Return the center newsize portion of the array.\n    newsize = np.asarray(newsize)\n    currsize = np.array(arr.shape)\n    startind = (currsize - newsize) // 2\n    endind = startind + newsize\n    myslice = [slice(startind[k], endind[k]) for k in range(len(endind))]\n    return arr[tuple(myslice)]\n\n\nscipy.signal.signaltools._centered = _centered\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.model_structure_selection import AOLS\nfrom sysidentpy.model_structure_selection import MetaMSS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\nfrom torch import nn\n\n# from sysidentpy.metrics import mean_squared_error\nfrom sysidentpy.neural_network import NARXNN\n\nfrom sktime.datasets import load_airline\nfrom sktime.forecasting.ets import AutoETS\nfrom sktime.forecasting.arima import ARIMA, AutoARIMA\nfrom sktime.forecasting.base import ForecastingHorizon\nfrom sktime.forecasting.exp_smoothing import ExponentialSmoothing\nfrom sktime.forecasting.fbprophet import Prophet\nfrom sktime.forecasting.tbats import TBATS\nfrom sktime.forecasting.bats import BATS\n\n# from sktime.forecasting.model_evaluation import evaluate\nfrom sktime.forecasting.model_selection import temporal_train_test_split\nfrom sktime.performance_metrics.forecasting import mean_squared_error\nfrom sktime.utils.plotting import plot_series\nfrom neuralprophet import NeuralProphet\nfrom neuralprophet import set_random_seed\n\n\nsimplefilter(\"ignore\", FutureWarning)\nnp.seterr(all=\"ignore\")\n\n%matplotlib inline\n\nloss = mean_squared_error\n</pre> from warnings import simplefilter import matplotlib.pyplot as plt import numpy as np import pandas as pd  import scipy.signal.signaltools   def _centered(arr, newsize):     # Return the center newsize portion of the array.     newsize = np.asarray(newsize)     currsize = np.array(arr.shape)     startind = (currsize - newsize) // 2     endind = startind + newsize     myslice = [slice(startind[k], endind[k]) for k in range(len(endind))]     return arr[tuple(myslice)]   scipy.signal.signaltools._centered = _centered  from sysidentpy.model_structure_selection import FROLS from sysidentpy.model_structure_selection import AOLS from sysidentpy.model_structure_selection import MetaMSS from sysidentpy.basis_function import Polynomial from sysidentpy.utils.plotting import plot_results from torch import nn  # from sysidentpy.metrics import mean_squared_error from sysidentpy.neural_network import NARXNN  from sktime.datasets import load_airline from sktime.forecasting.ets import AutoETS from sktime.forecasting.arima import ARIMA, AutoARIMA from sktime.forecasting.base import ForecastingHorizon from sktime.forecasting.exp_smoothing import ExponentialSmoothing from sktime.forecasting.fbprophet import Prophet from sktime.forecasting.tbats import TBATS from sktime.forecasting.bats import BATS  # from sktime.forecasting.model_evaluation import evaluate from sktime.forecasting.model_selection import temporal_train_test_split from sktime.performance_metrics.forecasting import mean_squared_error from sktime.utils.plotting import plot_series from neuralprophet import NeuralProphet from neuralprophet import set_random_seed   simplefilter(\"ignore\", FutureWarning) np.seterr(all=\"ignore\")  %matplotlib inline  loss = mean_squared_error <pre>c:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\datatypes\\_series\\_check.py:43: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\datatypes\\_panel\\_check.py:45: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\datatypes\\_panel\\_check.py:46: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  VALID_MULTIINDEX_TYPES = (pd.Int64Index, pd.RangeIndex)\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\utils\\validation\\series.py:18: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\forecasting\\base\\_fh.py:18: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  RELATIVE_TYPES = (pd.Int64Index, pd.RangeIndex)\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\forecasting\\base\\_fh.py:19: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  ABSOLUTE_TYPES = (pd.Int64Index, pd.RangeIndex, pd.DatetimeIndex, pd.PeriodIndex)\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:7: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:7: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n</pre> In\u00a0[2]: Copied! <pre>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)  # 23 samples for testing\nplot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"])\nfh = ForecastingHorizon(y_test.index, is_relative=False)\nprint(y_train.shape[0], y_test.shape[0])\n</pre> y = load_airline() y_train, y_test = temporal_train_test_split(y, test_size=23)  # 23 samples for testing plot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"]) fh = ForecastingHorizon(y_test.index, is_relative=False) print(y_train.shape[0], y_test.shape[0]) <pre>121 23\n</pre> In\u00a0[3]: Copied! <pre>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy = FROLS(\n    order_selection=True,\n    ylag=13,  # the lags for all models will be 13\n    basis_function=basis_function,\n    model_type=\"NAR\",\n)\nsysidentpy.fit(y=y_train)\ny_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])\n\nyhat = sysidentpy.predict(y=y_test, forecast_horizon=23)\nfrols_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy.max_lag :]),\n)\nprint(frols_loss)\n\nplot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :])\n</pre> y = load_airline() y_train, y_test = temporal_train_test_split(y, test_size=23) y_train = y_train.values.reshape(-1, 1) y_test = y_test.values.reshape(-1, 1)  basis_function = Polynomial(degree=1) sysidentpy = FROLS(     order_selection=True,     ylag=13,  # the lags for all models will be 13     basis_function=basis_function,     model_type=\"NAR\", ) sysidentpy.fit(y=y_train) y_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])  yhat = sysidentpy.predict(y=y_test, forecast_horizon=23) frols_loss = loss(     pd.Series(y_test.flatten()[sysidentpy.max_lag :]),     pd.Series(yhat.flatten()[sysidentpy.max_lag :]), ) print(frols_loss)  plot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :]) <pre>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:619: UserWarning:\n\nn_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 14\n\n</pre> <pre>805.9521186338106\n</pre> In\u00a0[4]: Copied! <pre>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\n\ndf_train, df_test = temporal_train_test_split(y, test_size=23)\ndf_train = df_train.reset_index()\ndf_train.columns = [\"ds\", \"y\"]\ndf_train[\"ds\"] = pd.to_datetime(df_train[\"ds\"].astype(str))\ndf_test = df_test.reset_index()\ndf_test.columns = [\"ds\", \"y\"]\ndf_test[\"ds\"] = pd.to_datetime(df_test[\"ds\"].astype(str))\n\nsysidentpy_AOLS = AOLS(\n    ylag=13, k=2, L=1, model_type=\"NAR\", basis_function=basis_function\n)\nsysidentpy_AOLS.fit(y=y_train)\ny_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])\n\nyhat = sysidentpy_AOLS.predict(y=y_test, steps_ahead=None, forecast_horizon=23)\naols_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]),\n)\nprint(aols_loss)\n\nplot_results(y=y_test[sysidentpy_AOLS.max_lag :], yhat=yhat[sysidentpy_AOLS.max_lag :])\n</pre> y = load_airline() y_train, y_test = temporal_train_test_split(y, test_size=23) y_train = y_train.values.reshape(-1, 1) y_test = y_test.values.reshape(-1, 1)  df_train, df_test = temporal_train_test_split(y, test_size=23) df_train = df_train.reset_index() df_train.columns = [\"ds\", \"y\"] df_train[\"ds\"] = pd.to_datetime(df_train[\"ds\"].astype(str)) df_test = df_test.reset_index() df_test.columns = [\"ds\", \"y\"] df_test[\"ds\"] = pd.to_datetime(df_test[\"ds\"].astype(str))  sysidentpy_AOLS = AOLS(     ylag=13, k=2, L=1, model_type=\"NAR\", basis_function=basis_function ) sysidentpy_AOLS.fit(y=y_train) y_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])  yhat = sysidentpy_AOLS.predict(y=y_test, steps_ahead=None, forecast_horizon=23) aols_loss = loss(     pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),     pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]), ) print(aols_loss)  plot_results(y=y_test[sysidentpy_AOLS.max_lag :], yhat=yhat[sysidentpy_AOLS.max_lag :]) <pre>476.64996316992523\n</pre> In\u00a0[5]: Copied! <pre>set_random_seed(42)\n\ny = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\n\nsysidentpy_metamss = MetaMSS(\n    basis_function=basis_function, ylag=13, model_type=\"NAR\", test_size=0.17\n)\nsysidentpy_metamss.fit(y=y_train)\n\ny_test = np.concatenate([y_train[-sysidentpy_metamss.max_lag :], y_test])\n\nyhat = sysidentpy_metamss.predict(y=y_test, steps_ahead=None, forecast_horizon=23)\nmetamss_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy_metamss.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]),\n)\nprint(metamss_loss)\n\nplot_results(\n    y=y_test[sysidentpy_metamss.max_lag :], yhat=yhat[sysidentpy_metamss.max_lag :]\n)\n</pre> set_random_seed(42)  y = load_airline() y_train, y_test = temporal_train_test_split(y, test_size=23) y_train = y_train.values.reshape(-1, 1) y_test = y_test.values.reshape(-1, 1)  sysidentpy_metamss = MetaMSS(     basis_function=basis_function, ylag=13, model_type=\"NAR\", test_size=0.17 ) sysidentpy_metamss.fit(y=y_train)  y_test = np.concatenate([y_train[-sysidentpy_metamss.max_lag :], y_test])  yhat = sysidentpy_metamss.predict(y=y_test, steps_ahead=None, forecast_horizon=23) metamss_loss = loss(     pd.Series(y_test.flatten()[sysidentpy_metamss.max_lag :]),     pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]), ) print(metamss_loss)  plot_results(     y=y_test[sysidentpy_metamss.max_lag :], yhat=yhat[sysidentpy_metamss.max_lag :] ) <pre>450.992127624293\n</pre> In\u00a0[6]: Copied! <pre>import torch\n\ntorch.manual_seed(42)\n\ny = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=36)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\nx_train = np.zeros_like(y_train)\nx_test = np.zeros_like(y_test)\n\n\nclass NARX(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(13, 20)\n        self.lin2 = nn.Linear(20, 20)\n        self.lin3 = nn.Linear(20, 20)\n        self.lin4 = nn.Linear(20, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, xb):\n        z = self.lin(xb)\n        z = self.relu(z)\n        z = self.lin2(z)\n        z = self.relu(z)\n        z = self.lin3(z)\n        z = self.relu(z)\n        z = self.lin4(z)\n        return z\n\n\nnarx_net = NARXNN(\n    net=NARX(),\n    ylag=13,\n    model_type=\"NAR\",\n    basis_function=Polynomial(degree=1),\n    epochs=900,\n    verbose=False,\n    learning_rate=2.5e-02,\n    optim_params={},  # optional parameters of the optimizer\n)\n\nnarx_net.fit(y=y_train)\nyhat = narx_net.predict(y=y_test, forecast_horizon=23)\nnarxnet_loss = loss(\n    pd.Series(y_test.flatten()[narx_net.max_lag :]),\n    pd.Series(yhat.flatten()[narx_net.max_lag :]),\n)\nprint(narxnet_loss)\nplot_results(y=y_test[narx_net.max_lag :], yhat=yhat[narx_net.max_lag :])\n</pre> import torch  torch.manual_seed(42)  y = load_airline() y_train, y_test = temporal_train_test_split(y, test_size=36) y_train = y_train.values.reshape(-1, 1) y_test = y_test.values.reshape(-1, 1) x_train = np.zeros_like(y_train) x_test = np.zeros_like(y_test)   class NARX(nn.Module):     def __init__(self):         super().__init__()         self.lin = nn.Linear(13, 20)         self.lin2 = nn.Linear(20, 20)         self.lin3 = nn.Linear(20, 20)         self.lin4 = nn.Linear(20, 1)         self.relu = nn.ReLU()      def forward(self, xb):         z = self.lin(xb)         z = self.relu(z)         z = self.lin2(z)         z = self.relu(z)         z = self.lin3(z)         z = self.relu(z)         z = self.lin4(z)         return z   narx_net = NARXNN(     net=NARX(),     ylag=13,     model_type=\"NAR\",     basis_function=Polynomial(degree=1),     epochs=900,     verbose=False,     learning_rate=2.5e-02,     optim_params={},  # optional parameters of the optimizer )  narx_net.fit(y=y_train) yhat = narx_net.predict(y=y_test, forecast_horizon=23) narxnet_loss = loss(     pd.Series(y_test.flatten()[narx_net.max_lag :]),     pd.Series(yhat.flatten()[narx_net.max_lag :]), ) print(narxnet_loss) plot_results(y=y_test[narx_net.max_lag :], yhat=yhat[narx_net.max_lag :]) <pre>316.54086775668776\n</pre> In\u00a0[9]: Copied! <pre>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)  # 23 samples for testing\nplot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"])\nfh = ForecastingHorizon(y_test.index, is_relative=False)\nprint(y_train.shape[0], y_test.shape[0])\n</pre> y = load_airline() y_train, y_test = temporal_train_test_split(y, test_size=23)  # 23 samples for testing plot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"]) fh = ForecastingHorizon(y_test.index, is_relative=False) print(y_train.shape[0], y_test.shape[0]) <pre>121 23\n</pre> In\u00a0[10]: Copied! <pre>es = ExponentialSmoothing(trend=\"add\", seasonal=\"multiplicative\", sp=12)\ny = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nes.fit(y_train)\ny_pred_es = es.predict(fh)\n\nplot_series(y_test, y_pred_es, labels=[\"y_test\", \"y_pred\"])\nes_loss = loss(y_test, y_pred_es)\nes_loss\n</pre> es = ExponentialSmoothing(trend=\"add\", seasonal=\"multiplicative\", sp=12) y = load_airline() y_train, y_test = temporal_train_test_split(y, test_size=23) es.fit(y_train) y_pred_es = es.predict(fh)  plot_series(y_test, y_pred_es, labels=[\"y_test\", \"y_pred\"]) es_loss = loss(y_test, y_pred_es) es_loss Out[10]: <pre>910.462659260655</pre> In\u00a0[11]: Copied! <pre>y = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nets = AutoETS(auto=True, sp=12, n_jobs=-1)\nets.fit(y_train)\ny_pred_ets = ets.predict(fh)\n\nplot_series(y_test, y_pred_ets, labels=[\"y_test\", \"y_pred\"])\nets_loss = loss(y_test, y_pred_ets)\nets_loss\n</pre> y = load_airline()  y_train, y_test = temporal_train_test_split(y, test_size=23) ets = AutoETS(auto=True, sp=12, n_jobs=-1) ets.fit(y_train) y_pred_ets = ets.predict(fh)  plot_series(y_test, y_pred_ets, labels=[\"y_test\", \"y_pred\"]) ets_loss = loss(y_test, y_pred_ets) ets_loss Out[11]: <pre>1739.117296439066</pre> In\u00a0[12]: Copied! <pre>auto_arima = AutoARIMA(sp=12, suppress_warnings=True)\ny = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nauto_arima.fit(y_train)\ny_pred_auto_arima = auto_arima.predict(fh)\n\nplot_series(y_test, y_pred_auto_arima, labels=[\"y_test\", \"y_pred\"])\nautoarima_loss = loss(y_test, y_pred_auto_arima)\nautoarima_loss\n</pre> auto_arima = AutoARIMA(sp=12, suppress_warnings=True) y = load_airline()  y_train, y_test = temporal_train_test_split(y, test_size=23) auto_arima.fit(y_train) y_pred_auto_arima = auto_arima.predict(fh)  plot_series(y_test, y_pred_auto_arima, labels=[\"y_test\", \"y_pred\"]) autoarima_loss = loss(y_test, y_pred_auto_arima) autoarima_loss Out[12]: <pre>1714.4753226965322</pre> In\u00a0[8]: Copied! <pre>y = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nmanual_arima = ARIMA(\n    order=(13, 1, 0), suppress_warnings=True\n)  # seasonal_order=(0, 1, 0, 12)\nmanual_arima.fit(y_train)\ny_pred_manual_arima = manual_arima.predict(fh)\nplot_series(y_test, y_pred_manual_arima, labels=[\"y_test\", \"y_pred\"])\nmanualarima_loss = loss(y_test, y_pred_manual_arima)\nmanualarima_loss\n</pre> y = load_airline()  y_train, y_test = temporal_train_test_split(y, test_size=23) manual_arima = ARIMA(     order=(13, 1, 0), suppress_warnings=True )  # seasonal_order=(0, 1, 0, 12) manual_arima.fit(y_train) y_pred_manual_arima = manual_arima.predict(fh) plot_series(y_test, y_pred_manual_arima, labels=[\"y_test\", \"y_pred\"]) manualarima_loss = loss(y_test, y_pred_manual_arima) manualarima_loss Out[8]: <pre>2085.425167938668</pre> In\u00a0[14]: Copied! <pre>y = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nbats = BATS(sp=12, use_trend=True, use_box_cox=False)\nbats.fit(y_train)\ny_pred_bats = bats.predict(fh)\n\nplot_series(y_test, y_pred_bats, labels=[\"y_test\", \"y_pred\"])\nbats_loss = loss(y_test, y_pred_bats)\nbats_loss\n</pre> y = load_airline()  y_train, y_test = temporal_train_test_split(y, test_size=23) bats = BATS(sp=12, use_trend=True, use_box_cox=False) bats.fit(y_train) y_pred_bats = bats.predict(fh)  plot_series(y_test, y_pred_bats, labels=[\"y_test\", \"y_pred\"]) bats_loss = loss(y_test, y_pred_bats) bats_loss Out[14]: <pre>7286.6484525676415</pre> In\u00a0[15]: Copied! <pre>y = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ntbats = TBATS(sp=12, use_trend=True, use_box_cox=False)\ntbats.fit(y_train)\ny_pred_tbats = tbats.predict(fh)\nplot_series(y_test, y_pred_tbats, labels=[\"y_test\", \"y_pred\"])\ntbats_loss = loss(y_test, y_pred_tbats)\ntbats_loss\n</pre> y = load_airline()  y_train, y_test = temporal_train_test_split(y, test_size=23) tbats = TBATS(sp=12, use_trend=True, use_box_cox=False) tbats.fit(y_train) y_pred_tbats = tbats.predict(fh) plot_series(y_test, y_pred_tbats, labels=[\"y_test\", \"y_pred\"]) tbats_loss = loss(y_test, y_pred_tbats) tbats_loss Out[15]: <pre>7448.434672875093</pre> In\u00a0[16]: Copied! <pre>set_random_seed(42)\n\ny = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nz = y.copy()\nz = z.to_timestamp(freq=\"M\")\nz_train, z_test = temporal_train_test_split(z, test_size=23)\n\n\nprophet = Prophet(\n    seasonality_mode=\"multiplicative\",\n    n_changepoints=int(len(y_train) / 12),\n    add_country_holidays={\"country_name\": \"Germany\"},\n    yearly_seasonality=True,\n    weekly_seasonality=False,\n    daily_seasonality=False,\n)\nprophet.fit(z_train)\ny_pred_prophet = prophet.predict(fh.to_relative(cutoff=y_train.index[-1]))\n\ny_pred_prophet.index = y_test.index\nplot_series(y_test, y_pred_prophet, labels=[\"y_test\", \"y_pred\"])\nprophet_loss = loss(y_test, y_pred_prophet)\nprophet_loss\n</pre> set_random_seed(42)  y = load_airline()  y_train, y_test = temporal_train_test_split(y, test_size=23) z = y.copy() z = z.to_timestamp(freq=\"M\") z_train, z_test = temporal_train_test_split(z, test_size=23)   prophet = Prophet(     seasonality_mode=\"multiplicative\",     n_changepoints=int(len(y_train) / 12),     add_country_holidays={\"country_name\": \"Germany\"},     yearly_seasonality=True,     weekly_seasonality=False,     daily_seasonality=False, ) prophet.fit(z_train) y_pred_prophet = prophet.predict(fh.to_relative(cutoff=y_train.index[-1]))  y_pred_prophet.index = y_test.index plot_series(y_test, y_pred_prophet, labels=[\"y_test\", \"y_pred\"]) prophet_loss = loss(y_test, y_pred_prophet) prophet_loss Out[16]: <pre>1186.0045566050442</pre> In\u00a0[35]: Copied! <pre>set_random_seed(42)\n\ndf = pd.read_csv(r\".\\datasets\\air_passengers.csv\")\nm = NeuralProphet(seasonality_mode=\"multiplicative\")\ndf_train = df.iloc[:-23, :].copy()\ndf_test = df.iloc[-23:, :].copy()\n\nm = NeuralProphet(seasonality_mode=\"multiplicative\")\n\nmetrics = m.fit(df_train, freq=\"MS\")\n\nfuture = m.make_future_dataframe(\n    df_train, periods=23, n_historic_predictions=len(df_train)\n)\n\nforecast = m.predict(future)\nplt.plot(forecast[\"yhat1\"].values[-23:])\nplt.plot(df_test[\"y\"].values)\nneuralprophet_loss = loss(forecast[\"yhat1\"].values[-23:], df_test[\"y\"].values)\nneuralprophet_loss\n</pre> set_random_seed(42)  df = pd.read_csv(r\".\\datasets\\air_passengers.csv\") m = NeuralProphet(seasonality_mode=\"multiplicative\") df_train = df.iloc[:-23, :].copy() df_test = df.iloc[-23:, :].copy()  m = NeuralProphet(seasonality_mode=\"multiplicative\")  metrics = m.fit(df_train, freq=\"MS\")  future = m.make_future_dataframe(     df_train, periods=23, n_historic_predictions=len(df_train) )  forecast = m.predict(future) plt.plot(forecast[\"yhat1\"].values[-23:]) plt.plot(df_test[\"y\"].values) neuralprophet_loss = loss(forecast[\"yhat1\"].values[-23:], df_test[\"y\"].values) neuralprophet_loss <pre>WARNING: nprophet - fit: Parts of code may break if using other than daily data.\n</pre> <pre>11-21 20:57:55 - WARNING - Parts of code may break if using other than daily data.\n</pre> <pre>INFO: nprophet.utils - set_auto_seasonalities: Disabling weekly seasonality. Run NeuralProphet with weekly_seasonality=True to override this.\n</pre> <pre>11-21 20:57:55 - INFO - Disabling weekly seasonality. Run NeuralProphet with weekly_seasonality=True to override this.\n</pre> <pre>INFO: nprophet.utils - set_auto_seasonalities: Disabling daily seasonality. Run NeuralProphet with daily_seasonality=True to override this.\n</pre> <pre>11-21 20:57:55 - INFO - Disabling daily seasonality. Run NeuralProphet with daily_seasonality=True to override this.\n</pre> <pre>INFO: nprophet.config - set_auto_batch_epoch: Auto-set batch_size to 8\n</pre> <pre>11-21 20:57:55 - INFO - Auto-set batch_size to 8\n</pre> <pre>INFO: nprophet.config - set_auto_batch_epoch: Auto-set epochs to 264\n</pre> <pre>11-21 20:57:55 - INFO - Auto-set epochs to 264\n</pre> <pre> 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 83/100 [00:00&lt;00:00, 1034.46it/s]\nINFO: nprophet - _lr_range_test: learning rate range test found optimal lr: 1.87E-01\n</pre> <pre>11-21 20:57:55 - INFO - learning rate range test found optimal lr: 1.87E-01\n</pre> <pre>Epoch[264/264]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 264/264 [00:03&lt;00:00, 66.42it/s, SmoothL1Loss=0.000325, MAE=6.38, RegLoss=0]\n</pre> Out[35]: <pre>501.24794023767436</pre> In\u00a0[19]: Copied! <pre>results = {\n    \"Exponential Smoothing\": es_loss,\n    \"ETS\": ets_loss,\n    \"AutoArima\": autoarima_loss,\n    \"Manual Arima\": manualarima_loss,\n    \"BATS\": bats_loss,\n    \"TBATS\": tbats_loss,\n    \"Prophet\": prophet_loss,\n    \"SysIdentPy (Polynomial Model)\": frols_loss,\n    \"SysIdentPy (Neural Model)\": narxnet_loss,\n    \"SysIdentPy (AOLS)\": aols_loss,\n    \"SysIdentPy (MetaMSS)\": metamss_loss,\n    \"NeuralProphet\": neuralprophet_loss,\n}\n\nsorted(results.items(), key=lambda result: result[1])\n</pre> results = {     \"Exponential Smoothing\": es_loss,     \"ETS\": ets_loss,     \"AutoArima\": autoarima_loss,     \"Manual Arima\": manualarima_loss,     \"BATS\": bats_loss,     \"TBATS\": tbats_loss,     \"Prophet\": prophet_loss,     \"SysIdentPy (Polynomial Model)\": frols_loss,     \"SysIdentPy (Neural Model)\": narxnet_loss,     \"SysIdentPy (AOLS)\": aols_loss,     \"SysIdentPy (MetaMSS)\": metamss_loss,     \"NeuralProphet\": neuralprophet_loss, }  sorted(results.items(), key=lambda result: result[1]) Out[19]: <pre>[('SysIdentPy (Neural Model)', 316.54086775668776),\n ('SysIdentPy (MetaMSS)', 450.992127624293),\n ('SysIdentPy (AOLS)', 476.64996316992523),\n ('NeuralProphet', 501.24794023767436),\n ('SysIdentPy (Polynomial Model)', 805.9521186338106),\n ('Exponential Smoothing', 910.462659260655),\n ('Prophet', 1186.0045566050442),\n ('AutoArima', 1714.4753226965322),\n ('ETS', 1739.117296439066),\n ('Manual Arima', 2085.425167938668),\n ('BATS', 7286.6484525676415),\n ('TBATS', 7448.434672875093)]</pre>"},{"location":"examples/air_passenger_benchmark/#air-passenger-benchmark","title":"Air Passenger benchmark\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"examples/air_passenger_benchmark/#note","title":"Note\u00b6","text":"<p>The following example is not intended to say that one library is better than another. The main focus of these examples is to show that SysIdentPy can be a good alternative for people looking to model time series.</p> <p>We will compare the results obtained using the sktime and neural prophet library.</p> <p>From sktime, the following models will be used:</p> <ul> <li><p>AutoARIMA</p> </li> <li><p>BATS</p> </li> <li><p>TBATS</p> </li> <li><p>Exponential Smoothing</p> </li> <li><p>Prophet</p> </li> <li><p>AutoETS</p> </li> </ul> <p>For the sake of brevity, from SysIdentPy only the MetaMSS, AOLS, FROLS (with polynomial base function) and NARXNN methods will be used. See the SysIdentPy documentation to learn other ways of modeling with the library.</p>"},{"location":"examples/air_passenger_benchmark/#air-passengers-data","title":"Air passengers data\u00b6","text":""},{"location":"examples/air_passenger_benchmark/#results","title":"Results\u00b6","text":"No. Package Mean Squared Error 1 SysIdentPy (Neural Model) 316.54 2 SysIdentPy (MetaMSS) 450.99 3 SysIdentPy (AOLS) 476.64 4 NeuralProphet 501.24 5 SysIdentPy (FROLS) 805.95 6 Exponential Smoothing 910.52 7 Prophet 1186.00 8 AutoArima 1714.47 9 Manual Arima 2085.42 10 ETS 2590.05 11 BATS 7286.64 12 TBATS 7448.43"},{"location":"examples/air_passenger_benchmark/#sysidentpy-frols","title":"SysIdentPy FROLS\u00b6","text":""},{"location":"examples/air_passenger_benchmark/#sysidentpy-aols","title":"SysIdentPy AOLS\u00b6","text":""},{"location":"examples/air_passenger_benchmark/#sysidentpy-metamss","title":"SysIdentPy MetaMSS\u00b6","text":""},{"location":"examples/air_passenger_benchmark/#sysidentpy-neural-narx","title":"SysIdentPy Neural NARX\u00b6","text":""},{"location":"examples/air_passenger_benchmark/#exponential-smoothing","title":"Exponential Smoothing\u00b6","text":""},{"location":"examples/air_passenger_benchmark/#autoets","title":"AutoETS\u00b6","text":""},{"location":"examples/air_passenger_benchmark/#autoarima","title":"AutoArima\u00b6","text":""},{"location":"examples/air_passenger_benchmark/#arima","title":"Arima\u00b6","text":""},{"location":"examples/air_passenger_benchmark/#bats","title":"BATS\u00b6","text":""},{"location":"examples/air_passenger_benchmark/#tbats","title":"TBATS\u00b6","text":""},{"location":"examples/air_passenger_benchmark/#prophet","title":"Prophet\u00b6","text":""},{"location":"examples/air_passenger_benchmark/#neural-prophet","title":"Neural Prophet\u00b6","text":""},{"location":"examples/aols/","title":"Using the Accelerated Orthogonal Least-Squares algorithm for building Polynomial NARX models","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\nfrom sysidentpy.model_structure_selection import AOLS\n\n# generating simulated data\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n</pre> import pandas as pd from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.metrics import root_relative_squared_error from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) from sysidentpy.model_structure_selection import AOLS  # generating simulated data x_train, x_test, y_train, y_test = get_siso_data(     n=1000, colored_noise=False, sigma=0.001, train_percentage=90 ) In\u00a0[2]: Copied! <pre>basis_function = Polynomial(degree=2)\nmodel = AOLS(xlag=3, ylag=3, k=5, L=1, basis_function=basis_function)\n\nmodel.fit(X=x_train, y=y_train)\n</pre> basis_function = Polynomial(degree=2) model = AOLS(xlag=3, ylag=3, k=5, L=1, basis_function=basis_function)  model.fit(X=x_train, y=y_train) Out[2]: <pre>&lt;sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS at 0x25cd3b406d0&gt;</pre> In\u00a0[3]: Copied! <pre>yhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</pre> yhat = model.predict(X=x_test, y=y_test) rrse = root_relative_squared_error(y_test, yhat) print(rrse) r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) <pre>0.0018996279285613828\n      Regressors   Parameters             ERR\n0         y(k-1)   1.9999E-01  0.00000000E+00\n1        x1(k-2)   9.0003E-01  0.00000000E+00\n2  x1(k-1)y(k-1)   9.9954E-02  0.00000000E+00\n3  x1(k-3)y(k-1)  -2.1442E-04  0.00000000E+00\n4      x1(k-1)^2   3.3714E-04  0.00000000E+00\n</pre> In\u00a0[4]: Copied! <pre>plot_results(y=y_test, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_test, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_test, yhat, x_test)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> plot_results(y=y_test, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_test, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_test, yhat, x_test) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")"},{"location":"examples/aols/#using-the-accelerated-orthogonal-least-squares-algorithm-for-building-polynomial-narx-models","title":"Using the Accelerated Orthogonal Least-Squares algorithm for building Polynomial NARX models\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"examples/basic_steps/","title":"Presenting main functionality","text":"<p>Here we import the NARMAX model, the metric for model evaluation and the methods to generate sample data for tests. Also, we import pandas for specific usage.</p> In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.parameter_estimation import LeastSquares from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) In\u00a0[2]: Copied! <pre>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.0001, train_percentage=90\n)\n</pre> x_train, x_valid, y_train, y_valid = get_siso_data(     n=1000, colored_noise=False, sigma=0.0001, train_percentage=90 ) <p>To obtain a NARMAX model we have to choose some values, e.g, the nonlinearity degree (degree), the maximum lag for the inputs and output (xlag and ylag).</p> <p>In addition, you can select the information criteria to be used with the Error Reduction Ratio to select the model order and the method to estimate the model parameters:</p> <ul> <li>Information Criteria: aic, aicc, bic, lilc, fpe</li> <li>Parameter Estimation: LeastSquares, TotalLeastSquares, RecursiveLeastSquares, NonNegativeLeastSquares, LeastMeanSquares and many more (see the docs)</li> </ul> <p>The n_terms values is optional. It refer to the number of terms to include in the final model. You can set this value based on the information criteria (see below) or based on priori information about the model structure. The default value is n_terms=None, so the algorithm will choose the minimum value reached by the information criteria.</p> <p>To use information criteria you have to set order_selection=True. You can also select n_info_values (default = 15).</p> In\u00a0[27]: Copied! <pre>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=3,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    err_tol=None,\n    basis_function=basis_function,\n)\n</pre> basis_function = Polynomial(degree=2) estimator = LeastSquares() model = FROLS(     order_selection=True,     n_info_values=3,     ylag=2,     xlag=2,     info_criteria=\"aic\",     estimator=estimator,     err_tol=None,     basis_function=basis_function, ) In\u00a0[28]: Copied! <pre>model.fit(X=x_train, y=y_train)\n</pre> model.fit(X=x_train, y=y_train) Out[28]: <pre>&lt;sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS at 0x1db932f5090&gt;</pre> In\u00a0[29]: Copied! <pre>yhat = model.predict(X=x_valid, y=y_valid)\n</pre> yhat = model.predict(X=x_valid, y=y_valid) In\u00a0[30]: Copied! <pre>rrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n</pre> rrse = root_relative_squared_error(y_valid, yhat) print(rrse) <pre>0.00017649882109753117\n</pre> <p>model_object.results return the selected model regressors, the estimated parameters and the ERR values. As shown below, the algorithm detect the exact model that was used for simulate the data.</p> In\u00a0[31]: Copied! <pre>r = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</pre> r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) <pre>      Regressors  Parameters             ERR\n0        x1(k-2)  9.0001E-01  9.57604864E-01\n1         y(k-1)  2.0000E-01  3.88976063E-02\n2  x1(k-1)y(k-1)  9.9992E-02  3.49749526E-03\n</pre> <p>In addition, you can access the residuals and plot_result methods to take a look at the prediction and two residual analysis. The extras and lam values below contain another residues analysis so you can plot it manually. This method will be improved soon.</p> In\u00a0[32]: Copied! <pre>plt.style.available\n</pre> plt.style.available Out[32]: <pre>['Solarize_Light2',\n '_classic_test_patch',\n '_mpl-gallery',\n '_mpl-gallery-nogrid',\n 'bmh',\n 'classic',\n 'dark_background',\n 'fast',\n 'fivethirtyeight',\n 'ggplot',\n 'grayscale',\n 'seaborn-v0_8',\n 'seaborn-v0_8-bright',\n 'seaborn-v0_8-colorblind',\n 'seaborn-v0_8-dark',\n 'seaborn-v0_8-dark-palette',\n 'seaborn-v0_8-darkgrid',\n 'seaborn-v0_8-deep',\n 'seaborn-v0_8-muted',\n 'seaborn-v0_8-notebook',\n 'seaborn-v0_8-paper',\n 'seaborn-v0_8-pastel',\n 'seaborn-v0_8-poster',\n 'seaborn-v0_8-talk',\n 'seaborn-v0_8-ticks',\n 'seaborn-v0_8-white',\n 'seaborn-v0_8-whitegrid',\n 'tableau-colorblind10']</pre> In\u00a0[33]: Copied! <pre>plot_results(\n    y=y_valid,\n    yhat=yhat,\n    n=1000,\n    title=\"test\",\n    xlabel=\"Samples\",\n    ylabel=r\"y, $\\hat{y}$\",\n    data_color=\"#1f77b4\",\n    model_color=\"#ff7f0e\",\n    marker=\"o\",\n    model_marker=\"*\",\n    linewidth=1.5,\n    figsize=(10, 6),\n    style=\"seaborn-v0_8-notebook\",\n    facecolor=\"white\",\n)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(\n    data=ee, title=\"Residues\", ylabel=\"$e^2$\", style=\"seaborn-v0_8-notebook\"\n)\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(\n    data=x1e, title=\"Residues\", ylabel=\"$x_1e$\", style=\"seaborn-v0_8-notebook\"\n)\n</pre> plot_results(     y=y_valid,     yhat=yhat,     n=1000,     title=\"test\",     xlabel=\"Samples\",     ylabel=r\"y, $\\hat{y}$\",     data_color=\"#1f77b4\",     model_color=\"#ff7f0e\",     marker=\"o\",     model_marker=\"*\",     linewidth=1.5,     figsize=(10, 6),     style=\"seaborn-v0_8-notebook\",     facecolor=\"white\", ) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(     data=ee, title=\"Residues\", ylabel=\"$e^2$\", style=\"seaborn-v0_8-notebook\" ) x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(     data=x1e, title=\"Residues\", ylabel=\"$x_1e$\", style=\"seaborn-v0_8-notebook\" ) In\u00a0[34]: Copied! <pre>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</pre> xaxis = np.arange(1, model.n_info_values + 1) plt.plot(xaxis, model.info_values) plt.xlabel(\"n_terms\") plt.ylabel(\"Information Criteria\") Out[34]: <pre>Text(0, 0.5, 'Information Criteria')</pre> <pre><code>{note}\n Here we are creating random samples with white noise and letting the algorithm choose\n the number of terms based on the minimum value of information criteria. \n This is not the best approach in System Identification, but serves as a simple example. \n The information criteria must be used as an __auxiliary tool__ to select *n_terms*. \n Plot the information values to help you on that!\n\n If you run the example above several times you might find some cases where the\n algorithm choose only the first two regressors, or four (depending on the information\n criteria method selected). This is because the minimum value of information criteria\n depends on residual variance (affected by noise) and have some limitations in nonlinear\n scenarios. However, if you check the ERR values (robust to noise) you will see that the\n ERR is ordering the regressors in the correct way!\n\n We have some examples on *information_criteria* notebook!\n</code></pre> <pre><code>{note}\nThis documentation and the examples below are written with MyST Markdown, a form\nof markdown that works with Sphinx. For more information about MyST markdown, and\nto use MyST markdown with your Sphinx website,\nsee [the MyST-parser documentation](https://myst-parser.readthedocs.io/)\n</code></pre> <p>The n_info_values limits the number of regressors to apply the information criteria. We choose $n_y = n_x = \\ell = 2$, so the candidate regressor is a list of 15 regressors. We can set n_info_values = 15 and see the information values for all regressors. This option can save some amount of computational resources when dealing with multiples inputs and large datasets.</p> In\u00a0[36]: Copied! <pre>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\n\nmodel.fit(X=x_train, y=y_train)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</pre> basis_function = Polynomial(degree=2) estimator = LeastSquares()  model = FROLS(     order_selection=True,     n_info_values=15,     ylag=2,     xlag=2,     info_criteria=\"aic\",     estimator=estimator,     basis_function=basis_function,     err_tol=None, )  model.fit(X=x_train, y=y_train)  xaxis = np.arange(1, model.n_info_values + 1) plt.plot(xaxis, model.info_values) plt.xlabel(\"n_terms\") plt.ylabel(\"Information Criteria\") Out[36]: <pre>Text(0, 0.5, 'Information Criteria')</pre> <p>Now running without executing information criteria methods (setting the n_terms) because we already know the optimal number of regressors</p> In\u00a0[37]: Copied! <pre>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=False,\n    n_info_values=15,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</pre> basis_function = Polynomial(degree=2) estimator = LeastSquares()  model = FROLS(     order_selection=False,     n_info_values=15,     n_terms=3,     ylag=2,     xlag=2,     info_criteria=\"aic\",     estimator=estimator,     basis_function=basis_function,     err_tol=None, ) model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) <pre>0.00017649882109753117\n      Regressors  Parameters             ERR\n0        x1(k-2)  9.0001E-01  9.57604864E-01\n1         y(k-1)  2.0000E-01  3.88976063E-02\n2  x1(k-1)y(k-1)  9.9992E-02  3.49749526E-03\n</pre> In\u00a0[38]: Copied! <pre>model.max_lag  # the number of initial conditions you should provide\n</pre> model.max_lag  # the number of initial conditions you should provide Out[38]: <pre>2</pre> In\u00a0[39]: Copied! <pre>yhat = model.predict(\n    X=x_valid, y=y_valid[: model.max_lag]\n)  # passing only the 2 initial values which will be used as initial conditions\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n</pre> yhat = model.predict(     X=x_valid, y=y_valid[: model.max_lag] )  # passing only the 2 initial values which will be used as initial conditions rrse = root_relative_squared_error(y_valid, yhat) print(rrse) <pre>0.00017649882109753117\n</pre> <p>As you can see, the rrse obtained is the same as the one obtained when we input the full test data. That is due the fact that even in the cases you provide the full test data, the prediction method only uses the first values as initial conditions and drop the other values in the backend.</p> <p>In the 1-step ahead prediction or n-steps ahead prediction you should provide the full test data because we handle all the computations in the background so the users don't need to care to implement the loops themselves.</p> <p>If you wants the check how your model performs in a 3-steps ahead scenario using 200 samples in the test data, that means that at each 3 iterations the model feedback will use the real data as initial conditions. Thats why the full data test is necessary, because otherwise the model won't find the real value to use as feedback in the n-iteration.</p> <p>This is the case where you have access the historical data so you can check how your model performs in n-steps ahead prediction. If you choose to use a 3-steps ahead model prediction in real life, you have to predict the next 3 samples, wait the 3 iterations, collect the real data and use the new data as initial condition to predict the next 3 values and so on.</p> <p>In the infinity-steps ahead prediction scenario, if your model has a input it will be able to make predictions for all inputs by only providing the initial conditions. If your model has no input (a NAR model, for example), you can set the forecast horizon and the model will make predictions by using only the initial conditions. All the feedback will be the predicted values (this is, by the way, one of the reasons that usually n-steps ahead models are better than infinity-ahead models).</p> <p>It is worth to mention that changing the initial condition doesn't mean you are changing your model. The only thing changing is the initial condition and that could make a real difference in many cases.</p> In\u00a0[40]: Copied! <pre># for now the list is returned as a codification. Here, $0$ is the constant term, $[1001]=y{k-1}, [100n]=y_{k-n}, [200n] = x1_{k-n}, [300n]=x2_{k-n}$ and so on\nmodel.regressor_code  # list of all possible regressors given non_degree, n_y and n_x values\n</pre> # for now the list is returned as a codification. Here, $0$ is the constant term, $[1001]=y{k-1}, [100n]=y_{k-n}, [200n] = x1_{k-n}, [300n]=x2_{k-n}$ and so on model.regressor_code  # list of all possible regressors given non_degree, n_y and n_x values Out[40]: <pre>array([[   0,    0],\n       [1001,    0],\n       [1002,    0],\n       [2001,    0],\n       [2002,    0],\n       [1001, 1001],\n       [1002, 1001],\n       [2001, 1001],\n       [2002, 1001],\n       [1002, 1002],\n       [2001, 1002],\n       [2002, 1002],\n       [2001, 2001],\n       [2002, 2001],\n       [2002, 2002]])</pre> In\u00a0[41]: Copied! <pre>print(model.err, \"\\n\\n\")  # err values for the selected terms\nprint(model.theta)  # estimated parameters for the final model structure\n</pre> print(model.err, \"\\n\\n\")  # err values for the selected terms print(model.theta)  # estimated parameters for the final model structure <pre>[9.57604864e-01 3.88976063e-02 3.49749526e-03 1.43420284e-10\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00] \n\n\n[[0.90000582]\n [0.20000142]\n [0.0999919 ]]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/basic_steps/#presenting-main-functionality","title":"Presenting main functionality\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"examples/basic_steps/#generating-1-input-1-output-sample-data","title":"Generating 1 input 1 output sample data\u00b6","text":"<p>The data is generated by simulating the following model:</p> <p>$y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_{k}$</p> <p>If colored_noise is set to True:</p> <p>$e_{k} = 0.8\\nu_{k-1} + \\nu_{k}$</p> <p>where $x$ is a uniformly distributed random variable and $\\nu$ is a gaussian distributed variable with $\\mu=0$ and $\\sigma=0.1$</p> <p>In the next example we will generate a data with 1000 samples with white noise and selecting 90% of the data to train the model.</p>"},{"location":"examples/basic_steps/#model-structure-selection","title":"Model Structure Selection\u00b6","text":"<p>The fit method executes the Error Reduction Ratio algorithm using Househoulder reflection to select the model structure.</p> <p>Enforcing keyword-only arguments in fit and predict methods as well. This is an effort to promote clear and non-ambiguous use of the library.</p>"},{"location":"examples/basic_steps/#free-run-simulation","title":"Free run simulation\u00b6","text":"<p>The predict method is use to generate the predictions. For now we only support free run simulation (also known as infinity steps ahead). Soon will let the user define a one-step ahead or k-step ahead prediction.</p>"},{"location":"examples/basic_steps/#evaluate-the-model","title":"Evaluate the model\u00b6","text":"<p>In this example we use the root_relative_squared_error metric because it is often used in System Identification. More metrics and information about it can be found on documentation.</p>"},{"location":"examples/basic_steps/#setting-the-n_terms-parameter","title":"Setting the n_terms parameter\u00b6","text":"<p>In the example above we let the number of terms to compose the final model to be defined as the minimum value of the information criteria. Once you ran the algorithm and choose the best number of parameters, you can turn order_selection to False and set the n_terms value (3 in this example). Here we have a small dataset, but in bigger data this can be critical because running information criteria algorithm is more computational expensive. Since we already know the best number of regressor, we set n_terms and we get the same result.</p> <p>However, this is not only critical because computational efficiency. In many situation, the minimum value of the information criteria can lead to overfitting. In some cases, the difference between choosing a model with 30 regressors or 10 is minimal, so you can take the model with 10 terms without loosing accuracy.</p> <p>In the following we use info_values to plot the information criteria values. As you can see, the minimum value relies where $xaxis = 5$</p>"},{"location":"examples/basic_steps/#predict-method","title":"Predict method\u00b6","text":"<p>One could ask why it is necessary to pass the test data on the predict method. The answers is: you don't need to pass the test data when you are running a infinity-steps ahead prediction, you just need to pass the initial conditions. However, if you wants to check how your model performs in a 1-step ahead prediction or n-step ahead prediction, you should provide the test data.</p> <p>To show you only need the initial condition, consider the following example using the previous trained model:</p>"},{"location":"examples/basic_steps/#extra-information","title":"Extra information\u00b6","text":"<p>You can access some extra information like the list of all candidate regressors</p>"},{"location":"examples/defining_lags/","title":"Setting specific lags","text":"<p>Different ways to set the maximum lag for input and output</p> In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[2]: Copied! <pre>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\n</pre> from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function._basis_function import Polynomial In\u00a0[3]: Copied! <pre>basis_function = Polynomial(degree=1)\n\nmodel = FROLS(\n    order_selection=True,\n    ylag=4,\n    xlag=4,\n    info_criteria=\"aic\",\n    basis_function=basis_function,\n)\n</pre> basis_function = Polynomial(degree=1)  model = FROLS(     order_selection=True,     ylag=4,     xlag=4,     info_criteria=\"aic\",     basis_function=basis_function, ) In\u00a0[7]: Copied! <pre>model = FROLS(\n    order_selection=True,\n    ylag=[1, 4],\n    xlag=[1, 4],\n    info_criteria=\"aic\",\n    basis_function=basis_function,\n)\n</pre> model = FROLS(     order_selection=True,     ylag=[1, 4],     xlag=[1, 4],     info_criteria=\"aic\",     basis_function=basis_function, ) In\u00a0[8]: Copied! <pre># The example considers a model with 2 inputs, but you can use the same for any amount of inputs.\n\nmodel = FROLS(\n    order_selection=True,\n    ylag=[1, 4],\n    xlag=[[1, 2, 3, 4], [1, 7]],\n    info_criteria=\"aic\",\n    basis_function=basis_function,\n)\n# The lags defined are:\n# x1(k-1), x1(k-2), x(k-3), x(k-4)\n# x2(k-1), x1(k-7)\n</pre> # The example considers a model with 2 inputs, but you can use the same for any amount of inputs.  model = FROLS(     order_selection=True,     ylag=[1, 4],     xlag=[[1, 2, 3, 4], [1, 7]],     info_criteria=\"aic\",     basis_function=basis_function, ) # The lags defined are: # x1(k-1), x1(k-2), x(k-3), x(k-4) # x2(k-1), x1(k-7) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/defining_lags/#setting-specific-lags","title":"Setting specific lags\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"examples/defining_lags/#setting-lags-using-a-range-of-values","title":"Setting lags using a range of values\u00b6","text":"<p>If you pass int values for ylag and xlag, the lags are defined as a range from 1-ylag and 1-xlag.</p> <p>For example: if ylag=4 then the candidate regressors are $y_{k-1}, y_{k-2}, y_{k-3}, y_{k-4}$</p>"},{"location":"examples/defining_lags/#setting-specific-lags-using-lists","title":"Setting specific lags using lists\u00b6","text":"<p>If you pass the ylag and xlag as a list, only the lags related to values in the list will be created. $y_{k-1}, y_{k-4}$,  $x_{k-1}, x_{k-4}$</p>"},{"location":"examples/defining_lags/#setting-lags-for-multiple-input-single-output-miso-models","title":"Setting lags for Multiple Input Single Output (MISO) models\u00b6","text":"<p>The following example shows how to define specific lags for each input. One should notice that we have to use a nested list in that case.</p>"},{"location":"examples/entropic_regression/","title":"Identification of an electromechanical system using Entropic Regression","text":"In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import ER\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import RecursiveLeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</pre> import numpy as np import pandas as pd from sysidentpy.model_structure_selection import ER from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.parameter_estimation import RecursiveLeastSquares from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) In\u00a0[2]: Copied! <pre>df1 = pd.read_csv(\"../examples/datasets/x_cc.csv\")\ndf2 = pd.read_csv(\"../examples/datasets/y_cc.csv\")\n</pre> df1 = pd.read_csv(\"../examples/datasets/x_cc.csv\") df2 = pd.read_csv(\"../examples/datasets/y_cc.csv\") In\u00a0[3]: Copied! <pre>df2[5000:80000].plot(figsize=(10, 4))\n</pre> df2[5000:80000].plot(figsize=(10, 4)) Out[3]: <pre>&lt;Axes: &gt;</pre> In\u00a0[4]: Copied! <pre># we will decimate the data using d=500 in this example\nx_train, x_valid = np.split(df1.iloc[::500].values, 2)\ny_train, y_valid = np.split(df2.iloc[::500].values, 2)\n</pre> # we will decimate the data using d=500 in this example x_train, x_valid = np.split(df1.iloc[::500].values, 2) y_train, y_valid = np.split(df2.iloc[::500].values, 2) In\u00a0[5]: Copied! <pre>basis_function = Polynomial(degree=2)\nestimator = RecursiveLeastSquares()\n\nmodel = ER(\n    ylag=6,\n    xlag=6,\n    n_perm=2,\n    k=2,\n    skip_forward=True,\n    estimator=estimator,\n    basis_function=basis_function,\n)\n</pre> basis_function = Polynomial(degree=2) estimator = RecursiveLeastSquares()  model = ER(     ylag=6,     xlag=6,     n_perm=2,     k=2,     skip_forward=True,     estimator=estimator,     basis_function=basis_function, ) <pre>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:40: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use ER(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning, stacklevel=1)\n</pre> In\u00a0[6]: Copied! <pre>model.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  plot_results(y=y_valid, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_20912\\4260657624.py:1: UserWarning: Given the higher number of possible regressors (91), the Entropic Regression algorithm may take long time to run. Consider reducing the number of regressors \n  model.fit(X=x_train, y=y_train)\n</pre> <pre>0.03276775133089435\n        Regressors   Parameters             ERR\n0                1  -6.7052E+02  0.00000000E+00\n1           y(k-1)   9.6022E-01  0.00000000E+00\n2           y(k-5)  -3.0769E-02  0.00000000E+00\n3          x1(k-2)   7.3733E+02  0.00000000E+00\n4         y(k-1)^2   1.5897E-04  0.00000000E+00\n5     y(k-2)y(k-1)  -2.2080E-04  0.00000000E+00\n6     y(k-3)y(k-1)   2.9946E-06  0.00000000E+00\n7     y(k-5)y(k-1)   4.9779E-06  0.00000000E+00\n8    x1(k-1)y(k-1)  -1.7036E-01  0.00000000E+00\n9    x1(k-2)y(k-1)  -2.0748E-01  0.00000000E+00\n10   x1(k-4)y(k-1)   8.3724E-03  0.00000000E+00\n11        y(k-2)^2   7.3635E-05  0.00000000E+00\n12   x1(k-1)y(k-2)   1.2028E-01  0.00000000E+00\n13   x1(k-2)y(k-2)   8.0270E-02  0.00000000E+00\n14   x1(k-3)y(k-2)  -3.0208E-03  0.00000000E+00\n15   x1(k-4)y(k-2)  -8.8307E-03  0.00000000E+00\n16   x1(k-1)y(k-3)  -4.9095E-02  0.00000000E+00\n17   x1(k-1)y(k-4)   1.2375E-02  0.00000000E+00\n18       x1(k-1)^2   1.1682E+02  0.00000000E+00\n19  x1(k-3)x1(k-2)   5.2777E+00  0.00000000E+00\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/entropic_regression/#identification-of-an-electromechanical-system-using-entropic-regression","title":"Identification of an electromechanical system using Entropic Regression\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>More details about this data can be found in the following paper (in Portuguese): https://www.researchgate.net/publication/320418710_Identificacao_de_um_motorgerador_CC_por_meio_de_modelos_polinomiais_autorregressivos_e_redes_neurais_artificiais</p>"},{"location":"examples/entropic_regression/#building-a-polynomial-narx-model-using-entropic-regression-algorithm","title":"Building a Polynomial NARX model using Entropic Regression Algorithm\u00b6","text":""},{"location":"examples/extended_least_squares/","title":"Extended Least Squares","text":"<p>Here we import the NARMAX model, the metric for model evaluation and the methods to generate sample data for tests. Also, we import pandas for specific usage.</p> In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\n</pre> import numpy as np import pandas as pd from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.parameter_estimation import LeastSquares from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.utils.display_results import results In\u00a0[2]: Copied! <pre>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=True, sigma=0.2, train_percentage=90\n)\n</pre> x_train, x_valid, y_train, y_valid = get_siso_data(     n=1000, colored_noise=True, sigma=0.2, train_percentage=90 ) In\u00a0[3]: Copied! <pre>basis_function = Polynomial(degree=2)\nestimator = LeastSquares(unbiased=False)\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\n</pre> basis_function = Polynomial(degree=2) estimator = LeastSquares(unbiased=False) model = FROLS(     order_selection=False,     n_terms=3,     ylag=2,     xlag=2,     info_criteria=\"aic\",     estimator=estimator,     basis_function=basis_function,     err_tol=None, ) In\u00a0[4]: Copied! <pre>model.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n</pre> model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse) <pre>0.5463756532162123\n</pre> <p>Clearly we have something wrong with the obtained model. See the basic_steps notebook to compare the results obtained using the same data but without colored noise. But let take a look in whats is wrong.</p> In\u00a0[5]: Copied! <pre>r = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</pre> r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) <pre>      Regressors  Parameters             ERR\n0        x1(k-2)  9.0807E-01  7.42958856E-01\n1         y(k-1)  2.8569E-01  8.25663362E-02\n2  x1(k-1)y(k-1)  1.0677E-01  4.01292463E-03\n</pre> In\u00a0[6]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nparameters = np.zeros([3, 50])\n\nfor i in range(50):\n    x_train, x_valid, y_train, y_valid = get_siso_data(\n        n=3000, colored_noise=True, train_percentage=90\n    )\n\n    model.fit(X=x_train, y=y_train)\n    parameters[:, i] = model.theta.flatten()\n\n# Set the theme for seaborn (optional)\nsns.set_theme()\n\nplt.figure(figsize=(14, 4))\n\n# Plot KDE for each parameter\nsns.kdeplot(parameters.T[:, 0], label=\"Parameter 1\")\nsns.kdeplot(parameters.T[:, 1], label=\"Parameter 2\")\nsns.kdeplot(parameters.T[:, 2], label=\"Parameter 3\")\n\n# Plot vertical lines where the real values must lie\nplt.axvline(x=0.1, color=\"k\", linestyle=\"--\", label=\"Real Value 0.1\")\nplt.axvline(x=0.2, color=\"k\", linestyle=\"--\", label=\"Real Value 0.2\")\nplt.axvline(x=0.9, color=\"k\", linestyle=\"--\", label=\"Real Value 0.9\")\n\nplt.xlabel(\"Parameter Value\")\nplt.ylabel(\"Density\")\nplt.title(\"Kernel Density Estimate of Parameters\")\nplt.legend()\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt import seaborn as sns  parameters = np.zeros([3, 50])  for i in range(50):     x_train, x_valid, y_train, y_valid = get_siso_data(         n=3000, colored_noise=True, train_percentage=90     )      model.fit(X=x_train, y=y_train)     parameters[:, i] = model.theta.flatten()  # Set the theme for seaborn (optional) sns.set_theme()  plt.figure(figsize=(14, 4))  # Plot KDE for each parameter sns.kdeplot(parameters.T[:, 0], label=\"Parameter 1\") sns.kdeplot(parameters.T[:, 1], label=\"Parameter 2\") sns.kdeplot(parameters.T[:, 2], label=\"Parameter 3\")  # Plot vertical lines where the real values must lie plt.axvline(x=0.1, color=\"k\", linestyle=\"--\", label=\"Real Value 0.1\") plt.axvline(x=0.2, color=\"k\", linestyle=\"--\", label=\"Real Value 0.2\") plt.axvline(x=0.9, color=\"k\", linestyle=\"--\", label=\"Real Value 0.9\")  plt.xlabel(\"Parameter Value\") plt.ylabel(\"Density\") plt.title(\"Kernel Density Estimate of Parameters\") plt.legend() plt.show() In\u00a0[7]: Copied! <pre>basis_function = Polynomial(degree=2)\nestimator = LeastSquares(unbiased=True)\nparameters = np.zeros([3, 50])\n\nfor i in range(50):\n    x_train, x_valid, y_train, y_valid = get_siso_data(\n        n=3000, colored_noise=True, train_percentage=90\n    )\n\n    model = FROLS(\n        order_selection=False,\n        n_terms=3,\n        ylag=2,\n        xlag=2,\n        elag=2,\n        info_criteria=\"aic\",\n        estimator=estimator,\n        basis_function=basis_function,\n    )\n\n    model.fit(X=x_train, y=y_train)\n    parameters[:, i] = model.theta.flatten()\n\n\nplt.figure(figsize=(14, 4))\n\n# Plot KDE for each parameter\nsns.kdeplot(parameters.T[:, 0], label=\"Parameter 1\")\nsns.kdeplot(parameters.T[:, 1], label=\"Parameter 2\")\nsns.kdeplot(parameters.T[:, 2], label=\"Parameter 3\")\n\n# Plot vertical lines where the real values must lie\nplt.axvline(x=0.1, color=\"k\", linestyle=\"--\", label=\"Real Value 0.1\")\nplt.axvline(x=0.2, color=\"k\", linestyle=\"--\", label=\"Real Value 0.2\")\nplt.axvline(x=0.9, color=\"k\", linestyle=\"--\", label=\"Real Value 0.9\")\n\nplt.xlabel(\"Parameter Value\")\nplt.ylabel(\"Density\")\nplt.title(\"Kernel Density Estimate of Parameters\")\nplt.legend()\nplt.show()\n</pre> basis_function = Polynomial(degree=2) estimator = LeastSquares(unbiased=True) parameters = np.zeros([3, 50])  for i in range(50):     x_train, x_valid, y_train, y_valid = get_siso_data(         n=3000, colored_noise=True, train_percentage=90     )      model = FROLS(         order_selection=False,         n_terms=3,         ylag=2,         xlag=2,         elag=2,         info_criteria=\"aic\",         estimator=estimator,         basis_function=basis_function,     )      model.fit(X=x_train, y=y_train)     parameters[:, i] = model.theta.flatten()   plt.figure(figsize=(14, 4))  # Plot KDE for each parameter sns.kdeplot(parameters.T[:, 0], label=\"Parameter 1\") sns.kdeplot(parameters.T[:, 1], label=\"Parameter 2\") sns.kdeplot(parameters.T[:, 2], label=\"Parameter 3\")  # Plot vertical lines where the real values must lie plt.axvline(x=0.1, color=\"k\", linestyle=\"--\", label=\"Real Value 0.1\") plt.axvline(x=0.2, color=\"k\", linestyle=\"--\", label=\"Real Value 0.2\") plt.axvline(x=0.9, color=\"k\", linestyle=\"--\", label=\"Real Value 0.9\")  plt.xlabel(\"Parameter Value\") plt.ylabel(\"Density\") plt.title(\"Kernel Density Estimate of Parameters\") plt.legend() plt.show() <p>Great! Now we have an unbiased estimation of the parameters!</p>"},{"location":"examples/extended_least_squares/#extended-least-squares","title":"Extended Least Squares\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"examples/extended_least_squares/#generating-1-input-1-output-sample-data","title":"Generating 1 input 1 output sample data\u00b6","text":"<p>The data is generated by simulating the following model: $y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-2} + e_{k}$</p> <p>If colored_noise is set to True:</p> <p>$e_{k} = 0.8\\nu_{k-1} + \\nu_{k}$</p> <p>where $x$ is a uniformly distributed random variable and $\\nu$ is a gaussian distributed variable with $\\mu=0$ and $\\sigma$ is defined by the user.</p> <p>In the next example we will generate a data with 3000 samples with white noise and selecting 90% of the data to train the model.</p>"},{"location":"examples/extended_least_squares/#build-the-model","title":"Build the model\u00b6","text":"<p>First we will train a model without the Extended Least Squares Algorithm for comparison purpose.</p>"},{"location":"examples/extended_least_squares/#biased-parameter-estimation","title":"Biased parameter estimation\u00b6","text":"<p>As we can observe above, the model structure is exact the same the one that generate the data. You can se that the ERR ordered the terms in the correct way. And this is an important note regarding the Error Reduction Ratio algorithm used here: it is very robust to colored noise!!</p> <p>That is a great feature! However, although the structure is correct, the model parameters are not ok! Here we have a biased estimation! The real parameter for $y_{k-1}$ is $0.2$, not $0.3$.</p> <p>In this case, we are actually modeling using a NARX model, not a NARMAX. The MA part exists to allow a unbiased estimation of the parameters. To achieve a unbiased estimation of the parameters we have the Extend Least Squares algorithm. Remember, if the data have only white noise, NARX is fine.</p> <p>Before applying the Extended Least Squares Algorithm we will run several NARX models to check how different the estimated parameters are from the real ones.</p>"},{"location":"examples/extended_least_squares/#using-the-extended-least-squares-algorithm","title":"Using the Extended Least Squares algorithm\u00b6","text":"<p>As shown in figure above, we have a problem to estimate the parameter for $y_{k-1}$. Now we will use the Extended Least Squares Algorithm.</p> <p>In SysIdentPy, just set extended_least_squares to True and the algorithm will be applied.</p>"},{"location":"examples/extended_least_squares/#note","title":"Note\u00b6","text":"<p>Note: The Extended Least Squares is an iterative algorithm. In SysIdentpy the default is 30 iterations (<code>uiter=30</code>) because it is known from literature that the algorithm converges quickly (about 10 or 20 iterations).</p>"},{"location":"examples/f_16_benchmark/","title":"Example: F-16 Ground Vibration Test benchmark","text":"In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.parameter_estimation import LeastSquares from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) In\u00a0[2]: Copied! <pre>f_16 = pd.read_csv(\n    r\"../examples/datasets/f-16.txt\", header=None, names=[\"x1\", \"x2\", \"y\"]\n)\n</pre> f_16 = pd.read_csv(     r\"../examples/datasets/f-16.txt\", header=None, names=[\"x1\", \"x2\", \"y\"] ) In\u00a0[3]: Copied! <pre>f_16.shape\n</pre> f_16.shape Out[3]: <pre>(32768, 3)</pre> In\u00a0[4]: Copied! <pre>f_16[[\"x1\", \"x2\"]][0:500].plot(figsize=(12, 8))\n</pre> f_16[[\"x1\", \"x2\"]][0:500].plot(figsize=(12, 8)) Out[4]: <pre>&lt;Axes: &gt;</pre> In\u00a0[5]: Copied! <pre>f_16[\"y\"][0:2000].plot(figsize=(12, 8))\n</pre> f_16[\"y\"][0:2000].plot(figsize=(12, 8)) Out[5]: <pre>&lt;Axes: &gt;</pre> In\u00a0[6]: Copied! <pre>x1_id, x1_val = f_16[\"x1\"][0:16384].values.reshape(-1, 1), f_16[\"x1\"][\n    16384::\n].values.reshape(-1, 1)\nx2_id, x2_val = f_16[\"x2\"][0:16384].values.reshape(-1, 1), f_16[\"x2\"][\n    16384::\n].values.reshape(-1, 1)\nx_id = np.concatenate([x1_id, x2_id], axis=1)\nx_val = np.concatenate([x1_val, x2_val], axis=1)\n\ny_id, y_val = f_16[\"y\"][0:16384].values.reshape(-1, 1), f_16[\"y\"][\n    16384::\n].values.reshape(-1, 1)\n</pre> x1_id, x1_val = f_16[\"x1\"][0:16384].values.reshape(-1, 1), f_16[\"x1\"][     16384:: ].values.reshape(-1, 1) x2_id, x2_val = f_16[\"x2\"][0:16384].values.reshape(-1, 1), f_16[\"x2\"][     16384:: ].values.reshape(-1, 1) x_id = np.concatenate([x1_id, x2_id], axis=1) x_val = np.concatenate([x1_val, x2_val], axis=1)  y_id, y_val = f_16[\"y\"][0:16384].values.reshape(-1, 1), f_16[\"y\"][     16384:: ].values.reshape(-1, 1) In\u00a0[7]: Copied! <pre>x1lag = list(range(1, 10))\nx2lag = list(range(1, 10))\nx2lag\n</pre> x1lag = list(range(1, 10)) x2lag = list(range(1, 10)) x2lag Out[7]: <pre>[1, 2, 3, 4, 5, 6, 7, 8, 9]</pre> In\u00a0[8]: Copied! <pre>basis_function = Polynomial(degree=1)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=39,\n    ylag=20,\n    xlag=[x1lag, x2lag],\n    info_criteria=\"bic\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_id, y=y_id)\ny_hat = model.predict(X=x_val, y=y_val)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</pre> basis_function = Polynomial(degree=1) estimator = LeastSquares()  model = FROLS(     order_selection=True,     n_info_values=39,     ylag=20,     xlag=[x1lag, x2lag],     info_criteria=\"bic\",     estimator=estimator,     basis_function=basis_function, )  model.fit(X=x_id, y=y_id) y_hat = model.predict(X=x_val, y=y_val) rrse = root_relative_squared_error(y_val, y_hat) print(rrse) r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) <pre>0.2910089654603829\n   Regressors   Parameters             ERR\n0      y(k-1)   1.8387E+00  9.43378253E-01\n1      y(k-2)  -1.8938E+00  1.95167599E-02\n2      y(k-3)   1.3337E+00  1.02432261E-02\n3      y(k-6)  -1.6038E+00  8.03485985E-03\n4      y(k-9)   2.6776E-01  9.27874557E-04\n5     x2(k-7)  -2.2385E+01  3.76837313E-04\n6     x1(k-1)   8.2709E+00  6.81508210E-04\n7     x2(k-3)   1.0587E+02  1.57459800E-03\n8     x1(k-8)  -3.7975E+00  7.35086279E-04\n9     x2(k-1)   8.5725E+01  4.85358786E-04\n10     y(k-7)   1.3955E+00  2.77245281E-04\n11     y(k-5)   1.3219E+00  8.64120037E-04\n12    y(k-10)  -2.9306E-01  8.51717688E-04\n13     y(k-4)  -9.5479E-01  7.23623116E-04\n14     y(k-8)  -7.1309E-01  4.44988077E-04\n15    y(k-12)  -3.0437E-01  1.49743148E-04\n16    y(k-11)   4.8602E-01  3.34613282E-04\n17    y(k-13)  -8.2442E-02  1.43738964E-04\n18    y(k-15)  -1.6762E-01  1.25546584E-04\n19    x1(k-2)  -8.9698E+00  9.76699739E-05\n20    y(k-17)   2.2036E-02  4.55983807E-05\n21    y(k-14)   2.4900E-01  1.10314107E-04\n22    y(k-19)  -6.8239E-03  1.99734771E-05\n23    x2(k-9)  -9.6265E+01  2.98523208E-05\n24    x2(k-8)   2.2620E+02  2.34402543E-04\n25    x2(k-2)  -2.3609E+02  1.04172323E-04\n26    y(k-20)  -5.4663E-02  5.37895336E-05\n27    x2(k-6)  -2.3651E+02  2.11392628E-05\n28    x2(k-4)   1.7378E+02  2.18396315E-05\n29    x1(k-7)   4.9862E+00  2.03811842E-05\n</pre> In\u00a0[9]: Copied! <pre>plot_results(y=y_val, yhat=y_hat, n=1000)\nee = compute_residues_autocorrelation(y_val, y_hat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_val, y_hat, x_val[:, 0])\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> plot_results(y=y_val, yhat=y_hat, n=1000) ee = compute_residues_autocorrelation(y_val, y_hat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_val, y_hat, x_val[:, 0]) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") In\u00a0[10]: Copied! <pre>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n\n# You can use the plot below to choose the \"n_terms\" and run the model again with the most adequate value of terms.\n</pre> xaxis = np.arange(1, model.n_info_values + 1) plt.plot(xaxis, model.info_values) plt.xlabel(\"n_terms\") plt.ylabel(\"Information Criteria\")  # You can use the plot below to choose the \"n_terms\" and run the model again with the most adequate value of terms. Out[10]: <pre>Text(0, 0.5, 'Information Criteria')</pre>"},{"location":"examples/f_16_benchmark/#example-f-16-ground-vibration-test-benchmark","title":"Example: F-16 Ground Vibration Test benchmark\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Note: The following examples do not try to replicate the results of the cited manuscripts. Even the model parameters such as ylag and xlag and size of identification and validation data are not the same of the cited papers. Moreover, sampling rate adjustment and other different data preparation are not handled here.</p>"},{"location":"examples/f_16_benchmark/#reference","title":"Reference\u00b6","text":"<p>The following text was taken from the link http://www.nonlinearbenchmark.org/#F16.</p> <p>Note: The reader is referred to the mentioned website for a complete reference concerning the experiment. For now, this notebook is just a simple example of the performance of SysIdentPy on a real world dataset. A more detailed study of this system will be published in the future.</p> <p>The F-16 Ground Vibration Test benchmark features a high order system with clearance and friction nonlinearities at the mounting interface of the payloads.</p> <p>The experimental data made available to the Workshop participants were acquired on a full-scale F-16 aircraft on the occasion of the Siemens LMS Ground Vibration Testing Master Class, held in September 2014 at the Saffraanberg military basis, Sint-Truiden, Belgium.</p> <p>During the test campaign, two dummy payloads were mounted at the wing tips to simulate the mass and inertia properties of real devices typically equipping an F-16 in \ufb02ight. The aircraft structure was instrumented with accelerometers. One shaker was attached underneath the right wing to apply input signals. The dominant source of nonlinearity in the structural dynamics was expected to originate from the mounting interfaces of the two payloads. These interfaces consist of T-shaped connecting elements on the payload side, slid through a rail attached to the wing side. A preliminary investigation showed that the back connection of the right-wing-to-payload interface was the predominant source of nonlinear distortions in the aircraft dynamics, and is therefore the focus of this benchmark study.</p> <p>A detailed formulation of the identification problem can be found here. All the provided files and information on the F-16 aircraft benchmark system are available for download here. This zip-file contains a detailed system description, the estimation and test data sets, and some pictures of the setup. The data is available in the .csv and .mat file format.</p> <p>Please refer to the F16 benchmark as:</p> <p>J.P. No\u00ebl and M. Schoukens, F-16 aircraft benchmark based on ground vibration test data, 2017 Workshop on Nonlinear System Identification Benchmarks, pp. 19-23, Brussels, Belgium, April 24-26, 2017.</p>"},{"location":"examples/f_16_benchmark/#visualizating-the-data","title":"Visualizating the data\u00b6","text":""},{"location":"examples/f_16_benchmark/#spliting-the-data","title":"Spliting the data\u00b6","text":""},{"location":"examples/f_16_benchmark/#setting-the-input-lags","title":"Setting the input lags\u00b6","text":""},{"location":"examples/f_16_benchmark/#model-training-and-evalutation","title":"Model training and evalutation\u00b6","text":""},{"location":"examples/f_16_benchmark/#information-criteria-plot","title":"Information Criteria plot\u00b6","text":""},{"location":"examples/fourier_basis_function/","title":"Fourier Basis Function","text":"<p>This example shows how changing or adding a new basis function could improve the model</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.parameter_estimation import LeastSquares, RecursiveLeastSquares\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.metrics import root_relative_squared_error\n\nnp.seterr(all=\"ignore\")\nnp.random.seed(1)\n\n%matplotlib inline\n</pre> import numpy as np import matplotlib.pyplot as plt  from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function import Polynomial, Fourier from sysidentpy.parameter_estimation import LeastSquares, RecursiveLeastSquares from sysidentpy.utils.plotting import plot_results from sysidentpy.metrics import root_relative_squared_error  np.seterr(all=\"ignore\") np.random.seed(1)  %matplotlib inline In\u00a0[2]: Copied! <pre># Simulated system\ndef system_equation(y, u):\n    yk = (\n        (0.2 - 0.75 * np.cos(-y[0] ** 2)) * np.cos(y[0])\n        - (0.15 + 0.45 * np.cos(-y[0] ** 2)) * np.cos(y[1])\n        + np.cos(u[0])\n        + 0.2 * u[1]\n        + 0.7 * u[0] * u[1]\n    )\n    return yk\n\n\nrepetition = 5\nrandom_samples = 200\ntotal_time = repetition * random_samples\nn = np.arange(0, total_time)\n\n# Generating input\nx = np.random.normal(size=(random_samples,)).repeat(repetition)\n\n\n_, ax = plt.subplots(figsize=(12, 6))\nax.step(n, x)\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$x[n]$\", fontsize=18)\nplt.show()\n</pre> # Simulated system def system_equation(y, u):     yk = (         (0.2 - 0.75 * np.cos(-y[0] ** 2)) * np.cos(y[0])         - (0.15 + 0.45 * np.cos(-y[0] ** 2)) * np.cos(y[1])         + np.cos(u[0])         + 0.2 * u[1]         + 0.7 * u[0] * u[1]     )     return yk   repetition = 5 random_samples = 200 total_time = repetition * random_samples n = np.arange(0, total_time)  # Generating input x = np.random.normal(size=(random_samples,)).repeat(repetition)   _, ax = plt.subplots(figsize=(12, 6)) ax.step(n, x) ax.set_xlabel(\"$n$\", fontsize=18) ax.set_ylabel(\"$x[n]$\", fontsize=18) plt.show() In\u00a0[3]: Copied! <pre>y = np.empty_like(x)\n# Initial Conditions\ny0 = [0, 0]\n\n# Simulate it\ny[0:2] = y0\nfor i in range(2, len(y)):\n    y[i] = system_equation(\n        [y[i - 1], y[i - 2]], [x[i - 1], x[i - 2]]\n    ) + np.random.normal(scale=0.1)\n\n# Plot\n_, ax = plt.subplots(figsize=(12, 6))\nax.plot(n, y)\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.grid()\nplt.show()\n</pre> y = np.empty_like(x) # Initial Conditions y0 = [0, 0]  # Simulate it y[0:2] = y0 for i in range(2, len(y)):     y[i] = system_equation(         [y[i - 1], y[i - 2]], [x[i - 1], x[i - 2]]     ) + np.random.normal(scale=0.1)  # Plot _, ax = plt.subplots(figsize=(12, 6)) ax.plot(n, y) ax.set_xlabel(\"$n$\", fontsize=18) ax.set_ylabel(\"$y[n]$\", fontsize=18) ax.grid() plt.show() In\u00a0[4]: Copied! <pre># Noise free data\nynoise_free = y.copy()\n\n# Generate noise\nv = np.random.normal(scale=0.5, size=y.shape)\n\n# Data corrupted with noise\nynoisy = ynoise_free + v\n\n# Plot\n_, ax = plt.subplots(figsize=(14, 8))\nax.plot(n, ynoise_free, label=\"Noise-free data\")\nax.plot(n, ynoisy, label=\"Corrupted data\")\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.legend(fontsize=18)\nplt.show()\n</pre> # Noise free data ynoise_free = y.copy()  # Generate noise v = np.random.normal(scale=0.5, size=y.shape)  # Data corrupted with noise ynoisy = ynoise_free + v  # Plot _, ax = plt.subplots(figsize=(14, 8)) ax.plot(n, ynoise_free, label=\"Noise-free data\") ax.plot(n, ynoisy, label=\"Corrupted data\") ax.set_xlabel(\"$n$\", fontsize=18) ax.set_ylabel(\"$y[n]$\", fontsize=18) ax.legend(fontsize=18) plt.show() In\u00a0[5]: Copied! <pre>n_train = 700\n\n# Identification data\ny_train = ynoisy[:n_train].reshape(-1, 1)\nx_train = x[:n_train].reshape(-1, 1)\n\n# Validation data\ny_test = ynoise_free[n_train:].reshape(-1, 1)\nx_test = x[n_train:].reshape(-1, 1)\n</pre> n_train = 700  # Identification data y_train = ynoisy[:n_train].reshape(-1, 1) x_train = x[:n_train].reshape(-1, 1)  # Validation data y_test = ynoise_free[n_train:].reshape(-1, 1) x_test = x[n_train:].reshape(-1, 1) In\u00a0[26]: Copied! <pre>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nsysidentpy = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    estimator=estimator,\n    err_tol=None,\n)\nsysidentpy.fit(X=x_train, y=y_train)\n\nyhat = sysidentpy.predict(X=x_test, y=y_test)\nfrols_loss = root_relative_squared_error(\n    y_test[sysidentpy.max_lag :], yhat[sysidentpy.max_lag :]\n)\nprint(frols_loss)\n\nplot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :])\n</pre> basis_function = Polynomial(degree=2) estimator = LeastSquares()  sysidentpy = FROLS(     order_selection=True,     n_info_values=15,     xlag=2,     ylag=2,     basis_function=basis_function,     model_type=\"NARMAX\",     estimator=estimator,     err_tol=None, ) sysidentpy.fit(X=x_train, y=y_train)  yhat = sysidentpy.predict(X=x_test, y=y_test) frols_loss = root_relative_squared_error(     y_test[sysidentpy.max_lag :], yhat[sysidentpy.max_lag :] ) print(frols_loss)  plot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :]) <pre>0.6768251106751224\n</pre> In\u00a0[13]: Copied! <pre>basis_function = Fourier(degree=2, n=2, p=2 * np.pi, ensemble=True)\nsysidentpy = FROLS(\n    order_selection=True,\n    n_info_values=70,\n    xlag=2,\n    ylag=2,  # the lags for all models will be 13\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    err_tol=None,\n)\nsysidentpy.fit(X=x_train, y=y_train)\n\nyhat = sysidentpy.predict(X=x_test, y=y_test)\nfrols_loss = root_relative_squared_error(\n    y_test[sysidentpy.max_lag :], yhat[sysidentpy.max_lag :]\n)\nprint(frols_loss)\n\nplot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :])\n</pre> basis_function = Fourier(degree=2, n=2, p=2 * np.pi, ensemble=True) sysidentpy = FROLS(     order_selection=True,     n_info_values=70,     xlag=2,     ylag=2,  # the lags for all models will be 13     basis_function=basis_function,     model_type=\"NARMAX\",     err_tol=None, ) sysidentpy.fit(X=x_train, y=y_train)  yhat = sysidentpy.predict(X=x_test, y=y_test) frols_loss = root_relative_squared_error(     y_test[sysidentpy.max_lag :], yhat[sysidentpy.max_lag :] ) print(frols_loss)  plot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :]) <pre>0.3742244715879492\n</pre>"},{"location":"examples/fourier_basis_function/#fourier-basis-function","title":"Fourier Basis Function\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"examples/fourier_basis_function/#defining-the-system","title":"Defining the system\u00b6","text":""},{"location":"examples/fourier_basis_function/#simulate-the-system","title":"Simulate the system\u00b6","text":""},{"location":"examples/fourier_basis_function/#adding-noise-to-the-system","title":"Adding noise to the system\u00b6","text":""},{"location":"examples/fourier_basis_function/#generating-training-and-test-data","title":"Generating training and test data\u00b6","text":""},{"location":"examples/fourier_basis_function/#polynomial-basis-function","title":"Polynomial Basis Function\u00b6","text":"<p>As you can see bellow, using only the polynomial basis function with the following parameters do not result in a bad model. However, lets check how is the performance using the Fourier Basis Function.</p>"},{"location":"examples/fourier_basis_function/#ensembling-a-fourier-basis-function","title":"Ensembling a Fourier Basis Function\u00b6","text":"<p>In this case, adding the Fourier Basis Function solves the problem and returns a model capable to predict the defined system</p>"},{"location":"examples/fourier_basis_function/#important","title":"Important\u00b6","text":"<p>Currently you can't get the model representation using <code>sysidentpy.regressor_code</code> for Fourier NARX models. Actually, you can use the method, but the representation is not accurate because we don't make clear what are the regressors related to the polynomial or related to the fourier basis function. This is a improvement to be done in future updates!</p>"},{"location":"examples/general_estimators/","title":"Building NARX models using general estimators","text":"<p>In this example we will create NARX models using different estimator like GradientBoostingRegressor, Bayesian Regression, Automatic Relevance Determination (ARD) Regression and Catboost</p> In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nfrom sysidentpy.metrics import mean_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.general_estimators import NARX\nfrom sklearn.linear_model import BayesianRidge, ARDRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom catboost import CatBoostRegressor\n\nfrom sysidentpy.basis_function._basis_function import Polynomial, Fourier\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</pre> import matplotlib.pyplot as plt from sysidentpy.metrics import mean_squared_error from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.general_estimators import NARX from sklearn.linear_model import BayesianRidge, ARDRegression from sklearn.ensemble import GradientBoostingRegressor from catboost import CatBoostRegressor  from sysidentpy.basis_function._basis_function import Polynomial, Fourier from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) In\u00a0[2]: Copied! <pre># simulated dataset\nx_train, x_valid, y_train, y_valid = get_siso_data(\n    n=10000, colored_noise=False, sigma=0.01, train_percentage=80\n)\n</pre> # simulated dataset x_train, x_valid, y_train, y_valid = get_siso_data(     n=10000, colored_noise=False, sigma=0.01, train_percentage=80 ) In\u00a0[3]: Copied! <pre>catboost = CatBoostRegressor(iterations=300, learning_rate=0.1, depth=6)\n</pre> catboost = CatBoostRegressor(iterations=300, learning_rate=0.1, depth=6) In\u00a0[4]: Copied! <pre>gb = GradientBoostingRegressor(\n    loss=\"quantile\",\n    alpha=0.90,\n    n_estimators=250,\n    max_depth=10,\n    learning_rate=0.1,\n    min_samples_leaf=9,\n    min_samples_split=9,\n)\n</pre> gb = GradientBoostingRegressor(     loss=\"quantile\",     alpha=0.90,     n_estimators=250,     max_depth=10,     learning_rate=0.1,     min_samples_leaf=9,     min_samples_split=9, ) In\u00a0[5]: Copied! <pre>def plot_results_tmp(y_valid, yhat):\n    _, ax = plt.subplots(figsize=(14, 8))\n    ax.plot(y_valid[:200], label=\"Data\", marker=\"o\")\n    ax.plot(yhat[:200], label=\"Prediction\", marker=\"*\")\n    ax.set_xlabel(\"$n$\", fontsize=18)\n    ax.set_ylabel(\"$y[n]$\", fontsize=18)\n    ax.grid()\n    ax.legend(fontsize=18)\n    plt.show()\n</pre> def plot_results_tmp(y_valid, yhat):     _, ax = plt.subplots(figsize=(14, 8))     ax.plot(y_valid[:200], label=\"Data\", marker=\"o\")     ax.plot(yhat[:200], label=\"Prediction\", marker=\"*\")     ax.set_xlabel(\"$n$\", fontsize=18)     ax.set_ylabel(\"$y[n]$\", fontsize=18)     ax.grid()     ax.legend(fontsize=18)     plt.show() In\u00a0[6]: Copied! <pre>catboost.fit(x_train, y_train, verbose=False)\nplot_results_tmp(y_valid, catboost.predict(x_valid))\n</pre> catboost.fit(x_train, y_train, verbose=False) plot_results_tmp(y_valid, catboost.predict(x_valid)) In\u00a0[7]: Copied! <pre>gb.fit(x_train, y_train.ravel())\nplot_results_tmp(y_valid, gb.predict(x_valid))\n</pre> gb.fit(x_train, y_train.ravel()) plot_results_tmp(y_valid, gb.predict(x_valid)) In\u00a0[8]: Copied! <pre>basis_function = Fourier(degree=1)\n\ncatboost_narx = NARX(\n    base_estimator=CatBoostRegressor(iterations=300, learning_rate=0.1, depth=8),\n    xlag=10,\n    ylag=10,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    fit_params={\"verbose\": False},\n)\n\ncatboost_narx.fit(X=x_train, y=y_train)\nyhat = catboost_narx.predict(X=x_valid, y=y_valid, steps_ahead=1)\nprint(\"MSE: \", mean_squared_error(y_valid, yhat))\nplot_results(y=y_valid, yhat=yhat, n=200)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> basis_function = Fourier(degree=1)  catboost_narx = NARX(     base_estimator=CatBoostRegressor(iterations=300, learning_rate=0.1, depth=8),     xlag=10,     ylag=10,     basis_function=basis_function,     model_type=\"NARMAX\",     fit_params={\"verbose\": False}, )  catboost_narx.fit(X=x_train, y=y_train) yhat = catboost_narx.predict(X=x_valid, y=y_valid, steps_ahead=1) print(\"MSE: \", mean_squared_error(y_valid, yhat)) plot_results(y=y_valid, yhat=yhat, n=200) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>MSE:  0.00024145290395678653\n</pre> In\u00a0[9]: Copied! <pre>basis_function = Fourier(degree=1)\n\ngb_narx = NARX(\n    base_estimator=GradientBoostingRegressor(\n        loss=\"quantile\",\n        alpha=0.90,\n        n_estimators=250,\n        max_depth=10,\n        learning_rate=0.1,\n        min_samples_leaf=9,\n        min_samples_split=9,\n    ),\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\ngb_narx.fit(X=x_train, y=y_train)\nyhat = gb_narx.predict(X=x_valid, y=y_valid)\nprint(mean_squared_error(y_valid, yhat))\n\nplot_results(y=y_valid, yhat=yhat, n=200)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> basis_function = Fourier(degree=1)  gb_narx = NARX(     base_estimator=GradientBoostingRegressor(         loss=\"quantile\",         alpha=0.90,         n_estimators=250,         max_depth=10,         learning_rate=0.1,         min_samples_leaf=9,         min_samples_split=9,     ),     xlag=2,     ylag=2,     basis_function=basis_function,     model_type=\"NARMAX\", )  gb_narx.fit(X=x_train, y=y_train) yhat = gb_narx.predict(X=x_valid, y=y_valid) print(mean_squared_error(y_valid, yhat))  plot_results(y=y_valid, yhat=yhat, n=200) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>0.0011824693986863938\n</pre> In\u00a0[10]: Copied! <pre>from sysidentpy.general_estimators import NARX\n\nARD_narx = NARX(\n    base_estimator=ARDRegression(),\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\nARD_narx.fit(X=x_train, y=y_train)\nyhat = ARD_narx.predict(X=x_valid, y=y_valid)\nprint(mean_squared_error(y_valid, yhat))\n\nplot_results(y=y_valid, yhat=yhat, n=200)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> from sysidentpy.general_estimators import NARX  ARD_narx = NARX(     base_estimator=ARDRegression(),     xlag=2,     ylag=2,     basis_function=basis_function,     model_type=\"NARMAX\", )  ARD_narx.fit(X=x_train, y=y_train) yhat = ARD_narx.predict(X=x_valid, y=y_valid) print(mean_squared_error(y_valid, yhat))  plot_results(y=y_valid, yhat=yhat, n=200) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>0.0011058934497373794\n</pre> In\u00a0[11]: Copied! <pre>from sysidentpy.general_estimators import NARX\n\nBayesianRidge_narx = NARX(\n    base_estimator=BayesianRidge(),\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\nBayesianRidge_narx.fit(X=x_train, y=y_train)\nyhat = BayesianRidge_narx.predict(X=x_valid, y=y_valid)\nprint(mean_squared_error(y_valid, yhat))\n\nplot_results(y=y_valid, yhat=yhat, n=200)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> from sysidentpy.general_estimators import NARX  BayesianRidge_narx = NARX(     base_estimator=BayesianRidge(),     xlag=2,     ylag=2,     basis_function=basis_function,     model_type=\"NARMAX\", )  BayesianRidge_narx.fit(X=x_train, y=y_train) yhat = BayesianRidge_narx.predict(X=x_valid, y=y_valid) print(mean_squared_error(y_valid, yhat))  plot_results(y=y_valid, yhat=yhat, n=200) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>0.0011077874945734536\n</pre>"},{"location":"examples/general_estimators/#building-narx-models-using-general-estimators","title":"Building NARX models using general estimators\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"examples/general_estimators/#importance-of-the-narx-architecture","title":"Importance of the NARX architecture\u00b6","text":"<p>To get an idea of the importance of the NARX architecture, lets take a look in the performance of the models without the NARX configuration.</p>"},{"location":"examples/general_estimators/#introducing-the-narx-configuration-using-sysidentpy","title":"Introducing the NARX configuration using SysIdentPy\u00b6","text":"<p>As you can see, you just need to pass the base estimator you want to the NARX class from SysIdentPy do build the NARX model! You can choose the lags of the input and output variables to build the regressor matrix.</p> <p>We keep the fit/predict method to make the process straightforward.</p>"},{"location":"examples/general_estimators/#narx-with-catboost","title":"NARX with Catboost\u00b6","text":""},{"location":"examples/general_estimators/#narx-with-gradient-boosting","title":"NARX with Gradient Boosting\u00b6","text":""},{"location":"examples/general_estimators/#narx-with-ard","title":"NARX with ARD\u00b6","text":""},{"location":"examples/general_estimators/#narx-with-bayesian-ridge","title":"NARX with Bayesian Ridge\u00b6","text":""},{"location":"examples/general_estimators/#note","title":"Note\u00b6","text":"<p>Remember you can use n-steps-ahead prediction and NAR and NFIR models now. Check how to use it in their respective examples.</p>"},{"location":"examples/identification_of_an_electromechanical_system/","title":"Identification of an electromechanical system","text":"In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import RecursiveLeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</pre> import numpy as np import pandas as pd from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.parameter_estimation import RecursiveLeastSquares from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) In\u00a0[2]: Copied! <pre>df1 = pd.read_csv(\"../examples/datasets/x_cc.csv\")\ndf2 = pd.read_csv(\"../examples/datasets/y_cc.csv\")\n</pre> df1 = pd.read_csv(\"../examples/datasets/x_cc.csv\") df2 = pd.read_csv(\"../examples/datasets/y_cc.csv\") In\u00a0[3]: Copied! <pre>df2[5000:80000].plot(figsize=(10, 4))\n</pre> df2[5000:80000].plot(figsize=(10, 4)) Out[3]: <pre>&lt;Axes: &gt;</pre> In\u00a0[4]: Copied! <pre># we will decimate the data using d=500 in this example\nx_train, x_valid = np.split(df1.iloc[::500].values, 2)\ny_train, y_valid = np.split(df2.iloc[::500].values, 2)\n</pre> # we will decimate the data using d=500 in this example x_train, x_valid = np.split(df1.iloc[::500].values, 2) y_train, y_valid = np.split(df2.iloc[::500].values, 2) In\u00a0[5]: Copied! <pre>basis_function = Polynomial(degree=2)\nestimator = RecursiveLeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=100,\n    ylag=5,\n    xlag=5,\n    info_criteria=\"bic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\n</pre> basis_function = Polynomial(degree=2) estimator = RecursiveLeastSquares()  model = FROLS(     order_selection=True,     n_info_values=100,     ylag=5,     xlag=5,     info_criteria=\"bic\",     estimator=estimator,     basis_function=basis_function,     err_tol=None, ) In\u00a0[6]: Copied! <pre>model.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  plot_results(y=y_valid, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:618: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 66\n  self.info_values = self.information_criterion(reg_matrix, y)\n</pre> <pre>0.05681502501595064\n        Regressors   Parameters             ERR\n0           y(k-1)   1.5935E+00  9.86000310E-01\n1        x1(k-1)^2   1.1202E+02  7.94813324E-03\n2         y(k-2)^2  -1.7469E-05  2.50921747E-03\n3    x1(k-1)y(k-1)  -1.5994E-01  1.43297462E-03\n4           y(k-2)  -7.4013E-01  1.02774988E-03\n5    x1(k-1)y(k-2)   1.0771E-01  5.35195948E-04\n6     y(k-3)y(k-1)   4.2578E-05  3.46258211E-04\n7        x1(k-4)^2  -6.1823E+00  6.91218347E-05\n8    x1(k-1)y(k-3)  -3.0064E-02  2.83751722E-05\n9     y(k-4)y(k-1)  -1.4505E-05  2.01620114E-05\n10  x1(k-4)x1(k-1)  -2.7490E+00  1.09189469E-05\n11    y(k-4)y(k-2)   7.2062E-06  1.27131624E-05\n12   x1(k-5)y(k-1)  -8.5557E-04  6.53111914E-06\n13  x1(k-3)x1(k-2)  -9.8645E-01  4.24331903E-06\n14  x1(k-2)x1(k-1)  -2.3609E+00  6.41299982E-06\n15         x1(k-3)  -2.0121E+02  6.43059002E-06\n16   x1(k-1)y(k-5)   3.0338E-03  2.76577885E-06\n17   x1(k-3)y(k-1)   3.2426E-02  2.79523223E-06\n18   x1(k-4)y(k-1)   5.9510E-03  1.62218750E-06\n19               1  -4.2071E+01  1.13359933E-06\n</pre> In\u00a0[7]: Copied! <pre>from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import (\n    RandomForestRegressor,\n    AdaBoostRegressor,\n    GradientBoostingRegressor,\n)\nfrom sklearn.naive_bayes import GaussianNB\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import BayesianRidge, ARDRegression\nfrom sysidentpy.general_estimators import NARX\n\nbasis_function = Polynomial(degree=2)\nxlag = 5\nylag = 5\n\nestimators = [\n    (\n        \"KNeighborsRegressor\",\n        NARX(\n            base_estimator=KNeighborsRegressor(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n            model_type=\"NARMAX\",\n        ),\n    ),\n    (\n        \"NARX-DecisionTreeRegressor\",\n        NARX(\n            base_estimator=DecisionTreeRegressor(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX-RandomForestRegressor\",\n        NARX(\n            base_estimator=RandomForestRegressor(n_estimators=200),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX-Catboost\",\n        NARX(\n            base_estimator=CatBoostRegressor(\n                iterations=800, learning_rate=0.1, depth=8\n            ),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n            fit_params={\"verbose\": False},\n        ),\n    ),\n    (\n        \"NARX-ARD\",\n        NARX(\n            base_estimator=ARDRegression(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"FROLS-Polynomial_NARX\",\n        FROLS(\n            order_selection=True,\n            n_info_values=50,\n            ylag=xlag,\n            xlag=ylag,\n            info_criteria=\"bic\",\n            estimator=estimator,\n            basis_function=basis_function,\n            err_tol=None,\n        ),\n    ),\n]\n\nresultados = {}\nfor nome_do_modelo, modelo in estimators:\n    resultados[\"%s\" % (nome_do_modelo)] = []\n    modelo.fit(X=x_train, y=y_train)\n    yhat = modelo.predict(X=x_valid, y=y_valid)\n    result = root_relative_squared_error(\n        y_valid[modelo.max_lag :], yhat[modelo.max_lag :]\n    )\n    resultados[\"%s\" % (nome_do_modelo)].append(result)\n    print(nome_do_modelo, \"%.3f\" % np.mean(result))\n</pre> from sklearn.neighbors import KNeighborsRegressor from sklearn.svm import SVC, LinearSVC, NuSVC from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import (     RandomForestRegressor,     AdaBoostRegressor,     GradientBoostingRegressor, ) from sklearn.naive_bayes import GaussianNB from catboost import CatBoostRegressor from sklearn.linear_model import BayesianRidge, ARDRegression from sysidentpy.general_estimators import NARX  basis_function = Polynomial(degree=2) xlag = 5 ylag = 5  estimators = [     (         \"KNeighborsRegressor\",         NARX(             base_estimator=KNeighborsRegressor(),             xlag=xlag,             ylag=ylag,             basis_function=basis_function,             model_type=\"NARMAX\",         ),     ),     (         \"NARX-DecisionTreeRegressor\",         NARX(             base_estimator=DecisionTreeRegressor(),             xlag=xlag,             ylag=ylag,             basis_function=basis_function,         ),     ),     (         \"NARX-RandomForestRegressor\",         NARX(             base_estimator=RandomForestRegressor(n_estimators=200),             xlag=xlag,             ylag=ylag,             basis_function=basis_function,         ),     ),     (         \"NARX-Catboost\",         NARX(             base_estimator=CatBoostRegressor(                 iterations=800, learning_rate=0.1, depth=8             ),             xlag=xlag,             ylag=ylag,             basis_function=basis_function,             fit_params={\"verbose\": False},         ),     ),     (         \"NARX-ARD\",         NARX(             base_estimator=ARDRegression(),             xlag=xlag,             ylag=ylag,             basis_function=basis_function,         ),     ),     (         \"FROLS-Polynomial_NARX\",         FROLS(             order_selection=True,             n_info_values=50,             ylag=xlag,             xlag=ylag,             info_criteria=\"bic\",             estimator=estimator,             basis_function=basis_function,             err_tol=None,         ),     ), ]  resultados = {} for nome_do_modelo, modelo in estimators:     resultados[\"%s\" % (nome_do_modelo)] = []     modelo.fit(X=x_train, y=y_train)     yhat = modelo.predict(X=x_valid, y=y_valid)     result = root_relative_squared_error(         y_valid[modelo.max_lag :], yhat[modelo.max_lag :]     )     resultados[\"%s\" % (nome_do_modelo)].append(result)     print(nome_do_modelo, \"%.3f\" % np.mean(result)) <pre>KNeighborsRegressor 1.168\nNARX-DecisionTreeRegressor 0.190\nNARX-RandomForestRegressor 0.151\nNARX-Catboost 0.121\nNARX-ARD 0.083\nFROLS-Polynomial_NARX 0.057\n</pre> In\u00a0[9]: Copied! <pre>for model_name, metric in sorted(\n    resultados.items(), key=lambda x: np.mean(x[1]), reverse=False\n):\n    print(model_name, np.mean(metric))\n</pre> for model_name, metric in sorted(     resultados.items(), key=lambda x: np.mean(x[1]), reverse=False ):     print(model_name, np.mean(metric)) <pre>FROLS-Polynomial_NARX 0.05729765719062527\nNARX-ARD 0.08336072971138789\nNARX-Catboost 0.12137085298392238\nNARX-RandomForestRegressor 0.15102205613876338\nNARX-DecisionTreeRegressor 0.19018792321900427\nKNeighborsRegressor 1.1676227184643708\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/identification_of_an_electromechanical_system/#identification-of-an-electromechanical-system","title":"Identification of an electromechanical system\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>More details about this data can be found in the following paper (in Portuguese): https://www.researchgate.net/publication/320418710_Identificacao_de_um_motorgerador_CC_por_meio_de_modelos_polinomiais_autorregressivos_e_redes_neurais_artificiais</p>"},{"location":"examples/identification_of_an_electromechanical_system/#building-a-polynomial-narx-model","title":"Building a Polynomial NARX model\u00b6","text":""},{"location":"examples/identification_of_an_electromechanical_system/#testing-different-autoregressive-models","title":"Testing different autoregressive models\u00b6","text":""},{"location":"examples/information_criteria_examples/","title":"Information Criteria - Examples","text":"<p>Here we import the NARMAX model, the metric for model evaluation and the methods to generate sample data for tests. Also, we import pandas for specific usage.</p> In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.parameter_estimation import LeastSquares from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_results In\u00a0[22]: Copied! <pre>x_train, x_test, y_train, y_test = get_siso_data(\n    n=100, colored_noise=False, sigma=0.1, train_percentage=70\n)\n</pre> x_train, x_test, y_train, y_test = get_siso_data(     n=100, colored_noise=False, sigma=0.1, train_percentage=70 ) <p>The idea is to show the impact of the information criteria to select the number of terms to compose the final model. You will se why it is an auxiliary tool and let the algorithm select the number of terms based on the minimum value is not a good idea when dealing with data highly corrupted by noise (even white noise)</p> <p>Note: You may find different results when running the examples. This is due the fact we are not setting a fixed random generator for the sample data. However, the main analysis remain.</p> In\u00a0[23]: Copied! <pre>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    # estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\nmodel.fit(X=x_train, y=y_train)\n\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_test, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</pre> basis_function = Polynomial(degree=2) estimator = LeastSquares()  model = FROLS(     order_selection=True,     n_info_values=15,     ylag=2,     xlag=2,     info_criteria=\"aic\",     # estimator=estimator,     basis_function=basis_function,     err_tol=None, ) model.fit(X=x_train, y=y_train)  yhat = model.predict(X=x_test, y=y_test) rrse = root_relative_squared_error(y_test, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) plot_results(y=y_test, yhat=yhat, n=1000)  xaxis = np.arange(1, model.n_info_values + 1) plt.plot(xaxis, model.info_values) plt.xlabel(\"n_terms\") plt.ylabel(\"Information Criteria\") <pre>0.1681621129389993\n       Regressors   Parameters             ERR\n0         x1(k-2)   9.2076E-01  9.41001395E-01\n1          y(k-1)   1.7063E-01  2.71018399E-02\n2   x1(k-1)y(k-1)   1.7342E-01  8.79812755E-03\n3   x1(k-1)y(k-2)  -9.7902E-02  2.75495842E-03\n4  x1(k-2)x1(k-1)   4.9319E-02  1.85339089E-03\n5        y(k-2)^2  -5.6743E-02  1.02439383E-03\n6         x1(k-1)  -2.0179E-02  6.78305323E-04\n</pre> Out[23]: <pre>Text(0, 0.5, 'Information Criteria')</pre> In\u00a0[24]: Copied! <pre>model.info_values\n</pre> model.info_values Out[24]: <pre>array([-273.81858224, -311.60797635, -331.34011486, -338.49936124,\n       -342.10339048, -342.27073244, -342.82764626, -342.16492383,\n       -341.04704839, -339.58437034, -337.79642875, -336.20531349,\n       -333.72427584, -331.48645717, -329.53042523])</pre> <p>As can be seen above, the minimum value make the algorithm choose a model with 4 terms. However, if you check the plot, 3 terms is the best choice. Increasing the number of terms from 3 upwards do not lead to a better model since the difference is very small.</p> <p>In this case, you should run the model again with the parameters n_terms=3! The ERR algorithm ordered the terms in a correct way, so you will get the exact model structure again!</p> In\u00a0[25]: Copied! <pre>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aicc\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_test, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</pre> basis_function = Polynomial(degree=2) estimator = LeastSquares()  model = FROLS(     order_selection=True,     n_info_values=15,     ylag=2,     xlag=2,     info_criteria=\"aicc\",     estimator=estimator,     basis_function=basis_function, ) model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_test, y=y_test) rrse = root_relative_squared_error(y_test, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) plot_results(y=y_test, yhat=yhat, n=1000)  xaxis = np.arange(1, model.n_info_values + 1) plt.plot(xaxis, model.info_values) plt.xlabel(\"n_terms\") plt.ylabel(\"Information Criteria\") <pre>0.1697650652880654\n       Regressors   Parameters             ERR\n0         x1(k-2)   9.2659E-01  9.41001395E-01\n1          y(k-1)   1.7219E-01  2.71018399E-02\n2   x1(k-1)y(k-1)   1.7454E-01  8.79812755E-03\n3   x1(k-1)y(k-2)  -1.0170E-01  2.75495842E-03\n4  x1(k-2)x1(k-1)   5.7955E-02  1.85339089E-03\n5        y(k-2)^2  -4.8117E-02  1.02439383E-03\n6         x1(k-1)  -2.4728E-02  6.78305323E-04\n</pre> Out[25]: <pre>Text(0, 0.5, 'Information Criteria')</pre> In\u00a0[26]: Copied! <pre>model.info_values\n</pre> model.info_values Out[26]: <pre>array([-273.99834706, -311.49708248, -331.28179881, -338.08184328,\n       -341.32119362, -341.3373076 , -341.51818541, -340.50119461,\n       -339.16231408, -336.96924137, -334.34018578, -331.63849434,\n       -328.92685178, -325.8181513 , -322.55039655])</pre> <p>As can be seen above, the minimum value make the algorithm choose a model with 4 terms. AICc, however, have major differences compared with AIC when the number of samples is small.</p> <p>In this case, you should run the model again with the parameters n_terms=3! The ERR algorithm ordered the terms in a correct way, so you will get the exact model structure again!</p> In\u00a0[31]: Copied! <pre>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"bic\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_test, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</pre> basis_function = Polynomial(degree=2) estimator = LeastSquares()  model = FROLS(     order_selection=True,     n_info_values=15,     ylag=2,     xlag=2,     info_criteria=\"bic\",     estimator=estimator,     basis_function=basis_function, )  model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_test, y=y_test) rrse = root_relative_squared_error(y_test, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) plot_results(y=y_test, yhat=yhat, n=1000)  xaxis = np.arange(1, model.n_info_values + 1) plt.plot(xaxis, model.info_values) plt.xlabel(\"n_terms\") plt.ylabel(\"Information Criteria\") <pre>0.16868631871856155\n       Regressors   Parameters             ERR\n0         x1(k-2)   9.3050E-01  9.41001395E-01\n1          y(k-1)   1.7980E-01  2.71018399E-02\n2   x1(k-1)y(k-1)   1.7026E-01  8.79812755E-03\n3   x1(k-1)y(k-2)  -8.5581E-02  2.75495842E-03\n4  x1(k-2)x1(k-1)   7.0047E-02  1.85339089E-03\n</pre> Out[31]: <pre>Text(0, 0.5, 'Information Criteria')</pre> In\u00a0[32]: Copied! <pre>model.info_values\n</pre> model.info_values Out[32]: <pre>array([-271.83944541, -307.24268246, -324.99827569, -329.8387331 ,\n       -331.19139703, -329.39731055, -327.84829815, -325.18581094,\n       -322.29019301, -318.63381344, -314.63988674, -310.67712915,\n       -306.81399235, -302.66957173, -298.4885502 ])</pre> <p>BIC did a better job in this case! The way it penalizes the model regarding the number of terms ensure that the minimum value here was exact the number of expected terms to compose the model. Good, but not always the best method!</p> In\u00a0[33]: Copied! <pre>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"lilc\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_test, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</pre> basis_function = Polynomial(degree=2) estimator = LeastSquares()  model = FROLS(     order_selection=True,     n_info_values=15,     ylag=2,     xlag=2,     info_criteria=\"lilc\",     estimator=estimator,     basis_function=basis_function, ) model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_test, y=y_test) rrse = root_relative_squared_error(y_test, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) plot_results(y=y_test, yhat=yhat, n=1000)  xaxis = np.arange(1, model.n_info_values + 1) plt.plot(xaxis, model.info_values) plt.xlabel(\"n_terms\") plt.ylabel(\"Information Criteria\") <pre>0.16868631871856155\n       Regressors   Parameters             ERR\n0         x1(k-2)   9.3050E-01  9.41001395E-01\n1          y(k-1)   1.7980E-01  2.71018399E-02\n2   x1(k-1)y(k-1)   1.7026E-01  8.79812755E-03\n3   x1(k-1)y(k-2)  -8.5581E-02  2.75495842E-03\n4  x1(k-2)x1(k-1)   7.0047E-02  1.85339089E-03\n</pre> Out[33]: <pre>Text(0, 0.5, 'Information Criteria')</pre> In\u00a0[34]: Copied! <pre>model.info_values\n</pre> model.info_values Out[34]: <pre>array([-273.17951619, -309.92282401, -329.01848803, -335.19901621,\n       -337.89175092, -337.43773522, -337.22879359, -335.90637716,\n       -334.35083001, -332.03452122, -329.3806653 , -326.75797849,\n       -324.23491246, -321.43056262, -318.58961187])</pre> <p>LILC also includes spurious terms. Like AIC, it fails to automatically select the correct terms but you could select the right number based on the plot above!</p> In\u00a0[35]: Copied! <pre>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"fpe\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</pre> basis_function = Polynomial(degree=2) estimator = LeastSquares()  model = FROLS(     order_selection=True,     n_info_values=15,     ylag=2,     xlag=2,     info_criteria=\"fpe\",     estimator=estimator,     basis_function=basis_function, ) model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_test, y=y_test) rrse = root_relative_squared_error(y_test, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  xaxis = np.arange(1, model.n_info_values + 1) plt.plot(xaxis, model.info_values) plt.xlabel(\"n_terms\") plt.ylabel(\"Information Criteria\") <pre>0.1697650652880654\n       Regressors   Parameters             ERR\n0         x1(k-2)   9.2659E-01  9.41001395E-01\n1          y(k-1)   1.7219E-01  2.71018399E-02\n2   x1(k-1)y(k-1)   1.7454E-01  8.79812755E-03\n3   x1(k-1)y(k-2)  -1.0170E-01  2.75495842E-03\n4  x1(k-2)x1(k-1)   5.7955E-02  1.85339089E-03\n5        y(k-2)^2  -4.8117E-02  1.02439383E-03\n6         x1(k-1)  -2.4728E-02  6.78305323E-04\n</pre> Out[35]: <pre>Text(0, 0.5, 'Information Criteria')</pre> In\u00a0[36]: Copied! <pre>model.info_values\n</pre> model.info_values Out[36]: <pre>array([-274.05880892, -311.68054386, -331.65290152, -338.70751749,\n       -342.27085495, -342.68306863, -343.33508312, -342.86743567,\n       -342.15953986, -340.68281499, -338.85950374, -337.05732543,\n       -335.3437066 , -333.33668596, -331.27985457])</pre> <p>FPE also failed to automatically select the right number of terms! But, as we pointed out before, Information Criteria is an auxiliary tool! If you look at the plots, all the methods allows you to choose the right numbers of terms!</p>"},{"location":"examples/information_criteria_examples/#information-criteria-examples","title":"Information Criteria - Examples\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"examples/information_criteria_examples/#comparing-different-information-criteria-methods","title":"Comparing different information criteria methods\u00b6","text":""},{"location":"examples/information_criteria_examples/#generating-sample-data","title":"Generating sample data\u00b6","text":"<p>The data is generated by simulating the following model: $y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_{k}$</p> <p>If colored_noise is set to True:</p> <p>$e_{k} = 0.8\\nu_{k-1} + \\nu_{k}$</p> <p>where $x$ is a uniformly distributed random variable and $\\nu$ is a gaussian distributed variable with $\\mu=0$ and $\\sigma=0.1$</p> <p>In the next example we will generate a data with 3000 samples with white noise and selecting 90% of the data to train the model.</p>"},{"location":"examples/information_criteria_examples/#aic","title":"AIC\u00b6","text":""},{"location":"examples/information_criteria_examples/#aicc","title":"AICc\u00b6","text":""},{"location":"examples/information_criteria_examples/#bic","title":"BIC\u00b6","text":""},{"location":"examples/information_criteria_examples/#lilc","title":"LILC\u00b6","text":""},{"location":"examples/information_criteria_examples/#fpe","title":"FPE\u00b6","text":""},{"location":"examples/information_criteria_examples/#important-note","title":"Important Note\u00b6","text":"<p>Here we are dealing with a known model structure! Concerning real data, we do not know the right number of terms so the methods above stands as excellent tools to help you out!</p> <p>If you check the metrics above, even with the models with more terms, you will see excellent metrics! But System Identification always search for the best model structure! Model Structure Selection is the core of NARMAX methods! In this respect, the examples are to show basic concepts and how the algorithms work!</p>"},{"location":"examples/load_forecasting_benchmark/","title":"Load forecasting benchmark","text":"<p>We will compare a 1-step ahead forecaster on electricity consumption of a building. The config of the neuralprophet model was taken from the neuralprophet documentation (https://neuralprophet.com/html/example_links/energy_data_example.html)</p> <p>The training will occur on 80% of the data, reserving the last 20% for the validation.</p> <p>Note: the data used in this example can be found in neuralprophet github.</p> In\u00a0[2]: Copied! <pre>from warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS, AOLS, MetaMSS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.neural_network import NARXNN\nfrom sysidentpy.metrics import mean_squared_error\n\nfrom sktime.datasets import load_airline\nfrom neuralprophet import NeuralProphet\nfrom neuralprophet import set_random_seed\n\n\nsimplefilter(\"ignore\", FutureWarning)\nnp.seterr(all=\"ignore\")\n\n%matplotlib inline\n\nloss = mean_squared_error\n\ndata_location = r\".\\datasets\"\n</pre> from warnings import simplefilter import matplotlib.pyplot as plt import numpy as np import pandas as pd  from sysidentpy.model_structure_selection import FROLS, AOLS, MetaMSS from sysidentpy.basis_function import Polynomial from sysidentpy.parameter_estimation import LeastSquares from sysidentpy.utils.plotting import plot_results from sysidentpy.neural_network import NARXNN from sysidentpy.metrics import mean_squared_error  from sktime.datasets import load_airline from neuralprophet import NeuralProphet from neuralprophet import set_random_seed   simplefilter(\"ignore\", FutureWarning) np.seterr(all=\"ignore\")  %matplotlib inline  loss = mean_squared_error  data_location = r\".\\datasets\" In\u00a0[3]: Copied! <pre>files = [\"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760))\ndf[\"y\"] = raw.iloc[:, 0].values\n\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\n\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\n\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy = FROLS(\n    order_selection=True,\n    info_criteria=\"bic\",\n    estimator=LeastSquares(),\n    basis_function=basis_function,\n)\nsysidentpy.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])\n\nyhat = sysidentpy.predict(X=x_test, y=y_test, steps_ahead=1)\nsysidentpy_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy.max_lag :]),\n)\nprint(sysidentpy_loss)\n\n\nplot_results(y=y_test[-504:], yhat=yhat[-504:], n=504, figsize=(18, 8))\n</pre> files = [\"\\SanFrancisco_Hospital.csv\"] raw = pd.read_csv(data_location + files[0]) df = pd.DataFrame() df[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760)) df[\"y\"] = raw.iloc[:, 0].values  df_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]  y = df[\"y\"].values.reshape(-1, 1) y_train = df_train[\"y\"].values.reshape(-1, 1) y_test = df_val[\"y\"].values.reshape(-1, 1)  x_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1) x_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)  basis_function = Polynomial(degree=1) sysidentpy = FROLS(     order_selection=True,     info_criteria=\"bic\",     estimator=LeastSquares(),     basis_function=basis_function, ) sysidentpy.fit(X=x_train, y=y_train) x_test = np.concatenate([x_train[-sysidentpy.max_lag :], x_test]) y_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])  yhat = sysidentpy.predict(X=x_test, y=y_test, steps_ahead=1) sysidentpy_loss = loss(     pd.Series(y_test.flatten()[sysidentpy.max_lag :]),     pd.Series(yhat.flatten()[sysidentpy.max_lag :]), ) print(sysidentpy_loss)   plot_results(y=y_test[-504:], yhat=yhat[-504:], n=504, figsize=(18, 8)) <pre>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:618: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 5\n  self.info_values = self.information_criterion(reg_matrix, y)\n</pre> <pre>4183.359498155755\n</pre> In\u00a0[4]: Copied! <pre># files = [\"\\SanFrancisco_Hospital.csv\"]\n# raw = pd.read_csv(data_location + files[0])\n# df = pd.DataFrame()\n# df[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760))\n# df[\"y\"] = raw.iloc[:, 0].values\n#\n# x_train, x_test, x_validation, _ = np.split(df[\"ds\"].dt.hour.values, [5000, 7008, 8760])\n# y_train, y_test, y_validation, _ = np.split(df[\"y\"].values, [5000, 7008, 8760])\n#\n# x_train = x_train.reshape(-1, 1)\n# x_test = x_test.reshape(-1, 1)\n# x_validation = x_validation.reshape(-1, 1)\n#\n# y_train = y_train.reshape(-1, 1)\n# y_test = y_test.reshape(-1, 1)\n# y_validation = y_validation.reshape(-1, 1)\n\nfiles = [\"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760))\ndf[\"y\"] = raw.iloc[:, 0].values\n\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\n\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\n\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy_metamss = MetaMSS(\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    steps_ahead=1,\n    n_agents=15,\n    random_state=42,\n)\nsysidentpy_metamss.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy_metamss.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy_metamss.max_lag :], y_test])\n\nyhat = sysidentpy_metamss.predict(X=x_test, y=y_test, steps_ahead=1)\nmetamss_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy_metamss.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]),\n)\nprint(metamss_loss)\n\n\nplot_results(y=y_test[:700], yhat=yhat[:700], n=504, figsize=(18, 8))\n</pre> # files = [\"\\SanFrancisco_Hospital.csv\"] # raw = pd.read_csv(data_location + files[0]) # df = pd.DataFrame() # df[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760)) # df[\"y\"] = raw.iloc[:, 0].values # # x_train, x_test, x_validation, _ = np.split(df[\"ds\"].dt.hour.values, [5000, 7008, 8760]) # y_train, y_test, y_validation, _ = np.split(df[\"y\"].values, [5000, 7008, 8760]) # # x_train = x_train.reshape(-1, 1) # x_test = x_test.reshape(-1, 1) # x_validation = x_validation.reshape(-1, 1) # # y_train = y_train.reshape(-1, 1) # y_test = y_test.reshape(-1, 1) # y_validation = y_validation.reshape(-1, 1)  files = [\"\\SanFrancisco_Hospital.csv\"] raw = pd.read_csv(data_location + files[0]) df = pd.DataFrame() df[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760)) df[\"y\"] = raw.iloc[:, 0].values  df_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]  y = df[\"y\"].values.reshape(-1, 1) y_train = df_train[\"y\"].values.reshape(-1, 1) y_test = df_val[\"y\"].values.reshape(-1, 1)  x_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1) x_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)  basis_function = Polynomial(degree=1) sysidentpy_metamss = MetaMSS(     xlag=2,     ylag=2,     basis_function=basis_function,     estimator=LeastSquares(),     steps_ahead=1,     n_agents=15,     random_state=42, ) sysidentpy_metamss.fit(X=x_train, y=y_train) x_test = np.concatenate([x_train[-sysidentpy_metamss.max_lag :], x_test]) y_test = np.concatenate([y_train[-sysidentpy_metamss.max_lag :], y_test])  yhat = sysidentpy_metamss.predict(X=x_test, y=y_test, steps_ahead=1) metamss_loss = loss(     pd.Series(y_test.flatten()[sysidentpy_metamss.max_lag :]),     pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]), ) print(metamss_loss)   plot_results(y=y_test[:700], yhat=yhat[:700], n=504, figsize=(18, 8)) <pre>5264.428783519863\n</pre> In\u00a0[5]: Copied! <pre>set_random_seed(42)\nfiles = [\"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760))\ndf[\"y\"] = raw.iloc[:, 0].values\n\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\n\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\n\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\nbasis_function = Polynomial(degree=1)\nsysidentpy_AOLS = AOLS(xlag=2, ylag=2, basis_function=basis_function)\nsysidentpy_AOLS.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy_AOLS.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])\n\nyhat = sysidentpy_AOLS.predict(X=x_test, y=y_test, steps_ahead=1)\naols_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]),\n)\nprint(aols_loss)\n\n\nplot_results(y=y_test[-504:], yhat=yhat[-504:], n=504, figsize=(18, 8))\n</pre> set_random_seed(42) files = [\"\\SanFrancisco_Hospital.csv\"] raw = pd.read_csv(data_location + files[0]) df = pd.DataFrame() df[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760)) df[\"y\"] = raw.iloc[:, 0].values  df_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]  y = df[\"y\"].values.reshape(-1, 1) y_train = df_train[\"y\"].values.reshape(-1, 1) y_test = df_val[\"y\"].values.reshape(-1, 1)  x_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1) x_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1) basis_function = Polynomial(degree=1) sysidentpy_AOLS = AOLS(xlag=2, ylag=2, basis_function=basis_function) sysidentpy_AOLS.fit(X=x_train, y=y_train) x_test = np.concatenate([x_train[-sysidentpy_AOLS.max_lag :], x_test]) y_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])  yhat = sysidentpy_AOLS.predict(X=x_test, y=y_test, steps_ahead=1) aols_loss = loss(     pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),     pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]), ) print(aols_loss)   plot_results(y=y_test[-504:], yhat=yhat[-504:], n=504, figsize=(18, 8)) <pre>5264.42917196841\n</pre> In\u00a0[6]: Copied! <pre>set_random_seed(42)\n\n# set_log_level(\"ERROR\")\nfiles = [\"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760))\ndf[\"y\"] = raw.iloc[:, 0].values\n\nm = NeuralProphet(\n    n_lags=24, ar_sparsity=0.5, num_hidden_layers=2, d_hidden=20, learning_rate=0.001\n)\nmetrics = m.fit(df, freq=\"H\", valid_p=0.2)\n\ndf_train, df_val = m.split_df(df, valid_p=0.2)\nm.test(df_val)\n\nfuture = m.make_future_dataframe(df_val, n_historic_predictions=True)\nforecast = m.predict(future)\n# fig = m.plot(forecast)\nprint(loss(forecast[\"y\"][24:-1], forecast[\"yhat1\"][24:-1]))\n\nneuralprophet_loss = loss(forecast[\"y\"][24:-1], forecast[\"yhat1\"][24:-1])\n</pre> set_random_seed(42)  # set_log_level(\"ERROR\") files = [\"\\SanFrancisco_Hospital.csv\"] raw = pd.read_csv(data_location + files[0]) df = pd.DataFrame() df[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760)) df[\"y\"] = raw.iloc[:, 0].values  m = NeuralProphet(     n_lags=24, ar_sparsity=0.5, num_hidden_layers=2, d_hidden=20, learning_rate=0.001 ) metrics = m.fit(df, freq=\"H\", valid_p=0.2)  df_train, df_val = m.split_df(df, valid_p=0.2) m.test(df_val)  future = m.make_future_dataframe(df_val, n_historic_predictions=True) forecast = m.predict(future) # fig = m.plot(forecast) print(loss(forecast[\"y\"][24:-1], forecast[\"yhat1\"][24:-1]))  neuralprophet_loss = loss(forecast[\"y\"][24:-1], forecast[\"yhat1\"][24:-1]) <pre>WARNING: nprophet - fit: Parts of code may break if using other than daily data.\n</pre> <pre>04-26 19:54:06 - WARNING - Parts of code may break if using other than daily data.\n</pre> <pre>INFO: nprophet.utils - set_auto_seasonalities: Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n</pre> <pre>04-26 19:54:06 - INFO - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n</pre> <pre>INFO: nprophet.config - set_auto_batch_epoch: Auto-set batch_size to 32\n</pre> <pre>04-26 19:54:06 - INFO - Auto-set batch_size to 32\n</pre> <pre>INFO: nprophet.config - set_auto_batch_epoch: Auto-set epochs to 7\n</pre> <pre>04-26 19:54:06 - INFO - Auto-set epochs to 7\n</pre> <pre>Epoch[7/7]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:03&lt;00:00,  1.82it/s, SmoothL1Loss=0.0102, MAE=81.6, RegLoss=0.011] \nINFO: nprophet - _evaluate: Validation metrics:    SmoothL1Loss    MAE\n1         0.011 84.733\n</pre> <pre>04-26 19:54:10 - INFO - Validation metrics:    SmoothL1Loss    MAE\n1         0.011 84.733\n11397.103026422525\n</pre> In\u00a0[7]: Copied! <pre>plt.figure(figsize=(18, 8))\nplt.plot(forecast[\"y\"][-504:], \"ro-\")\nplt.plot(forecast[\"yhat1\"][-504:], \"k*-\")\n</pre> plt.figure(figsize=(18, 8)) plt.plot(forecast[\"y\"][-504:], \"ro-\") plt.plot(forecast[\"yhat1\"][-504:], \"k*-\") Out[7]: <pre>[&lt;matplotlib.lines.Line2D at 0x237847417f0&gt;]</pre> In\u00a0[8]: Copied! <pre>results = {\n    \"SysIdentPy - FROLS\": sysidentpy_loss,\n    \"SysIdentPy (AOLS)\": aols_loss,\n    \"SysIdentPy (MetaMSS)\": metamss_loss,\n    \"NeuralProphet\": neuralprophet_loss,\n}\n\nsorted(results.items(), key=lambda result: result[1])\n</pre> results = {     \"SysIdentPy - FROLS\": sysidentpy_loss,     \"SysIdentPy (AOLS)\": aols_loss,     \"SysIdentPy (MetaMSS)\": metamss_loss,     \"NeuralProphet\": neuralprophet_loss, }  sorted(results.items(), key=lambda result: result[1]) Out[8]: <pre>[('SysIdentPy - FROLS', 4183.359498155755),\n ('SysIdentPy (MetaMSS)', 5264.429171346123),\n ('SysIdentPy (AOLS)', 5264.42917196841),\n ('NeuralProphet', 11397.103026422525)]</pre>"},{"location":"examples/load_forecasting_benchmark/#load-forecasting-benchmark","title":"Load forecasting benchmark\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"examples/load_forecasting_benchmark/#note","title":"Note\u00b6","text":"<p>The following example is not intended to say that one library is better than another. The main focus of these examples is to show that SysIdentPy can be a good alternative for people looking to model time series.</p> <p>We will compare the results obtained against neural prophet library.</p> <p>For the sake of brevity, from SysIdentPy only the MetaMSS, AOLS and FROLS (with polynomial base function) methods will be used. See the SysIdentPy documentation to learn other ways of modeling with the library.</p>"},{"location":"examples/load_forecasting_benchmark/#benchmark-results","title":"Benchmark results:\u00b6","text":"No. Package Mean Squared Error 1 SysIdentPy (FROLS) 4183 2 SysIdentPy (MetaMSS) 5264 3 SysIdentPy (AOLS) 5264 4 NeuralProphet 11471"},{"location":"examples/load_forecasting_benchmark/#frols","title":"FROLS\u00b6","text":""},{"location":"examples/load_forecasting_benchmark/#metamss","title":"MetaMSS\u00b6","text":""},{"location":"examples/load_forecasting_benchmark/#aols","title":"AOLS\u00b6","text":""},{"location":"examples/load_forecasting_benchmark/#neural-prophet","title":"Neural Prophet\u00b6","text":""},{"location":"examples/metamss/","title":"Meta-Model Structure Selection (MetaMSS) algorithm for building Polynomial NARX models","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import MetaMSS, FROLS\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import RecursiveLeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt from sysidentpy.model_structure_selection import MetaMSS, FROLS from sysidentpy.metrics import root_relative_squared_error from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.parameter_estimation import RecursiveLeastSquares from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) In\u00a0[2]: Copied! <pre>df1 = pd.read_csv(\"./datasets/x_cc.csv\")\ndf2 = pd.read_csv(\"./datasets/y_cc.csv\")\n\ndf2[5000:80000].plot(figsize=(10, 4))\n</pre> df1 = pd.read_csv(\"./datasets/x_cc.csv\") df2 = pd.read_csv(\"./datasets/y_cc.csv\")  df2[5000:80000].plot(figsize=(10, 4)) Out[2]: <pre>&lt;Axes: &gt;</pre> In\u00a0[3]: Copied! <pre>df1.iloc[::500].values.shape\n</pre> df1.iloc[::500].values.shape Out[3]: <pre>(1000, 1)</pre> <p>We will decimate the data using d=500 in this example. Besides, we separate the MetaMSS data to use the same amount of samples in the prediction validation. Because MetaMSS need a train and test data to optimize the parameters of the model, in this case, we'll use 400 samples to train instead of 500 samples used for the other models.</p> In\u00a0[7]: Copied! <pre># we will decimate the data using d=500 in this example\nx_train, x_test = np.split(df1.iloc[::500].values, 2)\ny_train, y_test = np.split(df2.iloc[::500].values, 2)\n</pre> # we will decimate the data using d=500 in this example x_train, x_test = np.split(df1.iloc[::500].values, 2) y_train, y_test = np.split(df2.iloc[::500].values, 2) In\u00a0[65]: Copied! <pre>basis_function = Polynomial(degree=2)\nestimator = RecursiveLeastSquares()\n\nmodel = MetaMSS(\n    xlag=5,\n    ylag=5,\n    estimator=estimator,\n    maxiter=5,\n    n_agents=15,\n    basis_function=basis_function,\n    random_state=42,\n)\n\nmodel.fit(X=x_train, y=y_train)\n</pre> basis_function = Polynomial(degree=2) estimator = RecursiveLeastSquares()  model = MetaMSS(     xlag=5,     ylag=5,     estimator=estimator,     maxiter=5,     n_agents=15,     basis_function=basis_function,     random_state=42, )  model.fit(X=x_train, y=y_train) <pre>c:\\Users\\wilso\\miniconda3\\envs\\sysidentpy334\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpy334\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\meta_model_structure_selection.py:455: RuntimeWarning: overflow encountered in square\n  sum_of_squared_residues = np.sum(residues**2)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\meta_model_structure_selection.py:465: RuntimeWarning: invalid value encountered in sqrt\n  se_theta = np.sqrt(var_e)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\metrics\\_regression.py:216: RuntimeWarning: overflow encountered in square\n  numerator = np.sum(np.square((yhat - y)))\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\narmax_base.py:724: RuntimeWarning: overflow encountered in power\n  regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpy334\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2590: RuntimeWarning: divide by zero encountered in power\n  absx **= ord\n</pre> Out[65]: <pre>&lt;sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS at 0x229e13e3150&gt;</pre> In\u00a0[67]: Copied! <pre>yhat = model.predict(X=x_test, y=y_test, steps_ahead=None)\nrrse = root_relative_squared_error(y_test[model.max_lag :, :], yhat[model.max_lag :, :])\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_test, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_test, yhat, x_test)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> yhat = model.predict(X=x_test, y=y_test, steps_ahead=None) rrse = root_relative_squared_error(y_test[model.max_lag :, :], yhat[model.max_lag :, :]) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  plot_results(y=y_test, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_test, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_test, yhat, x_test) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>0.035919583498004094\n        Regressors   Parameters             ERR\n0                1  -6.1606E+02  0.00000000E+00\n1           y(k-1)   1.3117E+00  0.00000000E+00\n2           y(k-2)  -3.0579E-01  0.00000000E+00\n3          x1(k-1)   5.7920E+02  0.00000000E+00\n4          x1(k-3)  -1.8750E-01  0.00000000E+00\n5    x1(k-1)y(k-1)  -1.7305E-01  0.00000000E+00\n6    x1(k-2)y(k-1)  -1.1660E-01  0.00000000E+00\n7    x1(k-1)y(k-2)   1.2182E-01  0.00000000E+00\n8    x1(k-2)y(k-2)   3.4112E-02  0.00000000E+00\n9    x1(k-1)y(k-3)  -4.8970E-02  0.00000000E+00\n10   x1(k-1)y(k-4)   1.3846E-02  0.00000000E+00\n11       x1(k-2)^2   1.0290E+02  0.00000000E+00\n12  x1(k-3)x1(k-2)   8.6745E-01  0.00000000E+00\n13  x1(k-4)x1(k-2)   3.4336E-01  0.00000000E+00\n14  x1(k-5)x1(k-2)   2.7815E-01  0.00000000E+00\n15       x1(k-3)^2  -9.3749E-01  0.00000000E+00\n16  x1(k-4)x1(k-3)   6.1039E-01  0.00000000E+00\n17  x1(k-5)x1(k-3)   3.9361E-02  0.00000000E+00\n18       x1(k-4)^2  -4.6335E-01  0.00000000E+00\n19  x1(k-5)x1(k-4)  -9.5668E-02  0.00000000E+00\n20       x1(k-5)^2   3.6922E-01  0.00000000E+00\n</pre> In\u00a0[68]: Copied! <pre># Plotting the evolution of the agents\nplt.plot(model.best_by_iter)\nmodel.best_by_iter[-1]\n</pre> # Plotting the evolution of the agents plt.plot(model.best_by_iter) model.best_by_iter[-1] Out[68]: <pre>0.0017530517788608157</pre> In\u00a0[69]: Copied! <pre># You have access to all tested models\n# model.tested_models\n</pre> # You have access to all tested models # model.tested_models In\u00a0[74]: Copied! <pre>from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import ARDRegression\nfrom sysidentpy.general_estimators import NARX\n\nxlag = ylag = 5\n\nestimators = [\n    (\n        \"NARX_KNeighborsRegressor\",\n        NARX(\n            base_estimator=KNeighborsRegressor(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX_DecisionTreeRegressor\",\n        NARX(\n            base_estimator=DecisionTreeRegressor(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX_RandomForestRegressor\",\n        NARX(\n            base_estimator=RandomForestRegressor(n_estimators=200),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX_Catboost\",\n        NARX(\n            base_estimator=CatBoostRegressor(\n                iterations=800, learning_rate=0.1, depth=8\n            ),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n            fit_params={\"verbose\": False},\n        ),\n    ),\n    (\n        \"NARX_ARD\",\n        NARX(\n            base_estimator=ARDRegression(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"FROLS-Polynomial_NARX\",\n        FROLS(\n            order_selection=True,\n            n_info_values=50,\n            ylag=ylag,\n            xlag=xlag,\n            basis_function=basis_function,\n            info_criteria=\"bic\",\n            err_tol=None,\n        ),\n    ),\n    (\n        \"MetaMSS\",\n        MetaMSS(\n            norm=-2,\n            xlag=xlag,\n            ylag=ylag,\n            estimator=estimator,\n            maxiter=5,\n            n_agents=15,\n            loss_func=\"metamss_loss\",\n            basis_function=basis_function,\n            random_state=42,\n        ),\n    ),\n]\n\n\nresultados = {}\nfor nome_do_modelo, modelo in estimators:\n    resultados[\"%s\" % (nome_do_modelo)] = []\n    modelo.fit(X=x_train, y=y_train)\n    yhat = modelo.predict(X=x_test, y=y_test)\n    if nome_do_modelo in [\"FROLS-Polynomial_NARX\", \"MetaMSS\"]:\n        result = root_relative_squared_error(\n            y_test[modelo.max_lag :], yhat[modelo.max_lag :]\n        )\n    else:\n        result = root_relative_squared_error(y_test, yhat)\n    resultados[\"%s\" % (nome_do_modelo)].append(result)\n    print(nome_do_modelo, \"%.3f\" % np.mean(result))\n</pre> from sklearn.neighbors import KNeighborsRegressor from sklearn.svm import SVC from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor from catboost import CatBoostRegressor from sklearn.linear_model import ARDRegression from sysidentpy.general_estimators import NARX  xlag = ylag = 5  estimators = [     (         \"NARX_KNeighborsRegressor\",         NARX(             base_estimator=KNeighborsRegressor(),             xlag=xlag,             ylag=ylag,             basis_function=basis_function,         ),     ),     (         \"NARX_DecisionTreeRegressor\",         NARX(             base_estimator=DecisionTreeRegressor(),             xlag=xlag,             ylag=ylag,             basis_function=basis_function,         ),     ),     (         \"NARX_RandomForestRegressor\",         NARX(             base_estimator=RandomForestRegressor(n_estimators=200),             xlag=xlag,             ylag=ylag,             basis_function=basis_function,         ),     ),     (         \"NARX_Catboost\",         NARX(             base_estimator=CatBoostRegressor(                 iterations=800, learning_rate=0.1, depth=8             ),             xlag=xlag,             ylag=ylag,             basis_function=basis_function,             fit_params={\"verbose\": False},         ),     ),     (         \"NARX_ARD\",         NARX(             base_estimator=ARDRegression(),             xlag=xlag,             ylag=ylag,             basis_function=basis_function,         ),     ),     (         \"FROLS-Polynomial_NARX\",         FROLS(             order_selection=True,             n_info_values=50,             ylag=ylag,             xlag=xlag,             basis_function=basis_function,             info_criteria=\"bic\",             err_tol=None,         ),     ),     (         \"MetaMSS\",         MetaMSS(             norm=-2,             xlag=xlag,             ylag=ylag,             estimator=estimator,             maxiter=5,             n_agents=15,             loss_func=\"metamss_loss\",             basis_function=basis_function,             random_state=42,         ),     ), ]   resultados = {} for nome_do_modelo, modelo in estimators:     resultados[\"%s\" % (nome_do_modelo)] = []     modelo.fit(X=x_train, y=y_train)     yhat = modelo.predict(X=x_test, y=y_test)     if nome_do_modelo in [\"FROLS-Polynomial_NARX\", \"MetaMSS\"]:         result = root_relative_squared_error(             y_test[modelo.max_lag :], yhat[modelo.max_lag :]         )     else:         result = root_relative_squared_error(y_test, yhat)     resultados[\"%s\" % (nome_do_modelo)].append(result)     print(nome_do_modelo, \"%.3f\" % np.mean(result)) <pre>NARX_KNeighborsRegressor 1.158\nNARX_DecisionTreeRegressor 0.203\nNARX_RandomForestRegressor 0.146\nNARX_Catboost 0.120\nNARX_ARD 0.083\nFROLS-Polynomial_NARX 0.057\n</pre> <pre>c:\\Users\\wilso\\miniconda3\\envs\\sysidentpy334\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpy334\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\meta_model_structure_selection.py:455: RuntimeWarning: overflow encountered in square\n  sum_of_squared_residues = np.sum(residues**2)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\meta_model_structure_selection.py:465: RuntimeWarning: invalid value encountered in sqrt\n  se_theta = np.sqrt(var_e)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\metrics\\_regression.py:216: RuntimeWarning: overflow encountered in square\n  numerator = np.sum(np.square((yhat - y)))\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\narmax_base.py:724: RuntimeWarning: overflow encountered in power\n  regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpy334\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2590: RuntimeWarning: divide by zero encountered in power\n  absx **= ord\n</pre> <pre>MetaMSS 0.036\n</pre> In\u00a0[75]: Copied! <pre>for model_name, metric in sorted(\n    resultados.items(), key=lambda x: np.mean(x[1]), reverse=False\n):\n    print(model_name, np.mean(metric))\n</pre> for model_name, metric in sorted(     resultados.items(), key=lambda x: np.mean(x[1]), reverse=False ):     print(model_name, np.mean(metric)) <pre>MetaMSS 0.035919583498004094\nFROLS-Polynomial_NARX 0.05729765719062527\nNARX_ARD 0.08265856190495872\nNARX_Catboost 0.12034851661643597\nNARX_RandomForestRegressor 0.14557973585496042\nNARX_DecisionTreeRegressor 0.203057724881072\nNARX_KNeighborsRegressor 1.157787546845798\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/metamss/#meta-model-structure-selection-metamss-algorithm-for-building-polynomial-narx-models","title":"Meta-Model Structure Selection (MetaMSS) algorithm for building Polynomial NARX models\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"examples/multiobjective_parameter_estimation/","title":"Multiobjective Parameter Estimation for NARMAX models - An Overview","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.multiobjective_parameter_estimation import AILS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.narmax_tools import set_weights\n</pre> import numpy as np import matplotlib.pyplot as plt import pandas as pd from sysidentpy.model_structure_selection import FROLS from sysidentpy.multiobjective_parameter_estimation import AILS from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_results from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.narmax_tools import set_weights In\u00a0[2]: Copied! <pre>df_train = pd.read_csv(r\"datasets/buck_id.csv\")\ndf_valid = pd.read_csv(r\"datasets/buck_valid.csv\")\n\n# Plotting the measured output (identification and validation data)\nplt.figure(1)\nplt.title(\"Output\")\nplt.plot(df_train.sampling_time, df_train.y, label=\"Identification\", linewidth=1.5)\nplt.plot(df_valid.sampling_time, df_valid.y, label=\"Validation\", linewidth=1.5)\nplt.xlabel(\"Samples\")\nplt.ylabel(\"Voltage\")\nplt.legend()\nplt.show()\n</pre> df_train = pd.read_csv(r\"datasets/buck_id.csv\") df_valid = pd.read_csv(r\"datasets/buck_valid.csv\")  # Plotting the measured output (identification and validation data) plt.figure(1) plt.title(\"Output\") plt.plot(df_train.sampling_time, df_train.y, label=\"Identification\", linewidth=1.5) plt.plot(df_valid.sampling_time, df_valid.y, label=\"Validation\", linewidth=1.5) plt.xlabel(\"Samples\") plt.ylabel(\"Voltage\") plt.legend() plt.show() In\u00a0[3]: Copied! <pre># Plotting the measured input (identification and validation data)\nplt.figure(2)\nplt.title(\"Input\")\nplt.plot(df_train.sampling_time, df_train.input, label=\"Identification\", linewidth=1.5)\nplt.plot(df_valid.sampling_time, df_valid.input, label=\"Validation\", linewidth=1.5)\nplt.ylim(2.1, 2.6)\nplt.ylabel(\"u\")\nplt.xlabel(\"Samples\")\nplt.legend()\nplt.show()\n</pre> # Plotting the measured input (identification and validation data) plt.figure(2) plt.title(\"Input\") plt.plot(df_train.sampling_time, df_train.input, label=\"Identification\", linewidth=1.5) plt.plot(df_valid.sampling_time, df_valid.input, label=\"Validation\", linewidth=1.5) plt.ylim(2.1, 2.6) plt.ylabel(\"u\") plt.xlabel(\"Samples\") plt.legend() plt.show() In\u00a0[4]: Copied! <pre># Static data\nVd = 24\nUo = np.linspace(0, 4, 50)\nYo = (4 - Uo) * Vd / 3\nUo = Uo.reshape(-1, 1)\nYo = Yo.reshape(-1, 1)\nplt.figure(3)\nplt.title(\"Buck Converter Static Curve\")\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{y}$\")\nplt.plot(Uo, Yo, linewidth=1.5, linestyle=\"-\", marker=\"o\")\nplt.show()\n</pre> # Static data Vd = 24 Uo = np.linspace(0, 4, 50) Yo = (4 - Uo) * Vd / 3 Uo = Uo.reshape(-1, 1) Yo = Yo.reshape(-1, 1) plt.figure(3) plt.title(\"Buck Converter Static Curve\") plt.xlabel(\"$\\\\bar{u}$\") plt.ylabel(\"$\\\\bar{y}$\") plt.plot(Uo, Yo, linewidth=1.5, linestyle=\"-\", marker=\"o\") plt.show() In\u00a0[5]: Copied! <pre># Defining the gain\ngain = -8 * np.ones(len(Uo)).reshape(-1, 1)\nplt.figure(3)\nplt.title(\"Buck Converter Static Gain\")\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{gain}$\")\nplt.plot(Uo, gain, linewidth=1.5, label=\"gain\", linestyle=\"-\", marker=\"o\")\nplt.legend()\nplt.show()\n</pre> # Defining the gain gain = -8 * np.ones(len(Uo)).reshape(-1, 1) plt.figure(3) plt.title(\"Buck Converter Static Gain\") plt.xlabel(\"$\\\\bar{u}$\") plt.ylabel(\"$\\\\bar{gain}$\") plt.plot(Uo, gain, linewidth=1.5, label=\"gain\", linestyle=\"-\", marker=\"o\") plt.legend() plt.show() In\u00a0[6]: Copied! <pre>x_train = df_train.input.values.reshape(-1, 1)\ny_train = df_train.y.values.reshape(-1, 1)\nx_valid = df_valid.input.values.reshape(-1, 1)\ny_valid = df_valid.y.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=2)\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=8,\n    extended_least_squares=False,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\n</pre> x_train = df_train.input.values.reshape(-1, 1) y_train = df_train.y.values.reshape(-1, 1) x_valid = df_valid.input.values.reshape(-1, 1) y_valid = df_valid.y.values.reshape(-1, 1)  basis_function = Polynomial(degree=2)  model = FROLS(     order_selection=True,     n_info_values=8,     extended_least_squares=False,     ylag=2,     xlag=2,     info_criteria=\"aic\",     estimator=\"least_squares\",     basis_function=basis_function, )  model.fit(X=x_train, y=y_train) <pre>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre> Out[6]: <pre>&lt;sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS at 0x14acaf77950&gt;</pre> In\u00a0[7]: Copied! <pre># you can use any set of model structure you want in your use case, but in this notebook we will use the one obtained above the compare with other work\nmo_estimator = AILS(final_model=model.final_model)\n\n# setting the log-spaced weights of each objective function\nw = set_weights(static_function=True, static_gain=True)\n\n# you can also use something like\n\n# w = np.array(\n#     [\n#         [0.98, 0.7, 0.5, 0.35, 0.25, 0.01, 0.15, 0.01],\n#         [0.01, 0.1, 0.3, 0.15, 0.25, 0.98, 0.35, 0.01],\n#         [0.01, 0.2, 0.2, 0.50, 0.50, 0.01, 0.50, 0.98],\n#     ]\n# )\n\n# to set the weights. Each row correspond to each objective\n</pre> # you can use any set of model structure you want in your use case, but in this notebook we will use the one obtained above the compare with other work mo_estimator = AILS(final_model=model.final_model)  # setting the log-spaced weights of each objective function w = set_weights(static_function=True, static_gain=True)  # you can also use something like  # w = np.array( #     [ #         [0.98, 0.7, 0.5, 0.35, 0.25, 0.01, 0.15, 0.01], #         [0.01, 0.1, 0.3, 0.15, 0.25, 0.98, 0.35, 0.01], #         [0.01, 0.2, 0.2, 0.50, 0.50, 0.01, 0.50, 0.98], #     ] # )  # to set the weights. Each row correspond to each objective <p>AILS has an <code>estimate</code> method that returns the cost functions (J), the Euclidean norm of the cost functions (E), the estimated parameters referring to each weight (theta), the regressor matrix of the gain and static_function affine information HR and QR, respectively.</p> In\u00a0[8]: Copied! <pre>J, E, theta, HR, QR, position = mo_estimator.estimate(\n    X=x_train, y=y_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\n    \"w1\": w[0, :],\n    \"w2\": w[2, :],\n    \"w3\": w[1, :],\n    \"J_ls\": J[0, :],\n    \"J_sg\": J[1, :],\n    \"J_sf\": J[2, :],\n    \"||J||:\": E,\n}\npd.DataFrame(result)\n</pre> J, E, theta, HR, QR, position = mo_estimator.estimate(     X=x_train, y=y_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w ) result = {     \"w1\": w[0, :],     \"w2\": w[2, :],     \"w3\": w[1, :],     \"J_ls\": J[0, :],     \"J_sg\": J[1, :],     \"J_sf\": J[2, :],     \"||J||:\": E, } pd.DataFrame(result) Out[8]: w1 w2 w3 J_ls J_sg J_sf ||J||: 0 0.006842 0.003078 0.990080 0.999970 1.095020e-05 0.000013 0.245244 1 0.007573 0.002347 0.990080 0.999938 2.294665e-05 0.000016 0.245236 2 0.008382 0.001538 0.990080 0.999885 6.504913e-05 0.000018 0.245223 3 0.009277 0.000642 0.990080 0.999717 4.505541e-04 0.000021 0.245182 4 0.006842 0.098663 0.894495 1.000000 7.393246e-08 0.000015 0.245251 ... ... ... ... ... ... ... ... 2290 0.659632 0.333527 0.006842 0.995896 3.965699e-04 1.000000 0.244489 2291 0.730119 0.263039 0.006842 0.995632 5.602981e-04 0.972842 0.244412 2292 0.808139 0.185020 0.006842 0.995364 8.321071e-04 0.868299 0.244300 2293 0.894495 0.098663 0.006842 0.995100 1.364999e-03 0.660486 0.244160 2294 0.990080 0.003078 0.006842 0.992584 9.825987e-02 0.305492 0.261455 <p>2295 rows \u00d7 7 columns</p> <p>Now we can set theta related to any weight results</p> In\u00a0[9]: Copied! <pre>model.theta = theta[-1, :].reshape(\n    -1, 1\n)  # setting the theta estimated for the last combination of the weights\n# the model structure is exactly the same, but the order of the regressors is changed in estimate method. Thats why you have to change the model.final_model\nmodel.final_model = mo_estimator.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</pre> model.theta = theta[-1, :].reshape(     -1, 1 )  # setting the theta estimated for the last combination of the weights # the model structure is exactly the same, but the order of the regressors is changed in estimate method. Thats why you have to change the model.final_model model.final_model = mo_estimator.final_model yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=3,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) r Out[9]: Regressors Parameters ERR 0 1 2.2930E+00 9.999E-01 1 y(k-1) 2.3307E-01 2.042E-05 2 y(k-2) 6.3209E-01 1.108E-06 3 x1(k-1) -5.9333E-01 4.688E-06 4 y(k-1)^2 2.7673E-01 3.922E-07 5 y(k-2)y(k-1) -5.3228E-01 8.389E-07 6 x1(k-1)y(k-1) 1.6667E-02 5.690E-07 7 y(k-2)^2 2.5766E-01 3.827E-06 In\u00a0[10]: Copied! <pre>plot_results(y=y_valid, yhat=yhat, n=1000)\n</pre> plot_results(y=y_valid, yhat=yhat, n=1000) In\u00a0[11]: Copied! <pre>plt.figure(4)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n</pre> plt.figure(4) plt.title(\"Gain\") plt.plot(     Uo,     gain,     linewidth=1.5,     linestyle=\"-\",     marker=\"o\",     label=\"Buck converter static gain\", ) plt.plot(     Uo,     HR.dot(model.theta),     linestyle=\"-\",     marker=\"^\",     linewidth=1.5,     label=\"NARX model gain\", ) plt.xlabel(\"$\\\\bar{u}$\") plt.ylabel(\"$\\\\bar{g}$\") plt.ylim(-16, 0) plt.legend() plt.show() In\u00a0[12]: Copied! <pre>plt.figure(5)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \u200b\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</pre> plt.figure(5) plt.title(\"Static Curve\") plt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\") plt.plot(     Uo,     QR.dot(model.theta),     linewidth=1.5,     label=\"NARX \u200b\u200bstatic representation\",     linestyle=\"-\",     marker=\"^\", ) plt.xlabel(\"$\\\\bar{u}$\") plt.xlabel(\"$\\\\bar{y}$\") plt.legend() plt.show() In\u00a0[13]: Copied! <pre># the variable `position` returned in `estimate` method give the position of the best weight combination\nmodel.theta = theta[position, :].reshape(\n    -1, 1\n)  # setting the theta estimated for the best combination of the weights\n# the model structure is exactly the same, but the order of the regressors is changed in estimate method. Thats why you have to change the model.final_model\nmodel.final_model = mo_estimator.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\n# The dynamic results for that chosen theta is\nplot_results(y=y_valid, yhat=yhat, n=1000)\n# The static gain result is\nplt.figure(4)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n# The static function result is\nplt.figure(5)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \u200b\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</pre> # the variable `position` returned in `estimate` method give the position of the best weight combination model.theta = theta[position, :].reshape(     -1, 1 )  # setting the theta estimated for the best combination of the weights # the model structure is exactly the same, but the order of the regressors is changed in estimate method. Thats why you have to change the model.final_model model.final_model = mo_estimator.final_model yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=3,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  # The dynamic results for that chosen theta is plot_results(y=y_valid, yhat=yhat, n=1000) # The static gain result is plt.figure(4) plt.title(\"Gain\") plt.plot(     Uo,     gain,     linewidth=1.5,     linestyle=\"-\",     marker=\"o\",     label=\"Buck converter static gain\", ) plt.plot(     Uo,     HR.dot(model.theta),     linestyle=\"-\",     marker=\"^\",     linewidth=1.5,     label=\"NARX model gain\", ) plt.xlabel(\"$\\\\bar{u}$\") plt.ylabel(\"$\\\\bar{g}$\") plt.ylim(-16, 0) plt.legend() plt.show() # The static function result is plt.figure(5) plt.title(\"Static Curve\") plt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\") plt.plot(     Uo,     QR.dot(model.theta),     linewidth=1.5,     label=\"NARX \u200b\u200bstatic representation\",     linestyle=\"-\",     marker=\"^\", ) plt.xlabel(\"$\\\\bar{u}$\") plt.xlabel(\"$\\\\bar{y}$\") plt.legend() plt.show() <pre>      Regressors   Parameters        ERR\n0              1   1.5405E+00  9.999E-01\n1         y(k-1)   2.9687E-01  2.042E-05\n2         y(k-2)   6.4693E-01  1.108E-06\n3        x1(k-1)  -4.1302E-01  4.688E-06\n4       y(k-1)^2   2.7671E-01  3.922E-07\n5   y(k-2)y(k-1)  -5.3474E-01  8.389E-07\n6  x1(k-1)y(k-1)   4.0624E-03  5.690E-07\n7       y(k-2)^2   2.5832E-01  3.827E-06\n</pre> <p>You can also plot the pareto-set solutions</p> In\u00a0[14]: Copied! <pre>plt.figure(6)\nax = plt.axes(projection=\"3d\")\nax.plot3D(J[0, :], J[1, :], J[2, :], \"o\", linewidth=0.1)\nax.set_title(\"Pareto-set solutions\", fontsize=15)\nax.set_xlabel(\"$J_{ls}$\", fontsize=10)\nax.set_ylabel(\"$J_{sg}$\", fontsize=10)\nax.set_zlabel(\"$J_{sf}$\", fontsize=10)\nplt.show()\n</pre> plt.figure(6) ax = plt.axes(projection=\"3d\") ax.plot3D(J[0, :], J[1, :], J[2, :], \"o\", linewidth=0.1) ax.set_title(\"Pareto-set solutions\", fontsize=15) ax.set_xlabel(\"$J_{ls}$\", fontsize=10) ax.set_ylabel(\"$J_{sg}$\", fontsize=10) ax.set_zlabel(\"$J_{sf}$\", fontsize=10) plt.show() <p>The polynomial NARX model built using the mono-objective approach has the following structure:</p> <p>$ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 u(k-1) y(k-1) + \\theta_4 + \\theta_5 y(k-1)^2 + \\theta_6 u(k-1) + \\theta_7 y(k-2)y(k-1) + \\theta_8 y(k-2)^2 $</p> <p>The, the goal when using the static function and static gain information in the multiobjective scenario is to estimate the vector $\\hat{\\theta}$ based on:</p> <p>$ \\theta = [w_1\\Psi^T\\Psi + w_2(HR)^T(HR) + w_3(QR)(QR)^T]^{-1} [w_1\\Psi^T y + w_2(HR)^T\\overline{g}+w_3(QR)^T\\overline{y}] $</p> <p>The $\\Psi$ matrix is built using the usual mono-objective dynamic modeling approach in SysIdentPy. However, it is still necessary to find the Q, H and R matrices. AILS have the methods to compute all of those matrices. Basically, to do that, $q_i^T$ is first estimated:</p> <p>$ q_i^T = \\begin{bmatrix} 1 &amp; \\overline{y_i} &amp; \\overline{u_1} &amp; \\overline{y_i}^2 &amp; \\cdots &amp; \\overline{y_i}^l &amp;  F_{yu} &amp; \\overline{u_i}^2 &amp; \\cdots &amp; \\overline{u_i}^l \\end{bmatrix} $</p> <p>where $F_{yu}$ stands for all non-linear monomials in the model that are related to $y(k)$ and $u(k)$, $l$ is the largest non-linearity in the model for input and output terms. For a model with a degree of nonlinearity equal to 2, we can obtain:</p> <p>$ q_i^T =  \\begin{bmatrix} 1 &amp; \\overline{y_i} &amp; \\overline{u_i} &amp; \\overline{y_i}^2 &amp; \\overline{u_i}\\:\\overline{y_i} &amp; \\overline{u_i}^2  \\end{bmatrix} $</p> <p>It is possible to encode the $q_i^T$ matrix so that it follows the model encoding defined in SysIdentPy. To do this, 0 is considered as a constant, $y_i$ equal to 1 and $u_i$ equal to 2. The number of columns indicates the degree of nonlinearity of the system and the number of rows reflects the number of terms:</p> <p>$ q_i =  \\begin{bmatrix} 0 &amp; 0\\\\ 1 &amp; 0\\\\ 2 &amp; 0\\\\ 1 &amp; 1\\\\ 2 &amp; 1\\\\ 2 &amp; 2\\\\ \\end{bmatrix} $ $ = $ $  \\begin{bmatrix} 1 \\\\ \\overline{y_i}\\\\ \\overline{u_i}\\\\ \\overline{y_i}^2\\\\ \\overline{u_i}\\:\\overline{y_i}\\\\ \\overline{u_i}^2\\\\ \\end{bmatrix} $</p> <p>Finally, the result can be easily obtained using the \u2018regressor_space\u2019 method of SysIdentPy</p> In\u00a0[15]: Copied! <pre>from sysidentpy.narmax_base import RegressorDictionary\n\nobject_qit = RegressorDictionary(xlag=1, ylag=1)\nR_example = object_qit.regressor_space(n_inputs=1) // 1000\nprint(f\"R = {R_example}\")\n</pre> from sysidentpy.narmax_base import RegressorDictionary  object_qit = RegressorDictionary(xlag=1, ylag=1) R_example = object_qit.regressor_space(n_inputs=1) // 1000 print(f\"R = {R_example}\") <pre>R = [[0 0]\n [1 0]\n [2 0]\n [1 1]\n [2 1]\n [2 2]]\n</pre> <p>such that:</p> <p>$ \\overline{y_i} = q_i^T R\\theta $</p> <p>and:</p> <p>$ \\overline{g_i} = H R\\theta $</p> <p>where $R$ is the linear mapping of the static regressors represented by $q_i^T$. In addition, the $H$ matrix holds affine information regarding $\\overline{g_i}$, which is equal to $\\overline{g_i} = \\frac{d\\overline{y}}{d\\overline{u}}{\\big |}_{(\\overline{u_i}\\:\\overline{y_i})}$.</p> <p>From now on, we will begin to apply the parameter estimation in a multiobjective manner. This will be done with the NARX polynomial model of the BUCK converter in mind. In this context, $q_i^T$ will be generic and will assume a specific format for the problem at hand. For this task, the $R_qit$ method will be used, whose objective is to return the $q_i^T$ related to the model and the matrix of the linear mapping $R$:</p> In\u00a0[16]: Copied! <pre>R, qit = mo_estimator.build_linear_mapping()\nprint(\"R matrix:\")\nprint(R)\nprint(\"qit matrix:\")\nprint(qit)\n</pre> R, qit = mo_estimator.build_linear_mapping() print(\"R matrix:\") print(R) print(\"qit matrix:\") print(qit) <pre>R matrix:\n[[1 0 0 0 0 0 0 0]\n [0 1 1 0 0 0 0 0]\n [0 0 0 1 0 0 0 0]\n [0 0 0 0 1 1 0 1]\n [0 0 0 0 0 0 1 0]]\nqit matrix:\n[[0 0]\n [1 0]\n [0 1]\n [2 0]\n [1 1]]\n</pre> <p>So</p> <p>$ q_i =  \\begin{bmatrix} 0 &amp; 0\\\\ 1 &amp; 0\\\\ 2 &amp; 0\\\\ 1 &amp; 1\\\\ 2 &amp; 1\\\\  \\end{bmatrix} $ $ =$ $ \\begin{bmatrix} 1\\\\ \\overline{y}\\\\ \\overline{u}\\\\ \\overline{y^2}\\\\ \\overline{u}\\:\\overline{y}\\\\  \\end{bmatrix} $</p> <p>You can notice that the method produces outputs consistent with what is expected:</p> <p>$ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 u(k-1) y(k-1) + \\theta_4 + \\theta_5 y(k-1)^2 + \\theta_6 u(k-1) + \\theta_7 y(k-2)y(k-1) + \\theta_8 y(k-2)^2 $</p> <p>and:</p> <p>$ R =  \\begin{bmatrix} term/\\theta &amp; \\theta_1 &amp; \\theta_2 &amp; \\theta_3 &amp; \\theta_4 &amp; \\theta_5 &amp; \\theta_6 &amp; \\theta_7 &amp; \\theta_8\\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\overline{y} &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\overline{u} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\\\ \\overline{y^2} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1\\\\ \\overline{y}\\:\\overline{u} &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\end{bmatrix} $</p> In\u00a0[17]: Copied! <pre>final_model = np.array(\n    [\n        [1001, 0],\n        [1002, 0],\n        [0, 0],\n        [2001, 0],\n        [2001, 2001],\n        [2002, 2001],\n        [2002, 0],\n        [2002, 2002],\n    ]\n)\nfinal_model\n</pre> final_model = np.array(     [         [1001, 0],         [1002, 0],         [0, 0],         [2001, 0],         [2001, 2001],         [2002, 2001],         [2002, 0],         [2002, 2002],     ] ) final_model Out[17]: <pre>array([[1001,    0],\n       [1002,    0],\n       [   0,    0],\n       [2001,    0],\n       [2001, 2001],\n       [2002, 2001],\n       [2002,    0],\n       [2002, 2002]])</pre> In\u00a0[18]: Copied! <pre>mult2 = AILS(final_model=final_model)\n</pre> mult2 = AILS(final_model=final_model) In\u00a0[19]: Copied! <pre>def psi(X, Y):\n    PSI = np.zeros((len(X), 8))\n    for k in range(2, len(Y)):\n        PSI[k, 0] = Y[k - 1]\n        PSI[k, 1] = Y[k - 2]\n        PSI[k, 2] = 1\n        PSI[k, 3] = X[k - 1]\n        PSI[k, 4] = X[k - 1] ** 2\n        PSI[k, 5] = X[k - 2] * X[k - 1]\n        PSI[k, 6] = X[k - 2]\n        PSI[k, 7] = X[k - 2] ** 2\n    return np.delete(PSI, [0, 1], axis=0)\n</pre> def psi(X, Y):     PSI = np.zeros((len(X), 8))     for k in range(2, len(Y)):         PSI[k, 0] = Y[k - 1]         PSI[k, 1] = Y[k - 2]         PSI[k, 2] = 1         PSI[k, 3] = X[k - 1]         PSI[k, 4] = X[k - 1] ** 2         PSI[k, 5] = X[k - 2] * X[k - 1]         PSI[k, 6] = X[k - 2]         PSI[k, 7] = X[k - 2] ** 2     return np.delete(PSI, [0, 1], axis=0) <p>The value of theta with the lowest mean squared error obtained with the same code implemented in Scilab was:</p> <p>$ W_{LS} = 0.3612343 $</p> <p>and:</p> <p>$ W_{SG} = 0.3548699 $</p> <p>and:</p> <p>$ W_{SF} = 0.3548699 $</p> In\u00a0[20]: Copied! <pre>PSI = psi(x_train, y_train)\nw = np.array([[0.3612343], [0.2838959], [0.3548699]])\n</pre> PSI = psi(x_train, y_train) w = np.array([[0.3612343], [0.2838959], [0.3548699]]) <pre>C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_1728\\3629615894.py:4: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 0] = Y[k - 1]\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_1728\\3629615894.py:5: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 1] = Y[k - 2]\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_1728\\3629615894.py:7: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 3] = X[k - 1]\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_1728\\3629615894.py:8: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 4] = X[k - 1] ** 2\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_1728\\3629615894.py:9: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 5] = X[k - 2] * X[k - 1]\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_1728\\3629615894.py:10: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 6] = X[k - 2]\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_1728\\3629615894.py:11: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 7] = X[k - 2] ** 2\n</pre> In\u00a0[21]: Copied! <pre>J, E, theta, HR, QR, position = mult2.estimate(\n    y=y_train, X=x_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\n    \"w1\": w[0, :],\n    \"w2\": w[2, :],\n    \"w3\": w[1, :],\n    \"J_ls\": J[0, :],\n    \"J_sg\": J[1, :],\n    \"J_sf\": J[2, :],\n    \"||J||:\": E,\n}\n# the order of the weights is different because the way we implemented in Python, but the results are very close as expected\npd.DataFrame(result)\n</pre> J, E, theta, HR, QR, position = mult2.estimate(     y=y_train, X=x_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w ) result = {     \"w1\": w[0, :],     \"w2\": w[2, :],     \"w3\": w[1, :],     \"J_ls\": J[0, :],     \"J_sg\": J[1, :],     \"J_sf\": J[2, :],     \"||J||:\": E, } # the order of the weights is different because the way we implemented in Python, but the results are very close as expected pd.DataFrame(result) Out[21]: w1 w2 w3 J_ls J_sg J_sf ||J||: 0 0.361234 0.35487 0.283896 1.0 1.0 1.0 1.0 <p>Dynamic results</p> In\u00a0[22]: Copied! <pre>model.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = mult2.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</pre> model.theta = theta[position, :].reshape(-1, 1) model.final_model = mult2.final_model yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=3,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) r Out[22]: Regressors Parameters ERR 0 1 1.4287E+00 9.999E-01 1 y(k-1) 5.5147E-01 2.042E-05 2 y(k-2) 4.0449E-01 1.108E-06 3 x1(k-1) -1.2605E+01 4.688E-06 4 x1(k-2) 1.2257E+01 3.922E-07 5 x1(k-1)^2 8.3274E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1416E+01 5.690E-07 7 x1(k-2)^2 3.0846E+00 3.827E-06 In\u00a0[23]: Copied! <pre>plot_results(y=y_valid, yhat=yhat, n=1000)\n</pre> plot_results(y=y_valid, yhat=yhat, n=1000) <p>Static gain</p> In\u00a0[24]: Copied! <pre>plt.figure(7)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n</pre> plt.figure(7) plt.title(\"Gain\") plt.plot(     Uo,     gain,     linewidth=1.5,     linestyle=\"-\",     marker=\"o\",     label=\"Buck converter static gain\", ) plt.plot(     Uo,     HR.dot(model.theta),     linestyle=\"-\",     marker=\"^\",     linewidth=1.5,     label=\"NARX model gain\", ) plt.xlabel(\"$\\\\bar{u}$\") plt.ylabel(\"$\\\\bar{g}$\") plt.ylim(-16, 0) plt.legend() plt.show() <p>Static function</p> In\u00a0[25]: Copied! <pre>plt.figure(8)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \u200b\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</pre> plt.figure(8) plt.title(\"Static Curve\") plt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\") plt.plot(     Uo,     QR.dot(model.theta),     linewidth=1.5,     label=\"NARX \u200b\u200bstatic representation\",     linestyle=\"-\",     marker=\"^\", ) plt.xlabel(\"$\\\\bar{u}$\") plt.xlabel(\"$\\\\bar{y}$\") plt.legend() plt.show() <p>Pareto-set solutions</p> In\u00a0[26]: Copied! <pre>plt.figure(9)\nax = plt.axes(projection=\"3d\")\nax.plot3D(J[0, :], J[1, :], J[2, :], \"o\", linewidth=0.1)\nax.set_title(\"Optimum pareto-curve\", fontsize=15)\nax.set_xlabel(\"$J_{ls}$\", fontsize=10)\nax.set_ylabel(\"$J_{sg}$\", fontsize=10)\nax.set_zlabel(\"$J_{sf}$\", fontsize=10)\nplt.show()\n</pre> plt.figure(9) ax = plt.axes(projection=\"3d\") ax.plot3D(J[0, :], J[1, :], J[2, :], \"o\", linewidth=0.1) ax.set_title(\"Optimum pareto-curve\", fontsize=15) ax.set_xlabel(\"$J_{ls}$\", fontsize=10) ax.set_ylabel(\"$J_{sg}$\", fontsize=10) ax.set_zlabel(\"$J_{sf}$\", fontsize=10) plt.show() In\u00a0[27]: Copied! <pre>theta[position, :]\n</pre> theta[position, :] Out[27]: <pre>array([  1.42867821,   0.55147249,   0.40449005, -12.60549001,\n        12.25730092,   8.32739876, -11.41573694,   3.08460955])</pre> <p>The following table show the results reported in \u2018IniciacaoCientifica2007\u2019 and the ones obtained with SysIdentPy implementation</p> Theta SysIdentPy IniciacaoCientifica2007 $\\theta_1$ 0.5514725 0.549144 $\\theta_2$ 0.40449005 0.408028 $\\theta_3$ 1.42867821 1.45097 $\\theta_4$ -12.60548863 -12.55788 $\\theta_5$ 8.32740057 8.1516315 $\\theta_6$ -11.41574116 -11.09728 $\\theta_7$ 12.25729955 12.215782 $\\theta_8$ 3.08461195 2.9319577 <p>where:</p> <p>$ E_{Scilab} =    17.426613 $</p> <p>and:</p> <p>$ E_{Python} = 17.474865 $</p> In\u00a0[28]: Copied! <pre>R, qit = mult2.build_linear_mapping()\nprint(\"R matrix:\")\nprint(R)\nprint(\"qit matrix:\")\nprint(qit)\n</pre> R, qit = mult2.build_linear_mapping() print(\"R matrix:\") print(R) print(\"qit matrix:\") print(qit) <pre>R matrix:\n[[1 0 0 0 0 0 0 0]\n [0 1 1 0 0 0 0 0]\n [0 0 0 1 1 0 0 0]\n [0 0 0 0 0 1 1 1]]\nqit matrix:\n[[0 0]\n [1 0]\n [0 1]\n [0 2]]\n</pre> <p>model's structure that will be utilized (\u2018IniciacaoCientifica2007\u2019):</p> <p>$ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 + \\theta_4 u(k-1) + \\theta_5 u(k-1)^2 + \\theta_6 u(k-2)u(k-1)+\\theta_7 u(k-2) + \\theta_8 u(k-2)^2 $</p> <p>$ q_i =  \\begin{bmatrix} 0 &amp; 0\\\\ 1 &amp; 0\\\\ 2 &amp; 0\\\\ 2 &amp; 2\\\\  \\end{bmatrix} $ $ = $ $ \\begin{bmatrix} 1\\\\ \\overline{y}\\\\ \\overline{u}\\\\ \\overline{u^2} \\end{bmatrix} $</p> <p>and:</p> <p>$ R =  \\begin{bmatrix} term/\\theta &amp; \\theta_1 &amp; \\theta_2 &amp; \\theta_3 &amp; \\theta_4 &amp; \\theta_5 &amp; \\theta_6 &amp; \\theta_7 &amp; \\theta_8\\\\ 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\overline{y} &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\overline{u} &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\\\ \\overline{u^2} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\end{bmatrix} $</p> <p>consistent with matrix R:</p> <p>R = [0 0 1 0 0 0 0 0;1 1 0 0 0 0 0 0;0 0 0 1 0 0 1 0;0 0 0 0 1 1 0 1]; // R</p> <p>or:</p> <p>$  R =  \\begin{bmatrix} 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\end{bmatrix} $</p> In\u00a0[29]: Copied! <pre>bi_objective = AILS(\n    static_function=True, static_gain=False, final_model=final_model, normalize=True\n)\n</pre> bi_objective = AILS(     static_function=True, static_gain=False, final_model=final_model, normalize=True ) <p>the value of theta with the lowest mean squared error obtained through the routine in Scilab was:</p> <p>$ W_{LS} = 0.9931126 $</p> <p>and:</p> <p>$ W_{SF} = 0.0068874 $</p> In\u00a0[30]: Copied! <pre>w = np.zeros((2, 2000))\nw[0, :] = np.logspace(-0.01, -6, num=2000, base=2.71)\nw[1, :] = np.ones(2000) - w[0, :]\nJ, E, theta, HR, QR, position = bi_objective.estimate(\n    y=y_train, X=x_train, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\"w1\": w[0, :], \"w2\": w[1, :], \"J_ls\": J[0, :], \"J_sg\": J[1, :], \"||J||:\": E}\npd.DataFrame(result)\n</pre> w = np.zeros((2, 2000)) w[0, :] = np.logspace(-0.01, -6, num=2000, base=2.71) w[1, :] = np.ones(2000) - w[0, :] J, E, theta, HR, QR, position = bi_objective.estimate(     y=y_train, X=x_train, y_static=Yo, X_static=Uo, weighing_matrix=w ) result = {\"w1\": w[0, :], \"w2\": w[1, :], \"J_ls\": J[0, :], \"J_sg\": J[1, :], \"||J||:\": E} pd.DataFrame(result) Out[30]: w1 w2 J_ls J_sg ||J||: 0 0.990080 0.009920 0.990863 1.000000 0.990939 1 0.987127 0.012873 0.990865 0.987032 0.990939 2 0.984182 0.015818 0.990867 0.974307 0.990939 3 0.981247 0.018753 0.990870 0.961803 0.990940 4 0.978320 0.021680 0.990873 0.949509 0.990941 ... ... ... ... ... ... 1995 0.002555 0.997445 0.999993 0.000072 0.999993 1996 0.002547 0.997453 0.999994 0.000072 0.999994 1997 0.002540 0.997460 0.999996 0.000071 0.999996 1998 0.002532 0.997468 0.999998 0.000071 0.999998 1999 0.002525 0.997475 1.000000 0.000070 1.000000 <p>2000 rows \u00d7 5 columns</p> In\u00a0[31]: Copied! <pre>model.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = bi_objective.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</pre> model.theta = theta[position, :].reshape(-1, 1) model.final_model = bi_objective.final_model yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=3,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) r Out[31]: Regressors Parameters ERR 0 1 1.3873E+00 9.999E-01 1 y(k-1) 5.4941E-01 2.042E-05 2 y(k-2) 4.0804E-01 1.108E-06 3 x1(k-1) -1.2515E+01 4.688E-06 4 x1(k-2) 1.2227E+01 3.922E-07 5 x1(k-1)^2 8.1171E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1047E+01 5.690E-07 7 x1(k-2)^2 2.9043E+00 3.827E-06 In\u00a0[32]: Copied! <pre>plot_results(y=y_valid, yhat=yhat, n=1000)\n</pre> plot_results(y=y_valid, yhat=yhat, n=1000) In\u00a0[33]: Copied! <pre>plt.figure(10)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \u200b\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</pre> plt.figure(10) plt.title(\"Static Curve\") plt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\") plt.plot(     Uo,     QR.dot(model.theta),     linewidth=1.5,     label=\"NARX \u200b\u200bstatic representation\",     linestyle=\"-\",     marker=\"^\", ) plt.xlabel(\"$\\\\bar{u}$\") plt.xlabel(\"$\\\\bar{y}$\") plt.legend() plt.show() In\u00a0[34]: Copied! <pre>plt.figure(11)\nplt.title(\"Costs Functions\")\nplt.plot(J[1, :], J[0, :], \"o\")\nplt.xlabel(\"Static Curve Information\")\nplt.ylabel(\"Prediction Error\")\nplt.show()\n</pre> plt.figure(11) plt.title(\"Costs Functions\") plt.plot(J[1, :], J[0, :], \"o\") plt.xlabel(\"Static Curve Information\") plt.ylabel(\"Prediction Error\") plt.show() <p>where the best estimated $\\theta$ is</p> Theta SysIdentPy IniciacaoCientifica2007 $\\theta_1$ 0.54940883 0.5494135 $\\theta_2$ 0.40803995 0.4080312 $\\theta_3$ 1.38725684 3.3857601 $\\theta_4$ -12.51466378 -12.513688 $\\theta_5$ 8.11712897 8.116575 $\\theta_6$ -11.04664789 -11.04592 $\\theta_7$ 12.22693907 12.227184 $\\theta_8$ 2.90425844 2.9038468 <p>where:</p> <p>$ E_{Scilab} = 17.408934 $</p> <p>and:</p> <p>$ E_{Python} = 17.408947 $</p> In\u00a0[35]: Copied! <pre>bi_objective_gain = AILS(\n    static_function=False, static_gain=True, final_model=final_model, normalize=False\n)\n</pre> bi_objective_gain = AILS(     static_function=False, static_gain=True, final_model=final_model, normalize=False ) <p>the value of theta with the lowest mean squared error obtained through the routine in Scilab was:</p> <p>$ W_{LS} = 0.9931126 $</p> <p>and:</p> <p>$ W_{SF} = 0.0068874 $</p> In\u00a0[36]: Copied! <pre>w = np.zeros((2, 2000))\nw[0, :] = np.logspace(0, -6, num=2000, base=2.71)\nw[1, :] = np.ones(2000) - w[0, :]\n# W = np.array([[0.9931126],\n# [0.0068874]])\nJ, E, theta, HR, QR, position = bi_objective_gain.estimate(\n    X=x_train, y=y_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\"w1\": w[0, :], \"w2\": w[1, :], \"J_ls\": J[0, :], \"J_sg\": J[1, :], \"||J||:\": E}\npd.DataFrame(result)\n</pre> w = np.zeros((2, 2000)) w[0, :] = np.logspace(0, -6, num=2000, base=2.71) w[1, :] = np.ones(2000) - w[0, :] # W = np.array([[0.9931126], # [0.0068874]]) J, E, theta, HR, QR, position = bi_objective_gain.estimate(     X=x_train, y=y_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w ) result = {\"w1\": w[0, :], \"w2\": w[1, :], \"J_ls\": J[0, :], \"J_sg\": J[1, :], \"||J||:\": E} pd.DataFrame(result) Out[36]: w1 w2 J_ls J_sg ||J||: 0 1.000000 0.000000 17.407256 3.579461e+01 39.802849 1 0.997012 0.002988 17.407528 2.109260e-01 17.408806 2 0.994033 0.005967 17.407540 2.082067e-01 17.408785 3 0.991063 0.008937 17.407559 2.056636e-01 17.408774 4 0.988102 0.011898 17.407585 2.031788e-01 17.408771 ... ... ... ... ... ... 1995 0.002555 0.997445 17.511596 3.340081e-07 17.511596 1996 0.002547 0.997453 17.511596 3.320125e-07 17.511596 1997 0.002540 0.997460 17.511597 3.300289e-07 17.511597 1998 0.002532 0.997468 17.511598 3.280571e-07 17.511598 1999 0.002525 0.997475 17.511599 3.260972e-07 17.511599 <p>2000 rows \u00d7 5 columns</p> In\u00a0[37]: Copied! <pre># Writing the results\nmodel.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = bi_objective_gain.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</pre> # Writing the results model.theta = theta[position, :].reshape(-1, 1) model.final_model = bi_objective_gain.final_model yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=3,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) r Out[37]: Regressors Parameters ERR 0 1 1.4853E+00 9.999E-01 1 y(k-1) 5.4940E-01 2.042E-05 2 y(k-2) 4.0806E-01 1.108E-06 3 x1(k-1) -1.2581E+01 4.688E-06 4 x1(k-2) 1.2210E+01 3.922E-07 5 x1(k-1)^2 8.1686E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1122E+01 5.690E-07 7 x1(k-2)^2 2.9455E+00 3.827E-06 In\u00a0[38]: Copied! <pre>plot_results(y=y_valid, yhat=yhat, n=1000)\n</pre> plot_results(y=y_valid, yhat=yhat, n=1000) In\u00a0[39]: Copied! <pre>plt.figure(12)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.legend()\nplt.show()\n</pre> plt.figure(12) plt.title(\"Gain\") plt.plot(     Uo,     gain,     linewidth=1.5,     linestyle=\"-\",     marker=\"o\",     label=\"Buck converter static gain\", ) plt.plot(     Uo,     HR.dot(model.theta),     linestyle=\"-\",     marker=\"^\",     linewidth=1.5,     label=\"NARX model gain\", ) plt.xlabel(\"$\\\\bar{u}$\") plt.ylabel(\"$\\\\bar{g}$\") plt.legend() plt.show() In\u00a0[40]: Copied! <pre>plt.figure(11)\nplt.title(\"Costs Functions\")\nplt.plot(J[1, :], J[0, :], \"o\")\nplt.xlabel(\"Gain Information\")\nplt.ylabel(\"Prediction Error\")\nplt.show()\n</pre> plt.figure(11) plt.title(\"Costs Functions\") plt.plot(J[1, :], J[0, :], \"o\") plt.xlabel(\"Gain Information\") plt.ylabel(\"Prediction Error\") plt.show() <p>being the selected $\\theta$:</p> Theta SysIdentPy IniciacaoCientifica2007 $\\theta_1$ 0.54939785 0.54937289 $\\theta_2$ 0.40805603 0.40810168 $\\theta_3$ 1.48525190 1.48663719 $\\theta_4$ -12.58066084 -12.58127183 $\\theta_5$ 8.16862622 8.16780294 $\\theta_6$ -11.12171897 -11.11998621 $\\theta_7$ 12.20954849 12.20927355 $\\theta_8$ 2.94548501 2.9446532 <p>where:</p> <p>$ E_{Scilab} =  17.408997 $</p> <p>and:</p> <p>$ E_{Python} = 17.408781 $</p> <p>Matrix Q:</p> In\u00a0[41]: Copied! <pre>bi_objective_gain.build_static_function_information(Uo, Yo)[1]\n</pre> bi_objective_gain.build_static_function_information(Uo, Yo)[1] Out[41]: <pre>array([[   50.        ,   800.        ,   800.        ,   100.        ,\n          100.        ,   269.3877551 ,   269.3877551 ,   269.3877551 ],\n       [  800.        , 17240.81632653, 17240.81632653,  1044.89795918,\n         1044.89795918,  2089.79591837,  2089.79591837,  2089.79591837],\n       [  800.        , 17240.81632653, 17240.81632653,  1044.89795918,\n         1044.89795918,  2089.79591837,  2089.79591837,  2089.79591837],\n       [  100.        ,  1044.89795918,  1044.89795918,   269.3877551 ,\n          269.3877551 ,   816.32653061,   816.32653061,   816.32653061],\n       [  100.        ,  1044.89795918,  1044.89795918,   269.3877551 ,\n          269.3877551 ,   816.32653061,   816.32653061,   816.32653061],\n       [  269.3877551 ,  2089.79591837,  2089.79591837,   816.32653061,\n          816.32653061,  2638.54142407,  2638.54142407,  2638.54142407],\n       [  269.3877551 ,  2089.79591837,  2089.79591837,   816.32653061,\n          816.32653061,  2638.54142407,  2638.54142407,  2638.54142407],\n       [  269.3877551 ,  2089.79591837,  2089.79591837,   816.32653061,\n          816.32653061,  2638.54142407,  2638.54142407,  2638.54142407]])</pre> <p>Matrix H+R:</p> In\u00a0[42]: Copied! <pre>bi_objective_gain.build_static_gain_information(Uo, Yo, gain)[1]\n</pre> bi_objective_gain.build_static_gain_information(Uo, Yo, gain)[1] Out[42]: <pre>array([[    0.        ,     0.        ,     0.        ,     0.        ,\n            0.        ,     0.        ,     0.        ,     0.        ],\n       [    0.        ,  3200.        ,  3200.        ,  -400.        ,\n         -400.        , -1600.        , -1600.        , -1600.        ],\n       [    0.        ,  3200.        ,  3200.        ,  -400.        ,\n         -400.        , -1600.        , -1600.        , -1600.        ],\n       [    0.        ,  -400.        ,  -400.        ,    50.        ,\n           50.        ,   200.        ,   200.        ,   200.        ],\n       [    0.        ,  -400.        ,  -400.        ,    50.        ,\n           50.        ,   200.        ,   200.        ,   200.        ],\n       [    0.        , -1600.        , -1600.        ,   200.        ,\n          200.        ,  1077.55102041,  1077.55102041,  1077.55102041],\n       [    0.        , -1600.        , -1600.        ,   200.        ,\n          200.        ,  1077.55102041,  1077.55102041,  1077.55102041],\n       [    0.        , -1600.        , -1600.        ,   200.        ,\n          200.        ,  1077.55102041,  1077.55102041,  1077.55102041]])</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/multiobjective_parameter_estimation/#multiobjective-parameter-estimation-for-narmax-models-an-overview","title":"Multiobjective Parameter Estimation for NARMAX models - An Overview\u00b6","text":"<p>Example created by Gabriel Bueno Leandro, Samir Milani Martins and Wilson Rocha Lacerda Junior</p> <p>Multiobjective parameter estimation represents a fundamental paradigm shift in the way we approach the parameter tuning problem for NARMAX models. Instead of seeking a single set of parameter values that optimally fits the model to the data, multiobjective approaches aim to identify a set of parameter solutions, known as the Pareto front, that provide a trade-off between competing objectives. These objectives often encompass a spectrum of model performance criteria, such as goodness-of-fit, model complexity, and robustness.</p>"},{"location":"examples/multiobjective_parameter_estimation/#reference","title":"Reference\u00b6","text":"<p>For further information, check this reference: https://doi.org/10.1080/00207170601185053.</p>"},{"location":"examples/multiobjective_parameter_estimation/#use-case-buck-converter","title":"Use case: Buck converter\u00b6","text":"A buck converter is a type of DC/DC converter that decreases the voltage (while increasing the current) from its input (power supply) to its output (load). It is similar to a boost converter (elevator) and is a type of switched-mode power supply (SMPS) that typically contains at least two semiconductors (a diode and a transistor, although modern buck converters replace the diode with a second transistor used for synchronous rectification) and at least one energy storage element, a capacitor, inductor or both combined."},{"location":"examples/multiobjective_parameter_estimation/#dynamic-behavior","title":"Dynamic Behavior\u00b6","text":""},{"location":"examples/multiobjective_parameter_estimation/#buck-converter-static-function","title":"Buck Converter Static Function\u00b6","text":"<p>The duty cycle, represented by the symbol $D$, is defined as the ratio of the time the system is on ($T_{on}$\u200b) to the total operation cycle time ($T$). Mathematically, this can be expressed as $D=\\frac{T_{on}}{T}$. The complement of the duty cycle, represented by $D'$, is defined as the ratio of the time the system is off ($T_{off}$) to the total operation cycle time ($T$) and can be expressed as $D'=\\frac{T_{off}}{T}$.</p> <p>The load voltage ($V_o$) is related to the source voltage ($V_d$) by the equation $V_o\u200b=D\u22c5V_d\u200b=(1\u2212D\u2019)\u22c5V_d$.  For this particular converter, it is known that $D\u2032=\\frac{\\bar{u}-1}{3}\u200b$,\u200b which means that the static function of this system can be derived from theory to be:</p> <p>$V_o = \\frac{4V_d}{3} - \\frac{V_d}{3}\\cdot \\bar{u}$</p> <p>If we assume that the source voltage $V_d$\u200b is equal to 24 V, then we can rewrite the above expression as follows:</p> <p>$V_o = (4 - \\bar{u})\\cdot 8$</p>"},{"location":"examples/multiobjective_parameter_estimation/#buck-converter-static-gain","title":"Buck converter Static Gain\u00b6","text":"<p>The gain of a Buck converter is a measure of how its output voltage changes in response to changes in its input voltage. Mathematically, the gain can be calculated as the derivative of the converter\u2019s static function, which describes the relationship between its input and output voltages. In this case, the static function of the Buck converter is given by the equation:</p> <p>$V_o = (4 - \\bar{u})\\cdot 8$</p> <p>Taking the derivative of this equation with respect to $\\hat{u}$, we find that the gain of the Buck converter is equal to \u22128. In other words, for every unit increase in the input voltage $\\hat{u}$, the output voltage Vo\u200b will decrease by 8 units.</p> <p>so $gain=V_o'=-8$</p>"},{"location":"examples/multiobjective_parameter_estimation/#building-a-dynamic-model-using-the-mono-objective-approach","title":"Building a dynamic model using the mono-objective approach\u00b6","text":""},{"location":"examples/multiobjective_parameter_estimation/#affine-information-least-squares-algorithm-ails","title":"Affine Information Least Squares Algorithm (AILS)\u00b6","text":"<p>AILS is a multiobjective parameter estimation algorithm, based on a set of affine information pairs. The multiobjective approach proposed in the mentioned paper and implemented in SysIdentPy leads to a convex multiobjective optimization problem, which can be solved by AILS. AILS is a LeastSquares-type non-iterative scheme for finding the Pareto-set solutions for the multiobjective problem.</p> <p>So, with the model structure defined (we will be using the one built using the dynamic data above), one can estimate the parameters using the multiobjective approach.</p> <p>The information about static function and static gain, besides the usual dynamic input/output data, can be used to build the pair of affine information to estimate the parameters of the model. We can model the cost function as:</p> <p>$ \\gamma(\\hat\\theta) = w_1\\cdot J_{LS}(\\hat{\\theta})+w_2\\cdot J_{SF}(\\hat{\\theta})+w_3\\cdot J_{SG}(\\hat{\\theta}) $</p>"},{"location":"examples/multiobjective_parameter_estimation/#multiobjective-parameter-estimation-considering-3-different-objectives-the-prediction-error-the-static-function-and-the-static-gain","title":"Multiobjective parameter estimation considering 3 different objectives: the prediction error, the static function and the static gain\u00b6","text":""},{"location":"examples/multiobjective_parameter_estimation/#the-dynamic-results-for-that-chosen-theta-is","title":"The dynamic results for that chosen theta is\u00b6","text":""},{"location":"examples/multiobjective_parameter_estimation/#the-static-gain-result-is","title":"The static gain result is\u00b6","text":""},{"location":"examples/multiobjective_parameter_estimation/#the-static-function-result-is","title":"The static function result is\u00b6","text":""},{"location":"examples/multiobjective_parameter_estimation/#getting-the-best-weight-combination-based-on-the-norm-of-the-cost-function","title":"Getting the best weight combination based on the norm of the cost function\u00b6","text":""},{"location":"examples/multiobjective_parameter_estimation/#detailing-ails","title":"Detailing AILS\u00b6","text":""},{"location":"examples/multiobjective_parameter_estimation/#validation","title":"Validation\u00b6","text":"<p>The following model structure will be used to validate the approach:</p> <p>$ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 + \\theta_4 u(k-1) + \\theta_5 u(k-1)^2 + \\theta_6 u(k-2)u(k-1)+\\theta_7 u(k-2) + \\theta_8 u(k-2)^2 $</p> <p>$\\therefore$</p> <p>$ final\\_model =  \\begin{bmatrix} 1001 &amp; 0\\\\ 1002 &amp; 0\\\\ 0 &amp; 0\\\\ 2001 &amp; 0\\\\ 2001 &amp; 2001\\\\ 2002 &amp; 2001\\\\ 2002 &amp; 0\\\\ 2002 &amp; 2002 \\end{bmatrix} $</p> <p>defining in code:</p>"},{"location":"examples/multiobjective_parameter_estimation/#note-as-mentioned-before-the-order-of-the-regressors-in-the-model-change-but-it-is-the-same-structure-the-tables-shows-the-respective-regressor-parameter-concerning-sysidentpy-and-iniciacaocientifica2007-but-the-order-1-2-and-so-on-are-not-the-same-of-the-ones-in-modelfinal_model","title":"Note: as mentioned before, the order of the regressors in the model change, but it is the same structure. The tables shows the respective regressor parameter concerning <code>SysIdentPy</code> and <code>IniciacaoCientifica2007</code>,  but the order <code>\u03b8\u2081</code>, <code>\u03b8\u2082</code> and so on are not the same of the ones in model.final_model\u00b6","text":""},{"location":"examples/multiobjective_parameter_estimation/#biobjective-optimization","title":"Biobjective optimization\u00b6","text":""},{"location":"examples/multiobjective_parameter_estimation/#an-use-case-applied-to-buck-converter-cc-cc-using-as-objectives-the-static-curve-information-and-the-prediction-error-dynamic","title":"An use case applied to Buck converter CC-CC using as objectives the static curve information and the prediction error (dynamic)\u00b6","text":""},{"location":"examples/multiobjective_parameter_estimation/#multiobjective-parameter-estimation","title":"Multiobjective parameter estimation\u00b6","text":""},{"location":"examples/multiobjective_parameter_estimation/#use-case-considering-2-different-objectives-the-prediction-error-and-the-static-gain","title":"Use case considering 2 different objectives: the prediction error and the static gain\u00b6","text":""},{"location":"examples/multiobjective_parameter_estimation/#additional-information","title":"Additional Information\u00b6","text":"<p>You can also access the matrix Q and H using the following methods</p>"},{"location":"examples/multiple_inputs_example/","title":"Multiple Inputs usage","text":"In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.utils.generate_data import get_miso_data\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.parameter_estimation import LeastSquares from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_results from sysidentpy.utils.generate_data import get_miso_data In\u00a0[2]: Copied! <pre>x_train, x_valid, y_train, y_valid = get_miso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n</pre> x_train, x_valid, y_train, y_valid = get_miso_data(     n=1000, colored_noise=False, sigma=0.001, train_percentage=90 ) <p>There is a specific difference for multiple input data.</p> <ul> <li>You have to pass the lags for each input in a nested list (e.g., [[1, 2], [1, 2]])</li> </ul> <p>The remainder settings remains the same.</p> In\u00a0[7]: Copied! <pre>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_terms=4,\n    ylag=2,\n    xlag=[[1, 2], [1, 2]],\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\n</pre> basis_function = Polynomial(degree=2) estimator = LeastSquares()  model = FROLS(     order_selection=True,     n_terms=4,     ylag=2,     xlag=[[1, 2], [1, 2]],     info_criteria=\"aic\",     estimator=estimator,     basis_function=basis_function,     err_tol=None, ) In\u00a0[8]: Copied! <pre>model.fit(X=x_train, y=y_train)\n</pre> model.fit(X=x_train, y=y_train) Out[8]: <pre>&lt;sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS at 0x1a88cc17350&gt;</pre> In\u00a0[11]: Copied! <pre>yhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n</pre> yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) plot_results(y=y_valid, yhat=yhat, n=1000) <pre>0.00314141814133057\n       Regressors   Parameters             ERR\n0         x2(k-1)   5.9999E-01  9.15006949E-01\n1  x2(k-2)x1(k-1)  -3.0010E-01  4.31748224E-02\n2        y(k-1)^2   3.9976E-01  4.15131661E-02\n3   x1(k-1)y(k-1)   1.0028E-01  2.96827987E-04\n</pre> In\u00a0[12]: Copied! <pre>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</pre> xaxis = np.arange(1, model.n_info_values + 1) plt.plot(xaxis, model.info_values) plt.xlabel(\"n_terms\") plt.ylabel(\"Information Criteria\") Out[12]: <pre>Text(0, 0.5, 'Information Criteria')</pre>"},{"location":"examples/multiple_inputs_example/#multiple-inputs-usage","title":"Multiple Inputs usage\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"examples/multiple_inputs_example/#generating-2-input-1-output-sample-data","title":"Generating 2 input 1 output sample data\u00b6","text":"<p>The data is generated by simulating the following model:</p> <p>$y_k = 0.4y_{k-1}^2 + 0.1y_{k-1}x1_{k-1} + 0.6x2_{k-1} -0.3x1_{k-1}x2_{k-2} + e_{k}$</p> <p>If colored_noise is set to True:</p> <p>$e_{k} = 0.8\\nu_{k-1} + \\nu_{k}$</p> <p>where $x$ is a uniformly distributed random variable and $\\nu$ is a gaussian distributed variable with $\\mu=0$ and $\\sigma=0.001$</p>"},{"location":"examples/multiple_inputs_example/#build-the-model","title":"Build the model\u00b6","text":""},{"location":"examples/multiple_inputs_example/#model-evaluation","title":"Model evaluation\u00b6","text":""},{"location":"examples/n_steps_ahead_prediction/","title":"Example: N-steps-ahead prediction - F-16 Ground Vibration Test benchmark","text":"<p>The following text was taken from the link http://www.nonlinearbenchmark.org/#F16.</p> <p>Note: The reader is reffered to the mentioned website for a complete reference concerning the experiment. For now, this notebook is just a simple example of the performance of SysIdentPy on a real world dataset. A more detailed study of this system will be published in the future.</p> <p>The F-16 Ground Vibration Test benchmark features a high order system with clearance and friction nonlinearities at the mounting interface of the payloads.</p> <p>The experimental data made available to the Workshop participants were acquired on a full-scale F-16 aircraft on the occasion of the Siemens LMS Ground Vibration Testing Master Class, held in September 2014 at the Saffraanberg military basis, Sint-Truiden, Belgium.</p> <p>During the test campaign, two dummy payloads were mounted at the wing tips to simulate the mass and inertia properties of real devices typically equipping an F-16 in \ufb02ight. The aircraft structure was instrumented with accelerometers. One shaker was attached underneath the right wing to apply input signals. The dominant source of nonlinearity in the structural dynamics was expected to originate from the mounting interfaces of the two payloads. These interfaces consist of T-shaped connecting elements on the payload side, slid through a rail attached to the wing side. A preliminary investigation showed that the back connection of the right-wing-to-payload interface was the predominant source of nonlinear distortions in the aircraft dynamics, and is therefore the focus of this benchmark study.</p> <p>A detailed formulation of the identification problem can be found here. All the provided files and information on the F-16 aircraft benchmark system are available for download here. This zip-file contains a detailed system description, the estimation and test data sets, and some pictures of the setup. The data is available in the .csv and .mat file format.</p> <p>Please refer to the F16 benchmark as:</p> <p>J.P. No\u00ebl and M. Schoukens, F-16 aircraft benchmark based on ground vibration test data, 2017 Workshop on Nonlinear System Identification Benchmarks, pp. 19-23, Brussels, Belgium, April 24-26, 2017.</p> In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\n</pre> import numpy as np import pandas as pd from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function import Polynomial from sysidentpy.parameter_estimation import LeastSquares from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_results In\u00a0[4]: Copied! <pre>f_16 = pd.read_csv(\n    r\"../examples/datasets/f-16.txt\", header=None, names=[\"x1\", \"x2\", \"y\"]\n)\n</pre> f_16 = pd.read_csv(     r\"../examples/datasets/f-16.txt\", header=None, names=[\"x1\", \"x2\", \"y\"] ) In\u00a0[5]: Copied! <pre>f_16.shape\n</pre> f_16.shape Out[5]: <pre>(32768, 3)</pre> In\u00a0[6]: Copied! <pre>f_16[[\"x1\", \"x2\"]][0:500].plot(figsize=(12, 8))\n</pre> f_16[[\"x1\", \"x2\"]][0:500].plot(figsize=(12, 8)) Out[6]: <pre>&lt;Axes: &gt;</pre> In\u00a0[7]: Copied! <pre>f_16[\"y\"][0:2000].plot(figsize=(12, 8))\n</pre> f_16[\"y\"][0:2000].plot(figsize=(12, 8)) Out[7]: <pre>&lt;Axes: &gt;</pre> In\u00a0[8]: Copied! <pre>x1_id, x1_val = f_16[\"x1\"][0:16384].values.reshape(-1, 1), f_16[\"x1\"][\n    16384::\n].values.reshape(-1, 1)\nx2_id, x2_val = f_16[\"x2\"][0:16384].values.reshape(-1, 1), f_16[\"x2\"][\n    16384::\n].values.reshape(-1, 1)\nx_id = np.concatenate([x1_id, x2_id], axis=1)\nx_val = np.concatenate([x1_val, x2_val], axis=1)\n\ny_id, y_val = f_16[\"y\"][0:16384].values.reshape(-1, 1), f_16[\"y\"][\n    16384::\n].values.reshape(-1, 1)\n</pre> x1_id, x1_val = f_16[\"x1\"][0:16384].values.reshape(-1, 1), f_16[\"x1\"][     16384:: ].values.reshape(-1, 1) x2_id, x2_val = f_16[\"x2\"][0:16384].values.reshape(-1, 1), f_16[\"x2\"][     16384:: ].values.reshape(-1, 1) x_id = np.concatenate([x1_id, x2_id], axis=1) x_val = np.concatenate([x1_val, x2_val], axis=1)  y_id, y_val = f_16[\"y\"][0:16384].values.reshape(-1, 1), f_16[\"y\"][     16384:: ].values.reshape(-1, 1) In\u00a0[9]: Copied! <pre>x1lag = list(range(1, 10))\nx2lag = list(range(1, 10))\nx2lag\n</pre> x1lag = list(range(1, 10)) x2lag = list(range(1, 10)) x2lag Out[9]: <pre>[1, 2, 3, 4, 5, 6, 7, 8, 9]</pre> In\u00a0[10]: Copied! <pre>basis_function = Polynomial(degree=1)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=39,\n    ylag=20,\n    xlag=[x1lag, x2lag],\n    info_criteria=\"bic\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_id, y=y_id)\n</pre> basis_function = Polynomial(degree=1) estimator = LeastSquares()  model = FROLS(     order_selection=True,     n_info_values=39,     ylag=20,     xlag=[x1lag, x2lag],     info_criteria=\"bic\",     estimator=estimator,     basis_function=basis_function, )  model.fit(X=x_id, y=y_id) Out[10]: <pre>&lt;sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS at 0x25d368a8910&gt;</pre> In\u00a0[11]: Copied! <pre>y_hat = model.predict(X=x_val, y=y_val, steps_ahead=1)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_val, yhat=y_hat, n=1000)\n</pre> y_hat = model.predict(X=x_val, y=y_val, steps_ahead=1) rrse = root_relative_squared_error(y_val, y_hat) print(rrse) r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  plot_results(y=y_val, yhat=y_hat, n=1000) <pre>0.09610207940697202\n   Regressors   Parameters             ERR\n0      y(k-1)   1.8387E+00  9.43378253E-01\n1      y(k-2)  -1.8938E+00  1.95167599E-02\n2      y(k-3)   1.3337E+00  1.02432261E-02\n3      y(k-6)  -1.6038E+00  8.03485985E-03\n4      y(k-9)   2.6776E-01  9.27874557E-04\n5     x2(k-7)  -2.2385E+01  3.76837313E-04\n6     x1(k-1)   8.2709E+00  6.81508210E-04\n7     x2(k-3)   1.0587E+02  1.57459800E-03\n8     x1(k-8)  -3.7975E+00  7.35086279E-04\n9     x2(k-1)   8.5725E+01  4.85358786E-04\n10     y(k-7)   1.3955E+00  2.77245281E-04\n11     y(k-5)   1.3219E+00  8.64120037E-04\n12    y(k-10)  -2.9306E-01  8.51717688E-04\n13     y(k-4)  -9.5479E-01  7.23623116E-04\n14     y(k-8)  -7.1309E-01  4.44988077E-04\n15    y(k-12)  -3.0437E-01  1.49743148E-04\n16    y(k-11)   4.8602E-01  3.34613282E-04\n17    y(k-13)  -8.2442E-02  1.43738964E-04\n18    y(k-15)  -1.6762E-01  1.25546584E-04\n19    x1(k-2)  -8.9698E+00  9.76699739E-05\n20    y(k-17)   2.2036E-02  4.55983807E-05\n21    y(k-14)   2.4900E-01  1.10314107E-04\n22    y(k-19)  -6.8239E-03  1.99734771E-05\n23    x2(k-9)  -9.6265E+01  2.98523208E-05\n24    x2(k-8)   2.2620E+02  2.34402543E-04\n25    x2(k-2)  -2.3609E+02  1.04172323E-04\n26    y(k-20)  -5.4663E-02  5.37895336E-05\n27    x2(k-6)  -2.3651E+02  2.11392628E-05\n28    x2(k-4)   1.7378E+02  2.18396315E-05\n29    x1(k-7)   4.9862E+00  2.03811842E-05\n</pre> In\u00a0[12]: Copied! <pre>y_hat = model.predict(X=x_val, y=y_val, steps_ahead=5)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nplot_results(y=y_val, yhat=y_hat, n=1000)\n</pre> y_hat = model.predict(X=x_val, y=y_val, steps_ahead=5) rrse = root_relative_squared_error(y_val, y_hat) print(rrse) plot_results(y=y_val, yhat=y_hat, n=1000) <pre>0.2168472873799118\n</pre> In\u00a0[13]: Copied! <pre>y_hat = model.predict(X=x_val, y=y_val, steps_ahead=None)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nplot_results(y=y_val, yhat=y_hat, n=1000)\n</pre> y_hat = model.predict(X=x_val, y=y_val, steps_ahead=None) rrse = root_relative_squared_error(y_val, y_hat) print(rrse) plot_results(y=y_val, yhat=y_hat, n=1000) <pre>0.2910089654603829\n</pre>"},{"location":"examples/n_steps_ahead_prediction/#example-n-steps-ahead-prediction-f-16-ground-vibration-test-benchmark","title":"Example: N-steps-ahead prediction - F-16 Ground Vibration Test benchmark\u00b6","text":"<p>Note: The following examples do not try to replicate the results of the cited manuscripts. Even the model parameters such as ylag and xlag and size of identification and validation data are not the same of the cited papers. Moreover, sampling rate adjustment and other different data preparation are not handled here.</p>"},{"location":"examples/n_steps_ahead_prediction/#preparing-the-data","title":"Preparing the data\u00b6","text":""},{"location":"examples/n_steps_ahead_prediction/#building-the-model","title":"Building the model\u00b6","text":""},{"location":"examples/n_steps_ahead_prediction/#defining-the-forecasting-horizon","title":"Defining the forecasting horizon\u00b6","text":"<p>To perform a n-steps-ahead prediction you just need to set the \"steps_ahead\" argument.</p> <p>Note The default value for steps_ahead is None and it performs a infinity-steps-ahead prediction</p>"},{"location":"examples/narx_neural_network/","title":"Building NARX Neural Network using Sysidentpy","text":"In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>from torch import nn\nfrom sysidentpy.metrics import mean_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.neural_network import NARXNN\n\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\nfrom sysidentpy.utils.narmax_tools import regressor_code\nimport torch\n</pre> from torch import nn from sysidentpy.metrics import mean_squared_error from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.neural_network import NARXNN  from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) from sysidentpy.utils.narmax_tools import regressor_code import torch In\u00a0[2]: Copied! <pre>torch.cuda.is_available()\n</pre> torch.cuda.is_available() Out[2]: <pre>False</pre> In\u00a0[3]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" print(f\"Using {device} device\") <pre>Using cpu device\n</pre> In\u00a0[4]: Copied! <pre>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.01, train_percentage=80\n)\n</pre> x_train, x_valid, y_train, y_valid = get_siso_data(     n=1000, colored_noise=False, sigma=0.01, train_percentage=80 ) In\u00a0[5]: Copied! <pre>basis_function = Polynomial(degree=1)\n\nnarx_net = NARXNN(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    loss_func=\"mse_loss\",\n    optimizer=\"Adam\",\n    epochs=2000,\n    verbose=False,\n    device=\"cuda\",\n    optim_params={\n        \"betas\": (0.9, 0.999),\n        \"eps\": 1e-05,\n    },  # optional parameters of the optimizer\n)\n</pre> basis_function = Polynomial(degree=1)  narx_net = NARXNN(     ylag=2,     xlag=2,     basis_function=basis_function,     model_type=\"NARMAX\",     loss_func=\"mse_loss\",     optimizer=\"Adam\",     epochs=2000,     verbose=False,     device=\"cuda\",     optim_params={         \"betas\": (0.9, 0.999),         \"eps\": 1e-05,     },  # optional parameters of the optimizer ) <pre>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\neural_network\\narx_nn.py:156: UserWarning: No CUDA available. We set the device as CPU\n  self.device = self._check_cuda(device)\n</pre> <p>Since we have defined our NARXNN using $ylag=2$, $xlag=2$ and a polynomial basis function with $degree=1$, we have a regressor matrix with 4 features. We need the size of the regressor matrix to build the layers of our network. Our input data(x_train) have only one feature, but since we are creating an NARX network, a regressor matrix is built behind the scenes with new features based on the xlag and ylag.</p> <p>If you need help finding how many regressors are created behind the scenes you can use the narmax_tools function regressor_code and take the size of the regressor code generated:</p> In\u00a0[6]: Copied! <pre>basis_function = Polynomial(degree=1)\n\nregressors = regressor_code(\n    X=x_train,\n    xlag=2,\n    ylag=2,\n    model_type=\"NARMAX\",\n    model_representation=\"neural_network\",\n    basis_function=basis_function,\n)\n</pre> basis_function = Polynomial(degree=1)  regressors = regressor_code(     X=x_train,     xlag=2,     ylag=2,     model_type=\"NARMAX\",     model_representation=\"neural_network\",     basis_function=basis_function, ) In\u00a0[7]: Copied! <pre>n_features = regressors.shape[0]  # the number of features of the NARX net\nn_features\n</pre> n_features = regressors.shape[0]  # the number of features of the NARX net n_features Out[7]: <pre>4</pre> In\u00a0[8]: Copied! <pre>regressors\n</pre> regressors Out[8]: <pre>array([[1001],\n       [1002],\n       [2001],\n       [2002]])</pre> In\u00a0[9]: Copied! <pre>class NARX(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(n_features, 30)\n        self.lin2 = nn.Linear(30, 30)\n        self.lin3 = nn.Linear(30, 1)\n        self.tanh = nn.Tanh()\n\n    def forward(self, xb):\n        z = self.lin(xb)\n        z = self.tanh(z)\n        z = self.lin2(z)\n        z = self.tanh(z)\n        z = self.lin3(z)\n        return z\n</pre> class NARX(nn.Module):     def __init__(self):         super().__init__()         self.lin = nn.Linear(n_features, 30)         self.lin2 = nn.Linear(30, 30)         self.lin3 = nn.Linear(30, 1)         self.tanh = nn.Tanh()      def forward(self, xb):         z = self.lin(xb)         z = self.tanh(z)         z = self.lin2(z)         z = self.tanh(z)         z = self.lin3(z)         return z <p>We have to pass the defined network to our NARXNN estimator.</p> In\u00a0[10]: Copied! <pre>narx_net.net = NARX()\n</pre> narx_net.net = NARX() In\u00a0[11]: Copied! <pre>if device == \"cuda\":\n    narx_net.net.to(torch.device(\"cuda\"))\n</pre> if device == \"cuda\":     narx_net.net.to(torch.device(\"cuda\")) In\u00a0[12]: outputPrepend Copied! <pre>narx_net.fit(X=x_train, y=y_train, X_test=x_valid, y_test=y_valid)\n</pre> narx_net.fit(X=x_train, y=y_train, X_test=x_valid, y_test=y_valid) Out[12]: <pre>&lt;sysidentpy.neural_network.narx_nn.NARXNN at 0x19ddfff3890&gt;</pre> In\u00a0[13]: Copied! <pre>yhat = narx_net.predict(X=x_valid, y=y_valid)\n</pre> yhat = narx_net.predict(X=x_valid, y=y_valid) In\u00a0[14]: Copied! <pre>print(\"MSE: \", mean_squared_error(y_valid, yhat))\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> print(\"MSE: \", mean_squared_error(y_valid, yhat)) plot_results(y=y_valid, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>MSE:  0.00013103585914746256\n</pre> In\u00a0[15]: outputPrepend Copied! <pre>class NARX(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(n_features, 30)\n        self.lin2 = nn.Linear(30, 30)\n        self.lin3 = nn.Linear(30, 1)\n        self.tanh = nn.Tanh()\n\n    def forward(self, xb):\n        z = self.lin(xb)\n        z = self.tanh(z)\n        z = self.lin2(z)\n        z = self.tanh(z)\n        z = self.lin3(z)\n        return z\n\n\nnarx_net2 = NARXNN(\n    net=NARX(),\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    loss_func=\"mse_loss\",\n    optimizer=\"Adam\",\n    epochs=2000,\n    verbose=False,\n    optim_params={\n        \"betas\": (0.9, 0.999),\n        \"eps\": 1e-05,\n    },  # optional parameters of the optimizer\n)\n\nnarx_net2.fit(X=x_train, y=y_train)\nyhat = narx_net2.predict(X=x_valid, y=y_valid)\nprint(\"MSE: \", mean_squared_error(y_valid, yhat))\n\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> class NARX(nn.Module):     def __init__(self):         super().__init__()         self.lin = nn.Linear(n_features, 30)         self.lin2 = nn.Linear(30, 30)         self.lin3 = nn.Linear(30, 1)         self.tanh = nn.Tanh()      def forward(self, xb):         z = self.lin(xb)         z = self.tanh(z)         z = self.lin2(z)         z = self.tanh(z)         z = self.lin3(z)         return z   narx_net2 = NARXNN(     net=NARX(),     ylag=2,     xlag=2,     basis_function=basis_function,     model_type=\"NARMAX\",     loss_func=\"mse_loss\",     optimizer=\"Adam\",     epochs=2000,     verbose=False,     optim_params={         \"betas\": (0.9, 0.999),         \"eps\": 1e-05,     },  # optional parameters of the optimizer )  narx_net2.fit(X=x_train, y=y_train) yhat = narx_net2.predict(X=x_valid, y=y_valid) print(\"MSE: \", mean_squared_error(y_valid, yhat))  plot_results(y=y_valid, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>MSE:  0.00010086796658327408\n</pre>"},{"location":"examples/narx_neural_network/#building-narx-neural-network-using-sysidentpy","title":"Building NARX Neural Network using Sysidentpy\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"examples/narx_neural_network/#series-parallel-training-and-parallel-prediction","title":"Series-Parallel Training and Parallel prediction\u00b6","text":"<p>Currently SysIdentPy support a Series-Parallel (open-loop) Feedforward Network training process, which make the training process easier. We convert the NARX network from Series-Parallel to the Parallel (closed-loop) configuration for prediction.</p> <p>Series-Parallel allows us to use Pytorch directly for training, so we can use all the power of the Pytorch library to build our NARX Neural Network model!</p> <p></p> <p>The reader is referred to the following paper for a more in depth discussion about Series-Parallel and Parallel configurations regarding NARX neural network:</p> <p>Parallel Training Considered Harmful?: Comparing series-parallel and parallel feedforward network training</p>"},{"location":"examples/narx_neural_network/#building-a-narx-neural-network","title":"Building a NARX Neural Network\u00b6","text":"<p>First, just import the necessary packages</p>"},{"location":"examples/narx_neural_network/#getting-the-data","title":"Getting the data\u00b6","text":"<p>The data is generated by simulating the following model:</p> <p>$y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_{k}$.</p> <p>If colored_noise is set to True:</p> <p>$e_{k} = 0.8\\nu_{k-1} + \\nu_{k}$,</p> <p>where $x$ is a uniformly distributed random variable and $\\nu$ is a gaussian distributed variable with $\\mu=0$ and $\\sigma=0.1$</p>"},{"location":"examples/narx_neural_network/#choosing-the-narx-parameters-loss-function-and-optimizer","title":"Choosing the NARX parameters, loss function and optimizer\u00b6","text":"<p>One can create a NARXNN object and choose the maximum lag of both input and output for building the regressor matrix to serve as input of the network.</p> <p>In addition, you can choose the loss function, the optimizer, the optional parameters of the optimizer, the number of epochs.</p> <p>Because we built this feature on top of Pytorch, you can choose any of the loss function of the torch.nn.functional. Click here for a list of the loss functions you can use. You just need to pass the name of the loss function you want.</p> <p>Similarly, you can choose any of the optimizers of the torch.optim. Click here for a list of optimizers available.</p>"},{"location":"examples/narx_neural_network/#building-the-narx-neural-network","title":"Building the NARX Neural Network\u00b6","text":"<p>The configuration of your network follows exactly the same pattern of a network defined in Pytorch. The following representing our NARX neural network.</p>"},{"location":"examples/narx_neural_network/#fit-and-predict","title":"Fit and Predict\u00b6","text":"<p>Because we have a fit (for training) and predict function for Polynomial NARMAX, we create the same pattern for the NARX net. So, you only have to fit and predict using the following:</p>"},{"location":"examples/narx_neural_network/#results","title":"Results\u00b6","text":"<p>Now we show the results</p>"},{"location":"examples/narx_neural_network/#note","title":"Note\u00b6","text":"<p>If you built the net configuration before calling the NARXNN, you can just pass the model to the NARXNN as follows:</p>"},{"location":"examples/narx_neural_network/#note","title":"Note\u00b6","text":"<p>Remember you can use n-steps-ahead prediction and NAR and NFIR models. Check how to use it in their respective examples.</p>"},{"location":"examples/parameter_estimation/","title":"Parameter Estimation","text":"<p>Here we import the NARMAX model, the metric for model evaluation and the methods to generate sample data for tests. Also, we import pandas for specific usage.</p> In\u00a0[1]: Copied! <pre>import pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import (\n    TotalLeastSquares,\n    RecursiveLeastSquares,\n    NonNegativeLeastSquares,\n    LeastMeanSquares,\n    AffineLeastMeanSquares,\n)\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\n</pre> import pandas as pd from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.parameter_estimation import (     TotalLeastSquares,     RecursiveLeastSquares,     NonNegativeLeastSquares,     LeastMeanSquares,     AffineLeastMeanSquares, ) from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.utils.display_results import results In\u00a0[2]: Copied! <pre>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n</pre> x_train, x_valid, y_train, y_valid = get_siso_data(     n=1000, colored_noise=False, sigma=0.001, train_percentage=90 ) In\u00a0[3]: Copied! <pre>basis_function = Polynomial(degree=2)\nestimator = TotalLeastSquares()\n\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</pre> basis_function = Polynomial(degree=2) estimator = TotalLeastSquares()  model = FROLS(     order_selection=False,     n_terms=3,     ylag=2,     xlag=2,     estimator=estimator,     basis_function=basis_function, ) model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) <pre>0.0021167167052431584\n      Regressors  Parameters             ERR\n0        x1(k-2)  9.0000E-01  9.56200123E-01\n1         y(k-1)  1.9995E-01  4.05078042E-02\n2  x1(k-1)y(k-1)  1.0004E-01  3.28866604E-03\n</pre> In\u00a0[4]: Copied! <pre># recursive least squares\nbasis_function = Polynomial(degree=2)\nestimator = RecursiveLeastSquares()\n\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</pre> # recursive least squares basis_function = Polynomial(degree=2) estimator = RecursiveLeastSquares()  model = FROLS(     order_selection=False,     n_terms=3,     ylag=2,     xlag=2,     estimator=estimator,     basis_function=basis_function, ) model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) <pre>0.0020703083403116164\n      Regressors  Parameters             ERR\n0        x1(k-2)  9.0012E-01  9.56200123E-01\n1         y(k-1)  2.0021E-01  4.05078042E-02\n2  x1(k-1)y(k-1)  9.9550E-02  3.28866604E-03\n</pre> In\u00a0[5]: Copied! <pre>basis_function = Polynomial(degree=2)\nestimator = LeastMeanSquares()\n\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</pre> basis_function = Polynomial(degree=2) estimator = LeastMeanSquares()  model = FROLS(     order_selection=False,     n_terms=3,     ylag=2,     xlag=2,     estimator=estimator,     basis_function=basis_function, ) model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) <pre>0.015488793944313425\n      Regressors  Parameters             ERR\n0        x1(k-2)  8.9775E-01  9.56200123E-01\n1         y(k-1)  2.0085E-01  4.05078042E-02\n2  x1(k-1)y(k-1)  7.5708E-02  3.28866604E-03\n</pre> In\u00a0[6]: Copied! <pre>basis_function = Polynomial(degree=2)\nestimator = AffineLeastMeanSquares()\n\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</pre> basis_function = Polynomial(degree=2) estimator = AffineLeastMeanSquares()  model = FROLS(     order_selection=False,     n_terms=3,     ylag=2,     xlag=2,     estimator=estimator,     basis_function=basis_function, ) model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) <pre>0.0021441596280611167\n      Regressors  Parameters             ERR\n0        x1(k-2)  8.9989E-01  9.56200123E-01\n1         y(k-1)  1.9992E-01  4.05078042E-02\n2  x1(k-1)y(k-1)  1.0003E-01  3.28866604E-03\n</pre> In\u00a0[7]: Copied! <pre>basis_function = Polynomial(degree=2)\nestimator = NonNegativeLeastSquares()\n\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</pre> basis_function = Polynomial(degree=2) estimator = NonNegativeLeastSquares()  model = FROLS(     order_selection=False,     n_terms=3,     ylag=2,     xlag=2,     estimator=estimator,     basis_function=basis_function, ) model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) <pre>0.0021170157359329173\n      Regressors  Parameters             ERR\n0        x1(k-2)  9.0000E-01  9.56200123E-01\n1         y(k-1)  1.9995E-01  4.05078042E-02\n2  x1(k-1)y(k-1)  1.0004E-01  3.28866604E-03\n</pre>"},{"location":"examples/parameter_estimation/#parameter-estimation","title":"Parameter Estimation\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"examples/parameter_estimation/#generating-1-input-1-output-sample-data","title":"Generating 1 input 1 output sample data\u00b6","text":"<p>The data is generated by simulating the following model:</p> <p>$y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_{k}$</p> <p>If colored_noise is set to True:</p> <p>$e_{k} = 0.8\\nu_{k-1} + \\nu_{k}$</p> <p>where $x$ is a uniformly distributed random variable and $\\nu$ is a gaussian distributed variable with $\\mu=0$ and $\\sigma=0.1$</p> <p>In the next example we will generate a data with 1000 samples with white noise and selecting 90% of the data to train the model.</p>"},{"location":"examples/parameter_estimation/#there-are-several-method-to-be-used-for-parameter-estimation","title":"There are several method to be used for parameter estimation.\u00b6","text":"<ul> <li>Least Squares;</li> <li>Total Least Squares;</li> <li>Recursive Least Squares</li> <li>Ridge Regression</li> <li>NonNegative Least Squares</li> <li>Least Squares Minimal Residues</li> <li>Bounded Variable Least Squares</li> <li>Least Mean Squares</li> <li>Affine Least Mean Squares</li> <li>Least Mean Squares Sign Error</li> <li>Normalized Least Mean Squares</li> <li>Least Mean Squares Normalized Sign Error</li> <li>Least Mean Squares Sign Regressor</li> <li>Least Mean Squares Normalized Sign Regressor</li> <li>Least Mean Squares Sign Sign</li> <li>Least Mean Squares Normalized Sign Sign</li> <li>Least Mean Squares Normalized Leaky</li> <li>Least Mean Squares Leaky</li> <li>Least Mean Squares Fourth</li> <li>Least Mean Squares Mixed Norm</li> </ul> <p>Polynomial NARMAX models are linear-in-the-parameter, so Least Squares based methods works well for most cases (using with extended least squares algorithm when dealing with colered noise).</p> <p>However, the user can choose some recursive and stochastic gradient descent methods (in this case, the least mean squares algorithm and its variants) to that task too.</p> <p>Choosing the method is straightforward: pass any of the methods mentioned above on estimator parameters.</p> <ul> <li>Note: Each algorithm have specifc parameter that need to be tunned. In the following examples we will use the default ones. More examples regarding tunned parameter will be available soon. For now, the user can read the method documentation for more information.</li> </ul>"},{"location":"examples/parameter_estimation/#total-least-squares","title":"Total Least Squares\u00b6","text":""},{"location":"examples/parameter_estimation/#recursive-least-squares","title":"Recursive Least Squares\u00b6","text":""},{"location":"examples/parameter_estimation/#least-mean-squares","title":"Least Mean Squares\u00b6","text":""},{"location":"examples/parameter_estimation/#affine-least-mean-squares","title":"Affine Least Mean Squares\u00b6","text":""},{"location":"examples/parameter_estimation/#nonnegative-least-squares","title":"NonNegative Least Squares\u00b6","text":""},{"location":"examples/save_and_load_models/","title":"Saving and Loading models using .syspy extension","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.utils.save_load import save_model, load_model\n\n\n# Generating 1 input 1 output sample data from a benchmark system\nx_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.0001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=3,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\n\nyhat = model.predict(X=x_valid, y=y_valid)\n\n# Gathering results\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n</pre> import pandas as pd from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.parameter_estimation import LeastSquares from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_results from sysidentpy.utils.save_load import save_model, load_model   # Generating 1 input 1 output sample data from a benchmark system x_train, x_valid, y_train, y_valid = get_siso_data(     n=1000, colored_noise=False, sigma=0.0001, train_percentage=90 )  basis_function = Polynomial(degree=2) estimator = LeastSquares()  model = FROLS(     order_selection=True,     n_info_values=3,     ylag=2,     xlag=2,     info_criteria=\"aic\",     estimator=estimator,     basis_function=basis_function, )  model.fit(X=x_train, y=y_train)  yhat = model.predict(X=x_valid, y=y_valid)  # Gathering results r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) In\u00a0[2]: Copied! <pre># save_model(model_variable, file_name.syspy, path (optional))\nsave_model(model=model, file_name=\"model_name.syspy\")\n</pre> # save_model(model_variable, file_name.syspy, path (optional)) save_model(model=model, file_name=\"model_name.syspy\") In\u00a0[3]: Copied! <pre># load_model(file_name.syspy, path (optional))\nloaded_model = load_model(file_name=\"model_name.syspy\")\n\n# Predicting output with loaded_model\nyhat_loaded = loaded_model.predict(X=x_valid, y=y_valid)\n\nr_loaded = pd.DataFrame(\n    results(\n        loaded_model.final_model,\n        loaded_model.theta,\n        loaded_model.err,\n        loaded_model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\n# Printing both: original model and model loaded from file\nprint(\"\\n Original model \\n\", r)\nprint(\"\\n Model Loaded from file \\n\", r_loaded)\n\n# Checking predictions from both: original model and model loaded from file\nif (yhat == yhat_loaded).all():\n    print(\"\\n Predictions are the same!\")\n\n# Ploting results\nplot_results(y=y_valid, yhat=yhat_loaded, n=1000)\n</pre> # load_model(file_name.syspy, path (optional)) loaded_model = load_model(file_name=\"model_name.syspy\")  # Predicting output with loaded_model yhat_loaded = loaded_model.predict(X=x_valid, y=y_valid)  r_loaded = pd.DataFrame(     results(         loaded_model.final_model,         loaded_model.theta,         loaded_model.err,         loaded_model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], )  # Printing both: original model and model loaded from file print(\"\\n Original model \\n\", r) print(\"\\n Model Loaded from file \\n\", r_loaded)  # Checking predictions from both: original model and model loaded from file if (yhat == yhat_loaded).all():     print(\"\\n Predictions are the same!\")  # Ploting results plot_results(y=y_valid, yhat=yhat_loaded, n=1000) <pre>\n Original model \n       Regressors  Parameters             ERR\n0        x1(k-2)  9.0000E-01  9.56631676E-01\n1         y(k-1)  1.9999E-01  3.99688899E-02\n2  x1(k-1)y(k-1)  1.0000E-01  3.39940092E-03\n\n Model Loaded from file \n       Regressors  Parameters             ERR\n0        x1(k-2)  9.0000E-01  9.56631676E-01\n1         y(k-1)  1.9999E-01  3.99688899E-02\n2  x1(k-1)y(k-1)  1.0000E-01  3.39940092E-03\n\n Predictions are the same!\n</pre>"},{"location":"examples/save_and_load_models/#saving-and-loading-models-using-syspy-extension","title":"Saving and Loading models using .syspy extension\u00b6","text":"<p>Example created by Samir Angelo Milani Martins</p>"},{"location":"examples/save_and_load_models/#obtaining-the-model-using-frols","title":"Obtaining the model using FROLS.\u00b6","text":""},{"location":"examples/save_and_load_models/#saving-obtained-model-in-file-model_namesyspy","title":"Saving obtained model in file \"model_name.syspy\"\u00b6","text":""},{"location":"examples/save_and_load_models/#loading-model-and-checking-if-everything-went-smoothly","title":"Loading model and checking if everything went smoothly\u00b6","text":""},{"location":"examples/simulating_a_predefined_model/","title":"Simulate a Predefined Model","text":"In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom sysidentpy.simulation import SimulateNARMAX\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</pre> import numpy as np import pandas as pd from sysidentpy.simulation import SimulateNARMAX from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.basis_function import Polynomial from sysidentpy.parameter_estimation import LeastSquares from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) In\u00a0[2]: Copied! <pre>x_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n</pre> x_train, x_test, y_train, y_test = get_siso_data(     n=1000, colored_noise=False, sigma=0.001, train_percentage=90 ) In\u00a0[3]: Copied! <pre>s = SimulateNARMAX(\n    basis_function=Polynomial(), calculate_err=True, estimate_parameter=False\n)\n\n# the model must be a numpy array\nmodel = np.array(\n    [\n        [1001, 0],  # y(k-1)\n        [2001, 1001],  # x1(k-1)y(k-1)\n        [2002, 0],  # x1(k-2)\n    ]\n)\n# theta must be a numpy array of shape (n, 1) where n is the number of regressors\ntheta = np.array([[0.2, 0.9, 0.1]]).T\n</pre> s = SimulateNARMAX(     basis_function=Polynomial(), calculate_err=True, estimate_parameter=False )  # the model must be a numpy array model = np.array(     [         [1001, 0],  # y(k-1)         [2001, 1001],  # x1(k-1)y(k-1)         [2002, 0],  # x1(k-2)     ] ) # theta must be a numpy array of shape (n, 1) where n is the number of regressors theta = np.array([[0.2, 0.9, 0.1]]).T In\u00a0[4]: Copied! <pre>yhat = s.simulate(\n    X_test=x_test,\n    y_test=y_test,\n    model_code=model,\n    theta=theta,\n)\n\nr = pd.DataFrame(\n    results(s.final_model, s.theta, s.err, s.n_terms, err_precision=8, dtype=\"sci\"),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_test, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_test, yhat, x_test)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> yhat = s.simulate(     X_test=x_test,     y_test=y_test,     model_code=model,     theta=theta, )  r = pd.DataFrame(     results(s.final_model, s.theta, s.err, s.n_terms, err_precision=8, dtype=\"sci\"),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  plot_results(y=y_test, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_test, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_test, yhat, x_test) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>      Regressors  Parameters             ERR\n0         y(k-1)  2.0000E-01  0.00000000E+00\n1        x1(k-2)  9.0000E-01  0.00000000E+00\n2  x1(k-1)y(k-1)  1.0000E-01  0.00000000E+00\n</pre> In\u00a0[5]: Copied! <pre>yhat = s.simulate(\n    X_test=x_test,\n    y_test=y_test,\n    model_code=model,\n    theta=theta,\n    steps_ahead=1,\n)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n</pre> yhat = s.simulate(     X_test=x_test,     y_test=y_test,     model_code=model,     theta=theta,     steps_ahead=1, ) rrse = root_relative_squared_error(y_test, yhat) print(rrse) <pre>0.001980394341423956\n</pre> In\u00a0[6]: Copied! <pre>yhat = s.simulate(\n    X_test=x_test,\n    y_test=y_test,\n    model_code=model,\n    theta=theta,\n    steps_ahead=21,\n)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n</pre> yhat = s.simulate(     X_test=x_test,     y_test=y_test,     model_code=model,     theta=theta,     steps_ahead=21, ) rrse = root_relative_squared_error(y_test, yhat) print(rrse) <pre>0.0019394741034286557\n</pre> In\u00a0[7]: Copied! <pre>s = SimulateNARMAX(\n    basis_function=Polynomial(),\n    estimate_parameter=True,\n    estimator=LeastSquares(),\n    calculate_err=True,\n)\n\nyhat = s.simulate(\n    X_train=x_train,\n    y_train=y_train,\n    X_test=x_test,\n    y_test=y_test,\n    model_code=model,\n    # theta will be estimated using the defined estimator\n)\n\nr = pd.DataFrame(\n    results(s.final_model, s.theta, s.err, s.n_terms, err_precision=8, dtype=\"sci\"),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_test, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_test, yhat, x_test)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> s = SimulateNARMAX(     basis_function=Polynomial(),     estimate_parameter=True,     estimator=LeastSquares(),     calculate_err=True, )  yhat = s.simulate(     X_train=x_train,     y_train=y_train,     X_test=x_test,     y_test=y_test,     model_code=model,     # theta will be estimated using the defined estimator )  r = pd.DataFrame(     results(s.final_model, s.theta, s.err, s.n_terms, err_precision=8, dtype=\"sci\"),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  plot_results(y=y_test, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_test, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_test, yhat, x_test) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>      Regressors  Parameters             ERR\n0         y(k-1)  1.9999E-01  9.57682046E-01\n1        x1(k-2)  9.0003E-01  3.87716434E-02\n2  x1(k-1)y(k-1)  1.0009E-01  3.54306118E-03\n</pre>"},{"location":"examples/simulating_a_predefined_model/#simulate-a-predefined-model","title":"Simulate a Predefined Model\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"examples/simulating_a_predefined_model/#generating-1-input-1-output-sample-data","title":"Generating 1 input 1 output sample data\u00b6","text":""},{"location":"examples/simulating_a_predefined_model/#the-data-is-generated-by-simulating-the-following-model","title":"The data is generated by simulating the following model:\u00b6","text":"<p>$y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-2} + e_{k}$</p> <p>If colored_noise is set to True:</p> <p>$e_{k} = 0.8\\nu_{k-1} + \\nu_{k}$</p> <p>where $x$ is a uniformly distributed random variable and $\\nu$ is a gaussian distributed variable with $\\mu=0$ and $\\sigma=0.1$</p> <p>In the next example we will generate a data with 1000 samples with white noise and selecting 90% of the data to train the model.</p>"},{"location":"examples/simulating_a_predefined_model/#defining-the-model","title":"Defining the model\u00b6","text":"<p>We already know that the generated data is a result of the model  $\ud835\udc66_\ud835\udc58=0.2\ud835\udc66_{\ud835\udc58\u22121}+0.1\ud835\udc66_{\ud835\udc58\u22121}\ud835\udc65_{\ud835\udc58\u22121}+0.9\ud835\udc65_{\ud835\udc58\u22122}+\ud835\udc52_\ud835\udc58$ . Thus, we can create a model with those regressors follwing a codification pattern:</p> <ul> <li>$0$ is the constant term,</li> <li>$[1001] = y_{k-1}$</li> <li>$[100n] = y_{k-n}$</li> <li>$[200n] = x1_{k-n}$</li> <li>$[300n] = x2_{k-n}$</li> <li>$[1011, 1001] = y_{k-11} \\times y_{k-1}$</li> <li>$[100n, 100m] = y_{k-n} \\times y_{k-m}$</li> <li>$[12001, 1003, 1001] = x11_{k-1} \\times y_{k-3} \\times y_{k-1}$</li> <li>and so on</li> </ul>"},{"location":"examples/simulating_a_predefined_model/#important-note","title":"Important Note\u00b6","text":"<p>The order of the arrays matter.</p> <p>If you use [2001, 1001], it will work, but [1001, 2001] will not (the regressor will be ignored). Always put the highest value first:</p> <ul> <li>$[2003, 2001]$ works</li> <li>$[2001, 2003]$ do not work</li> </ul> <p>We will handle this limitation in upcoming update.</p>"},{"location":"examples/simulating_a_predefined_model/#simulating-the-model","title":"Simulating the model\u00b6","text":"<p>After defining the model and theta we just need to use the simulate method.</p> <p>The simulate method returns the predicted values and the results where we can look at regressors, parameters and ERR values.</p>"},{"location":"examples/simulating_a_predefined_model/#options","title":"Options\u00b6","text":"<p>You can set the <code>steps_ahead</code> to run the prediction/simulation:</p>"},{"location":"examples/simulating_a_predefined_model/#estimating-the-parameters","title":"Estimating the parameters\u00b6","text":"<p>If you have only the model strucuture, you can create an object with <code>estimate_parameter=True</code> and choose the methed for estimation using <code>estimator</code>. In this case, you have to pass the training data for parameters estimation.</p> <p>When <code>estimate_parameter=True</code>, we also computate the ERR considering only the regressors defined by the user.</p>"},{"location":"landing-page/about-us/","title":"Project History","text":"<p>The project was started by Wilson R. L. Junior, Luan Pascoal and Samir A. M. Martins as a project for System Identification discipline. We have been working with System Identification for several years (Nonlinear Systems, Machine Learning, Chaotic Systems, Hysteretic models, etc) for several years.</p> <p> Every work we did was using a great tool, but a paid one: Matlab. We started looking for some free alternatives to build NARMAX and its variants (AR, ARX, ARMAX, NAR, NARX, NFIR, Neural NARX, etc.) models using the methods known in System Identification community, but we didn't find any package written in Python with the features we needed to keep doing our research.  Besides, it was always too difficult to find source code of the papers working with NARMAX models and reproduce results was a really hard thing to do.</p> <p> In that context, SysIdentPy was idealized with the following goal: be a free and open source package to help the community design NARMAX models. More than that, be a free and robust alternative to one of the most used tools to build NARMAX models, which is the Matlab's System Identification Toolbox.</p> <p> Samuel joined early in 2019 to help us achieve our goal.  </p> <p></p>"},{"location":"landing-page/about-us/#active-maintainers","title":"Active Maintainers","text":"<p>The project is actively maintained by Wilson Rocha Lacerda Junior and looking for contributors.</p>"},{"location":"landing-page/about-us/#citation","title":"Citation","text":"<p>If you use SysIdentPy on your project, please drop me a line.</p> <p>Send email </p> <p>If you use SysIdentPy on your scientific publication, we would appreciate citations to the following paper:</p> <p>Lacerda et al., (2020). SysIdentPy: A Python package for System Identification using NARMAX models. Journal of Open Source Software, 5(54), 2384, https://doi.org/10.21105/joss.02384 <pre><code>    @article{Lacerda2020,\n      doi = {10.21105/joss.02384},\n      url = {https://doi.org/10.21105/joss.02384},\n      year = {2020},\n      publisher = {The Open Journal},\n      volume = {5},\n      number = {54},\n      pages = {2384},\n      author = {Wilson Rocha Lacerda Junior and Luan Pascoal Costa da Andrade and Samuel Carlos Pessoa Oliveira and Samir Angelo Milani Martins},\n      title = {SysIdentPy: A Python package for System Identification using NARMAX models},\n      journal = {Journal of Open Source Software}\n    }\n</code></pre></p>"},{"location":"landing-page/about-us/#inspiration","title":"Inspiration","text":"<p>The documentation and structure (even this section) is openly inspired by sklearn, einsteinpy, and many others as we used (and keep using) them to learn.</p>"},{"location":"landing-page/about-us/#future","title":"Future","text":"<p>SysIdentPy is already useful for many researchers and companies to build NARX models for dynamical systems. But still, there are many improvements and features to come. SysIdentPy has a great future ahead, and your help is greatly appreciated.</p>"},{"location":"landing-page/about-us/#contributors","title":"Contributors","text":""},{"location":"landing-page/attribute/","title":"Attribute","text":"<p>The image in landing page is adapted from:</p> <p>Image by storyset on Freepik</p>"},{"location":"landing-page/basic-usage/","title":"Basic Usage","text":"<p>The NARMAX model is described as:</p> \\[     y_k= F[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>where \\(n_y\\in \\mathbb{N}\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\), are the maximum lags for the system output and input respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}\\) is some nonlinear function of the input and output regressors and \\(d\\) is a time delay typically set to \\(d=1\\).</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.metrics import mean_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\n\n\n# Generate a dataset of a simulated dynamical system\nx_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000,\n    colored_noise=False,\n    sigma=0.001,\n    train_percentage=80\n)\n</code></pre>"},{"location":"landing-page/basic-usage/#build-a-polynomial-narx-model","title":"Build a Polynomial NARX model","text":"<pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import compute_residues_autocorrelation, compute_cross_correlation\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=10,\n    extended_least_squares=False,\n    ylag=2,\n    xlag=2,\n    info_criteria='aic',\n    estimator='least_squares',\n    basis_function=basis_function\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nmse = mean_squared_error(y_valid, yhat)\nprint(mse)\nr = pd.DataFrame(\n    results(\n        model.final_model, model.theta, model.err,\n        model.n_terms, err_precision=8, dtype='sci'\n        ),\n    columns=['Regressors', 'Parameters', 'ERR'])\nprint(r)\n\nRegressors     Parameters        ERR\n0        x1(k-2)     0.9000  0.95556574\n1         y(k-1)     0.1999  0.04107943\n2  x1(k-1)y(k-1)     0.1000  0.00335113\n\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x2_val)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> Validation of the Polynomial NARMAX model"},{"location":"landing-page/basic-usage/#narx-neural-network","title":"NARX Neural Network","text":"<pre><code>from torch import nn\nfrom sysidentpy.neural_network import NARXNN\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import compute_residues_autocorrelation, compute_cross_correlation\n\nclass NARX(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(4, 10)\n        self.lin2 = nn.Linear(10, 10)\n        self.lin3 = nn.Linear(10, 1)\n        self.tanh = nn.Tanh()\n\n    def forward(self, xb):\n        z = self.lin(xb)\n        z = self.tanh(z)\n        z = self.lin2(z)\n        z = self.tanh(z)\n        z = self.lin3(z)\n        return z\n\nbasis_function=Polynomial(degree=1)\n\nnarx_net = NARXNN(\n    net=NARX(),\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    loss_func='mse_loss',\n    optimizer='Adam',\n    epochs=200,\n    verbose=False,\n    optim_params={'betas': (0.9, 0.999), 'eps': 1e-05} # optional parameters of the optimizer\n)\n\nnarx_net.fit(X=x_train, y=y_train)\nyhat = narx_net.predict(X=x_valid, y=y_valid)\nplot_results(y=y_valid, yhat=yhat, n=200)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> Validation of the Neural NARX model"},{"location":"landing-page/basic-usage/#catboost-narx","title":"Catboost-narx","text":"<pre><code>from sysidentpy.general_estimators import NARX\nfrom catboost import CatBoostRegressor\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import compute_residues_autocorrelation, compute_cross_correlation\n\ncatboost_narx = NARX(\n    base_estimator=CatBoostRegressor(\n        iterations=300,\n        learning_rate=0.1,\n        depth=6),\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    fit_params={'verbose': False}\n)\n\ncatboost_narx.fit(X=x_train, y=y_train)\nyhat = catboost_narx.predict(X=x_valid, y=y_valid)\nplot_results(y=y_valid, yhat=yhat, n=200)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> Validation of the Catboost NARX model"},{"location":"landing-page/basic-usage/#catboost-without-narx-configuration","title":"Catboost without NARX configuration","text":"<p>The following is the Catboost performance without the NARX configuration.</p> <pre><code>def plot_results_tmp(y_valid, yhat):\n    _, ax = plt.subplots(figsize=(14, 8))\n    ax.plot(y_valid[:200], label='Data', marker='o')\n    ax.plot(yhat[:200], label='Prediction', marker='*')\n    ax.set_xlabel(\"$n$\", fontsize=18)\n    ax.set_ylabel(\"$y[n]$\", fontsize=18)\n    ax.grid()\n    ax.legend(fontsize=18)\n    plt.show()\n\ncatboost = CatBoostRegressor(\n    iterations=300,\n    learning_rate=0.1,\n    depth=6\n)\n\ncatboost.fit(x_train, y_train, verbose=False)\nplot_results(y_valid, catboost.predict(x_valid))\n</code></pre> <p> </p> Validation of the Catboost model without NARX configuration"},{"location":"landing-page/ch0-narmax-intro/","title":"Introduction","text":"<p>Author: Wilson Rocha Lacerda Junior</p> <p>This is the first in a series of publications explaining a little bit about NARMAX<sup>1</sup> models. I hope the content of these publications will help those who use or would like to use the SysIdentPy library.</p>"},{"location":"landing-page/ch0-narmax-intro/#system-identification","title":"System Identification","text":"<p>As I will use the term Systems Identification here and there, let me make a brief definition regarding these terms.</p> <p> Systems identification is one of the major areas that deals with the modeling of data-based processes. In this context, the term \"system\" can be interpreted as any set of operations that process one or more inputs and return one or more outputs. Examples include electrical systems, mechanical systems, biological systems, financial systems, chemical systems \u2026 literally anything you can relate to input and output data. The electricity demand is part of a system whose inputs can be, for example, quantity of the population, quantity of water in the reservoirs, season, events. The price of a property is the output of a system whose entries can be the city, per capita income, neighborhood, number of rooms, how old the house is, and many others. You got the idea.  </p> <p> Although there are many things related with Machine Learning, Statistical Learning and other fields,  each field has its particularities.  </p>"},{"location":"landing-page/ch0-narmax-intro/#so-what-is-a-narmax-model","title":"So, what is a NARMAX model?","text":"<p>You may have noticed the similarity between the acronym NARMAX with the well-known models ARX, ARMAX, etc., which are widely used for forecasting time series. And this resemblance is not by chance. The Autoregressive models with Moving Average and Exogenous Input (ARMAX) and their variations AR, ARX, ARMA (to name just a few) are one of the most used mathematical representations for identifying linear systems.</p> <p> Let's go back to the model. I said that the ARX family of models is commonly used to model linear systems. Linear is the key word here. For nonlinear scenarios we have the NARMAX class. As reported by Billings (one of the creators of NARMAX model) in the book Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains,  NARMAX started out as a model name, but soon became a philosophy when it comes to identifying nonlinear systems. Obtaining NARMAX models consists of performing the following steps:</p> <ul> <li>Dynamical tests and collecting data;</li> <li>Choice of mathematical representation;</li> <li>Detection of the model structure;</li> <li>Estimation of parameters;</li> <li>Validation;</li> <li>Analysis of the model.</li> </ul> <p>We will cover each of these steps in further publications. The idea of this text is to present an overview of NARMAX models.</p> <p> NARMAX models are not, however, a simple extension of ARMAX models. NARMAX models are able to represent the most different and complex nonlinear systems. Introduced in 1981 by the Electrical Engineer Stephen A. Billings, NARMAX models can be described as:</p> \\[     y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>where \\(n_y\\in \\mathbb{N}\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\) , are the maximum lags for the system output and input respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}^\\ell\\) is some nonlinear function of the input and output regressors with nonlinearity degree \\(\\ell \\in \\mathbb{N}\\) and \\(d\\) is a time delay typically set to \\(d=1\\).</p> <p>If we do not include noise terms, \\(e_{k-n_e}\\), we have NARX models. If we set \\(\\ell = 1\\) then we deal with ARMAX models; if \\(\\ell = 1\\) and we do not include input and noise terms, it turns to AR model (ARX if we include inputs, ARMA if we include noise terms instead); if \\(\\ell&gt;1\\) and there is no input terms, we have the NARMA. If there is no input or noise terms, we have NAR. There are several variants, but that is sufficient for now.</p>"},{"location":"landing-page/ch0-narmax-intro/#narmax-representation","title":"NARMAX Representation","text":"<p>There are several nonlinear functions representations to approximate the unknown mapping \\(\\mathrm{f}[\\cdot]\\) in the NARMAX methods, e.g.,</p> <ul> <li>neural networks;</li> <li>fuzzy logic-based models;</li> <li>radial basis functions;</li> <li>wavelet basis;</li> <li>polynomial basis;</li> <li>generalized additive models;</li> </ul> <p>The remainder of this post contemplates methods related to the power-form polynomial models, which is the most common used representation. Polynomial NARMAX is a mathematical model based on difference equations and relates the current output as a function of past inputs and outputs.</p>"},{"location":"landing-page/ch0-narmax-intro/#polynomial-narmax","title":"Polynomial NARMAX","text":"<p>The polynomial NARMAX model with asymptotically stable equilibrium points can be described as:</p> \\[\\begin{align}     y_k =&amp; \\sum_{0} + \\sum_{i=1}^{p}\\Theta_{y}^{i}y_{k-i} + \\sum_{j=1}^{q}\\Theta_{e}^{j}e_{k-j} + \\sum_{m=1}^{r}\\Theta_{x}^{m}x_{k-m}\\\\     &amp;+ \\sum_{i=1}^{p}\\sum_{j=1}^{q}\\Theta_{ye}^{ij}y_{k-i} e_{k-j} + \\sum_{i=1}^{p}\\sum_{m=1}^{r}\\Theta_{yx}^{im}y_{k-i} x_{k-m} \\\\     &amp;+ \\sum_{j=1}^{q}\\sum_{m=1}^{r}\\Theta_{e x}^{jm}e_{k-j} x_{k-m} \\\\     &amp;+ \\sum_{i=1}^{p}\\sum_{j=1}^{q}\\sum_{m=1}^{r}\\Theta_{y e x}^{ijm}y_{k-i} e_{k-j} x_{k-m} \\\\     &amp;+ \\sum_{m_1=1}^{r} \\sum_{m_2=m_1}^{r}\\Theta_{x^2}^{m_1 m_2} x_{k-m_1} x_{k-m_2} \\dotsc \\\\     &amp;+ \\sum_{m_1=1}^{r} \\dotsc \\sum_{m_l=m_{l-1}}^{r} \\Theta_{x^l}^{m_1, \\dotsc, m_2} x_{k-m_1} x_{k-m_l} \\end{align}\\] <p>where \\(\\sum\\nolimits_{0}\\), \\(c_{y}^{i}\\), \\(c_{e}^{j}\\), \\(c_{x}^{m}\\), \\(c_{y\\e}^{ij}\\), \\(c_{yx}^{im}\\), \\(c_{e x}^{jm}\\), \\(c_{y e x}^{ijm}\\), \\(c_{x^2}^{m_1 m_2} \\dotsc c_{x^l}^{m_1, \\dotsc, ml}\\) are constant parameters.</p> <p> Let's take a look at an example of a NARMAX model for an easy understanding. The following is a NARMAX model of degree~\\(2\\), identified from experimental data of a DC motor/generator with no prior knowledge of the model form. If you want more information about the identification process, I wrote a paper comparing a polynomial NARMAX with a neural NARX model using that data (IN PORTUGUESE: Identifica\u00e7\u00e3o de um motor/gerador CC por meio de modelos polinomiais autorregressivos e redes neurais artificiais)</p> \\[\\begin{align}     y_k =&amp; 1.7813y_{k-1}-0,7962y_{k-2}+0,0339x_{k-1} -0,1597x_{k-1} y_{k-1} +0,0338x_{k-2} \\\\     &amp; + 0,1297x_{k-1}y_{k-2} - 0,1396x_{k-2}y_{k-1}+ 0,1086x_{k-2}y_{k-2}+0,0085y_{k-2}^2 + 0.1938e_{k-1}e_{k-2} \\end{align}\\] <p>But how those terms were selected? How the parameters were estimated? These questions will lead us to model structure selection and parameter estimation topics, but, for now,  let us discuss about those topics in a more simple manner.</p> <p> First, the \"structure\" of a model is the set of terms (also called regressors) included in the final model. The parameters are the values multiplying each of theses terms. And looking at the example above we can notice an really important thing regarding polynomial NARMAX models dealt in this text: they have a non-linear structure, but they are linear-in-the-parameters. You will see how this note is important in the post about parameter estimation.</p> <p> In this respect, consider the case where we have the input and output data of some system. For the sake of simplicity, suppose one input and one output. We have the data, but we do not know which lags to choose for the input or the output. Also, we know nothing about the system non-linearity. So, we have to define some values for maximum lags of the input, output and the noise terms, besides the choice of the \\(\\ell\\) value. It's worth to notice that many assumptions taken for linear cases are not valid in the nonlinear scenario and therefore select the maximum lags is not straightforward. So, how those values can make the modeling harder?</p> <p> So we have one input and one output (disregard the noise terms for now). What if we choose the \\(n_y = n_x = \\ell = 2\\)? With these values, we have the following possibilities for compose the final model:</p> \\[\\begin{align}     &amp; constant, y_{k-1}, y_{k-2}, y_{k-1}^2, y_{k-2}^2, x_{k-1}, x_{k-2}, x_{k-1}^2, x_{k-2}^2,y_{k-1}y_{k-2},\\\\     &amp; y_{k-1}x_{k-1}, y_{k-1}x_{k-2}, y_{k-2}x_{k-1}, y_{k-2}x_{k-2}, x_{k-1}x_{k-2} . \\end{align}\\] <p>So we have \\(15\\) candidate terms to compose the final model.</p> <p> Again, we do not know how of those terms are significant to compose the model. One should decide to use all the terms because there are only \\(15\\). This, even in a simple scenario like this, can lead to a very wrong representation of the system that you are trying to modeling. Ok, what if we run a brute force algorithm to test the candidate regressors so we can select only the significant ones? In this case, we have \\(2^{15} = 32768\\) possible model structures to be tested.</p> <p> You can think that it is ok, we have computer power for that. But this case is very simple and the system might have lags equal to \\(10\\) for input and output. If we define \\(n_y = n_x = 10\\) and \\(\\ell=2\\), the number of possible models to be tested increases to \\(2^{231}=3.4508732\\times10^{69}\\). If the non-linearity is set to \\(3\\) then we have \\(2^{1771} = 1.3308291989700907535925992... \\times 10^{533}\\) candidate models.</p> <p> Now, think about the case when we have not 1, but 5, 10 or more inputs... and have to include terms for the noise, and maximum lags are higher than 10... and nonlinearity is higher than 3...</p> <p> And the problem is not solved by only identifying the most significant terms. How do you choose the number of terms to include in the final model. It is not just about check the relevance of each regressor, we have to think about the impact of including \\(5\\), \\(10\\) or \\(50\\) regressors in the model. And do not forget: after selecting the terms, we have to estimate its parameters.</p> <p> As you can see, to select the most significant terms from a huge dictionary of possible terms is not an easy task. And it is hard not only because the complex combinatorial problem and the uncertainty concerning the model order. Identifying the most significant terms in a nonlinear scenario is very difficult because depends on the type of the non-linearity (sparse singularity or near-singular behavior, memory or dumping effects and many others), dynamical response (spatial-temporal systems, time-dependent), the steady-state response,  frequency of the data, the noise...</p> <p> Despite all this complexity, NARMAX models are widely used because it is able to represent complex system with simple and transparent models, which terms are selected using robust algorithms for model structure selection. Model structure selection is the core of NARMAX methods and the scientific community is very active on improving classical methods and developing new ones. As I said, I will introduce some of those methods in another post.</p> <p> I hope this publication served as a brief introduction to NARMAX models. Furthermore, I hope I have sparked your interest in this model class. The link to the other texts will be made available soon, but feel free to contact us if you are interested in collaborating with the SysIdentPy library or if you want to address any questions.</p> <ol> <li> <p>Non-linear Autoregressive Models with Moving Average and Exogenous Input.\u00a0\u21a9</p> </li> </ol>"},{"location":"landing-page/contribute/","title":"Contributing","text":"<p>SysIdentPy is intended to be a community project, hence all contributions are welcome! There exist many possible use cases in System Identification field and we can not test all scenarios without your help! If you find any bugs or have suggestions, please report them on issue tracker on GitHub.</p> <p>We welcome new contributors of all experience levels. The SysIdentPy community goals are to be helpful, welcoming, and effective.</p>"},{"location":"landing-page/contribute/#help-others-with-issues-in-github","title":"Help others with issues in GitHub","text":"<p>You can see existing issues and try and help others, most of the times they are questions that you might already know the answer for.</p>"},{"location":"landing-page/contribute/#watch-the-github-repository","title":"Watch the GitHub repository","text":"<p>You can watch SysIdentPy in GitHub (clicking the \"watch\" button at the top right):</p> <p>If you select \"Watching\" instead of \"Releases only\" you will receive notifications when someone creates a new issue.</p> <p>Then you can try and help them solve those issues.</p>"},{"location":"landing-page/contribute/#documentation","title":"Documentation","text":"<p>Documentation is as important as the library itself. English is not the primary language of the main authors, so if you find any typo or anything wrong do not hesitate to point out to us.</p>"},{"location":"landing-page/contribute/#create-a-pull-request","title":"Create a Pull Request","text":"<p>You can contribute to the source code with Pull Requests, for example:</p> <ul> <li>To fix a typo you found on the documentation.</li> <li>To share an article, video, or podcast you created or found about SysIdentPy.</li> <li>To propose new documentation sections.</li> <li>To fix an existing issue/bug.</li> <li>To add a new feature.</li> </ul>"},{"location":"landing-page/contribute/#development-environment","title":"Development environment","text":"<p>These are some basic steps to help us with code:</p> <ul> <li> Install and Setup Git on your computer.</li> <li> Fork SysIdentPy.</li> <li> Clone the fork on your local machine.</li> <li> Create a new branch.</li> <li> Make changes following the coding style of the project (or suggesting improvements).</li> <li> Run the tests.</li> <li> Write and/or adapt existing test if needed.</li> <li> Add documentation if needed.</li> <li> Commit.</li> <li> Push to your fork.</li> <li> Open a pull_request.</li> </ul>"},{"location":"landing-page/contribute/#environment","title":"Environment","text":"<p>Clone the repository using</p> <pre><code>git clone https://github.com/wilsonrljr/sysidentpy.git\n</code></pre> <p>If you already cloned the repository and you know that you need to deep dive in the code, here are some guidelines to set up your environment.</p>"},{"location":"landing-page/contribute/#virtual-environment-with-venv","title":"Virtual environment with <code>venv</code>","text":"<p>You can create a virtual environment in a directory using Python's <code>venv</code> module or Conda:</p> venvconda <pre><code>$ python -m venv env\n</code></pre> <pre><code>conda create -n env\n</code></pre> <p>That will create a directory <code>./env/</code> with the Python binaries and then you will be able to install packages for that isolated environment.</p>"},{"location":"landing-page/contribute/#activate-the-environment","title":"Activate the environment","text":"<p>If you created the environment using Python's <code>venv</code> module, activate it with:</p> Linux, macOSWindows PowerShellWindows Bash <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\env\\Scripts\\Activate.ps1\n</code></pre> <p>Or if you use Bash for Windows (e.g. Git Bash):</p> <pre><code>source ./env/Scripts/activate\n</code></pre> <p>If you created the environment using Conda, activate it with:</p> Conda Bash <p>Or if you use Bash for Windows (e.g. Git Bash):</p> <pre><code>conda activate env\n</code></pre> <p>To check it worked, use:</p> Linux, macOS, Windows BashWindows PowerShellWindows Bash <pre><code>$ which pip\n\nsome/directory/sysidentpy/env/Scripts/pip\n</code></pre> <pre><code>$ Get-Command pip\n\nsome/directory/sysidentpy/env/Scripts/pip\n</code></pre> <pre><code>$ where pip\n\nsome/directory/sysidentpy/env/Scripts/pip\n</code></pre> <p>If it shows the <code>pip</code> binary at <code>env/bin/pip</code> then it worked.</p> <p>Tip</p> <p>Every time you install a new package with <code>pip</code> under that environment, activate the environment again.</p> <p>Note</p> <p>We use the <code>pytest</code> package for testing. The test functions are located in tests subdirectories at each folder inside SysIdentPy, which check the validity of the algorithms.</p>"},{"location":"landing-page/contribute/#dependencies","title":"Dependencies","text":"<p>Install SysIdentPy with the <code>dev</code> and <code>docs</code> option to get all the necessary dependencies to run the tests</p> Dev and Docs dependencies <pre><code>pip install \"sysidentpy[dev, docs]\"\n</code></pre>"},{"location":"landing-page/contribute/#docs","title":"Docs","text":"<p>First, make sure you set up your environment as described above, that will install all the requirements.</p> <p>The documentation uses MkDocs and Material for MKDocs.</p> <p>All the documentation is in Markdown format in the directory <code>./docs/</code>.</p>"},{"location":"landing-page/contribute/#check-the-changes","title":"Check the changes","text":"<p>During local development, you can serve the website locally and checks for any changes. This helps making sure that:</p> <ul> <li>All of your modifications were applied.</li> <li>The unmodified files are displaying as expected.</li> </ul> <pre><code>$ mkdocs serve\n\nINFO     -  [13:25:00] Browser connected: http://127.0.0.1:8000\n</code></pre> <p>It will serve the documentation on <code>http://127.0.0.1:8008</code>.</p> <p>That way, you can keep editing the source files and see the changes live.</p> <p>Warning</p> <p>If any modification break the build, you have to serve the website again. Always check your <code>console</code> to make sure you are serving the website.</p>"},{"location":"landing-page/contribute/#run-tests-locally","title":"Run tests locally","text":"<p>Its always good to check if your implementations/modifications does not break any other part of the package. You can run the SysIdentPy tests locally using <code>pytest</code> in the respective folder to perform all the tests of the corresponding sub-packages.</p>"},{"location":"landing-page/contribute/#example-of-how-to-run-the-tests","title":"Example of how to run the tests:","text":"<p>Open a terminal emulator of your choice and go to a subdirectory, e.g,</p> <pre><code>\\sysidentpy\\metrics\\\n</code></pre> <p>Just type <code>pytest</code> in the terminal emulator</p> <pre><code>pytest\n</code></pre> <p>and you get a result like:</p> <pre><code>========== test session starts ==========\n\nplatform linux -- Python 3.7.6, pytest-5.4.2, py-1.8.1, pluggy-0.13.1\n\nrootdir: ~/sysidentpy\n\nplugins: cov-2.8.1\n\ncollected 12 items\n\ntests/test_regression.py ............ [100%]\n\n========== 12 passed in 2.45s ==================\n</code></pre>"},{"location":"landing-page/get-help/","title":"Get Help","text":"<p>Before asking others for help, it\u2019s generally a good idea for you to try to help yourself. SysIdentPy includes several examples in the documentation with tips and notes about the package that might help you. However, if you have any issues and you can't find the answer, reach out using any method described below.</p>"},{"location":"landing-page/get-help/#connect-with-the-author","title":"Connect with the author","text":"<p>You can:</p> <ul> <li>Follow me on GitHub.</li> <li>Follow me on</li> <li>Connect with me on Linkedin.<ul> <li>I'll start to use Twitter more often \ud83e\udd37\u200d\u2642 (probably).</li> </ul> </li> <li>Read what I write (or follow me) on Medium.</li> </ul>"},{"location":"landing-page/get-help/#create-issues","title":"Create issues","text":"<p>You can create a new issue in the GitHub repository, for example to:</p> <ul> <li>Ask a question or ask about a problem.</li> <li>Suggest a new feature.</li> </ul>"},{"location":"landing-page/get-help/#join-the-chat","title":"Join the chat","text":"<p>Join the \ud83d\udc65 Discord chat server \ud83d\udc65 and hang out with others in the SysIdentPy community.</p> <p>You can use the chat for anything</p> <p>Have in mind that you can use the chat to talk about anything related to SysIdentPy. Conversations about system identification, dynamical systems, new papers, issues, new features are allowed, but have in mind that if some of the questions could help other users, I'll kindly ask you to open an discussion or an issue on Github as well.</p> <p>I can make sure I always answer everything, even if it takes some time.</p>"},{"location":"landing-page/getting-started/","title":"Getting Started","text":"<p>SysIdentPy is a Python module for System Identification using NARMAX models built on top of numpy and is distributed under the 3-Clause BSD license.</p>"},{"location":"landing-page/getting-started/#do-you-like-sysidentpy","title":"Do you like SysIdentPy?","text":"<p>Would you like to help SysIdentPy, other users, and the author? You can \"star\" SysIdentPy in GitHub by clicking in the star button at the top right of the page: https://github.com/wilsonrljr/sysidentpy. \u2b50\ufe0f</p> <p>Starring a repository makes it easy to find it later and help you to find similar projects on GitHub based on Github recommendation contents. Besides, by starring a repository also shows appreciation to the SysIdentPy maintainer for their work.</p> <p> \u00a0 Join our  \"Star\" in github</p>"},{"location":"landing-page/getting-started/#requirements","title":"Requirements","text":"<p>SysIdentPy requires:</p> Dependency version Comment python &gt;=3.7,&lt;3.10 numpy &gt;=1.9.2 for all numerical algorithms scipy &gt;=1.7.0 for some linear regression methods matplotlib &gt;=3.3.2 for static plotting and visualizations torch &gt;=1.7.1 Only necessary if you want to use Neural NARX models Platform Status Windows ok Linux ok Mac OS ok <p>SysIdentPy do not to support Python 2.7.</p> <p>A few examples require pandas &gt;= 0.18.0. However, it is not required to use SysIdentPy.</p>"},{"location":"landing-page/getting-started/#installation","title":"Installation","text":""},{"location":"landing-page/getting-started/#with-pip","title":"with pip recommended","text":"<p>SysIdentPy is published as a Python package and can be installed with <code>pip</code>, ideally by using a virtual environment. If not, scroll down and expand the help box. Install with:</p> LatestNeural NARX Supportv0.1.6 <pre><code>pip install sysidentpy\n</code></pre> <pre><code>pip install sysidentpy[\"all\"]\n</code></pre> <pre><code>pip install sysidentpy==\"0.1.6\"\n</code></pre> How to manage my projects dependencies? <p>If you don't have prior experience with Python, we recommend reading Using Python's pip to Manage Your Projects' Dependencies, which is a really good introduction on the mechanics of Python package management and helps you troubleshoot if you run into errors.</p>"},{"location":"landing-page/getting-started/#with-git","title":"with git","text":"<p>SysIdentPy can be used directly from GitHub by cloning the repository into a subfolder of your project root which might be useful if you want to use the very latest version:</p> <pre><code>git clone https://github.com/wilsonrljr/sysidentpy.git\n</code></pre>"},{"location":"landing-page/license/","title":"License","text":"<p>BSD 3-Clause License</p> <p>Copyright \u00a9 2019, Wilson Rocha; Luan Pascoal; Samuel Oliveira; Samir Martins All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ul> <li> <p>Redistributions of source code must retain the above copyright notice, this   list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright notice,   this list of conditions and the following disclaimer in the documentation   and/or other materials provided with the distribution.</p> </li> <li> <p>Neither the name of the copyright holder nor the names of its   contributors may be used to endorse or promote products derived from   this software without specific prior written permission.</p> </li> </ul> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"landing-page/sponsor/","title":"Sponsors","text":"<p>As a free and open source project, SysIdentPy relies on the support of the community for its development. If you work for an organization that uses and benefits from SysIdentPy, please consider supporting us.</p> <p>SysIdentPy does not follow the sponsorware release strategy, which means that all features are released to the public at the same time. SysIdentPy is a community driven project, however sponsorships will help to assure its sustainability.</p> <p>The main goal of sponsorships it to make this project sustainable. Your donation goes to support a variety of services and development as they buy the maintainers of this project time to work on the development of new features, bug fixing, stability improvement, issue triage and general support.</p> <p>Read on to learn how to become a sponsor!</p>"},{"location":"landing-page/sponsor/#sponsorships","title":"Sponsorships","text":"<p>Every donation counts and would be greatly appreciated!</p> <p>Sponsorships start as low as $1 a month.<sup>1</sup></p>"},{"location":"landing-page/sponsor/#how-to-become-a-sponsor","title":"How to become a sponsor","text":"<p>Thanks for your interest in sponsoring! In order to become an eligible sponsor with your GitHub account, visit wilsonrljr's sponsor profile, and complete a sponsorship of $1 a month or more. You can use your individual or organization GitHub account for sponsoring.</p> <p> \u00a0 Join our  awesome sponsors</p> <p>Special thanks to our sponsors:</p> <p> Gold Sponsors</p> <p> </p> <p> Individual Sponsors</p> <p> </p>"},{"location":"landing-page/sponsor/#goals","title":"Goals","text":"<p>The following section lists all funding goals. Each goal contains a list of features prefixed with a checkmark symbol, denoting whether a feature is  already available or  planned, but not yet implemented. When the funding goal is hit, the features are released for general availability.</p>"},{"location":"landing-page/sponsor/#frequently-asked-questions","title":"Frequently asked questions","text":"I don't want to sponsor anymore. Can I cancel my sponsorship? <p>Yes, you can cancel your sponsorship anytime! If you no longer want to sponsor SysIdentPy in GitHub Sponsors, you can request a cancellation which will become effective at the end of the billing cycle. Just remember: sponsorships are non-refundable!</p> <p>If you have any problems or further questions, please reach out to wilsonrljr@outlook.com.</p> We don't want to pay for sponsorship every month. Are there any other options? <p>Yes. You can sponsor on a yearly basis by switching your GitHub account to a yearly billing cycle or just choose an one time donation.</p> <p>If you have any problems or further questions, please reach out to wilsonrljr@outlook.com.</p> <ol> <li> <p>Note that $1 a month is the minimum amount to become a sponsor in Github Sponsor Program.\u00a0\u21a9</p> </li> </ol>"},{"location":"old_version/NFIR/","title":"NFIR Example","text":"<p>This example shows how to use SysIdentPy to build NFIR models. NFIR models are models with no output feedback. In other words, there are no $y(k-n_y)$ regressors, only $x(k-n_x)$.</p> <p>The NFIR model can be described as:</p> <p>$$     y_k= F^\\ell[x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k $$</p> <p>where $n_x \\in \\mathbb{N}$ is the maximum lag for the system input; $x_k \\in \\mathbb{R}^{n_x}$ is the system input at discrete time $k \\in \\mathbb{N}^n$; $e_k \\in \\mathbb{R}^{n_e}$ stands for uncertainties and possible noise at discrete time $k$. In this case, $\\mathcal{F}^\\ell$ is some nonlinear function of the input regressors with nonlinearity degree $\\ell \\in \\mathbb{N}$ and $d$ is a time delay typically set to $d=1$.</p> <p>It is important to note that NFIR model size is generally significantly higher compared to their counterpart NARMAX model size. This drawback can be noted in linear models dimensionality and it leads to even more complex scenarios in the nonlinear case.</p> <p>So, if you are looking for parsimonious and compact models, consider using NARMAX models. However, when comparing NFIR and NARMAX models, it's generally more challenging to establish stability, particularly in a control-oriented context, with NARMAX models than with NFIR models.</p> In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.basis_function._basis_function import Polynomial, Fourier\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\nfrom sysidentpy.model_structure_selection import AOLS, FROLS\n</pre> import pandas as pd import numpy as np from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.metrics import root_relative_squared_error from sysidentpy.basis_function._basis_function import Polynomial, Fourier from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) from sysidentpy.model_structure_selection import AOLS, FROLS <p>NFIR x NARMAX</p> <p>We will reproduce the same example provided in the \"presenting main functionality section\". In that example, we use a NARX model with xlag and ylag equal to 2 and a nonlinearity degree equal to 2. That resulted in a model with 3 regressors and a RRSE (validation metric) equal to $0.000184$</p> Regressors Parameters ERR x1(k-2) 9.0001E-01 9.57505011E-01 y(k-1) 2.0001E-01 3.89117583E-02 x1(k-1)y(k-1) 9.9992E-02 3.58319976E-03 In\u00a0[2]: Copied! <pre>np.random.seed(seed=42)\n# generating simulated data\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    order_selection=True,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n    model_type=\"NFIR\",\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\n</pre> np.random.seed(seed=42) # generating simulated data x_train, x_test, y_train, y_test = get_siso_data(     n=1000, colored_noise=False, sigma=0.001, train_percentage=90 )  basis_function = Polynomial(degree=2) model = FROLS(     order_selection=True,     xlag=2,     info_criteria=\"aic\",     estimator=\"least_squares\",     basis_function=basis_function,     model_type=\"NFIR\", )  model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_test, y=y_test) rrse = root_relative_squared_error(y_test, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  plot_results(y=y_test, yhat=yhat, n=1000) <pre>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:40: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning, stacklevel=1)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:589: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 6\n  self.info_values = self.information_criterion(reg_matrix, y)\n</pre> <pre>0.2129700627690414\n  Regressors  Parameters             ERR\n0    x1(k-2)  8.9017E-01  9.55432286E-01\n</pre> <p>In the NFIR case, we got a model with 1 regressor, but with significantly worse RRSE ($0.21$)</p> Regressors Parameters ERR x1(k-2) 8.9017E-01 9.55432286E-01 <p>So, to get a better NFIR model, we have to set a higher order model. In other words, we have to set a higher maximum lag to build the model.</p> <p>Lets set xlag=3.</p> In\u00a0[3]: Copied! <pre>np.random.seed(seed=42)\n# generating simulated data\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    order_selection=True,\n    xlag=3,\n    info_criteria=\"aic\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n    model_type=\"NFIR\",\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\n</pre> np.random.seed(seed=42) # generating simulated data x_train, x_test, y_train, y_test = get_siso_data(     n=1000, colored_noise=False, sigma=0.001, train_percentage=90 )  basis_function = Polynomial(degree=2) model = FROLS(     order_selection=True,     xlag=3,     info_criteria=\"aic\",     estimator=\"least_squares\",     basis_function=basis_function,     model_type=\"NFIR\", )  model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_test, y=y_test) rrse = root_relative_squared_error(y_test, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  plot_results(y=y_test, yhat=yhat, n=1000) <pre>0.04314951932710626\n       Regressors  Parameters             ERR\n0         x1(k-2)  8.9980E-01  9.55367779E-01\n1         x1(k-3)  1.7832E-01  3.94348076E-02\n2  x1(k-3)x1(k-1)  9.1104E-02  3.33315478E-03\n</pre> <pre>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:40: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning, stacklevel=1)\n</pre> <p>Now, the model have 3 regressors, but the RRSE is still worse ($0.04$).</p> Regressors Parameters ERR x1(k-2) 8.9980E-01 9.55367779E-01 x1(k-3) 1.7832E-01 3.94348076E-02 x1(k-3)x1(k-1) 9.1104E-02 3.33315478E-03 <p>Lets set xlag=5.</p> In\u00a0[4]: Copied! <pre>np.random.seed(seed=42)\n# generating simulated data\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    order_selection=True,\n    xlag=5,\n    info_criteria=\"aic\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n    model_type=\"NFIR\",\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\n</pre> np.random.seed(seed=42) # generating simulated data x_train, x_test, y_train, y_test = get_siso_data(     n=1000, colored_noise=False, sigma=0.001, train_percentage=90 )  basis_function = Polynomial(degree=2) model = FROLS(     order_selection=True,     xlag=5,     info_criteria=\"aic\",     estimator=\"least_squares\",     basis_function=basis_function,     model_type=\"NFIR\", )  model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_test, y=y_test) rrse = root_relative_squared_error(y_test, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  plot_results(y=y_test, yhat=yhat, n=1000) <pre>0.004209451216121233\n       Regressors  Parameters             ERR\n0         x1(k-2)  8.9978E-01  9.55485306E-01\n1         x1(k-3)  1.7979E-01  3.93181813E-02\n2  x1(k-3)x1(k-1)  8.9706E-02  3.33141271E-03\n3         x1(k-4)  3.5772E-02  1.54789285E-03\n4  x1(k-4)x1(k-2)  1.7615E-02  1.09675506E-04\n5  x1(k-4)x1(k-1)  1.7871E-02  1.13215338E-04\n6         x1(k-5)  6.9594E-03  6.23773643E-05\n7  x1(k-5)x1(k-1)  4.1353E-03  6.10794551E-06\n8  x1(k-5)x1(k-3)  3.4007E-03  3.98364615E-06\n9  x1(k-5)x1(k-2)  2.9798E-03  3.42693984E-06\n</pre> <pre>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:40: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning, stacklevel=1)\n</pre> <p>Now the RRSE is closer to the NARMAX model, but the NFIR model have 10 regressors. So, as mentioned before, the order of NFIR models is generally higher than NARMAX model to get comparable results.</p> <p>Here's the markdown code for the provided table:</p> Regressors Parameters ERR x1(k-2) 8.9978E-01 9.55485306E-01 x1(k-3) 1.7979E-01 3.93181813E-02 x1(k-3)x1(k-1) 8.9706E-02 3.33141271E-03 x1(k-4) 3.5772E-02 1.54789285E-03 x1(k-4)x1(k-2) 1.7615E-02 1.09675506E-04 x1(k-4)x1(k-1) 1.7871E-02 1.13215338E-04 x1(k-5) 6.9594E-03 6.23773643E-05 x1(k-5)x1(k-1) 4.1353E-03 6.10794551E-06 x1(k-5)x1(k-3) 3.4007E-03 3.98364615E-06 x1(k-5)x1(k-2) 2.9798E-03 3.42693984E-06 <p>xlag = 35</p> In\u00a0[5]: Copied! <pre>np.random.seed(seed=42)\n# generating simulated data\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    order_selection=True,\n    xlag=35,\n    n_info_values=200,\n    info_criteria=\"aic\",\n    estimator=\"recursive_least_squares\",\n    basis_function=basis_function,\n    model_type=\"NFIR\",\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\n</pre> np.random.seed(seed=42) # generating simulated data x_train, x_test, y_train, y_test = get_siso_data(     n=1000, colored_noise=False, sigma=0.001, train_percentage=90 )  basis_function = Polynomial(degree=2) model = FROLS(     order_selection=True,     xlag=35,     n_info_values=200,     info_criteria=\"aic\",     estimator=\"recursive_least_squares\",     basis_function=basis_function,     model_type=\"NFIR\", )  model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_test, y=y_test) rrse = root_relative_squared_error(y_test, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  plot_results(y=y_test, yhat=yhat, n=1000) <pre>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:40: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning, stacklevel=1)\n</pre> <pre>0.0038421247000040536\n          Regressors   Parameters             ERR\n0            x1(k-2)   8.9996E-01  9.55386378E-01\n1            x1(k-3)   1.8005E-01  3.94178379E-02\n2     x1(k-3)x1(k-1)   9.0796E-02  3.32874170E-03\n3            x1(k-4)   3.5468E-02  1.54540871E-03\n4     x1(k-4)x1(k-2)   1.8637E-02  1.12751104E-04\n5     x1(k-4)x1(k-1)   1.8337E-02  1.13878189E-04\n6            x1(k-5)   7.0057E-03  6.27406151E-05\n7     x1(k-5)x1(k-1)   4.3851E-03  6.32909200E-06\n8     x1(k-5)x1(k-3)   3.8095E-03  3.95779051E-06\n9     x1(k-5)x1(k-2)   4.8296E-03  3.39231220E-06\n10           x1(k-6)   1.0914E-03  2.37202762E-06\n11    x1(k-6)x1(k-2)   2.4944E-03  3.65196792E-07\n12    x1(k-6)x1(k-3)   9.8303E-04  2.92529290E-07\n13    x1(k-6)x1(k-1)   7.9133E-04  2.55676107E-07\n14    x1(k-6)x1(k-4)   4.4939E-04  3.03449240E-07\n15         x1(k-3)^2   1.1930E-03  2.23371798E-07\n16   x1(k-33)x1(k-2)   1.0851E-03  1.83491782E-07\n17   x1(k-11)x1(k-3)   2.0589E-03  1.65999771E-07\n18   x1(k-26)x1(k-1)  -1.8883E-04  1.59339882E-07\n19   x1(k-14)x1(k-8)  -9.4483E-04  1.36917637E-07\n20   x1(k-18)x1(k-9)   5.2852E-04  1.43200990E-07\n21  x1(k-29)x1(k-22)   2.9389E-04  1.32772541E-07\n22   x1(k-35)x1(k-4)  -3.7559E-04  1.37651677E-07\n23  x1(k-19)x1(k-17)  -2.9479E-04  1.40605871E-07\n24  x1(k-33)x1(k-28)  -1.3863E-03  1.21220227E-07\n25  x1(k-20)x1(k-12)   3.9743E-04  1.24584593E-07\n</pre> <p>Now the RRSE is closer to the NARMAX model, but the NFIR model have 25 regressors. So, as you can check in these examples, the order of NFIR models is generally higher than NARMAX model to get comparable results.</p> <p>Here's the markdown code for the provided table:</p> Regressors Parameters ERR x1(k-2) 8.9996E-01 9.55386378E-01 x1(k-3) 1.8005E-01 3.94178379E-02 x1(k-3)x1(k-1) 9.0796E-02 3.32874170E-03 x1(k-4) 3.5468E-02 1.54540871E-03 x1(k-4)x1(k-2) 1.8637E-02 1.12751104E-04 x1(k-4)x1(k-1) 1.8337E-02 1.13878189E-04 x1(k-5) 7.0057E-03 6.27406151E-05 x1(k-5)x1(k-1) 4.3851E-03 6.32909200E-06 x1(k-5)x1(k-3) 3.8095E-03 3.95779051E-06 x1(k-5)x1(k-2) 4.8296E-03 3.39231220E-06 x1(k-6) 1.0914E-03 2.37202762E-06 x1(k-6)x1(k-2) 2.4944E-03 3.65196792E-07 x1(k-6)x1(k-3) 9.8303E-04 2.92529290E-07 x1(k-6)x1(k-1) 7.9133E-04 2.55676107E-07 x1(k-6)x1(k-4) 4.4939E-04 3.03449240E-07 x1(k-3)^2 1.1930E-03 2.23371798E-07 x1(k-33)x1(k-2) 1.0851E-03 1.83491782E-07 x1(k-11)x1(k-3) 2.0589E-03 1.65999771E-07 x1(k-26)x1(k-1) -1.8883E-04 1.59339882E-07 x1(k-14)x1(k-8) -9.4483E-04 1.36917637E-07 x1(k-18)x1(k-9) 5.2852E-04 1.43200990E-07 x1(k-29)x1(k-22) 2.9389E-04 1.32772541E-07 x1(k-35)x1(k-4) -3.7559E-04 1.37651677E-07 x1(k-19)x1(k-17) -2.9479E-04 1.40605871E-07 x1(k-33)x1(k-28) -1.3863E-03 1.21220227E-07 x1(k-20)x1(k-12) 3.9743E-04 1.24584593E-07"},{"location":"old_version/NFIR/#nfir-example","title":"NFIR Example\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"old_version/NFIR/#so-what-happens-if-i-use-a-nfir-model-with-the-same-configuration","title":"So, what happens if I use a NFIR model with the same configuration?\u00b6","text":""},{"location":"old_version/PV_forecasting_benchmark/","title":"PV forecasting benchmark","text":"<p>We will compare a 1-step ahead forecaster on solar irradiance data (that can be a proxy for solar PV production). The config of the neuralprophet model was taken from the neuralprophet documentation (https://neuralprophet.com/html/example_links/energy_data_example.html)</p> <p>The training will occur on 80% of the data, reserving the last 20% for the validation.</p> <p>Note: the data used in this example can be found in neuralprophet github.</p> In\u00a0[1]: Copied! <pre>from warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.model_structure_selection import AOLS\nfrom sysidentpy.model_structure_selection import MetaMSS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.neural_network import NARXNN\nfrom sysidentpy.metrics import mean_squared_error\n\nfrom sktime.datasets import load_airline\nfrom neuralprophet import NeuralProphet\nfrom neuralprophet import set_random_seed\n\n\nsimplefilter(\"ignore\", FutureWarning)\nnp.seterr(all=\"ignore\")\n\n%matplotlib inline\n\nloss = mean_squared_error\n\ndata_location = r\".\\datasets\"\n</pre> from warnings import simplefilter import matplotlib.pyplot as plt import numpy as np import pandas as pd  from sysidentpy.model_structure_selection import FROLS from sysidentpy.model_structure_selection import AOLS from sysidentpy.model_structure_selection import MetaMSS from sysidentpy.basis_function import Polynomial from sysidentpy.utils.plotting import plot_results from sysidentpy.neural_network import NARXNN from sysidentpy.metrics import mean_squared_error  from sktime.datasets import load_airline from neuralprophet import NeuralProphet from neuralprophet import set_random_seed   simplefilter(\"ignore\", FutureWarning) np.seterr(all=\"ignore\")  %matplotlib inline  loss = mean_squared_error  data_location = r\".\\datasets\" <pre>c:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\datatypes\\_series\\_check.py:43: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\datatypes\\_panel\\_check.py:45: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\datatypes\\_panel\\_check.py:46: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  VALID_MULTIINDEX_TYPES = (pd.Int64Index, pd.RangeIndex)\n</pre> In\u00a0[2]: Copied! <pre>files = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760))\ndf[\"y\"] = raw.iloc[:, 0].values\n\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\n\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\n\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy = FROLS(\n    order_selection=True,\n    ylag=24,\n    xlag=24,\n    info_criteria=\"bic\",\n    estimator=\"recursive_least_squares\",\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\nsysidentpy.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])\n\nyhat = sysidentpy.predict(X=x_test, y=y_test, steps_ahead=1)\nsysidentpy_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy.max_lag :]),\n)\nprint(sysidentpy_loss)\n\n\nplot_results(y=y_test[-104:], yhat=yhat[-104:])\n</pre> files = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"] raw = pd.read_csv(data_location + files[0]) df = pd.DataFrame() df[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760)) df[\"y\"] = raw.iloc[:, 0].values  df_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]  y = df[\"y\"].values.reshape(-1, 1) y_train = df_train[\"y\"].values.reshape(-1, 1) y_test = df_val[\"y\"].values.reshape(-1, 1)  x_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1) x_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)  basis_function = Polynomial(degree=1) sysidentpy = FROLS(     order_selection=True,     ylag=24,     xlag=24,     info_criteria=\"bic\",     estimator=\"recursive_least_squares\",     basis_function=basis_function,     model_type=\"NARMAX\", ) sysidentpy.fit(X=x_train, y=y_train) x_test = np.concatenate([x_train[-sysidentpy.max_lag :], x_test]) y_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])  yhat = sysidentpy.predict(X=x_test, y=y_test, steps_ahead=1) sysidentpy_loss = loss(     pd.Series(y_test.flatten()[sysidentpy.max_lag :]),     pd.Series(yhat.flatten()[sysidentpy.max_lag :]), ) print(sysidentpy_loss)   plot_results(y=y_test[-104:], yhat=yhat[-104:]) <pre>3755.242530079209\n</pre> In\u00a0[3]: Copied! <pre>set_random_seed(42)\nfiles = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760))\ndf[\"y\"] = raw.iloc[:, 0].values\n\nx_train, x_test, x_validation, _ = np.split(df[\"ds\"].dt.hour.values, [5000, 7008, 8760])\ny_train, y_test, y_validation, _ = np.split(df[\"y\"].values, [5000, 7008, 8760])\n\nx_train = x_train.reshape(-1, 1)\nx_test = x_test.reshape(-1, 1)\nx_validation = x_validation.reshape(-1, 1)\n\ny_train = y_train.reshape(-1, 1)\ny_test = y_test.reshape(-1, 1)\ny_validation = y_validation.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy_metamss = MetaMSS(\n    basis_function=basis_function,\n    norm=-2,\n    xlag=24,\n    ylag=24,\n    estimator=\"least_squares\",\n    k_agents_percent=2,\n    estimate_parameter=True,\n    maxiter=10,\n    steps_ahead=1,\n    n_agents=15,\n    p_value=0.05,\n    loss_func=\"metamss_loss\",\n    p_ones=0.5,\n    p_zeros=0.5,\n    model_type=\"NARMAX\",\n    random_state=42,\n)\nsysidentpy_metamss.fit(X=x_train, X_test=x_test, y=y_train, y_test=y_test)\nx_validation = np.concatenate([x_test[-sysidentpy_metamss.max_lag :], x_validation])\ny_validation = np.concatenate([y_test[-sysidentpy_metamss.max_lag :], y_validation])\n\nyhat = sysidentpy_metamss.predict(X=x_validation, y=y_validation, steps_ahead=1)\nmetamss_loss = loss(\n    pd.Series(y_validation.flatten()[sysidentpy_metamss.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]),\n)\nprint(metamss_loss)\n\n\nplot_results(y=y_validation[-104:], yhat=yhat[-104:])\n</pre> set_random_seed(42) files = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"] raw = pd.read_csv(data_location + files[0]) df = pd.DataFrame() df[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760)) df[\"y\"] = raw.iloc[:, 0].values  x_train, x_test, x_validation, _ = np.split(df[\"ds\"].dt.hour.values, [5000, 7008, 8760]) y_train, y_test, y_validation, _ = np.split(df[\"y\"].values, [5000, 7008, 8760])  x_train = x_train.reshape(-1, 1) x_test = x_test.reshape(-1, 1) x_validation = x_validation.reshape(-1, 1)  y_train = y_train.reshape(-1, 1) y_test = y_test.reshape(-1, 1) y_validation = y_validation.reshape(-1, 1)  basis_function = Polynomial(degree=1) sysidentpy_metamss = MetaMSS(     basis_function=basis_function,     norm=-2,     xlag=24,     ylag=24,     estimator=\"least_squares\",     k_agents_percent=2,     estimate_parameter=True,     maxiter=10,     steps_ahead=1,     n_agents=15,     p_value=0.05,     loss_func=\"metamss_loss\",     p_ones=0.5,     p_zeros=0.5,     model_type=\"NARMAX\",     random_state=42, ) sysidentpy_metamss.fit(X=x_train, X_test=x_test, y=y_train, y_test=y_test) x_validation = np.concatenate([x_test[-sysidentpy_metamss.max_lag :], x_validation]) y_validation = np.concatenate([y_test[-sysidentpy_metamss.max_lag :], y_validation])  yhat = sysidentpy_metamss.predict(X=x_validation, y=y_validation, steps_ahead=1) metamss_loss = loss(     pd.Series(y_validation.flatten()[sysidentpy_metamss.max_lag :]),     pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]), ) print(metamss_loss)   plot_results(y=y_validation[-104:], yhat=yhat[-104:]) <pre>2203.92644661679\n</pre> In\u00a0[4]: Copied! <pre>set_random_seed(42)\nfiles = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760))\ndf[\"y\"] = raw.iloc[:, 0].values\n\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\n\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\n\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\nbasis_function = Polynomial(degree=1)\nsysidentpy_AOLS = AOLS(\n    ylag=24, xlag=24, k=2, L=1, model_type=\"NARMAX\", basis_function=basis_function\n)\nsysidentpy_AOLS.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy_AOLS.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])\n\nyhat = sysidentpy_AOLS.predict(X=x_test, y=y_test, steps_ahead=1)\naols_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]),\n)\nprint(aols_loss)\n\n\nplot_results(y=y_test[-104:], yhat=yhat[-104:])\n</pre> set_random_seed(42) files = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"] raw = pd.read_csv(data_location + files[0]) df = pd.DataFrame() df[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760)) df[\"y\"] = raw.iloc[:, 0].values  df_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]  y = df[\"y\"].values.reshape(-1, 1) y_train = df_train[\"y\"].values.reshape(-1, 1) y_test = df_val[\"y\"].values.reshape(-1, 1)  x_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1) x_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1) basis_function = Polynomial(degree=1) sysidentpy_AOLS = AOLS(     ylag=24, xlag=24, k=2, L=1, model_type=\"NARMAX\", basis_function=basis_function ) sysidentpy_AOLS.fit(X=x_train, y=y_train) x_test = np.concatenate([x_train[-sysidentpy_AOLS.max_lag :], x_test]) y_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])  yhat = sysidentpy_AOLS.predict(X=x_test, y=y_test, steps_ahead=1) aols_loss = loss(     pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),     pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]), ) print(aols_loss)   plot_results(y=y_test[-104:], yhat=yhat[-104:]) <pre>2361.561682547365\n</pre> In\u00a0[6]: Copied! <pre>set_random_seed(42)\n\n# set_log_level(\"ERROR\")\nfiles = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760))\ndf[\"y\"] = raw.iloc[:, 0].values\n\nm = NeuralProphet(\n    n_lags=24,\n    ar_sparsity=0.5,\n    # num_hidden_layers = 2,\n    # d_hidden=20,\n)\nmetrics = m.fit(df, freq=\"H\", valid_p=0.2)\n\ndf_train, df_val = m.split_df(df, valid_p=0.2)\nm.test(df_val)\n\nfuture = m.make_future_dataframe(df_val, n_historic_predictions=True)\nforecast = m.predict(future)\n# fig = m.plot(forecast)\nprint(loss(forecast[\"y\"][24:-1], forecast[\"yhat1\"][24:-1]))\n</pre> set_random_seed(42)  # set_log_level(\"ERROR\") files = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"] raw = pd.read_csv(data_location + files[0]) df = pd.DataFrame() df[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760)) df[\"y\"] = raw.iloc[:, 0].values  m = NeuralProphet(     n_lags=24,     ar_sparsity=0.5,     # num_hidden_layers = 2,     # d_hidden=20, ) metrics = m.fit(df, freq=\"H\", valid_p=0.2)  df_train, df_val = m.split_df(df, valid_p=0.2) m.test(df_val)  future = m.make_future_dataframe(df_val, n_historic_predictions=True) forecast = m.predict(future) # fig = m.plot(forecast) print(loss(forecast[\"y\"][24:-1], forecast[\"yhat1\"][24:-1])) <pre>WARNING: nprophet - fit: Parts of code may break if using other than daily data.\n</pre> <pre>04-26 20:16:52 - WARNING - Parts of code may break if using other than daily data.\n</pre> <pre>INFO: nprophet.utils - set_auto_seasonalities: Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n</pre> <pre>04-26 20:16:52 - INFO - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n</pre> <pre>INFO: nprophet.config - set_auto_batch_epoch: Auto-set batch_size to 32\n</pre> <pre>04-26 20:16:52 - INFO - Auto-set batch_size to 32\n</pre> <pre>INFO: nprophet.config - set_auto_batch_epoch: Auto-set epochs to 7\n</pre> <pre>04-26 20:16:52 - INFO - Auto-set epochs to 7\n</pre> <pre> 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 87/100 [00:00&lt;00:00, 617.37it/s]\nINFO: nprophet - _lr_range_test: learning rate range test found optimal lr: 1.23E-01\n</pre> <pre>04-26 20:16:52 - INFO - learning rate range test found optimal lr: 1.23E-01\n</pre> <pre>Epoch[7/7]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:02&lt;00:00,  2.61it/s, SmoothL1Loss=0.00415, MAE=58.8, RegLoss=0.0112]\nINFO: nprophet - _evaluate: Validation metrics:    SmoothL1Loss    MAE\n1         0.003 48.746\n</pre> <pre>04-26 20:16:55 - INFO - Validation metrics:    SmoothL1Loss    MAE\n1         0.003 48.746\n4642.234763049609\n</pre> In\u00a0[7]: Copied! <pre>plt.plot(forecast[\"y\"][-104:], \"ro-\")\nplt.plot(forecast[\"yhat1\"][-104:], \"k*-\")\n</pre> plt.plot(forecast[\"y\"][-104:], \"ro-\") plt.plot(forecast[\"yhat1\"][-104:], \"k*-\") Out[7]: <pre>[&lt;matplotlib.lines.Line2D at 0x2618e76ebe0&gt;]</pre>"},{"location":"old_version/PV_forecasting_benchmark/#pv-forecasting-benchmark","title":"PV forecasting benchmark\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"old_version/PV_forecasting_benchmark/#note","title":"Note\u00b6","text":"<p>The following example is not intended to say that one library is better than another. The main focus of these examples is to show that SysIdentPy can be a good alternative for people looking to model time series.</p> <p>We will compare the results obtained against neural prophet library.</p> <p>For the sake of brevity, from SysIdentPy only the MetaMSS, AOLS and FROLS (with polynomial base function) methods will be used. See the SysIdentPy documentation to learn other ways of modeling with the library.</p>"},{"location":"old_version/PV_forecasting_benchmark/#frols","title":"FROLS\u00b6","text":""},{"location":"old_version/PV_forecasting_benchmark/#metamss","title":"MetaMSS\u00b6","text":""},{"location":"old_version/PV_forecasting_benchmark/#aols","title":"AOLS\u00b6","text":""},{"location":"old_version/PV_forecasting_benchmark/#neural-prophet","title":"Neural Prophet\u00b6","text":""},{"location":"old_version/air_passenger_benchmark/","title":"Air Passenger benchmark","text":"In\u00a0[\u00a0]: Copied! <pre>from warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport scipy.signal.signaltools\n\n\ndef _centered(arr, newsize):\n    # Return the center newsize portion of the array.\n    newsize = np.asarray(newsize)\n    currsize = np.array(arr.shape)\n    startind = (currsize - newsize) // 2\n    endind = startind + newsize\n    myslice = [slice(startind[k], endind[k]) for k in range(len(endind))]\n    return arr[tuple(myslice)]\n\n\nscipy.signal.signaltools._centered = _centered\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.model_structure_selection import AOLS\nfrom sysidentpy.model_structure_selection import MetaMSS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\nfrom torch import nn\n\n# from sysidentpy.metrics import mean_squared_error\nfrom sysidentpy.neural_network import NARXNN\n\nfrom sktime.datasets import load_airline\nfrom sktime.forecasting.ets import AutoETS\nfrom sktime.forecasting.arima import ARIMA, AutoARIMA\nfrom sktime.forecasting.base import ForecastingHorizon\nfrom sktime.forecasting.exp_smoothing import ExponentialSmoothing\nfrom sktime.forecasting.fbprophet import Prophet\nfrom sktime.forecasting.tbats import TBATS\nfrom sktime.forecasting.bats import BATS\n\n# from sktime.forecasting.model_evaluation import evaluate\nfrom sktime.forecasting.model_selection import temporal_train_test_split\nfrom sktime.performance_metrics.forecasting import mean_squared_error\nfrom sktime.utils.plotting import plot_series\nfrom neuralprophet import NeuralProphet\nfrom neuralprophet import set_random_seed\n\n\nsimplefilter(\"ignore\", FutureWarning)\nnp.seterr(all=\"ignore\")\n\n%matplotlib inline\n\nloss = mean_squared_error\n</pre> from warnings import simplefilter import matplotlib.pyplot as plt import numpy as np import pandas as pd  import scipy.signal.signaltools   def _centered(arr, newsize):     # Return the center newsize portion of the array.     newsize = np.asarray(newsize)     currsize = np.array(arr.shape)     startind = (currsize - newsize) // 2     endind = startind + newsize     myslice = [slice(startind[k], endind[k]) for k in range(len(endind))]     return arr[tuple(myslice)]   scipy.signal.signaltools._centered = _centered  from sysidentpy.model_structure_selection import FROLS from sysidentpy.model_structure_selection import AOLS from sysidentpy.model_structure_selection import MetaMSS from sysidentpy.basis_function import Polynomial from sysidentpy.utils.plotting import plot_results from torch import nn  # from sysidentpy.metrics import mean_squared_error from sysidentpy.neural_network import NARXNN  from sktime.datasets import load_airline from sktime.forecasting.ets import AutoETS from sktime.forecasting.arima import ARIMA, AutoARIMA from sktime.forecasting.base import ForecastingHorizon from sktime.forecasting.exp_smoothing import ExponentialSmoothing from sktime.forecasting.fbprophet import Prophet from sktime.forecasting.tbats import TBATS from sktime.forecasting.bats import BATS  # from sktime.forecasting.model_evaluation import evaluate from sktime.forecasting.model_selection import temporal_train_test_split from sktime.performance_metrics.forecasting import mean_squared_error from sktime.utils.plotting import plot_series from neuralprophet import NeuralProphet from neuralprophet import set_random_seed   simplefilter(\"ignore\", FutureWarning) np.seterr(all=\"ignore\")  %matplotlib inline  loss = mean_squared_error In\u00a0[2]: Copied! <pre>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)  # 23 samples for testing\nplot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"])\nfh = ForecastingHorizon(y_test.index, is_relative=False)\nprint(y_train.shape[0], y_test.shape[0])\n</pre> y = load_airline() y_train, y_test = temporal_train_test_split(y, test_size=23)  # 23 samples for testing plot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"]) fh = ForecastingHorizon(y_test.index, is_relative=False) print(y_train.shape[0], y_test.shape[0]) <pre>121 23\n</pre> In\u00a0[3]: Copied! <pre>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy = FROLS(\n    order_selection=True,\n    ylag=13,  # the lags for all models will be 13\n    basis_function=basis_function,\n    model_type=\"NAR\",\n)\nsysidentpy.fit(y=y_train)\ny_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])\n\nyhat = sysidentpy.predict(y=y_test, forecast_horizon=23)\nfrols_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy.max_lag :]),\n)\nprint(frols_loss)\n\nplot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :])\n</pre> y = load_airline() y_train, y_test = temporal_train_test_split(y, test_size=23) y_train = y_train.values.reshape(-1, 1) y_test = y_test.values.reshape(-1, 1)  basis_function = Polynomial(degree=1) sysidentpy = FROLS(     order_selection=True,     ylag=13,  # the lags for all models will be 13     basis_function=basis_function,     model_type=\"NAR\", ) sysidentpy.fit(y=y_train) y_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])  yhat = sysidentpy.predict(y=y_test, forecast_horizon=23) frols_loss = loss(     pd.Series(y_test.flatten()[sysidentpy.max_lag :]),     pd.Series(yhat.flatten()[sysidentpy.max_lag :]), ) print(frols_loss)  plot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :]) <pre>805.9521186338106\n</pre> In\u00a0[4]: Copied! <pre>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\n\ndf_train, df_test = temporal_train_test_split(y, test_size=23)\ndf_train = df_train.reset_index()\ndf_train.columns = [\"ds\", \"y\"]\ndf_train[\"ds\"] = pd.to_datetime(df_train[\"ds\"].astype(str))\ndf_test = df_test.reset_index()\ndf_test.columns = [\"ds\", \"y\"]\ndf_test[\"ds\"] = pd.to_datetime(df_test[\"ds\"].astype(str))\n\nsysidentpy_AOLS = AOLS(\n    ylag=13, k=2, L=1, model_type=\"NAR\", basis_function=basis_function\n)\nsysidentpy_AOLS.fit(y=y_train)\ny_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])\n\nyhat = sysidentpy_AOLS.predict(y=y_test, steps_ahead=None, forecast_horizon=23)\naols_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]),\n)\nprint(aols_loss)\n\nplot_results(y=y_test[sysidentpy_AOLS.max_lag :], yhat=yhat[sysidentpy_AOLS.max_lag :])\n</pre> y = load_airline() y_train, y_test = temporal_train_test_split(y, test_size=23) y_train = y_train.values.reshape(-1, 1) y_test = y_test.values.reshape(-1, 1)  df_train, df_test = temporal_train_test_split(y, test_size=23) df_train = df_train.reset_index() df_train.columns = [\"ds\", \"y\"] df_train[\"ds\"] = pd.to_datetime(df_train[\"ds\"].astype(str)) df_test = df_test.reset_index() df_test.columns = [\"ds\", \"y\"] df_test[\"ds\"] = pd.to_datetime(df_test[\"ds\"].astype(str))  sysidentpy_AOLS = AOLS(     ylag=13, k=2, L=1, model_type=\"NAR\", basis_function=basis_function ) sysidentpy_AOLS.fit(y=y_train) y_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])  yhat = sysidentpy_AOLS.predict(y=y_test, steps_ahead=None, forecast_horizon=23) aols_loss = loss(     pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),     pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]), ) print(aols_loss)  plot_results(y=y_test[sysidentpy_AOLS.max_lag :], yhat=yhat[sysidentpy_AOLS.max_lag :]) <pre>476.64996316992523\n</pre> In\u00a0[34]: Copied! <pre>set_random_seed(42)\n\ny = load_airline()\ny_train, y_test, y_validation, _ = np.split(y, [100, 121, 144])\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\ny_validation = y_validation.values.reshape(-1, 1)\n\nsysidentpy_metamss = MetaMSS(basis_function=basis_function, ylag=13, model_type=\"NAR\")\nsysidentpy_metamss.fit(y=y_train, y_test=y_test)\n\ny_validation = np.concatenate([y_test[-sysidentpy_metamss.max_lag :], y_validation])\n\nyhat = sysidentpy_metamss.predict(y=y_validation, steps_ahead=None, forecast_horizon=23)\nmetamss_loss = loss(\n    pd.Series(y_validation.flatten()[sysidentpy_metamss.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]),\n)\nprint(metamss_loss)\n\nplot_results(\n    y=y_validation[sysidentpy_metamss.max_lag :],\n    yhat=yhat[sysidentpy_metamss.max_lag :],\n)\n</pre> set_random_seed(42)  y = load_airline() y_train, y_test, y_validation, _ = np.split(y, [100, 121, 144]) y_train = y_train.values.reshape(-1, 1) y_test = y_test.values.reshape(-1, 1) y_validation = y_validation.values.reshape(-1, 1)  sysidentpy_metamss = MetaMSS(basis_function=basis_function, ylag=13, model_type=\"NAR\") sysidentpy_metamss.fit(y=y_train, y_test=y_test)  y_validation = np.concatenate([y_test[-sysidentpy_metamss.max_lag :], y_validation])  yhat = sysidentpy_metamss.predict(y=y_validation, steps_ahead=None, forecast_horizon=23) metamss_loss = loss(     pd.Series(y_validation.flatten()[sysidentpy_metamss.max_lag :]),     pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]), ) print(metamss_loss)  plot_results(     y=y_validation[sysidentpy_metamss.max_lag :],     yhat=yhat[sysidentpy_metamss.max_lag :], ) <pre>450.992127624293\n</pre> In\u00a0[8]: Copied! <pre>import torch\n\ntorch.manual_seed(42)\n\ny = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=36)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\nx_train = np.zeros_like(y_train)\nx_test = np.zeros_like(y_test)\n\n\nclass NARX(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(13, 20)\n        self.lin2 = nn.Linear(20, 20)\n        self.lin3 = nn.Linear(20, 20)\n        self.lin4 = nn.Linear(20, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, xb):\n        z = self.lin(xb)\n        z = self.relu(z)\n        z = self.lin2(z)\n        z = self.relu(z)\n        z = self.lin3(z)\n        z = self.relu(z)\n        z = self.lin4(z)\n        return z\n\n\nnarx_net = NARXNN(\n    net=NARX(),\n    ylag=13,\n    model_type=\"NAR\",\n    basis_function=Polynomial(degree=1),\n    epochs=900,\n    verbose=False,\n    learning_rate=2.5e-02,\n    optim_params={},  # optional parameters of the optimizer\n)\n\nnarx_net.fit(y=y_train)\nyhat = narx_net.predict(y=y_test, forecast_horizon=23)\nnarxnet_loss = loss(\n    pd.Series(y_test.flatten()[narx_net.max_lag :]),\n    pd.Series(yhat.flatten()[narx_net.max_lag :]),\n)\nprint(narxnet_loss)\nplot_results(y=y_test[narx_net.max_lag :], yhat=yhat[narx_net.max_lag :])\n</pre> import torch  torch.manual_seed(42)  y = load_airline() y_train, y_test = temporal_train_test_split(y, test_size=36) y_train = y_train.values.reshape(-1, 1) y_test = y_test.values.reshape(-1, 1) x_train = np.zeros_like(y_train) x_test = np.zeros_like(y_test)   class NARX(nn.Module):     def __init__(self):         super().__init__()         self.lin = nn.Linear(13, 20)         self.lin2 = nn.Linear(20, 20)         self.lin3 = nn.Linear(20, 20)         self.lin4 = nn.Linear(20, 1)         self.relu = nn.ReLU()      def forward(self, xb):         z = self.lin(xb)         z = self.relu(z)         z = self.lin2(z)         z = self.relu(z)         z = self.lin3(z)         z = self.relu(z)         z = self.lin4(z)         return z   narx_net = NARXNN(     net=NARX(),     ylag=13,     model_type=\"NAR\",     basis_function=Polynomial(degree=1),     epochs=900,     verbose=False,     learning_rate=2.5e-02,     optim_params={},  # optional parameters of the optimizer )  narx_net.fit(y=y_train) yhat = narx_net.predict(y=y_test, forecast_horizon=23) narxnet_loss = loss(     pd.Series(y_test.flatten()[narx_net.max_lag :]),     pd.Series(yhat.flatten()[narx_net.max_lag :]), ) print(narxnet_loss) plot_results(y=y_test[narx_net.max_lag :], yhat=yhat[narx_net.max_lag :]) <pre>316.54086775668776\n</pre> In\u00a0[9]: Copied! <pre>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)  # 23 samples for testing\nplot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"])\nfh = ForecastingHorizon(y_test.index, is_relative=False)\nprint(y_train.shape[0], y_test.shape[0])\n</pre> y = load_airline() y_train, y_test = temporal_train_test_split(y, test_size=23)  # 23 samples for testing plot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"]) fh = ForecastingHorizon(y_test.index, is_relative=False) print(y_train.shape[0], y_test.shape[0]) <pre>121 23\n</pre> In\u00a0[10]: Copied! <pre>es = ExponentialSmoothing(trend=\"add\", seasonal=\"multiplicative\", sp=12)\ny = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nes.fit(y_train)\ny_pred_es = es.predict(fh)\n\nplot_series(y_test, y_pred_es, labels=[\"y_test\", \"y_pred\"])\nes_loss = loss(y_test, y_pred_es)\nes_loss\n</pre> es = ExponentialSmoothing(trend=\"add\", seasonal=\"multiplicative\", sp=12) y = load_airline() y_train, y_test = temporal_train_test_split(y, test_size=23) es.fit(y_train) y_pred_es = es.predict(fh)  plot_series(y_test, y_pred_es, labels=[\"y_test\", \"y_pred\"]) es_loss = loss(y_test, y_pred_es) es_loss Out[10]: <pre>910.462659260655</pre> In\u00a0[11]: Copied! <pre>y = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nets = AutoETS(auto=True, sp=12, n_jobs=-1)\nets.fit(y_train)\ny_pred_ets = ets.predict(fh)\n\nplot_series(y_test, y_pred_ets, labels=[\"y_test\", \"y_pred\"])\nets_loss = loss(y_test, y_pred_ets)\nets_loss\n</pre> y = load_airline()  y_train, y_test = temporal_train_test_split(y, test_size=23) ets = AutoETS(auto=True, sp=12, n_jobs=-1) ets.fit(y_train) y_pred_ets = ets.predict(fh)  plot_series(y_test, y_pred_ets, labels=[\"y_test\", \"y_pred\"]) ets_loss = loss(y_test, y_pred_ets) ets_loss Out[11]: <pre>1739.117296439066</pre> In\u00a0[12]: Copied! <pre>auto_arima = AutoARIMA(sp=12, suppress_warnings=True)\ny = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nauto_arima.fit(y_train)\ny_pred_auto_arima = auto_arima.predict(fh)\n\nplot_series(y_test, y_pred_auto_arima, labels=[\"y_test\", \"y_pred\"])\nautoarima_loss = loss(y_test, y_pred_auto_arima)\nautoarima_loss\n</pre> auto_arima = AutoARIMA(sp=12, suppress_warnings=True) y = load_airline()  y_train, y_test = temporal_train_test_split(y, test_size=23) auto_arima.fit(y_train) y_pred_auto_arima = auto_arima.predict(fh)  plot_series(y_test, y_pred_auto_arima, labels=[\"y_test\", \"y_pred\"]) autoarima_loss = loss(y_test, y_pred_auto_arima) autoarima_loss Out[12]: <pre>1714.4753226965322</pre> In\u00a0[13]: Copied! <pre>y = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nmanual_arima = ARIMA(\n    order=(13, 1, 0), suppress_warnings=True\n)  # seasonal_order=(0, 1, 0, 12)\nmanual_arima.fit(y_train)\ny_pred_manual_arima = manual_arima.predict(fh)\nplot_series(y_test, y_pred_manual_arima, labels=[\"y_test\", \"y_pred\"])\nmanualarima_loss = loss(y_test, y_pred_manual_arima)\nmanualarima_loss\n</pre> y = load_airline()  y_train, y_test = temporal_train_test_split(y, test_size=23) manual_arima = ARIMA(     order=(13, 1, 0), suppress_warnings=True )  # seasonal_order=(0, 1, 0, 12) manual_arima.fit(y_train) y_pred_manual_arima = manual_arima.predict(fh) plot_series(y_test, y_pred_manual_arima, labels=[\"y_test\", \"y_pred\"]) manualarima_loss = loss(y_test, y_pred_manual_arima) manualarima_loss Out[13]: <pre>2085.425167938668</pre> In\u00a0[14]: Copied! <pre>y = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nbats = BATS(sp=12, use_trend=True, use_box_cox=False)\nbats.fit(y_train)\ny_pred_bats = bats.predict(fh)\n\nplot_series(y_test, y_pred_bats, labels=[\"y_test\", \"y_pred\"])\nbats_loss = loss(y_test, y_pred_bats)\nbats_loss\n</pre> y = load_airline()  y_train, y_test = temporal_train_test_split(y, test_size=23) bats = BATS(sp=12, use_trend=True, use_box_cox=False) bats.fit(y_train) y_pred_bats = bats.predict(fh)  plot_series(y_test, y_pred_bats, labels=[\"y_test\", \"y_pred\"]) bats_loss = loss(y_test, y_pred_bats) bats_loss Out[14]: <pre>7286.6484525676415</pre> In\u00a0[15]: Copied! <pre>y = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ntbats = TBATS(sp=12, use_trend=True, use_box_cox=False)\ntbats.fit(y_train)\ny_pred_tbats = tbats.predict(fh)\nplot_series(y_test, y_pred_tbats, labels=[\"y_test\", \"y_pred\"])\ntbats_loss = loss(y_test, y_pred_tbats)\ntbats_loss\n</pre> y = load_airline()  y_train, y_test = temporal_train_test_split(y, test_size=23) tbats = TBATS(sp=12, use_trend=True, use_box_cox=False) tbats.fit(y_train) y_pred_tbats = tbats.predict(fh) plot_series(y_test, y_pred_tbats, labels=[\"y_test\", \"y_pred\"]) tbats_loss = loss(y_test, y_pred_tbats) tbats_loss Out[15]: <pre>7448.434672875093</pre> In\u00a0[16]: Copied! <pre>set_random_seed(42)\n\ny = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nz = y.copy()\nz = z.to_timestamp(freq=\"M\")\nz_train, z_test = temporal_train_test_split(z, test_size=23)\n\n\nprophet = Prophet(\n    seasonality_mode=\"multiplicative\",\n    n_changepoints=int(len(y_train) / 12),\n    add_country_holidays={\"country_name\": \"Germany\"},\n    yearly_seasonality=True,\n    weekly_seasonality=False,\n    daily_seasonality=False,\n)\nprophet.fit(z_train)\ny_pred_prophet = prophet.predict(fh.to_relative(cutoff=y_train.index[-1]))\n\ny_pred_prophet.index = y_test.index\nplot_series(y_test, y_pred_prophet, labels=[\"y_test\", \"y_pred\"])\nprophet_loss = loss(y_test, y_pred_prophet)\nprophet_loss\n</pre> set_random_seed(42)  y = load_airline()  y_train, y_test = temporal_train_test_split(y, test_size=23) z = y.copy() z = z.to_timestamp(freq=\"M\") z_train, z_test = temporal_train_test_split(z, test_size=23)   prophet = Prophet(     seasonality_mode=\"multiplicative\",     n_changepoints=int(len(y_train) / 12),     add_country_holidays={\"country_name\": \"Germany\"},     yearly_seasonality=True,     weekly_seasonality=False,     daily_seasonality=False, ) prophet.fit(z_train) y_pred_prophet = prophet.predict(fh.to_relative(cutoff=y_train.index[-1]))  y_pred_prophet.index = y_test.index plot_series(y_test, y_pred_prophet, labels=[\"y_test\", \"y_pred\"]) prophet_loss = loss(y_test, y_pred_prophet) prophet_loss Out[16]: <pre>1186.0045566050442</pre> In\u00a0[35]: Copied! <pre>set_random_seed(42)\n\ndf = pd.read_csv(r\".\\datasets\\air_passengers.csv\")\nm = NeuralProphet(seasonality_mode=\"multiplicative\")\ndf_train = df.iloc[:-23, :].copy()\ndf_test = df.iloc[-23:, :].copy()\n# df_val['y'] = None\n\nm = NeuralProphet(seasonality_mode=\"multiplicative\")\n\nmetrics = m.fit(df_train, freq=\"MS\")\n\nfuture = m.make_future_dataframe(\n    df_train, periods=23, n_historic_predictions=len(df_train)\n)\n\nforecast = m.predict(future)\n# fig = m.plot(forecast)\nplt.plot(forecast[\"yhat1\"].values[-23:])\nplt.plot(df_test[\"y\"].values)\nneuralprophet_loss = loss(forecast[\"yhat1\"].values[-23:], df_test[\"y\"].values)\nneuralprophet_loss\n</pre> set_random_seed(42)  df = pd.read_csv(r\".\\datasets\\air_passengers.csv\") m = NeuralProphet(seasonality_mode=\"multiplicative\") df_train = df.iloc[:-23, :].copy() df_test = df.iloc[-23:, :].copy() # df_val['y'] = None  m = NeuralProphet(seasonality_mode=\"multiplicative\")  metrics = m.fit(df_train, freq=\"MS\")  future = m.make_future_dataframe(     df_train, periods=23, n_historic_predictions=len(df_train) )  forecast = m.predict(future) # fig = m.plot(forecast) plt.plot(forecast[\"yhat1\"].values[-23:]) plt.plot(df_test[\"y\"].values) neuralprophet_loss = loss(forecast[\"yhat1\"].values[-23:], df_test[\"y\"].values) neuralprophet_loss <pre>WARNING: nprophet - fit: Parts of code may break if using other than daily data.\n</pre> <pre>11-21 20:57:55 - WARNING - Parts of code may break if using other than daily data.\n</pre> <pre>INFO: nprophet.utils - set_auto_seasonalities: Disabling weekly seasonality. Run NeuralProphet with weekly_seasonality=True to override this.\n</pre> <pre>11-21 20:57:55 - INFO - Disabling weekly seasonality. Run NeuralProphet with weekly_seasonality=True to override this.\n</pre> <pre>INFO: nprophet.utils - set_auto_seasonalities: Disabling daily seasonality. Run NeuralProphet with daily_seasonality=True to override this.\n</pre> <pre>11-21 20:57:55 - INFO - Disabling daily seasonality. Run NeuralProphet with daily_seasonality=True to override this.\n</pre> <pre>INFO: nprophet.config - set_auto_batch_epoch: Auto-set batch_size to 8\n</pre> <pre>11-21 20:57:55 - INFO - Auto-set batch_size to 8\n</pre> <pre>INFO: nprophet.config - set_auto_batch_epoch: Auto-set epochs to 264\n</pre> <pre>11-21 20:57:55 - INFO - Auto-set epochs to 264\n</pre> <pre> 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 83/100 [00:00&lt;00:00, 1034.46it/s]\nINFO: nprophet - _lr_range_test: learning rate range test found optimal lr: 1.87E-01\n</pre> <pre>11-21 20:57:55 - INFO - learning rate range test found optimal lr: 1.87E-01\n</pre> <pre>Epoch[264/264]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 264/264 [00:03&lt;00:00, 66.42it/s, SmoothL1Loss=0.000325, MAE=6.38, RegLoss=0]\n</pre> Out[35]: <pre>501.24794023767436</pre> In\u00a0[19]: Copied! <pre>results = {\n    \"Exponential Smoothing\": es_loss,\n    \"ETS\": ets_loss,\n    \"AutoArima\": autoarima_loss,\n    \"Manual Arima\": manualarima_loss,\n    \"BATS\": bats_loss,\n    \"TBATS\": tbats_loss,\n    \"Prophet\": prophet_loss,\n    \"SysIdentPy (Polynomial Model)\": frols_loss,\n    \"SysIdentPy (Neural Model)\": narxnet_loss,\n    \"SysIdentPy (AOLS)\": aols_loss,\n    \"SysIdentPy (MetaMSS)\": metamss_loss,\n    \"NeuralProphet\": neuralprophet_loss,\n}\n\nsorted(results.items(), key=lambda result: result[1])\n</pre> results = {     \"Exponential Smoothing\": es_loss,     \"ETS\": ets_loss,     \"AutoArima\": autoarima_loss,     \"Manual Arima\": manualarima_loss,     \"BATS\": bats_loss,     \"TBATS\": tbats_loss,     \"Prophet\": prophet_loss,     \"SysIdentPy (Polynomial Model)\": frols_loss,     \"SysIdentPy (Neural Model)\": narxnet_loss,     \"SysIdentPy (AOLS)\": aols_loss,     \"SysIdentPy (MetaMSS)\": metamss_loss,     \"NeuralProphet\": neuralprophet_loss, }  sorted(results.items(), key=lambda result: result[1]) Out[19]: <pre>[('SysIdentPy (Neural Model)', 316.54086775668776),\n ('SysIdentPy (MetaMSS)', 450.992127624293),\n ('SysIdentPy (AOLS)', 476.64996316992523),\n ('NeuralProphet', 501.24794023767436),\n ('SysIdentPy (Polynomial Model)', 805.9521186338106),\n ('Exponential Smoothing', 910.462659260655),\n ('Prophet', 1186.0045566050442),\n ('AutoArima', 1714.4753226965322),\n ('ETS', 1739.117296439066),\n ('Manual Arima', 2085.425167938668),\n ('BATS', 7286.6484525676415),\n ('TBATS', 7448.434672875093)]</pre>"},{"location":"old_version/air_passenger_benchmark/#air-passenger-benchmark","title":"Air Passenger benchmark\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"old_version/air_passenger_benchmark/#note","title":"Note\u00b6","text":"<p>The following example is not intended to say that one library is better than another. The main focus of these examples is to show that SysIdentPy can be a good alternative for people looking to model time series.</p> <p>We will compare the results obtained using the sktime and neural prophet library.</p> <p>From sktime, the following models will be used:</p> <ul> <li><p>AutoARIMA</p> </li> <li><p>BATS</p> </li> <li><p>TBATS</p> </li> <li><p>Exponential Smoothing</p> </li> <li><p>Prophet</p> </li> <li><p>AutoETS</p> </li> </ul> <p>For the sake of brevity, from SysIdentPy only the MetaMSS, AOLS, FROLS (with polynomial base function) and NARXNN methods will be used. See the SysIdentPy documentation to learn other ways of modeling with the library.</p>"},{"location":"old_version/air_passenger_benchmark/#air-passengers-data","title":"Air passengers data\u00b6","text":""},{"location":"old_version/air_passenger_benchmark/#results","title":"Results\u00b6","text":"No. Package Mean Squared Error 1 SysIdentPy (Neural Model) 316.54 2 SysIdentPy (MetaMSS) 450.99 3 SysIdentPy (AOLS) 476.64 4 NeuralProphet 501.24 5 SysIdentPy (FROLS) 805.95 6 Exponential Smoothing 910.52 7 Prophet 1186.00 8 AutoArima 1714.47 9 Manual Arima 2085.42 10 ETS 2590.05 11 BATS 7286.64 12 TBATS 7448.43"},{"location":"old_version/air_passenger_benchmark/#sysidentpy-frols","title":"SysIdentPy FROLS\u00b6","text":""},{"location":"old_version/air_passenger_benchmark/#sysidentpy-aols","title":"SysIdentPy AOLS\u00b6","text":""},{"location":"old_version/air_passenger_benchmark/#sysidentpy-metamss","title":"SysIdentPy MetaMSS\u00b6","text":""},{"location":"old_version/air_passenger_benchmark/#sysidentpy-neural-narx","title":"SysIdentPy Neural NARX\u00b6","text":""},{"location":"old_version/air_passenger_benchmark/#exponential-smoothing","title":"Exponential Smoothing\u00b6","text":""},{"location":"old_version/air_passenger_benchmark/#autoets","title":"AutoETS\u00b6","text":""},{"location":"old_version/air_passenger_benchmark/#autoarima","title":"AutoArima\u00b6","text":""},{"location":"old_version/air_passenger_benchmark/#arima","title":"Arima\u00b6","text":""},{"location":"old_version/air_passenger_benchmark/#bats","title":"BATS\u00b6","text":""},{"location":"old_version/air_passenger_benchmark/#tbats","title":"TBATS\u00b6","text":""},{"location":"old_version/air_passenger_benchmark/#prophet","title":"Prophet\u00b6","text":""},{"location":"old_version/air_passenger_benchmark/#neural-prophet","title":"Neural Prophet\u00b6","text":""},{"location":"old_version/aols/","title":"Using the Accelerated Orthogonal Least-Squares algorithm for building Polynomial NARX models","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\nfrom sysidentpy.model_structure_selection import AOLS\n\n# generating simulated data\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n</pre> import pandas as pd from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.metrics import root_relative_squared_error from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) from sysidentpy.model_structure_selection import AOLS  # generating simulated data x_train, x_test, y_train, y_test = get_siso_data(     n=1000, colored_noise=False, sigma=0.001, train_percentage=90 ) In\u00a0[2]: Copied! <pre>basis_function = Polynomial(degree=2)\nmodel = AOLS(xlag=3, ylag=3, k=5, L=1, basis_function=basis_function)\n\nmodel.fit(X=x_train, y=y_train)\n</pre> basis_function = Polynomial(degree=2) model = AOLS(xlag=3, ylag=3, k=5, L=1, basis_function=basis_function)  model.fit(X=x_train, y=y_train) <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use AOLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre> Out[2]: <pre>&lt;sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS at 0x1fb7f6b2cd0&gt;</pre> In\u00a0[3]: Copied! <pre>yhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\nresults = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(results)\n</pre> yhat = model.predict(X=x_test, y=y_test) rrse = root_relative_squared_error(y_test, yhat) print(rrse) results = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(results) <pre>0.002246729829172727\n      Regressors   Parameters             ERR\n0         y(k-1)   2.0014E-01  0.00000000E+00\n1         y(k-2)  -1.4763E-04  0.00000000E+00\n2        x1(k-2)   9.0004E-01  0.00000000E+00\n3  x1(k-1)y(k-1)   9.9921E-02  0.00000000E+00\n4       y(k-2)^2   1.1732E-04  0.00000000E+00\n</pre> In\u00a0[4]: Copied! <pre>plot_results(y=y_test, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_test, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_test, yhat, x_test)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> plot_results(y=y_test, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_test, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_test, yhat, x_test) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")"},{"location":"old_version/aols/#using-the-accelerated-orthogonal-least-squares-algorithm-for-building-polynomial-narx-models","title":"Using the Accelerated Orthogonal Least-Squares algorithm for building Polynomial NARX models\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"old_version/basic_steps/","title":"Presenting main functionality","text":"<p>Here we import the NARMAX model, the metric for model evaluation and the methods to generate sample data for tests. Also, we import pandas for specific usage.</p> In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[2]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) In\u00a0[3]: Copied! <pre>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.0001, train_percentage=90\n)\n</pre> x_train, x_valid, y_train, y_valid = get_siso_data(     n=1000, colored_noise=False, sigma=0.0001, train_percentage=90 ) <p>To obtain a NARMAX model we have to choose some values, e.g, the nonlinearity degree (non_degree), the maximum lag for the inputs and output (xlag and ylag).</p> <p>In addition, you can select the information criteria to be used with the Error Reduction Ratio to select the model order and the method to estimate the model parameters:</p> <ul> <li>Information Criteria: aic, bic, lilc, fpe</li> <li>Parameter Estimation: least_squares, total_least_squares, recursive_least_squares, least_mean_squares and many other (see the docs)</li> </ul> <p>The n_terms values is optional. It refer to the number of terms to include in the final model. You can set this value based on the information criteria (see below) or based on priori information about the model structure. The default value is n_terms=None, so the algorithm will choose the minimum value reached by the information criteria.</p> <p>To use information criteria you have to set order_selection=True. You can also select n_info_values (default = 15).</p> In\u00a0[4]: Copied! <pre>basis_function = Polynomial(degree=2)\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=3,\n    extended_least_squares=False,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n)\n</pre> basis_function = Polynomial(degree=2)  model = FROLS(     order_selection=True,     n_info_values=3,     extended_least_squares=False,     ylag=2,     xlag=2,     info_criteria=\"aic\",     estimator=\"least_squares\",     basis_function=basis_function, ) <pre>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:40: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning, stacklevel=1)\n</pre> In\u00a0[5]: Copied! <pre>model.fit(X=x_train, y=y_train)\n</pre> model.fit(X=x_train, y=y_train) Out[5]: <pre>&lt;sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS at 0x20eb3f8ced0&gt;</pre> In\u00a0[6]: Copied! <pre>yhat = model.predict(X=x_valid, y=y_valid)\n</pre> yhat = model.predict(X=x_valid, y=y_valid) In\u00a0[7]: Copied! <pre>rrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n</pre> rrse = root_relative_squared_error(y_valid, yhat) print(rrse) <pre>0.000236625360270206\n</pre> <p>model_object.results return the selected model regressors, the estimated parameters and the ERR values. As shown below, the algorithm detect the exact model that was used for simulate the data.</p> In\u00a0[8]: Copied! <pre>r = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</pre> r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) <pre>      Regressors  Parameters             ERR\n0        x1(k-2)  9.0001E-01  9.56885108E-01\n1         y(k-1)  2.0000E-01  3.96313039E-02\n2  x1(k-1)y(k-1)  1.0001E-01  3.48355000E-03\n</pre> <p>In addition, you can access the residuals and plot_result methods to take a look at the prediction and two residual analysis. The extras and lam values below contain another residues analysis so you can plot it manually. This method will be improved soon.</p> In\u00a0[9]: Copied! <pre>plt.style.available\n</pre> plt.style.available Out[9]: <pre>['Solarize_Light2',\n '_classic_test_patch',\n '_mpl-gallery',\n '_mpl-gallery-nogrid',\n 'bmh',\n 'classic',\n 'dark_background',\n 'fast',\n 'fivethirtyeight',\n 'ggplot',\n 'grayscale',\n 'seaborn-v0_8',\n 'seaborn-v0_8-bright',\n 'seaborn-v0_8-colorblind',\n 'seaborn-v0_8-dark',\n 'seaborn-v0_8-dark-palette',\n 'seaborn-v0_8-darkgrid',\n 'seaborn-v0_8-deep',\n 'seaborn-v0_8-muted',\n 'seaborn-v0_8-notebook',\n 'seaborn-v0_8-paper',\n 'seaborn-v0_8-pastel',\n 'seaborn-v0_8-poster',\n 'seaborn-v0_8-talk',\n 'seaborn-v0_8-ticks',\n 'seaborn-v0_8-white',\n 'seaborn-v0_8-whitegrid',\n 'tableau-colorblind10']</pre> In\u00a0[10]: Copied! <pre>plot_results(\n    y=y_valid,\n    yhat=yhat,\n    n=1000,\n    title=\"test\",\n    xlabel=\"Samples\",\n    ylabel=r\"y, $\\hat{y}$\",\n    data_color=\"#1f77b4\",\n    model_color=\"#ff7f0e\",\n    marker=\"o\",\n    model_marker=\"*\",\n    linewidth=1.5,\n    figsize=(10, 6),\n    style=\"seaborn-v0_8-notebook\",\n    facecolor=\"white\",\n)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(\n    data=ee, title=\"Residues\", ylabel=\"$e^2$\", style=\"seaborn-v0_8-notebook\"\n)\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(\n    data=x1e, title=\"Residues\", ylabel=\"$x_1e$\", style=\"seaborn-v0_8-notebook\"\n)\n</pre> plot_results(     y=y_valid,     yhat=yhat,     n=1000,     title=\"test\",     xlabel=\"Samples\",     ylabel=r\"y, $\\hat{y}$\",     data_color=\"#1f77b4\",     model_color=\"#ff7f0e\",     marker=\"o\",     model_marker=\"*\",     linewidth=1.5,     figsize=(10, 6),     style=\"seaborn-v0_8-notebook\",     facecolor=\"white\", ) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(     data=ee, title=\"Residues\", ylabel=\"$e^2$\", style=\"seaborn-v0_8-notebook\" ) x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(     data=x1e, title=\"Residues\", ylabel=\"$x_1e$\", style=\"seaborn-v0_8-notebook\" ) In\u00a0[11]: Copied! <pre>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</pre> xaxis = np.arange(1, model.n_info_values + 1) plt.plot(xaxis, model.info_values) plt.xlabel(\"n_terms\") plt.ylabel(\"Information Criteria\") Out[11]: <pre>Text(0, 0.5, 'Information Criteria')</pre> <pre><code>{note}\n Here we are creating random samples with white noise and letting the algorithm choose\n the number of terms based on the minimum value of information criteria. \n This is not the best approach in System Identification, but serves as a simple example. \n The information criteria must be used as an __auxiliary tool__ to select *n_terms*. \n Plot the information values to help you on that!\n\n If you run the example above several times you might find some cases where the\n algorithm choose only the first two regressors, or four (depending on the information\n criteria method selected). This is because the minimum value of information criteria\n depends on residual variance (affected by noise) and have some limitations in nonlinear\n scenarios. However, if you check the ERR values (robust to noise) you will see that the\n ERR is ordering the regressors in the correct way!\n\n We have some examples on *information_criteria* notebook!\n</code></pre> <pre><code>{note}\nThis documentation and the examples below are written with MyST Markdown, a form\nof markdown that works with Sphinx. For more information about MyST markdown, and\nto use MyST markdown with your Sphinx website,\nsee [the MyST-parser documentation](https://myst-parser.readthedocs.io/)\n</code></pre> <p>The n_info_values limits the number of regressors to apply the information criteria. We choose $n_y = n_x = \\ell = 2$, so the candidate regressor is a list of 15 regressors. We can set n_info_values = 15 and see the information values for all regressors. This option can save some amount of computational resources when dealing with multiples inputs and large datasets.</p> In\u00a0[12]: Copied! <pre>basis_function = Polynomial(degree=2)\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    extended_least_squares=False,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</pre> basis_function = Polynomial(degree=2)  model = FROLS(     order_selection=True,     n_info_values=15,     extended_least_squares=False,     ylag=2,     xlag=2,     info_criteria=\"aic\",     estimator=\"least_squares\",     basis_function=basis_function, )  model.fit(X=x_train, y=y_train)  xaxis = np.arange(1, model.n_info_values + 1) plt.plot(xaxis, model.info_values) plt.xlabel(\"n_terms\") plt.ylabel(\"Information Criteria\") <pre>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:40: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning, stacklevel=1)\n</pre> Out[12]: <pre>Text(0, 0.5, 'Information Criteria')</pre> <p>Now running without executing information criteria methods (setting the n_terms) because we already know the optimal number of regressors</p> In\u00a0[13]: Copied! <pre>basis_function = Polynomial(degree=2)\n\nmodel = FROLS(\n    order_selection=False,\n    n_info_values=15,\n    n_terms=3,\n    extended_least_squares=False,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</pre> basis_function = Polynomial(degree=2)  model = FROLS(     order_selection=False,     n_info_values=15,     n_terms=3,     extended_least_squares=False,     ylag=2,     xlag=2,     info_criteria=\"aic\",     estimator=\"least_squares\",     basis_function=basis_function, ) model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) <pre>0.000236625360270206\n      Regressors  Parameters             ERR\n0        x1(k-2)  9.0001E-01  9.56885108E-01\n1         y(k-1)  2.0000E-01  3.96313039E-02\n2  x1(k-1)y(k-1)  1.0001E-01  3.48355000E-03\n</pre> In\u00a0[14]: Copied! <pre>model.max_lag  # the number of initial conditions you should provide\n</pre> model.max_lag  # the number of initial conditions you should provide Out[14]: <pre>2</pre> In\u00a0[15]: Copied! <pre>yhat = model.predict(\n    X=x_valid, y=y_valid[: model.max_lag]\n)  # passing only the 2 initial values which will be used as initial conditions\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n</pre> yhat = model.predict(     X=x_valid, y=y_valid[: model.max_lag] )  # passing only the 2 initial values which will be used as initial conditions rrse = root_relative_squared_error(y_valid, yhat) print(rrse) <pre>0.000236625360270206\n</pre> <p>As you can see, the rrse obtained is the same as the one obtained when we input the full test data. That is due the fact that even in the cases you provide the full test data, the prediction method only uses the first values as initial conditions and drop the other values in the backend.</p> <p>In the 1-step ahead prediction or n-steps ahead prediction you should provide the full test data because we handle all the computations in the background so the users don't need to care to implement the loops themselves.</p> <p>If you wants the check how your model performs in a 3-steps ahead scenario using 200 samples in the test data, that means that at each 3 iterations the model feedback will use the real data as initial conditions. Thats why the full data test is necessary, because otherwise the model won't find the real value to use as feedback in the n-iteration.</p> <p>This is the case where you have access the historical data so you can check how your model performs in n-steps ahead prediction. If you choose to use a 3-steps ahead model prediction in real life, you have to predict the next 3 samples, wait the 3 iterations, collect the real data and use the new data as initial condition to predict the next 3 values and so on.</p> <p>In the infinity-steps ahead prediction scenario, if your model has a input it will be able to make predictions for all inputs by only providing the initial conditions. If your model has no input (a NAR model, for example), you can set the forecast horizon and the model will make predictions by using only the initial conditions. All the feedback will be the predicted values (this is, by the way, one of the reasons that usually n-steps ahead models are better than infinity-ahead models).</p> <p>It is worth to mention that changing the initial condition doesn't mean you are changing your model. The only thing changing is the initial condition and that could make a real difference in many cases.</p> In\u00a0[16]: Copied! <pre># for now the list is returned as a codification. Here, $0$ is the constant term, $[1001]=y{k-1}, [100n]=y_{k-n}, [200n] = x1_{k-n}, [300n]=x2_{k-n}$ and so on\nmodel.regressor_code  # list of all possible regressors given non_degree, n_y and n_x values\n</pre> # for now the list is returned as a codification. Here, $0$ is the constant term, $[1001]=y{k-1}, [100n]=y_{k-n}, [200n] = x1_{k-n}, [300n]=x2_{k-n}$ and so on model.regressor_code  # list of all possible regressors given non_degree, n_y and n_x values Out[16]: <pre>array([[   0,    0],\n       [1001,    0],\n       [1002,    0],\n       [2001,    0],\n       [2002,    0],\n       [1001, 1001],\n       [1002, 1001],\n       [2001, 1001],\n       [2002, 1001],\n       [1002, 1002],\n       [2001, 1002],\n       [2002, 1002],\n       [2001, 2001],\n       [2002, 2001],\n       [2002, 2002]])</pre> In\u00a0[17]: Copied! <pre>print(model.err, \"\\n\\n\")  # err values for the selected terms\nprint(model.theta)  # estimated parameters for the final model structure\n</pre> print(model.err, \"\\n\\n\")  # err values for the selected terms print(model.theta)  # estimated parameters for the final model structure <pre>[0.95688511 0.0396313  0.00348355 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.        ] \n\n\n[[0.90000632]\n [0.20000258]\n [0.10001084]]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"old_version/basic_steps/#presenting-main-functionality","title":"Presenting main functionality\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"old_version/basic_steps/#generating-1-input-1-output-sample-data","title":"Generating 1 input 1 output sample data\u00b6","text":"<p>The data is generated by simulating the following model:</p> <p>$y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_{k}$</p> <p>If colored_noise is set to True:</p> <p>$e_{k} = 0.8\\nu_{k-1} + \\nu_{k}$</p> <p>where $x$ is a uniformly distributed random variable and $\\nu$ is a gaussian distributed variable with $\\mu=0$ and $\\sigma=0.1$</p> <p>In the next example we will generate a data with 1000 samples with white noise and selecting 90% of the data to train the model.</p>"},{"location":"old_version/basic_steps/#model-structure-selection","title":"Model Structure Selection\u00b6","text":"<p>The fit method executes the Error Reduction Ratio algorithm using Househoulder reflection to select the model structure.</p> <p>Enforcing keyword-only arguments in fit and predict methods as well. This is an effort to promote clear and non-ambiguous use of the library.</p>"},{"location":"old_version/basic_steps/#free-run-simulation","title":"Free run simulation\u00b6","text":"<p>The predict method is use to generate the predictions. For now we only support free run simulation (also known as infinity steps ahead). Soon will let the user define a one-step ahead or k-step ahead prediction.</p>"},{"location":"old_version/basic_steps/#evaluate-the-model","title":"Evaluate the model\u00b6","text":"<p>In this example we use the root_relative_squared_error metric because it is often used in System Identification. More metrics and information about it can be found on documentation.</p>"},{"location":"old_version/basic_steps/#setting-the-n_terms-parameter","title":"Setting the n_terms parameter\u00b6","text":"<p>In the example above we let the number of terms to compose the final model to be defined as the minimum value of the information criteria. Once you ran the algorithm and choose the best number of parameters, you can turn order_selection to False and set the n_terms value (3 in this example). Here we have a small dataset, but in bigger data this can be critical because running information criteria algorithm is more computational expensive. Since we already know the best number of regressor, we set n_terms and we get the same result.</p> <p>However, this is not only critical because computational efficiency. In many situation, the minimum value of the information criteria can lead to overfitting. In some cases, the difference between choosing a model with 30 regressors or 10 is minimal, so you can take the model with 10 terms without loosing accuracy.</p> <p>In the following we use info_values to plot the information criteria values. As you can see, the minimum value relies where $xaxis = 5$</p>"},{"location":"old_version/basic_steps/#predict-method","title":"Predict method\u00b6","text":"<p>One could ask why it is necessary to pass the test data on the predict method. The answers is: you don't need to pass the test data when you are running a infinity-steps ahead prediction, you just need to pass the initial conditions. However, if you wants to check how your model performs in a 1-step ahead prediction or n-step ahead prediction, you should provide the test data.</p> <p>To show you only need the initial condition, consider the following example using the previous trained model:</p>"},{"location":"old_version/basic_steps/#extra-information","title":"Extra information\u00b6","text":"<p>You can access some extra information like the list of all candidate regressors</p>"},{"location":"old_version/defining_lags/","title":"Setting specific lags","text":"<p>Different ways to set the maximum lag for input and output</p> In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) In\u00a0[2]: Copied! <pre>basis_function = Polynomial(degree=1)\n\nmodel = FROLS(\n    order_selection=True,\n    extended_least_squares=False,\n    ylag=4,\n    xlag=4,\n    info_criteria=\"aic\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n)\n</pre> basis_function = Polynomial(degree=1)  model = FROLS(     order_selection=True,     extended_least_squares=False,     ylag=4,     xlag=4,     info_criteria=\"aic\",     estimator=\"least_squares\",     basis_function=basis_function, ) <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre> In\u00a0[3]: Copied! <pre>model = FROLS(\n    order_selection=True,\n    extended_least_squares=False,\n    ylag=[1, 4],\n    xlag=[1, 4],\n    info_criteria=\"aic\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n)\n</pre> model = FROLS(     order_selection=True,     extended_least_squares=False,     ylag=[1, 4],     xlag=[1, 4],     info_criteria=\"aic\",     estimator=\"least_squares\",     basis_function=basis_function, ) In\u00a0[4]: Copied! <pre># The example considers a model with 2 inputs, but you can use the same for any amount of inputs.\nmodel = FROLS(\n    order_selection=True,\n    extended_least_squares=False,\n    ylag=[1, 4],\n    xlag=[[1, 2, 3, 4], [1, 7]],\n    info_criteria=\"aic\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n)\n# The lags defined are:\n# x1(k-1), x1(k-2), x(k-3), x(k-4)\n# x2(k-1), x1(k-7)\n</pre> # The example considers a model with 2 inputs, but you can use the same for any amount of inputs. model = FROLS(     order_selection=True,     extended_least_squares=False,     ylag=[1, 4],     xlag=[[1, 2, 3, 4], [1, 7]],     info_criteria=\"aic\",     estimator=\"least_squares\",     basis_function=basis_function, ) # The lags defined are: # x1(k-1), x1(k-2), x(k-3), x(k-4) # x2(k-1), x1(k-7)"},{"location":"old_version/defining_lags/#setting-specific-lags","title":"Setting specific lags\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"old_version/defining_lags/#setting-lags-using-a-range-of-values","title":"Setting lags using a range of values\u00b6","text":"<p>If you pass int values for ylag and xlag, the lags are defined as a range from 1-ylag and 1-xlag.</p> <p>For example: if ylag=4 then the candidate regressors are $y_{k-1}, y_{k-2}, y_{k-3}, y_{k-4}$</p>"},{"location":"old_version/defining_lags/#setting-specific-lags-using-lists","title":"Setting specific lags using lists\u00b6","text":"<p>If you pass the ylag and xlag as a list, only the lags related to values in the list will be created.</p>"},{"location":"old_version/defining_lags/#setting-lags-for-multiple-input-single-output-miso-models","title":"Setting lags for Multiple Input Single Output (MISO) models\u00b6","text":"<p>The following example shows how to define specific lags for each input. One should notice that we have to use a nested list in that case.</p>"},{"location":"old_version/entropic_regression/","title":"Identification of an electromechanical system using Entropic Regression","text":"In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import ER\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sysidentpy.model_structure_selection import ER from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) In\u00a0[2]: Copied! <pre>df1 = pd.read_csv(\"examples/datasets/x_cc.csv\")\ndf2 = pd.read_csv(\"examples/datasets/y_cc.csv\")\n</pre> df1 = pd.read_csv(\"examples/datasets/x_cc.csv\") df2 = pd.read_csv(\"examples/datasets/y_cc.csv\") In\u00a0[3]: Copied! <pre>df2[5000:80000].plot(figsize=(10, 4))\n</pre> df2[5000:80000].plot(figsize=(10, 4)) Out[3]: <pre>&lt;AxesSubplot:&gt;</pre> In\u00a0[4]: Copied! <pre># we will decimate the data using d=500 in this example\nx_train, x_valid = np.split(df1.iloc[::500].values, 2)\ny_train, y_valid = np.split(df2.iloc[::500].values, 2)\n</pre> # we will decimate the data using d=500 in this example x_train, x_valid = np.split(df1.iloc[::500].values, 2) y_train, y_valid = np.split(df2.iloc[::500].values, 2) In\u00a0[5]: Copied! <pre>basis_function = Polynomial(degree=2)\n\nmodel = ER(\n    ylag=6,\n    xlag=6,\n    n_perm=2,\n    k=2,\n    skip_forward=True,\n    estimator=\"recursive_least_squares\",\n    basis_function=basis_function,\n)\n</pre> basis_function = Polynomial(degree=2)  model = ER(     ylag=6,     xlag=6,     n_perm=2,     k=2,     skip_forward=True,     estimator=\"recursive_least_squares\",     basis_function=basis_function, ) <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use ER(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre> In\u00a0[6]: Copied! <pre>model.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  plot_results(y=y_valid, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>C:\\Users\\wilso\\AppData\\Local\\Temp/ipykernel_35768/1917592845.py:1: UserWarning: Given the higher number of possible regressors (91), the Entropic Regression algorithm may take long time to run. Consider reducing the number of regressors \n  model.fit(X=x_train, y=y_train)\n</pre> <pre>0.026821345664204566\n        Regressors   Parameters             ERR\n0                1  -5.0893E+02  0.00000000E+00\n1           y(k-1)   9.4303E-01  0.00000000E+00\n2           y(k-5)  -5.8675E-02  0.00000000E+00\n3          x1(k-2)   5.2611E+02  0.00000000E+00\n4          x1(k-5)  -4.8028E+01  0.00000000E+00\n5          x1(k-6)  -8.2871E+00  0.00000000E+00\n6         y(k-1)^2   2.2206E-04  0.00000000E+00\n7     y(k-2)y(k-1)  -3.8730E-04  0.00000000E+00\n8     y(k-3)y(k-1)   7.0514E-05  0.00000000E+00\n9     y(k-5)y(k-1)   1.3006E-05  0.00000000E+00\n10   x1(k-1)y(k-1)  -1.6708E-01  0.00000000E+00\n11   x1(k-2)y(k-1)  -1.5839E-01  0.00000000E+00\n12   x1(k-4)y(k-1)   1.1017E-02  0.00000000E+00\n13        y(k-2)^2   1.6803E-04  0.00000000E+00\n14    y(k-3)y(k-2)  -5.1021E-05  0.00000000E+00\n15    y(k-4)y(k-2)  -6.8171E-06  0.00000000E+00\n16   x1(k-1)y(k-2)   1.1750E-01  0.00000000E+00\n17   x1(k-2)y(k-2)   6.9008E-02  0.00000000E+00\n18   x1(k-3)y(k-2)  -7.1654E-03  0.00000000E+00\n19   x1(k-4)y(k-2)  -1.6124E-02  0.00000000E+00\n20   x1(k-5)y(k-2)   8.1489E-03  0.00000000E+00\n21   x1(k-6)y(k-2)   1.1681E-03  0.00000000E+00\n22   x1(k-1)y(k-3)  -4.7176E-02  0.00000000E+00\n23   x1(k-1)y(k-4)   1.7867E-02  0.00000000E+00\n24   x1(k-4)y(k-4)   2.4316E-03  0.00000000E+00\n25   x1(k-1)y(k-5)  -7.1066E-03  0.00000000E+00\n26   x1(k-2)y(k-5)  -3.2368E-03  0.00000000E+00\n27   x1(k-1)y(k-6)   2.5080E-03  0.00000000E+00\n28       x1(k-1)^2   1.1453E+02  0.00000000E+00\n29  x1(k-2)x1(k-1)  -1.1003E+00  0.00000000E+00\n30  x1(k-4)x1(k-1)  -9.2887E-01  0.00000000E+00\n31  x1(k-3)x1(k-2)   4.7982E+00  0.00000000E+00\n32  x1(k-4)x1(k-3)   2.5953E+00  0.00000000E+00\n</pre>"},{"location":"old_version/entropic_regression/#identification-of-an-electromechanical-system-using-entropic-regression","title":"Identification of an electromechanical system using Entropic Regression\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>More details about this data can be found in the following paper (in Portuguese): https://www.researchgate.net/publication/320418710_Identificacao_de_um_motorgerador_CC_por_meio_de_modelos_polinomiais_autorregressivos_e_redes_neurais_artificiais</p>"},{"location":"old_version/entropic_regression/#building-a-polynomial-narx-model-using-entropic-regression-algorithm","title":"Building a Polynomial NARX model using Entropic Regression Algorithm\u00b6","text":""},{"location":"old_version/extended_least_squares/","title":"Extended Least Squares","text":"<p>Here we import the NARMAX model, the metric for model evaluation and the methods to generate sample data for tests. Also, we import pandas for specific usage.</p> In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) In\u00a0[2]: Copied! <pre>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=True, sigma=0.2, train_percentage=90\n)\n</pre> x_train, x_valid, y_train, y_valid = get_siso_data(     n=1000, colored_noise=True, sigma=0.2, train_percentage=90 ) In\u00a0[3]: Copied! <pre>basis_function = Polynomial(degree=2)\n\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    extended_least_squares=False,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n)\n</pre> basis_function = Polynomial(degree=2)  model = FROLS(     order_selection=False,     n_terms=3,     extended_least_squares=False,     ylag=2,     xlag=2,     info_criteria=\"aic\",     estimator=\"least_squares\",     basis_function=basis_function, ) <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre> In\u00a0[4]: Copied! <pre>model.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n</pre> model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse) <pre>0.4864715504437732\n</pre> <p>Clearly we have something wrong with the obtained model. See the basic_steps notebook to compare the results obtained using the same data but without colored noise. But let take a look in whats is wrong.</p> In\u00a0[5]: Copied! <pre>r = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</pre> r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) <pre>      Regressors  Parameters             ERR\n0        x1(k-2)  9.1595E-01  7.59322318E-01\n1         y(k-1)  2.6843E-01  7.29718483E-02\n2  x1(k-1)y(k-1)  7.0081E-02  1.68008211E-03\n</pre> In\u00a0[6]: Copied! <pre>import seaborn as sns\nimport matplotlib.pyplot as plt\n\nparameters = np.zeros([3, 50])\n\nfor i in range(50):\n    x_train, x_valid, y_train, y_valid = get_siso_data(\n        n=3000, colored_noise=True, train_percentage=90\n    )\n\n    model.fit(X=x_train, y=y_train)\n    parameters[:, i] = model.theta.flatten()\n\nsns.set()\npal = sns.cubehelix_palette(3, rot=-0.5, dark=0.3)\n\nax = sns.kdeplot(parameters.T[:, 0])\nax = sns.kdeplot(parameters.T[:, 1])\nax = sns.kdeplot(parameters.T[:, 2])\n# plotting a vertical line where the real values must lie\nax = plt.axvline(x=0.1, c=\"k\")\nax = plt.axvline(x=0.2, c=\"k\")\nax = plt.axvline(x=0.9, c=\"k\")\n</pre> import seaborn as sns import matplotlib.pyplot as plt  parameters = np.zeros([3, 50])  for i in range(50):     x_train, x_valid, y_train, y_valid = get_siso_data(         n=3000, colored_noise=True, train_percentage=90     )      model.fit(X=x_train, y=y_train)     parameters[:, i] = model.theta.flatten()  sns.set() pal = sns.cubehelix_palette(3, rot=-0.5, dark=0.3)  ax = sns.kdeplot(parameters.T[:, 0]) ax = sns.kdeplot(parameters.T[:, 1]) ax = sns.kdeplot(parameters.T[:, 2]) # plotting a vertical line where the real values must lie ax = plt.axvline(x=0.1, c=\"k\") ax = plt.axvline(x=0.2, c=\"k\") ax = plt.axvline(x=0.9, c=\"k\") In\u00a0[7]: Copied! <pre>basis_function = Polynomial(degree=2)\nparameters = np.zeros([3, 50])\n\nfor i in range(50):\n    x_train, x_valid, y_train, y_valid = get_siso_data(\n        n=3000, colored_noise=True, train_percentage=90\n    )\n\n    model = FROLS(\n        order_selection=False,\n        n_terms=3,\n        extended_least_squares=True,\n        ylag=2,\n        xlag=2,\n        elag=2,\n        info_criteria=\"aic\",\n        estimator=\"least_squares\",\n        basis_function=basis_function,\n    )\n\n    model.fit(X=x_train, y=y_train)\n    parameters[:, i] = model.theta.flatten()\n\nsns.set()\npal = sns.cubehelix_palette(3, rot=-0.5, dark=0.3)\n\nax = sns.kdeplot(parameters.T[:, 0])\nax = sns.kdeplot(parameters.T[:, 1])\nax = sns.kdeplot(parameters.T[:, 2])\n# plotting a vertical line where the real values must lie\nax = plt.axvline(x=0.1, c=\"k\")\nax = plt.axvline(x=0.2, c=\"k\")\nax = plt.axvline(x=0.9, c=\"k\")\n</pre> basis_function = Polynomial(degree=2) parameters = np.zeros([3, 50])  for i in range(50):     x_train, x_valid, y_train, y_valid = get_siso_data(         n=3000, colored_noise=True, train_percentage=90     )      model = FROLS(         order_selection=False,         n_terms=3,         extended_least_squares=True,         ylag=2,         xlag=2,         elag=2,         info_criteria=\"aic\",         estimator=\"least_squares\",         basis_function=basis_function,     )      model.fit(X=x_train, y=y_train)     parameters[:, i] = model.theta.flatten()  sns.set() pal = sns.cubehelix_palette(3, rot=-0.5, dark=0.3)  ax = sns.kdeplot(parameters.T[:, 0]) ax = sns.kdeplot(parameters.T[:, 1]) ax = sns.kdeplot(parameters.T[:, 2]) # plotting a vertical line where the real values must lie ax = plt.axvline(x=0.1, c=\"k\") ax = plt.axvline(x=0.2, c=\"k\") ax = plt.axvline(x=0.9, c=\"k\") <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre> <p>Great! Now we have an unbiased estimation of the parameters!</p>"},{"location":"old_version/extended_least_squares/#extended-least-squares","title":"Extended Least Squares\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"old_version/extended_least_squares/#generating-1-input-1-output-sample-data","title":"Generating 1 input 1 output sample data\u00b6","text":"<p>The data is generated by simulating the following model: $y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-2} + e_{k}$</p> <p>If colored_noise is set to True:</p> <p>$e_{k} = 0.8\\nu_{k-1} + \\nu_{k}$</p> <p>where $x$ is a uniformly distributed random variable and $\\nu$ is a gaussian distributed variable with $\\mu=0$ and $\\sigma$ is defined by the user.</p> <p>In the next example we will generate a data with 3000 samples with white noise and selecting 90% of the data to train the model.</p>"},{"location":"old_version/extended_least_squares/#build-the-model","title":"Build the model\u00b6","text":"<p>First we will train a model without the Extended Least Squares Algorithm for comparison purpose.</p>"},{"location":"old_version/extended_least_squares/#biased-parameter-estimation","title":"Biased parameter estimation\u00b6","text":"<p>As we can observe above, the model structure is exact the same the one that generate the data. You can se that the ERR ordered the terms in the correct way. And this is an important note regarding the Error Reduction Ratio algorithm used here: it is very robust to colored noise!!</p> <p>That is a great feature! However, although the structure is correct, the model parameters are not ok! Here we have a biased estimation! The real parameter for $y_{k-1}$ is $0.2$, not $0.3$.</p> <p>In this case, we are actually modeling using a NARX model, not a NARMAX. The MA part exists to allow a unbiased estimation of the parameters. To achieve a unbiased estimation of the parameters we have the Extend Least Squares algorithm. Remember, if the data have only white noise, NARX is fine.</p> <p>Before applying the Extended Least Squares Algorithm we will run several NARX models to check how different the estimated parameters are from the real ones.</p>"},{"location":"old_version/extended_least_squares/#using-the-extended-least-squares-algorithm","title":"Using the Extended Least Squares algorithm\u00b6","text":"<p>As shown in figure above, we have a problem to estimate the parameter for $y_{k-1}$. Now we will use the Extended Least Squares Algorithm.</p> <p>In SysIdentPy, just set extended_least_squares to True and the algorithm will be applied.</p>"},{"location":"old_version/extended_least_squares/#note","title":"Note\u00b6","text":"<p>Note: The Extended Least Squares is an iterative algorithm. In SysIdentpy we fixed 30 iterations because it is known from literature that the algorithm converges quickly (about 10 or 20 iterations).</p>"},{"location":"old_version/f_16_benchmark/","title":"Example: F-16 Ground Vibration Test benchmark","text":"In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) In\u00a0[2]: Copied! <pre>f_16 = pd.read_csv(r\"examples/datasets/f-16.txt\", header=None, names=[\"x1\", \"x2\", \"y\"])\n</pre> f_16 = pd.read_csv(r\"examples/datasets/f-16.txt\", header=None, names=[\"x1\", \"x2\", \"y\"]) In\u00a0[3]: Copied! <pre>f_16.shape\n</pre> f_16.shape Out[3]: <pre>(32768, 3)</pre> In\u00a0[4]: Copied! <pre>f_16[[\"x1\", \"x2\"]][0:500].plot(figsize=(12, 8))\n</pre> f_16[[\"x1\", \"x2\"]][0:500].plot(figsize=(12, 8)) Out[4]: <pre>&lt;AxesSubplot:&gt;</pre> In\u00a0[5]: Copied! <pre>f_16[\"y\"][0:2000].plot(figsize=(12, 8))\n</pre> f_16[\"y\"][0:2000].plot(figsize=(12, 8)) Out[5]: <pre>&lt;AxesSubplot:&gt;</pre> In\u00a0[6]: Copied! <pre>x1_id, x1_val = f_16[\"x1\"][0:16384].values.reshape(-1, 1), f_16[\"x1\"][\n    16384::\n].values.reshape(-1, 1)\nx2_id, x2_val = f_16[\"x2\"][0:16384].values.reshape(-1, 1), f_16[\"x2\"][\n    16384::\n].values.reshape(-1, 1)\nx_id = np.concatenate([x1_id, x2_id], axis=1)\nx_val = np.concatenate([x1_val, x2_val], axis=1)\n\ny_id, y_val = f_16[\"y\"][0:16384].values.reshape(-1, 1), f_16[\"y\"][\n    16384::\n].values.reshape(-1, 1)\n</pre> x1_id, x1_val = f_16[\"x1\"][0:16384].values.reshape(-1, 1), f_16[\"x1\"][     16384:: ].values.reshape(-1, 1) x2_id, x2_val = f_16[\"x2\"][0:16384].values.reshape(-1, 1), f_16[\"x2\"][     16384:: ].values.reshape(-1, 1) x_id = np.concatenate([x1_id, x2_id], axis=1) x_val = np.concatenate([x1_val, x2_val], axis=1)  y_id, y_val = f_16[\"y\"][0:16384].values.reshape(-1, 1), f_16[\"y\"][     16384:: ].values.reshape(-1, 1) In\u00a0[7]: Copied! <pre>x1lag = list(range(1, 10))\nx2lag = list(range(1, 10))\nx2lag\n</pre> x1lag = list(range(1, 10)) x2lag = list(range(1, 10)) x2lag Out[7]: <pre>[1, 2, 3, 4, 5, 6, 7, 8, 9]</pre> In\u00a0[8]: Copied! <pre>basis_function = Polynomial(degree=1)\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=39,\n    extended_least_squares=False,\n    ylag=20,\n    xlag=[x1lag, x2lag],\n    info_criteria=\"bic\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_id, y=y_id)\ny_hat = model.predict(X=x_val, y=y_val)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</pre> basis_function = Polynomial(degree=1)  model = FROLS(     order_selection=True,     n_info_values=39,     extended_least_squares=False,     ylag=20,     xlag=[x1lag, x2lag],     info_criteria=\"bic\",     estimator=\"least_squares\",     basis_function=basis_function, )  model.fit(X=x_id, y=y_id) y_hat = model.predict(X=x_val, y=y_val) rrse = root_relative_squared_error(y_val, y_hat) print(rrse) r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre> <pre>0.2910089654603829\n   Regressors   Parameters             ERR\n0      y(k-1)   1.8387E+00  9.43378253E-01\n1      y(k-2)  -1.8938E+00  1.95167599E-02\n2      y(k-3)   1.3337E+00  1.02432261E-02\n3      y(k-6)  -1.6038E+00  8.03485985E-03\n4      y(k-9)   2.6776E-01  9.27874557E-04\n5     x2(k-7)  -2.2385E+01  3.76837313E-04\n6     x1(k-1)   8.2709E+00  6.81508210E-04\n7     x2(k-3)   1.0587E+02  1.57459800E-03\n8     x1(k-8)  -3.7975E+00  7.35086279E-04\n9     x2(k-1)   8.5725E+01  4.85358786E-04\n10     y(k-7)   1.3955E+00  2.77245281E-04\n11     y(k-5)   1.3219E+00  8.64120037E-04\n12    y(k-10)  -2.9306E-01  8.51717688E-04\n13     y(k-4)  -9.5479E-01  7.23623116E-04\n14     y(k-8)  -7.1309E-01  4.44988077E-04\n15    y(k-12)  -3.0437E-01  1.49743148E-04\n16    y(k-11)   4.8602E-01  3.34613282E-04\n17    y(k-13)  -8.2442E-02  1.43738964E-04\n18    y(k-15)  -1.6762E-01  1.25546584E-04\n19    x1(k-2)  -8.9698E+00  9.76699739E-05\n20    y(k-17)   2.2036E-02  4.55983807E-05\n21    y(k-14)   2.4900E-01  1.10314107E-04\n22    y(k-19)  -6.8239E-03  1.99734771E-05\n23    x2(k-9)  -9.6265E+01  2.98523208E-05\n24    x2(k-8)   2.2620E+02  2.34402543E-04\n25    x2(k-2)  -2.3609E+02  1.04172323E-04\n26    y(k-20)  -5.4663E-02  5.37895336E-05\n27    x2(k-6)  -2.3651E+02  2.11392628E-05\n28    x2(k-4)   1.7378E+02  2.18396315E-05\n29    x1(k-7)   4.9862E+00  2.03811842E-05\n</pre> In\u00a0[9]: Copied! <pre>plot_results(y=y_val, yhat=y_hat, n=1000)\nee = compute_residues_autocorrelation(y_val, y_hat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_val, y_hat, x_val[:, 0])\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> plot_results(y=y_val, yhat=y_hat, n=1000) ee = compute_residues_autocorrelation(y_val, y_hat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_val, y_hat, x_val[:, 0]) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") In\u00a0[10]: Copied! <pre>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n\n# You can use the plot below to choose the \"n_terms\" and run the model again with the most adequate value of terms.\n</pre> xaxis = np.arange(1, model.n_info_values + 1) plt.plot(xaxis, model.info_values) plt.xlabel(\"n_terms\") plt.ylabel(\"Information Criteria\")  # You can use the plot below to choose the \"n_terms\" and run the model again with the most adequate value of terms. Out[10]: <pre>Text(0, 0.5, 'Information Criteria')</pre>"},{"location":"old_version/f_16_benchmark/#example-f-16-ground-vibration-test-benchmark","title":"Example: F-16 Ground Vibration Test benchmark\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Note: The following examples do not try to replicate the results of the cited manuscripts. Even the model parameters such as ylag and xlag and size of identification and validation data are not the same of the cited papers. Moreover, sampling rate adjustment and other different data preparation are not handled here.</p>"},{"location":"old_version/f_16_benchmark/#reference","title":"Reference\u00b6","text":"<p>The following text was taken from the link http://www.nonlinearbenchmark.org/#F16.</p> <p>Note: The reader is referred to the mentioned website for a complete reference concerning the experiment. For now, this notebook is just a simple example of the performance of SysIdentPy on a real world dataset. A more detailed study of this system will be published in the future.</p> <p>The F-16 Ground Vibration Test benchmark features a high order system with clearance and friction nonlinearities at the mounting interface of the payloads.</p> <p>The experimental data made available to the Workshop participants were acquired on a full-scale F-16 aircraft on the occasion of the Siemens LMS Ground Vibration Testing Master Class, held in September 2014 at the Saffraanberg military basis, Sint-Truiden, Belgium.</p> <p>During the test campaign, two dummy payloads were mounted at the wing tips to simulate the mass and inertia properties of real devices typically equipping an F-16 in \ufb02ight. The aircraft structure was instrumented with accelerometers. One shaker was attached underneath the right wing to apply input signals. The dominant source of nonlinearity in the structural dynamics was expected to originate from the mounting interfaces of the two payloads. These interfaces consist of T-shaped connecting elements on the payload side, slid through a rail attached to the wing side. A preliminary investigation showed that the back connection of the right-wing-to-payload interface was the predominant source of nonlinear distortions in the aircraft dynamics, and is therefore the focus of this benchmark study.</p> <p>A detailed formulation of the identification problem can be found here. All the provided files and information on the F-16 aircraft benchmark system are available for download here. This zip-file contains a detailed system description, the estimation and test data sets, and some pictures of the setup. The data is available in the .csv and .mat file format.</p> <p>Please refer to the F16 benchmark as:</p> <p>J.P. No\u00ebl and M. Schoukens, F-16 aircraft benchmark based on ground vibration test data, 2017 Workshop on Nonlinear System Identification Benchmarks, pp. 19-23, Brussels, Belgium, April 24-26, 2017.</p>"},{"location":"old_version/f_16_benchmark/#visualizating-the-data","title":"Visualizating the data\u00b6","text":""},{"location":"old_version/f_16_benchmark/#spliting-the-data","title":"Spliting the data\u00b6","text":""},{"location":"old_version/f_16_benchmark/#setting-the-input-lags","title":"Setting the input lags\u00b6","text":""},{"location":"old_version/f_16_benchmark/#model-training-and-evalutation","title":"Model training and evalutation\u00b6","text":""},{"location":"old_version/f_16_benchmark/#information-criteria-plot","title":"Information Criteria plot\u00b6","text":""},{"location":"old_version/fourier_basis_function/","title":"Fourier Basis Function","text":"<p>This example shows how changing or adding a new basis function could improve the model</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.metrics import root_relative_squared_error\n\nnp.seterr(all=\"ignore\")\nnp.random.seed(1)\n\n%matplotlib inline\n</pre> import numpy as np import matplotlib.pyplot as plt  from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function import Polynomial, Fourier from sysidentpy.utils.plotting import plot_results from sysidentpy.metrics import root_relative_squared_error  np.seterr(all=\"ignore\") np.random.seed(1)  %matplotlib inline In\u00a0[2]: Copied! <pre># Simulated system\ndef system_equation(y, u):\n    yk = (\n        (0.2 - 0.75 * np.cos(-y[0] ** 2)) * np.cos(y[0])\n        - (0.15 + 0.45 * np.cos(-y[0] ** 2)) * np.cos(y[1])\n        + np.cos(u[0])\n        + 0.2 * u[1]\n        + 0.7 * u[0] * u[1]\n    )\n    return yk\n\n\nrepetition = 5\nrandom_samples = 200\ntotal_time = repetition * random_samples\nn = np.arange(0, total_time)\n\n# Generating input\nx = np.random.normal(size=(random_samples,)).repeat(repetition)\n\n\n_, ax = plt.subplots(figsize=(12, 6))\nax.step(n, x)\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$x[n]$\", fontsize=18)\nplt.show()\n</pre> # Simulated system def system_equation(y, u):     yk = (         (0.2 - 0.75 * np.cos(-y[0] ** 2)) * np.cos(y[0])         - (0.15 + 0.45 * np.cos(-y[0] ** 2)) * np.cos(y[1])         + np.cos(u[0])         + 0.2 * u[1]         + 0.7 * u[0] * u[1]     )     return yk   repetition = 5 random_samples = 200 total_time = repetition * random_samples n = np.arange(0, total_time)  # Generating input x = np.random.normal(size=(random_samples,)).repeat(repetition)   _, ax = plt.subplots(figsize=(12, 6)) ax.step(n, x) ax.set_xlabel(\"$n$\", fontsize=18) ax.set_ylabel(\"$x[n]$\", fontsize=18) plt.show() In\u00a0[3]: Copied! <pre>y = np.empty_like(x)\n# Initial Conditions\ny0 = [0, 0]\n\n# Simulate it\ny[0:2] = y0\nfor i in range(2, len(y)):\n    y[i] = system_equation(\n        [y[i - 1], y[i - 2]], [x[i - 1], x[i - 2]]\n    ) + np.random.normal(scale=0.1)\n\n# Plot\n_, ax = plt.subplots(figsize=(12, 6))\nax.plot(n, y)\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.grid()\nplt.show()\n</pre> y = np.empty_like(x) # Initial Conditions y0 = [0, 0]  # Simulate it y[0:2] = y0 for i in range(2, len(y)):     y[i] = system_equation(         [y[i - 1], y[i - 2]], [x[i - 1], x[i - 2]]     ) + np.random.normal(scale=0.1)  # Plot _, ax = plt.subplots(figsize=(12, 6)) ax.plot(n, y) ax.set_xlabel(\"$n$\", fontsize=18) ax.set_ylabel(\"$y[n]$\", fontsize=18) ax.grid() plt.show() In\u00a0[4]: Copied! <pre># Noise free data\nynoise_free = y.copy()\n\n# Generate noise\nv = np.random.normal(scale=0.5, size=y.shape)\n\n# Data corrupted with noise\nynoisy = ynoise_free + v\n\n# Plot\n_, ax = plt.subplots(figsize=(14, 8))\nax.plot(n, ynoise_free, label=\"Noise-free data\")\nax.plot(n, ynoisy, label=\"Corrupted data\")\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.legend(fontsize=18)\nplt.show()\n</pre> # Noise free data ynoise_free = y.copy()  # Generate noise v = np.random.normal(scale=0.5, size=y.shape)  # Data corrupted with noise ynoisy = ynoise_free + v  # Plot _, ax = plt.subplots(figsize=(14, 8)) ax.plot(n, ynoise_free, label=\"Noise-free data\") ax.plot(n, ynoisy, label=\"Corrupted data\") ax.set_xlabel(\"$n$\", fontsize=18) ax.set_ylabel(\"$y[n]$\", fontsize=18) ax.legend(fontsize=18) plt.show() In\u00a0[5]: Copied! <pre>n_train = 700\n\n# Identification data\ny_train = ynoisy[:n_train].reshape(-1, 1)\nx_train = x[:n_train].reshape(-1, 1)\n\n# Validation data\ny_test = ynoise_free[n_train:].reshape(-1, 1)\nx_test = x[n_train:].reshape(-1, 1)\n</pre> n_train = 700  # Identification data y_train = ynoisy[:n_train].reshape(-1, 1) x_train = x[:n_train].reshape(-1, 1)  # Validation data y_test = ynoise_free[n_train:].reshape(-1, 1) x_test = x[n_train:].reshape(-1, 1) In\u00a0[6]: Copied! <pre>basis_function = Polynomial(degree=2)\nsysidentpy = FROLS(\n    order_selection=True,\n    n_info_values=70,\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\nsysidentpy.fit(X=x_train, y=y_train)\n\nyhat = sysidentpy.predict(X=x_test, y=y_test)\nfrols_loss = root_relative_squared_error(\n    y_test[sysidentpy.max_lag :], yhat[sysidentpy.max_lag :]\n)\nprint(frols_loss)\n\nplot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :])\n</pre> basis_function = Polynomial(degree=2) sysidentpy = FROLS(     order_selection=True,     n_info_values=70,     xlag=2,     ylag=2,     basis_function=basis_function,     model_type=\"NARMAX\", ) sysidentpy.fit(X=x_train, y=y_train)  yhat = sysidentpy.predict(X=x_test, y=y_test) frols_loss = root_relative_squared_error(     y_test[sysidentpy.max_lag :], yhat[sysidentpy.max_lag :] ) print(frols_loss)  plot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :]) <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\nc:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:569: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 15\n  self.info_values = self.information_criterion(reg_matrix, y)\n</pre> <pre>nan\n</pre> In\u00a0[7]: Copied! <pre>basis_function = Fourier(degree=2, n=2, p=2 * np.pi, ensemble=True)\nsysidentpy = FROLS(\n    order_selection=True,\n    n_info_values=70,\n    xlag=2,\n    ylag=2,  # the lags for all models will be 13\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\nsysidentpy.fit(X=x_train, y=y_train)\n\nyhat = sysidentpy.predict(X=x_test, y=y_test)\nfrols_loss = root_relative_squared_error(\n    y_test[sysidentpy.max_lag :], yhat[sysidentpy.max_lag :]\n)\nprint(frols_loss)\n\nplot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :])\n</pre> basis_function = Fourier(degree=2, n=2, p=2 * np.pi, ensemble=True) sysidentpy = FROLS(     order_selection=True,     n_info_values=70,     xlag=2,     ylag=2,  # the lags for all models will be 13     basis_function=basis_function,     model_type=\"NARMAX\", ) sysidentpy.fit(X=x_train, y=y_train)  yhat = sysidentpy.predict(X=x_test, y=y_test) frols_loss = root_relative_squared_error(     y_test[sysidentpy.max_lag :], yhat[sysidentpy.max_lag :] ) print(frols_loss)  plot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :]) <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre> <pre>0.3742244715879481\n</pre>"},{"location":"old_version/fourier_basis_function/#fourier-basis-function","title":"Fourier Basis Function\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"old_version/fourier_basis_function/#defining-the-system","title":"Defining the system\u00b6","text":""},{"location":"old_version/fourier_basis_function/#simulate-the-system","title":"Simulate the system\u00b6","text":""},{"location":"old_version/fourier_basis_function/#adding-noise-to-the-system","title":"Adding noise to the system\u00b6","text":""},{"location":"old_version/fourier_basis_function/#generating-training-and-test-data","title":"Generating training and test data\u00b6","text":""},{"location":"old_version/fourier_basis_function/#polynomial-basis-function","title":"Polynomial Basis Function\u00b6","text":"<p>As you can see bellow, using only the polynomial basis function with the following parameters results in a divergent model</p>"},{"location":"old_version/fourier_basis_function/#ensembling-a-fourier-basis-function","title":"Ensembling a Fourier Basis Function\u00b6","text":"<p>In this case, adding the Fourier Basis Function solves the problem and returns a model capable to predict the defined system</p>"},{"location":"old_version/fourier_basis_function/#important","title":"Important\u00b6","text":"<p>Currently you can't get the model representation using <code>sysidentpy.regressor_code</code> for Fourier NARX models. Actually, you can use the method, but the representation is not accurate because we don't make clear what are the regressors related to the polynomial or related to the fourier basis function. This is a improvement to be done in future updates!</p>"},{"location":"old_version/general_estimators/","title":"Building NARX models using general estimators","text":"<p>In this example we will create NARX models using different estimator like GradientBoostingRegressor, Bayesian Regression, Automatic Relevance Determination (ARD) Regression and Catboost</p> In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nfrom sysidentpy.metrics import mean_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.general_estimators import NARX\nfrom sklearn.linear_model import BayesianRidge, ARDRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom catboost import CatBoostRegressor\n\nfrom sysidentpy.basis_function._basis_function import Polynomial, Fourier\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</pre> import matplotlib.pyplot as plt from sysidentpy.metrics import mean_squared_error from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.general_estimators import NARX from sklearn.linear_model import BayesianRidge, ARDRegression from sklearn.ensemble import GradientBoostingRegressor from catboost import CatBoostRegressor  from sysidentpy.basis_function._basis_function import Polynomial, Fourier from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) In\u00a0[2]: Copied! <pre># simulated dataset\nx_train, x_valid, y_train, y_valid = get_siso_data(\n    n=10000, colored_noise=False, sigma=0.01, train_percentage=80\n)\n</pre> # simulated dataset x_train, x_valid, y_train, y_valid = get_siso_data(     n=10000, colored_noise=False, sigma=0.01, train_percentage=80 ) In\u00a0[3]: Copied! <pre>catboost = CatBoostRegressor(iterations=300, learning_rate=0.1, depth=6)\n</pre> catboost = CatBoostRegressor(iterations=300, learning_rate=0.1, depth=6) In\u00a0[4]: Copied! <pre>gb = GradientBoostingRegressor(\n    loss=\"quantile\",\n    alpha=0.90,\n    n_estimators=250,\n    max_depth=10,\n    learning_rate=0.1,\n    min_samples_leaf=9,\n    min_samples_split=9,\n)\n</pre> gb = GradientBoostingRegressor(     loss=\"quantile\",     alpha=0.90,     n_estimators=250,     max_depth=10,     learning_rate=0.1,     min_samples_leaf=9,     min_samples_split=9, ) In\u00a0[5]: Copied! <pre>def plot_results_tmp(y_valid, yhat):\n    _, ax = plt.subplots(figsize=(14, 8))\n    ax.plot(y_valid[:200], label=\"Data\", marker=\"o\")\n    ax.plot(yhat[:200], label=\"Prediction\", marker=\"*\")\n    ax.set_xlabel(\"$n$\", fontsize=18)\n    ax.set_ylabel(\"$y[n]$\", fontsize=18)\n    ax.grid()\n    ax.legend(fontsize=18)\n    plt.show()\n</pre> def plot_results_tmp(y_valid, yhat):     _, ax = plt.subplots(figsize=(14, 8))     ax.plot(y_valid[:200], label=\"Data\", marker=\"o\")     ax.plot(yhat[:200], label=\"Prediction\", marker=\"*\")     ax.set_xlabel(\"$n$\", fontsize=18)     ax.set_ylabel(\"$y[n]$\", fontsize=18)     ax.grid()     ax.legend(fontsize=18)     plt.show() In\u00a0[6]: Copied! <pre>catboost.fit(x_train, y_train, verbose=False)\nplot_results_tmp(y_valid, catboost.predict(x_valid))\n</pre> catboost.fit(x_train, y_train, verbose=False) plot_results_tmp(y_valid, catboost.predict(x_valid)) In\u00a0[7]: Copied! <pre>gb.fit(x_train, y_train.ravel())\nplot_results_tmp(y_valid, gb.predict(x_valid))\n</pre> gb.fit(x_train, y_train.ravel()) plot_results_tmp(y_valid, gb.predict(x_valid)) In\u00a0[8]: Copied! <pre>basis_function = Fourier(degree=1)\n\ncatboost_narx = NARX(\n    base_estimator=CatBoostRegressor(iterations=300, learning_rate=0.1, depth=8),\n    xlag=10,\n    ylag=10,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    fit_params={\"verbose\": False},\n)\n\ncatboost_narx.fit(X=x_train, y=y_train)\nyhat = catboost_narx.predict(X=x_valid, y=y_valid, steps_ahead=1)\nprint(\"MSE: \", mean_squared_error(y_valid, yhat))\nplot_results(y=y_valid, yhat=yhat, n=200)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> basis_function = Fourier(degree=1)  catboost_narx = NARX(     base_estimator=CatBoostRegressor(iterations=300, learning_rate=0.1, depth=8),     xlag=10,     ylag=10,     basis_function=basis_function,     model_type=\"NARMAX\",     fit_params={\"verbose\": False}, )  catboost_narx.fit(X=x_train, y=y_train) yhat = catboost_narx.predict(X=x_valid, y=y_valid, steps_ahead=1) print(\"MSE: \", mean_squared_error(y_valid, yhat)) plot_results(y=y_valid, yhat=yhat, n=200) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>MSE:  0.00022083855101248805\n</pre> In\u00a0[9]: Copied! <pre>basis_function = Fourier(degree=1)\n\ngb_narx = NARX(\n    base_estimator=GradientBoostingRegressor(\n        loss=\"quantile\",\n        alpha=0.90,\n        n_estimators=250,\n        max_depth=10,\n        learning_rate=0.1,\n        min_samples_leaf=9,\n        min_samples_split=9,\n    ),\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\ngb_narx.fit(X=x_train, y=y_train)\nyhat = gb_narx.predict(X=x_valid, y=y_valid)\nprint(mean_squared_error(y_valid, yhat))\n\nplot_results(y=y_valid, yhat=yhat, n=200)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> basis_function = Fourier(degree=1)  gb_narx = NARX(     base_estimator=GradientBoostingRegressor(         loss=\"quantile\",         alpha=0.90,         n_estimators=250,         max_depth=10,         learning_rate=0.1,         min_samples_leaf=9,         min_samples_split=9,     ),     xlag=2,     ylag=2,     basis_function=basis_function,     model_type=\"NARMAX\", )  gb_narx.fit(X=x_train, y=y_train) yhat = gb_narx.predict(X=x_valid, y=y_valid) print(mean_squared_error(y_valid, yhat))  plot_results(y=y_valid, yhat=yhat, n=200) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>0.0013519489212683146\n</pre> In\u00a0[10]: Copied! <pre>from sysidentpy.general_estimators import NARX\n\nARD_narx = NARX(\n    base_estimator=ARDRegression(),\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\nARD_narx.fit(X=x_train, y=y_train)\nyhat = ARD_narx.predict(X=x_valid, y=y_valid)\nprint(mean_squared_error(y_valid, yhat))\n\nplot_results(y=y_valid, yhat=yhat, n=200)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> from sysidentpy.general_estimators import NARX  ARD_narx = NARX(     base_estimator=ARDRegression(),     xlag=2,     ylag=2,     basis_function=basis_function,     model_type=\"NARMAX\", )  ARD_narx.fit(X=x_train, y=y_train) yhat = ARD_narx.predict(X=x_valid, y=y_valid) print(mean_squared_error(y_valid, yhat))  plot_results(y=y_valid, yhat=yhat, n=200) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>0.001087497212624252\n</pre> In\u00a0[11]: Copied! <pre>from sysidentpy.general_estimators import NARX\n\nBayesianRidge_narx = NARX(\n    base_estimator=BayesianRidge(),\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\nBayesianRidge_narx.fit(X=x_train, y=y_train)\nyhat = BayesianRidge_narx.predict(X=x_valid, y=y_valid)\nprint(mean_squared_error(y_valid, yhat))\n\nplot_results(y=y_valid, yhat=yhat, n=200)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> from sysidentpy.general_estimators import NARX  BayesianRidge_narx = NARX(     base_estimator=BayesianRidge(),     xlag=2,     ylag=2,     basis_function=basis_function,     model_type=\"NARMAX\", )  BayesianRidge_narx.fit(X=x_train, y=y_train) yhat = BayesianRidge_narx.predict(X=x_valid, y=y_valid) print(mean_squared_error(y_valid, yhat))  plot_results(y=y_valid, yhat=yhat, n=200) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>0.0010898810785159086\n</pre>"},{"location":"old_version/general_estimators/#building-narx-models-using-general-estimators","title":"Building NARX models using general estimators\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"old_version/general_estimators/#importance-of-the-narx-architecture","title":"Importance of the NARX architecture\u00b6","text":"<p>To get an idea of the importance of the NARX architecture, lets take a look in the performance of the models without the NARX configuration.</p>"},{"location":"old_version/general_estimators/#introducing-the-narx-configuration-using-sysidentpy","title":"Introducing the NARX configuration using SysIdentPy\u00b6","text":"<p>As you can see, you just need to pass the base estimator you want to the NARX class from SysIdentPy do build the NARX model! You can choose the lags of the input and output variables to build the regressor matrix.</p> <p>We keep the fit/predict method to make the process straightforward.</p>"},{"location":"old_version/general_estimators/#narx-with-catboost","title":"NARX with Catboost\u00b6","text":""},{"location":"old_version/general_estimators/#narx-with-gradient-boosting","title":"NARX with Gradient Boosting\u00b6","text":""},{"location":"old_version/general_estimators/#narx-with-ard","title":"NARX with ARD\u00b6","text":""},{"location":"old_version/general_estimators/#narx-with-bayesian-ridge","title":"NARX with Bayesian Ridge\u00b6","text":""},{"location":"old_version/general_estimators/#note","title":"Note\u00b6","text":"<p>Remember you can use n-steps-ahead prediction and NAR and NFIR models now. Check how to use it in their respective examples.</p>"},{"location":"old_version/identification_of_an_electromechanical_system/","title":"Identification of an electromechanical system","text":"In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) In\u00a0[2]: Copied! <pre>df1 = pd.read_csv(\"examples/datasets/x_cc.csv\")\ndf2 = pd.read_csv(\"examples/datasets/y_cc.csv\")\n</pre> df1 = pd.read_csv(\"examples/datasets/x_cc.csv\") df2 = pd.read_csv(\"examples/datasets/y_cc.csv\") In\u00a0[3]: Copied! <pre>df2[5000:80000].plot(figsize=(10, 4))\n</pre> df2[5000:80000].plot(figsize=(10, 4)) Out[3]: <pre>&lt;AxesSubplot:&gt;</pre> In\u00a0[4]: Copied! <pre># we will decimate the data using d=500 in this example\nx_train, x_valid = np.split(df1.iloc[::500].values, 2)\ny_train, y_valid = np.split(df2.iloc[::500].values, 2)\n</pre> # we will decimate the data using d=500 in this example x_train, x_valid = np.split(df1.iloc[::500].values, 2) y_train, y_valid = np.split(df2.iloc[::500].values, 2) In\u00a0[5]: Copied! <pre>basis_function = Polynomial(degree=2)\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=40,\n    extended_least_squares=False,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"bic\",\n    estimator=\"recursive_least_squares\",\n    basis_function=basis_function,\n)\n</pre> basis_function = Polynomial(degree=2)  model = FROLS(     order_selection=True,     n_info_values=40,     extended_least_squares=False,     ylag=2,     xlag=2,     info_criteria=\"bic\",     estimator=\"recursive_least_squares\",     basis_function=basis_function, ) <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre> In\u00a0[6]: Copied! <pre>model.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  plot_results(y=y_valid, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:569: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 15\n  self.info_values = self.information_criterion(reg_matrix, y)\n</pre> <pre>0.07912218629573997\n       Regressors   Parameters             ERR\n0          y(k-1)   1.3016E+00  9.86000384E-01\n1       x1(k-1)^2   1.0393E+02  7.94805130E-03\n2        y(k-2)^2   1.6288E-05  2.50905908E-03\n3   x1(k-1)y(k-1)  -1.2567E-01  1.43301039E-03\n4          y(k-2)  -5.0784E-01  1.02781443E-03\n5   x1(k-1)y(k-2)   5.6049E-02  5.35200312E-04\n6         x1(k-2)   3.4986E+02  2.79648078E-04\n7   x1(k-2)y(k-1)  -8.4030E-02  1.12211942E-04\n8  x1(k-2)x1(k-1)  -7.8186E+00  4.54743448E-05\n9   x1(k-2)y(k-2)   3.4050E-02  3.25346101E-05\n</pre> In\u00a0[7]: Copied! <pre>from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import (\n    RandomForestRegressor,\n    AdaBoostRegressor,\n    GradientBoostingRegressor,\n)\nfrom sklearn.naive_bayes import GaussianNB\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import BayesianRidge, ARDRegression\nfrom sysidentpy.general_estimators import NARX\n\nbasis_function = Polynomial(degree=2)\n\n\nestimators = [\n    (\n        \"KNeighborsRegressor\",\n        NARX(\n            base_estimator=KNeighborsRegressor(),\n            xlag=10,\n            ylag=10,\n            basis_function=basis_function,\n            model_type=\"NARMAX\",\n        ),\n    ),\n    (\n        \"NARX-DecisionTreeRegressor\",\n        NARX(\n            base_estimator=DecisionTreeRegressor(),\n            xlag=10,\n            ylag=10,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX-RandomForestRegressor\",\n        NARX(\n            base_estimator=RandomForestRegressor(n_estimators=200),\n            xlag=10,\n            ylag=10,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX-Catboost\",\n        NARX(\n            base_estimator=CatBoostRegressor(\n                iterations=800, learning_rate=0.1, depth=8\n            ),\n            xlag=10,\n            ylag=10,\n            basis_function=basis_function,\n            fit_params={\"verbose\": False},\n        ),\n    ),\n    (\n        \"NARX-ARD\",\n        NARX(\n            base_estimator=ARDRegression(),\n            xlag=10,\n            ylag=10,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"FROLS-Polynomial_NARX\",\n        FROLS(\n            order_selection=True,\n            n_info_values=50,\n            extended_least_squares=False,\n            ylag=10,\n            xlag=10,\n            info_criteria=\"bic\",\n            estimator=\"recursive_least_squares\",\n            basis_function=basis_function,\n        ),\n    ),\n]\n\nresultados = {}\nfor nome_do_modelo, modelo in estimators:\n    resultados[\"%s\" % (nome_do_modelo)] = []\n    modelo.fit(X=x_train, y=y_train)\n    yhat = modelo.predict(X=x_valid, y=y_valid)\n    result = root_relative_squared_error(\n        y_valid[modelo.max_lag :], yhat[modelo.max_lag :]\n    )\n    resultados[\"%s\" % (nome_do_modelo)].append(result)\n    print(nome_do_modelo, \"%.3f\" % np.mean(result))\n</pre> from sklearn.neighbors import KNeighborsRegressor from sklearn.svm import SVC, LinearSVC, NuSVC from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import (     RandomForestRegressor,     AdaBoostRegressor,     GradientBoostingRegressor, ) from sklearn.naive_bayes import GaussianNB from catboost import CatBoostRegressor from sklearn.linear_model import BayesianRidge, ARDRegression from sysidentpy.general_estimators import NARX  basis_function = Polynomial(degree=2)   estimators = [     (         \"KNeighborsRegressor\",         NARX(             base_estimator=KNeighborsRegressor(),             xlag=10,             ylag=10,             basis_function=basis_function,             model_type=\"NARMAX\",         ),     ),     (         \"NARX-DecisionTreeRegressor\",         NARX(             base_estimator=DecisionTreeRegressor(),             xlag=10,             ylag=10,             basis_function=basis_function,         ),     ),     (         \"NARX-RandomForestRegressor\",         NARX(             base_estimator=RandomForestRegressor(n_estimators=200),             xlag=10,             ylag=10,             basis_function=basis_function,         ),     ),     (         \"NARX-Catboost\",         NARX(             base_estimator=CatBoostRegressor(                 iterations=800, learning_rate=0.1, depth=8             ),             xlag=10,             ylag=10,             basis_function=basis_function,             fit_params={\"verbose\": False},         ),     ),     (         \"NARX-ARD\",         NARX(             base_estimator=ARDRegression(),             xlag=10,             ylag=10,             basis_function=basis_function,         ),     ),     (         \"FROLS-Polynomial_NARX\",         FROLS(             order_selection=True,             n_info_values=50,             extended_least_squares=False,             ylag=10,             xlag=10,             info_criteria=\"bic\",             estimator=\"recursive_least_squares\",             basis_function=basis_function,         ),     ), ]  resultados = {} for nome_do_modelo, modelo in estimators:     resultados[\"%s\" % (nome_do_modelo)] = []     modelo.fit(X=x_train, y=y_train)     yhat = modelo.predict(X=x_valid, y=y_valid)     result = root_relative_squared_error(         y_valid[modelo.max_lag :], yhat[modelo.max_lag :]     )     resultados[\"%s\" % (nome_do_modelo)].append(result)     print(nome_do_modelo, \"%.3f\" % np.mean(result)) <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre> <pre>KNeighborsRegressor 1.178\nNARX-DecisionTreeRegressor 0.263\nNARX-RandomForestRegressor 0.226\nNARX-Catboost 0.159\nNARX-ARD 0.072\nFROLS-Polynomial_NARX 0.046\n</pre> In\u00a0[8]: Copied! <pre>for aux_results, results in sorted(\n    resultados.items(), key=lambda x: np.mean(x[1]), reverse=False\n):\n    print(aux_results, np.mean(results))\n</pre> for aux_results, results in sorted(     resultados.items(), key=lambda x: np.mean(x[1]), reverse=False ):     print(aux_results, np.mean(results)) <pre>FROLS-Polynomial_NARX 0.045954494047661276\nNARX-ARD 0.07153877637255217\nNARX-Catboost 0.15909412163898615\nNARX-RandomForestRegressor 0.22640167414558635\nNARX-DecisionTreeRegressor 0.2631297612484716\nKNeighborsRegressor 1.17783796557386\n</pre>"},{"location":"old_version/identification_of_an_electromechanical_system/#identification-of-an-electromechanical-system","title":"Identification of an electromechanical system\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>More details about this data can be found in the following paper (in Portuguese): https://www.researchgate.net/publication/320418710_Identificacao_de_um_motorgerador_CC_por_meio_de_modelos_polinomiais_autorregressivos_e_redes_neurais_artificiais</p>"},{"location":"old_version/identification_of_an_electromechanical_system/#building-a-polynomial-narx-model","title":"Building a Polynomial NARX model\u00b6","text":""},{"location":"old_version/identification_of_an_electromechanical_system/#testing-different-autoregressive-models","title":"Testing different autoregressive models\u00b6","text":""},{"location":"old_version/information_criteria_examples/","title":"Information Criteria - Examples","text":"<p>Here we import the NARMAX model, the metric for model evaluation and the methods to generate sample data for tests. Also, we import pandas for specific usage.</p> In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) In\u00a0[2]: Copied! <pre>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.2, train_percentage=90\n)\n</pre> x_train, x_valid, y_train, y_valid = get_siso_data(     n=1000, colored_noise=False, sigma=0.2, train_percentage=90 ) <p>The idea is to show the impact of the information criteria to select the number of terms to compose the final model. You will se why it is an auxiliary tool and let the algorithm select the number of terms based on the minimum value is not a good idea when dealing with data highly corrupted by noise (even white noise)</p> <p>Note: You may find different results when running the examples. This is due the fact we are not setting a fixed random generator for the sample data. However, the main analysis remain.</p> In\u00a0[3]: Copied! <pre>basis_function = Polynomial(degree=2)\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    extended_least_squares=False,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</pre> basis_function = Polynomial(degree=2)  model = FROLS(     order_selection=True,     n_info_values=15,     extended_least_squares=False,     ylag=2,     xlag=2,     info_criteria=\"aic\",     estimator=\"least_squares\",     basis_function=basis_function, ) model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) plot_results(y=y_valid, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")  xaxis = np.arange(1, model.n_info_values + 1) plt.plot(xaxis, model.info_values) plt.xlabel(\"n_terms\") plt.ylabel(\"Information Criteria\") <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre> <pre>0.4013781100113083\n      Regressors  Parameters             ERR\n0        x1(k-2)  8.9927E-01  8.31470481E-01\n1         y(k-1)  2.0168E-01  4.10001326E-02\n2  x1(k-1)y(k-1)  9.2625E-02  2.71330347E-03\n</pre> Out[3]: <pre>Text(0, 0.5, 'Information Criteria')</pre> In\u00a0[4]: Copied! <pre>model.info_values\n</pre> model.info_values Out[4]: <pre>array([-2641.78942999, -2890.08785144, -2907.8115072 , -2907.62848919,\n       -2907.33194129, -2906.49131563, -2905.28985273, -2904.19842636,\n       -2902.82919405, -2901.13658068, -2899.20612214, -2897.22138478,\n       -2895.23592902, -2893.24339593, -2891.25015639])</pre> <p>As can be seen above, the minimum value make the algorithm choose a model with 5 terms. However, if you check the plot, 3 terms is the best choice. Increasing the number of terms from 3 upwards do not lead to a better model since the difference is very small.</p> <p>In this case, you should run the model again with the parameters n_terms=3! The ERR algorithm ordered the terms in a correct way, so you will get the exact model structure again!</p> In\u00a0[5]: Copied! <pre>basis_function = Polynomial(degree=2)\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    extended_least_squares=False,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aicc\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</pre> basis_function = Polynomial(degree=2)  model = FROLS(     order_selection=True,     n_info_values=15,     extended_least_squares=False,     ylag=2,     xlag=2,     info_criteria=\"aicc\",     estimator=\"least_squares\",     basis_function=basis_function, ) model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) plot_results(y=y_valid, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")  xaxis = np.arange(1, model.n_info_values + 1) plt.plot(xaxis, model.info_values) plt.xlabel(\"n_terms\") plt.ylabel(\"Information Criteria\") <pre>0.4013781100113083\n      Regressors  Parameters             ERR\n0        x1(k-2)  8.9927E-01  8.31470481E-01\n1         y(k-1)  2.0168E-01  4.10001326E-02\n2  x1(k-1)y(k-1)  9.2625E-02  2.71330347E-03\n</pre> <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre> Out[5]: <pre>Text(0, 0.5, 'Information Criteria')</pre> In\u00a0[6]: Copied! <pre>model.info_values\n</pre> model.info_values Out[6]: <pre>array([-2641.78496571, -2890.07444362, -2907.78466157, -2907.58369636,\n       -2907.26467671, -2906.39703954, -2905.16401003, -2904.03644661,\n       -2902.62649135, -2900.88855362, -2898.90815375, -2896.86884241,\n       -2894.82416432, -2892.76774474, -2890.7059387 ])</pre> <p>As can be seen above, the minimum value make the algorithm choose a model with 4 terms. AICc, however, have major differences compared with AIC when the number of samples is small.</p> <p>In this case, you should run the model again with the parameters n_terms=3! The ERR algorithm ordered the terms in a correct way, so you will get the exact model structure again!</p> In\u00a0[7]: Copied! <pre>basis_function = Polynomial(degree=2)\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    extended_least_squares=False,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"bic\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</pre> basis_function = Polynomial(degree=2)  model = FROLS(     order_selection=True,     n_info_values=15,     extended_least_squares=False,     ylag=2,     xlag=2,     info_criteria=\"bic\",     estimator=\"least_squares\",     basis_function=basis_function, ) model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) plot_results(y=y_valid, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")  xaxis = np.arange(1, model.n_info_values + 1) plt.plot(xaxis, model.info_values) plt.xlabel(\"n_terms\") plt.ylabel(\"Information Criteria\") <pre>0.4013781100113083\n      Regressors  Parameters             ERR\n0        x1(k-2)  8.9927E-01  8.31470481E-01\n1         y(k-1)  2.0168E-01  4.10001326E-02\n2  x1(k-1)y(k-1)  9.2625E-02  2.71330347E-03\n</pre> <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre> Out[7]: <pre>Text(0, 0.5, 'Information Criteria')</pre> In\u00a0[8]: Copied! <pre>model.info_values\n</pre> model.info_values Out[8]: <pre>array([-2636.98925993, -2880.4875113 , -2893.410997  , -2888.42780892,\n       -2883.33109095, -2877.69029522, -2871.68866225, -2865.79706581,\n       -2859.62766344, -2853.13487999, -2846.40425139, -2839.61934396,\n       -2832.83371813, -2826.04101497, -2819.24760536])</pre> <p>BIC did a better job in this case! The way it penalizes the model regarding the number of terms ensure that the minimum value here was exact the number of expected terms to compose the model. Good, but not always the best method!</p> In\u00a0[9]: Copied! <pre>basis_function = Polynomial(degree=2)\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    extended_least_squares=False,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"lilc\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</pre> basis_function = Polynomial(degree=2)  model = FROLS(     order_selection=True,     n_info_values=15,     extended_least_squares=False,     ylag=2,     xlag=2,     info_criteria=\"lilc\",     estimator=\"least_squares\",     basis_function=basis_function, ) model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) plot_results(y=y_valid, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")  xaxis = np.arange(1, model.n_info_values + 1) plt.plot(xaxis, model.info_values) plt.xlabel(\"n_terms\") plt.ylabel(\"Information Criteria\") <pre>0.4013781100113083\n      Regressors  Parameters             ERR\n0        x1(k-2)  8.9927E-01  8.31470481E-01\n1         y(k-1)  2.0168E-01  4.10001326E-02\n2  x1(k-1)y(k-1)  9.2625E-02  2.71330347E-03\n</pre> <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre> Out[9]: <pre>Text(0, 0.5, 'Information Criteria')</pre> In\u00a0[10]: Copied! <pre>model.info_values\n</pre> model.info_values Out[10]: <pre>array([-2639.95553475, -2886.42006095, -2902.30982147, -2900.29290822,\n       -2898.16246507, -2895.48794417, -2892.45258602, -2889.52726441,\n       -2886.32413686, -2882.79762824, -2879.03327446, -2875.21464186,\n       -2871.39529085, -2867.56886251, -2863.74172773])</pre> <p>LILC also includes spurious terms. Like AIC, it fails to automatically select the correct terms but you could select the right number based on the plot above!</p> In\u00a0[11]: Copied! <pre>basis_function = Polynomial(degree=2)\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    extended_least_squares=False,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"fpe\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</pre> basis_function = Polynomial(degree=2)  model = FROLS(     order_selection=True,     n_info_values=15,     extended_least_squares=False,     ylag=2,     xlag=2,     info_criteria=\"fpe\",     estimator=\"least_squares\",     basis_function=basis_function, ) model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) plot_results(y=y_valid, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")  xaxis = np.arange(1, model.n_info_values + 1) plt.plot(xaxis, model.info_values) plt.xlabel(\"n_terms\") plt.ylabel(\"Information Criteria\") <pre>0.4013781100113083\n      Regressors  Parameters             ERR\n0        x1(k-2)  8.9927E-01  8.31470481E-01\n1         y(k-1)  2.0168E-01  4.10001326E-02\n2  x1(k-1)y(k-1)  9.2625E-02  2.71330347E-03\n</pre> <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre> Out[11]: <pre>Text(0, 0.5, 'Information Criteria')</pre> In\u00a0[12]: Copied! <pre>model.info_values\n</pre> model.info_values Out[12]: <pre>array([-2641.78942917, -2890.08784482, -2907.81148488, -2907.62843628,\n       -2907.33183795, -2906.49113706, -2905.28956915, -2904.19800306,\n       -2902.82859134, -2901.1357539 , -2899.20502169, -2897.21995607,\n       -2895.2341125 , -2893.24112709, -2891.24736576])</pre> <p>FPE also failed to automatically select the right number of terms! But, as we pointed out before, Information Criteria is an auxiliary tool! If you look at the plots, all the methods allows you to choose the right numbers of terms!</p>"},{"location":"old_version/information_criteria_examples/#information-criteria-examples","title":"Information Criteria - Examples\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"old_version/information_criteria_examples/#comparing-different-information-criteria-methods","title":"Comparing different information criteria methods\u00b6","text":""},{"location":"old_version/information_criteria_examples/#generating-sample-data","title":"Generating sample data\u00b6","text":"<p>The data is generated by simulating the following model: $y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_{k}$</p> <p>If colored_noise is set to True:</p> <p>$e_{k} = 0.8\\nu_{k-1} + \\nu_{k}$</p> <p>where $x$ is a uniformly distributed random variable and $\\nu$ is a gaussian distributed variable with $\\mu=0$ and $\\sigma=0.1$</p> <p>In the next example we will generate a data with 3000 samples with white noise and selecting 90% of the data to train the model.</p>"},{"location":"old_version/information_criteria_examples/#aic","title":"AIC\u00b6","text":""},{"location":"old_version/information_criteria_examples/#aicc","title":"AICc\u00b6","text":""},{"location":"old_version/information_criteria_examples/#bic","title":"BIC\u00b6","text":""},{"location":"old_version/information_criteria_examples/#lilc","title":"LILC\u00b6","text":""},{"location":"old_version/information_criteria_examples/#fpe","title":"FPE\u00b6","text":""},{"location":"old_version/information_criteria_examples/#important-note","title":"Important Note\u00b6","text":"<p>Here we are dealing with a known model structure! Concerning real data, we do not know the right number of terms so the methods above stands as excellent tools to help you out!</p> <p>If you check the metrics above, even with the models with more terms, you will see excellent metrics! But System Identification always search for the best model structure! Model Structure Selection is the core of NARMAX methods! In this respect, the examples are to show basic concepts and how the algorithms work!</p>"},{"location":"old_version/load_forecasting_benchmark/","title":"Load forecasting benchmark","text":"<p>We will compare a 1-step ahead forecaster on electricity consumption of a building. The config of the neuralprophet model was taken from the neuralprophet documentation (https://neuralprophet.com/html/example_links/energy_data_example.html)</p> <p>The training will occur on 80% of the data, reserving the last 20% for the validation.</p> <p>Note: the data used in this example can be found in neuralprophet github.</p> In\u00a0[1]: Copied! <pre>from warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.model_structure_selection import AOLS\nfrom sysidentpy.model_structure_selection import MetaMSS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.neural_network import NARXNN\nfrom sysidentpy.metrics import mean_squared_error\n\nfrom sktime.datasets import load_airline\nfrom neuralprophet import NeuralProphet\nfrom neuralprophet import set_random_seed\n\n\nsimplefilter(\"ignore\", FutureWarning)\nnp.seterr(all=\"ignore\")\n\n%matplotlib inline\n\nloss = mean_squared_error\n\ndata_location = r\".\\datasets\"\n</pre> from warnings import simplefilter import matplotlib.pyplot as plt import numpy as np import pandas as pd  from sysidentpy.model_structure_selection import FROLS from sysidentpy.model_structure_selection import AOLS from sysidentpy.model_structure_selection import MetaMSS from sysidentpy.basis_function import Polynomial from sysidentpy.utils.plotting import plot_results from sysidentpy.neural_network import NARXNN from sysidentpy.metrics import mean_squared_error  from sktime.datasets import load_airline from neuralprophet import NeuralProphet from neuralprophet import set_random_seed   simplefilter(\"ignore\", FutureWarning) np.seterr(all=\"ignore\")  %matplotlib inline  loss = mean_squared_error  data_location = r\".\\datasets\" <pre>c:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\datatypes\\_series\\_check.py:43: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\datatypes\\_panel\\_check.py:45: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\datatypes\\_panel\\_check.py:46: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  VALID_MULTIINDEX_TYPES = (pd.Int64Index, pd.RangeIndex)\n</pre> In\u00a0[2]: Copied! <pre>files = [\"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760))\ndf[\"y\"] = raw.iloc[:, 0].values\n\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\n\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\n\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy = FROLS(\n    order_selection=True,\n    info_criteria=\"bic\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n)\nsysidentpy.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])\n\nyhat = sysidentpy.predict(X=x_test, y=y_test, steps_ahead=1)\nsysidentpy_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy.max_lag :]),\n)\nprint(sysidentpy_loss)\n\n\nplot_results(y=y_test[-504:], yhat=yhat[-504:], n=504, figsize=(18, 8))\n</pre> files = [\"\\SanFrancisco_Hospital.csv\"] raw = pd.read_csv(data_location + files[0]) df = pd.DataFrame() df[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760)) df[\"y\"] = raw.iloc[:, 0].values  df_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]  y = df[\"y\"].values.reshape(-1, 1) y_train = df_train[\"y\"].values.reshape(-1, 1) y_test = df_val[\"y\"].values.reshape(-1, 1)  x_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1) x_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)  basis_function = Polynomial(degree=1) sysidentpy = FROLS(     order_selection=True,     info_criteria=\"bic\",     estimator=\"least_squares\",     basis_function=basis_function, ) sysidentpy.fit(X=x_train, y=y_train) x_test = np.concatenate([x_train[-sysidentpy.max_lag :], x_test]) y_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])  yhat = sysidentpy.predict(X=x_test, y=y_test, steps_ahead=1) sysidentpy_loss = loss(     pd.Series(y_test.flatten()[sysidentpy.max_lag :]),     pd.Series(yhat.flatten()[sysidentpy.max_lag :]), ) print(sysidentpy_loss)   plot_results(y=y_test[-504:], yhat=yhat[-504:], n=504, figsize=(18, 8)) <pre>c:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:569: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 5\n  self.info_values = self.information_criterion(reg_matrix, y)\n</pre> <pre>4183.359498155755\n</pre> In\u00a0[3]: Copied! <pre>df.shape\n</pre> df.shape Out[3]: <pre>(8760, 2)</pre> In\u00a0[10]: Copied! <pre>files = [\"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760))\ndf[\"y\"] = raw.iloc[:, 0].values\n\nx_train, x_test, x_validation, _ = np.split(df[\"ds\"].dt.hour.values, [5000, 7008, 8760])\ny_train, y_test, y_validation, _ = np.split(df[\"y\"].values, [5000, 7008, 8760])\n\nx_train = x_train.reshape(-1, 1)\nx_test = x_test.reshape(-1, 1)\nx_validation = x_validation.reshape(-1, 1)\n\ny_train = y_train.reshape(-1, 1)\ny_test = y_test.reshape(-1, 1)\ny_validation = y_validation.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy_metamss = MetaMSS(\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    estimator=\"least_squares\",\n    steps_ahead=1,\n    n_agents=15,\n    random_state=42,\n)\nsysidentpy_metamss.fit(X=x_train, X_test=x_test, y=y_train, y_test=y_test)\nx_validation = np.concatenate([x_test[-sysidentpy_metamss.max_lag :], x_validation])\ny_validation = np.concatenate([y_test[-sysidentpy_metamss.max_lag :], y_validation])\n\nyhat = sysidentpy_metamss.predict(X=x_validation, y=y_validation, steps_ahead=1)\nmetamss_loss = loss(\n    pd.Series(y_validation.flatten()[sysidentpy_metamss.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]),\n)\nprint(metamss_loss)\n\n\nplot_results(y=y_validation[:700], yhat=yhat[:700], n=504, figsize=(18, 8))\n</pre> files = [\"\\SanFrancisco_Hospital.csv\"] raw = pd.read_csv(data_location + files[0]) df = pd.DataFrame() df[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760)) df[\"y\"] = raw.iloc[:, 0].values  x_train, x_test, x_validation, _ = np.split(df[\"ds\"].dt.hour.values, [5000, 7008, 8760]) y_train, y_test, y_validation, _ = np.split(df[\"y\"].values, [5000, 7008, 8760])  x_train = x_train.reshape(-1, 1) x_test = x_test.reshape(-1, 1) x_validation = x_validation.reshape(-1, 1)  y_train = y_train.reshape(-1, 1) y_test = y_test.reshape(-1, 1) y_validation = y_validation.reshape(-1, 1)  basis_function = Polynomial(degree=1) sysidentpy_metamss = MetaMSS(     xlag=2,     ylag=2,     basis_function=basis_function,     estimator=\"least_squares\",     steps_ahead=1,     n_agents=15,     random_state=42, ) sysidentpy_metamss.fit(X=x_train, X_test=x_test, y=y_train, y_test=y_test) x_validation = np.concatenate([x_test[-sysidentpy_metamss.max_lag :], x_validation]) y_validation = np.concatenate([y_test[-sysidentpy_metamss.max_lag :], y_validation])  yhat = sysidentpy_metamss.predict(X=x_validation, y=y_validation, steps_ahead=1) metamss_loss = loss(     pd.Series(y_validation.flatten()[sysidentpy_metamss.max_lag :]),     pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]), ) print(metamss_loss)   plot_results(y=y_validation[:700], yhat=yhat[:700], n=504, figsize=(18, 8)) <pre>5264.434488104671\n</pre> In\u00a0[11]: Copied! <pre>set_random_seed(42)\nfiles = [\"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760))\ndf[\"y\"] = raw.iloc[:, 0].values\n\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\n\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\n\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\nbasis_function = Polynomial(degree=1)\nsysidentpy_AOLS = AOLS(xlag=2, ylag=2, basis_function=basis_function)\nsysidentpy_AOLS.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy_AOLS.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])\n\nyhat = sysidentpy_AOLS.predict(X=x_test, y=y_test, steps_ahead=1)\naols_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]),\n)\nprint(aols_loss)\n\n\nplot_results(y=y_test[-504:], yhat=yhat[-504:], n=504, figsize=(18, 8))\n</pre> set_random_seed(42) files = [\"\\SanFrancisco_Hospital.csv\"] raw = pd.read_csv(data_location + files[0]) df = pd.DataFrame() df[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760)) df[\"y\"] = raw.iloc[:, 0].values  df_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]  y = df[\"y\"].values.reshape(-1, 1) y_train = df_train[\"y\"].values.reshape(-1, 1) y_test = df_val[\"y\"].values.reshape(-1, 1)  x_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1) x_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1) basis_function = Polynomial(degree=1) sysidentpy_AOLS = AOLS(xlag=2, ylag=2, basis_function=basis_function) sysidentpy_AOLS.fit(X=x_train, y=y_train) x_test = np.concatenate([x_train[-sysidentpy_AOLS.max_lag :], x_test]) y_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])  yhat = sysidentpy_AOLS.predict(X=x_test, y=y_test, steps_ahead=1) aols_loss = loss(     pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),     pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]), ) print(aols_loss)   plot_results(y=y_test[-504:], yhat=yhat[-504:], n=504, figsize=(18, 8)) <pre>5264.42917196841\n</pre> In\u00a0[6]: Copied! <pre>set_random_seed(42)\n\n# set_log_level(\"ERROR\")\nfiles = [\"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760))\ndf[\"y\"] = raw.iloc[:, 0].values\n\nm = NeuralProphet(\n    n_lags=24, ar_sparsity=0.5, num_hidden_layers=2, d_hidden=20, learning_rate=0.001\n)\nmetrics = m.fit(df, freq=\"H\", valid_p=0.2)\n\ndf_train, df_val = m.split_df(df, valid_p=0.2)\nm.test(df_val)\n\nfuture = m.make_future_dataframe(df_val, n_historic_predictions=True)\nforecast = m.predict(future)\n# fig = m.plot(forecast)\nprint(loss(forecast[\"y\"][24:-1], forecast[\"yhat1\"][24:-1]))\n\nneuralprophet_loss = loss(forecast[\"y\"][24:-1], forecast[\"yhat1\"][24:-1])\n</pre> set_random_seed(42)  # set_log_level(\"ERROR\") files = [\"\\SanFrancisco_Hospital.csv\"] raw = pd.read_csv(data_location + files[0]) df = pd.DataFrame() df[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=(8760)) df[\"y\"] = raw.iloc[:, 0].values  m = NeuralProphet(     n_lags=24, ar_sparsity=0.5, num_hidden_layers=2, d_hidden=20, learning_rate=0.001 ) metrics = m.fit(df, freq=\"H\", valid_p=0.2)  df_train, df_val = m.split_df(df, valid_p=0.2) m.test(df_val)  future = m.make_future_dataframe(df_val, n_historic_predictions=True) forecast = m.predict(future) # fig = m.plot(forecast) print(loss(forecast[\"y\"][24:-1], forecast[\"yhat1\"][24:-1]))  neuralprophet_loss = loss(forecast[\"y\"][24:-1], forecast[\"yhat1\"][24:-1]) <pre>WARNING: nprophet - fit: Parts of code may break if using other than daily data.\n</pre> <pre>04-26 19:54:06 - WARNING - Parts of code may break if using other than daily data.\n</pre> <pre>INFO: nprophet.utils - set_auto_seasonalities: Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n</pre> <pre>04-26 19:54:06 - INFO - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n</pre> <pre>INFO: nprophet.config - set_auto_batch_epoch: Auto-set batch_size to 32\n</pre> <pre>04-26 19:54:06 - INFO - Auto-set batch_size to 32\n</pre> <pre>INFO: nprophet.config - set_auto_batch_epoch: Auto-set epochs to 7\n</pre> <pre>04-26 19:54:06 - INFO - Auto-set epochs to 7\n</pre> <pre>Epoch[7/7]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:03&lt;00:00,  1.82it/s, SmoothL1Loss=0.0102, MAE=81.6, RegLoss=0.011] \nINFO: nprophet - _evaluate: Validation metrics:    SmoothL1Loss    MAE\n1         0.011 84.733\n</pre> <pre>04-26 19:54:10 - INFO - Validation metrics:    SmoothL1Loss    MAE\n1         0.011 84.733\n11397.103026422525\n</pre> In\u00a0[7]: Copied! <pre>plt.figure(figsize=(18, 8))\nplt.plot(forecast[\"y\"][-504:], \"ro-\")\nplt.plot(forecast[\"yhat1\"][-504:], \"k*-\")\n</pre> plt.figure(figsize=(18, 8)) plt.plot(forecast[\"y\"][-504:], \"ro-\") plt.plot(forecast[\"yhat1\"][-504:], \"k*-\") Out[7]: <pre>[&lt;matplotlib.lines.Line2D at 0x237847417f0&gt;]</pre> In\u00a0[8]: Copied! <pre>results = {\n    \"SysIdentPy - FROLS\": sysidentpy_loss,\n    \"SysIdentPy (AOLS)\": aols_loss,\n    \"SysIdentPy (MetaMSS)\": metamss_loss,\n    \"NeuralProphet\": neuralprophet_loss,\n}\n\nsorted(results.items(), key=lambda result: result[1])\n</pre> results = {     \"SysIdentPy - FROLS\": sysidentpy_loss,     \"SysIdentPy (AOLS)\": aols_loss,     \"SysIdentPy (MetaMSS)\": metamss_loss,     \"NeuralProphet\": neuralprophet_loss, }  sorted(results.items(), key=lambda result: result[1]) Out[8]: <pre>[('SysIdentPy - FROLS', 4183.359498155755),\n ('SysIdentPy (MetaMSS)', 5264.429171346123),\n ('SysIdentPy (AOLS)', 5264.42917196841),\n ('NeuralProphet', 11397.103026422525)]</pre>"},{"location":"old_version/load_forecasting_benchmark/#load-forecasting-benchmark","title":"Load forecasting benchmark\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"old_version/load_forecasting_benchmark/#note","title":"Note\u00b6","text":"<p>The following example is not intended to say that one library is better than another. The main focus of these examples is to show that SysIdentPy can be a good alternative for people looking to model time series.</p> <p>We will compare the results obtained against neural prophet library.</p> <p>For the sake of brevity, from SysIdentPy only the MetaMSS, AOLS and FROLS (with polynomial base function) methods will be used. See the SysIdentPy documentation to learn other ways of modeling with the library.</p>"},{"location":"old_version/load_forecasting_benchmark/#benchmark-results","title":"Benchmark results:\u00b6","text":"No. Package Mean Squared Error 1 SysIdentPy (FROLS) 4183 2 SysIdentPy (MetaMSS) 5264 3 SysIdentPy (AOLS) 5264 4 NeuralProphet 11471"},{"location":"old_version/load_forecasting_benchmark/#frols","title":"FROLS\u00b6","text":""},{"location":"old_version/load_forecasting_benchmark/#metamss","title":"MetaMSS\u00b6","text":""},{"location":"old_version/load_forecasting_benchmark/#aols","title":"AOLS\u00b6","text":""},{"location":"old_version/load_forecasting_benchmark/#neural-prophet","title":"Neural Prophet\u00b6","text":""},{"location":"old_version/metamss/","title":"Meta-Model Structure Selection (MetaMSS) algorithm for building Polynomial NARX models","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import MetaMSS, FROLS\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt from sysidentpy.model_structure_selection import MetaMSS, FROLS from sysidentpy.metrics import root_relative_squared_error from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) In\u00a0[2]: Copied! <pre>df1 = pd.read_csv(\"./datasets/x_cc.csv\")\ndf2 = pd.read_csv(\"./datasets/y_cc.csv\")\n\ndf2[5000:80000].plot(figsize=(10, 4))\n</pre> df1 = pd.read_csv(\"./datasets/x_cc.csv\") df2 = pd.read_csv(\"./datasets/y_cc.csv\")  df2[5000:80000].plot(figsize=(10, 4)) Out[2]: <pre>&lt;Axes: &gt;</pre> In\u00a0[3]: Copied! <pre>df1.iloc[::500].values.shape\n</pre> df1.iloc[::500].values.shape Out[3]: <pre>(1000, 1)</pre> <p>We will decimate the data using d=500 in this example. Besides, we separate the MetaMSS data to use the same amount of samples in the prediction validation. Because MetaMSS need a train and test data to optimize the parameters of the model, in this case, we'll use 400 samples to train instead of 500 samples used for the other models.</p> In\u00a0[4]: Copied! <pre>x_train_meta, x_test_meta, x_validation_meta, _ = np.split(\n    df1.iloc[::500].values, [400, 500, 1000]\n)\ny_train_meta, y_test_meta, y_validation_meta, _ = np.split(\n    df2.iloc[::500].values, [400, 500, 1000]\n)\n\nx_train, x_test = np.split(df1.iloc[::500].values, 2)\ny_train, y_test = np.split(df2.iloc[::500].values, 2)\n</pre> x_train_meta, x_test_meta, x_validation_meta, _ = np.split(     df1.iloc[::500].values, [400, 500, 1000] ) y_train_meta, y_test_meta, y_validation_meta, _ = np.split(     df2.iloc[::500].values, [400, 500, 1000] )  x_train, x_test = np.split(df1.iloc[::500].values, 2) y_train, y_test = np.split(df2.iloc[::500].values, 2) In\u00a0[5]: Copied! <pre>basis_function = Polynomial(degree=2)\n\nmodel = MetaMSS(\n    norm=-2,\n    xlag=3,\n    ylag=3,\n    estimator=\"recursive_least_squares\",\n    k_agents_percent=10,\n    estimate_parameter=True,\n    maxiter=30,\n    n_agents=10,\n    loss_func=\"metamss_loss\",\n    basis_function=basis_function,\n    random_state=42,\n)\nmodel.fit(\n    X=x_train_meta, y=y_train_meta, X_test=x_validation_meta, y_test=y_validation_meta\n)\n</pre> basis_function = Polynomial(degree=2)  model = MetaMSS(     norm=-2,     xlag=3,     ylag=3,     estimator=\"recursive_least_squares\",     k_agents_percent=10,     estimate_parameter=True,     maxiter=30,     n_agents=10,     loss_func=\"metamss_loss\",     basis_function=basis_function,     random_state=42, ) model.fit(     X=x_train_meta, y=y_train_meta, X_test=x_validation_meta, y_test=y_validation_meta ) <pre>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use MetaMSS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: You will not need to pass X_test and y_test in v0.4.0. \n You'll have to use MetaMSS(test_size=0.25) instead. \n This change will make easier to use the MetaMSS model and will follow the same structure of the other methods.\n  warnings.warn(message, FutureWarning)\nc:\\Users\\wilso\\miniconda3\\envs\\v0.3.3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\nc:\\Users\\wilso\\miniconda3\\envs\\v0.3.3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\narmax_base.py:719: RuntimeWarning: overflow encountered in power\n  regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\meta_model_structure_selection.py:465: RuntimeWarning: overflow encountered in square\n  sum_of_squared_residues = np.sum(residues**2)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\metrics\\_regression.py:216: RuntimeWarning: overflow encountered in square\n  numerator = np.sum(np.square((yhat - y)))\nc:\\Users\\wilso\\miniconda3\\envs\\v0.3.3\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2590: RuntimeWarning: divide by zero encountered in power\n  absx **= ord\n</pre> Out[5]: <pre>&lt;sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS at 0x1c4772bfd90&gt;</pre> In\u00a0[6]: Copied! <pre>yhat = model.predict(X=x_validation_meta, y=y_validation_meta, steps_ahead=None)\nrrse = root_relative_squared_error(y_validation_meta, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_validation_meta, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_validation_meta, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_validation_meta, yhat, x_validation_meta)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> yhat = model.predict(X=x_validation_meta, y=y_validation_meta, steps_ahead=None) rrse = root_relative_squared_error(y_validation_meta, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  plot_results(y=y_validation_meta, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_validation_meta, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_validation_meta, yhat, x_validation_meta) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>0.0333419637006627\n        Regressors   Parameters             ERR\n0                1  -3.5513E+02  0.00000000E+00\n1           y(k-1)   1.4835E+00  0.00000000E+00\n2           y(k-2)  -5.3744E-01  0.00000000E+00\n3          x1(k-2)   2.6056E+02  0.00000000E+00\n4          x1(k-3)  -4.6979E+00  0.00000000E+00\n5     y(k-3)y(k-1)   1.3104E-05  0.00000000E+00\n6    x1(k-1)y(k-1)  -1.6041E-01  0.00000000E+00\n7    x1(k-2)y(k-1)  -6.8079E-02  0.00000000E+00\n8    x1(k-3)y(k-1)   1.3481E-02  0.00000000E+00\n9     y(k-3)y(k-2)  -1.9122E-05  0.00000000E+00\n10   x1(k-1)y(k-2)   9.1429E-02  0.00000000E+00\n11   x1(k-2)y(k-2)   3.5621E-02  0.00000000E+00\n12   x1(k-3)y(k-2)   5.7288E-03  0.00000000E+00\n13        y(k-3)^2   1.3004E-05  0.00000000E+00\n14   x1(k-1)y(k-3)  -1.9402E-02  0.00000000E+00\n15   x1(k-2)y(k-3)  -8.7897E-03  0.00000000E+00\n16       x1(k-1)^2   1.1771E+02  0.00000000E+00\n17  x1(k-2)x1(k-1)  -2.4822E+00  0.00000000E+00\n18  x1(k-3)x1(k-1)   2.8744E+00  0.00000000E+00\n19       x1(k-3)^2  -2.3489E+01  0.00000000E+00\n</pre> In\u00a0[7]: Copied! <pre># Plotting the evolution of the agents\nplt.plot(model.best_by_iter)\nmodel.best_by_iter[-1]\n</pre> # Plotting the evolution of the agents plt.plot(model.best_by_iter) model.best_by_iter[-1] Out[7]: <pre>0.003898009726092222</pre> In\u00a0[8]: Copied! <pre># You have access to all tested models\n# model.tested_models\n</pre> # You have access to all tested models # model.tested_models In\u00a0[9]: Copied! <pre>from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import (\n    RandomForestRegressor,\n    AdaBoostRegressor,\n    GradientBoostingRegressor,\n)\nfrom sklearn.naive_bayes import GaussianNB\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import BayesianRidge, ARDRegression\nfrom sysidentpy.general_estimators import NARX\n\nxlag = ylag = 10\n\nestimators = [\n    (\n        \"NARX_KNeighborsRegressor\",\n        NARX(\n            base_estimator=KNeighborsRegressor(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX_DecisionTreeRegressor\",\n        NARX(\n            base_estimator=DecisionTreeRegressor(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX_RandomForestRegressor\",\n        NARX(\n            base_estimator=RandomForestRegressor(n_estimators=200),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX_Catboost\",\n        NARX(\n            base_estimator=CatBoostRegressor(\n                iterations=800, learning_rate=0.1, depth=8\n            ),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n            fit_params={\"verbose\": False},\n        ),\n    ),\n    (\n        \"NARX_ARD\",\n        NARX(\n            base_estimator=ARDRegression(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"FROLS-Polynomial_NARX\",\n        FROLS(\n            order_selection=True,\n            n_info_values=50,\n            extended_least_squares=False,\n            ylag=ylag,\n            xlag=xlag,\n            info_criteria=\"bic\",\n            estimator=\"recursive_least_squares\",\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"MetaMSS\",\n        MetaMSS(\n            norm=-2,\n            xlag=xlag,\n            ylag=ylag,\n            estimator=\"recursive_least_squares\",\n            k_agents_percent=10,\n            estimate_parameter=True,\n            maxiter=20,\n            n_agents=15,\n            loss_func=\"metamss_loss\",\n            basis_function=basis_function,\n            random_state=42,\n        ),\n    ),\n]\n\n\nresultados = {}\nfor nome_do_modelo, modelo in estimators:\n    resultados[\"%s\" % (nome_do_modelo)] = []\n    if nome_do_modelo == \"MetaMSS\":\n        modelo.fit(X=x_train, y=y_train, X_test=x_test, y_test=y_test)\n        yhat = modelo.predict(X=x_validation_meta, y=y_validation_meta)\n    else:\n        modelo.fit(X=x_train, y=y_train)\n        yhat = modelo.predict(X=x_test, y=y_test)\n    if nome_do_modelo == \"MetaMSS\":\n        result = root_relative_squared_error(\n            y_validation_meta[modelo.max_lag :], yhat[modelo.max_lag :]\n        )\n    if nome_do_modelo == \"FROLS-Polynomial_NARX\":\n        result = root_relative_squared_error(\n            y_test[modelo.max_lag :], yhat[modelo.max_lag :]\n        )\n    else:\n        result = root_relative_squared_error(y_test, yhat)\n    resultados[\"%s\" % (nome_do_modelo)].append(result)\n    print(nome_do_modelo, \"%.3f\" % np.mean(result))\n</pre> from sklearn.neighbors import KNeighborsRegressor from sklearn.svm import SVC, LinearSVC, NuSVC from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import (     RandomForestRegressor,     AdaBoostRegressor,     GradientBoostingRegressor, ) from sklearn.naive_bayes import GaussianNB from catboost import CatBoostRegressor from sklearn.linear_model import BayesianRidge, ARDRegression from sysidentpy.general_estimators import NARX  xlag = ylag = 10  estimators = [     (         \"NARX_KNeighborsRegressor\",         NARX(             base_estimator=KNeighborsRegressor(),             xlag=xlag,             ylag=ylag,             basis_function=basis_function,         ),     ),     (         \"NARX_DecisionTreeRegressor\",         NARX(             base_estimator=DecisionTreeRegressor(),             xlag=xlag,             ylag=ylag,             basis_function=basis_function,         ),     ),     (         \"NARX_RandomForestRegressor\",         NARX(             base_estimator=RandomForestRegressor(n_estimators=200),             xlag=xlag,             ylag=ylag,             basis_function=basis_function,         ),     ),     (         \"NARX_Catboost\",         NARX(             base_estimator=CatBoostRegressor(                 iterations=800, learning_rate=0.1, depth=8             ),             xlag=xlag,             ylag=ylag,             basis_function=basis_function,             fit_params={\"verbose\": False},         ),     ),     (         \"NARX_ARD\",         NARX(             base_estimator=ARDRegression(),             xlag=xlag,             ylag=ylag,             basis_function=basis_function,         ),     ),     (         \"FROLS-Polynomial_NARX\",         FROLS(             order_selection=True,             n_info_values=50,             extended_least_squares=False,             ylag=ylag,             xlag=xlag,             info_criteria=\"bic\",             estimator=\"recursive_least_squares\",             basis_function=basis_function,         ),     ),     (         \"MetaMSS\",         MetaMSS(             norm=-2,             xlag=xlag,             ylag=ylag,             estimator=\"recursive_least_squares\",             k_agents_percent=10,             estimate_parameter=True,             maxiter=20,             n_agents=15,             loss_func=\"metamss_loss\",             basis_function=basis_function,             random_state=42,         ),     ), ]   resultados = {} for nome_do_modelo, modelo in estimators:     resultados[\"%s\" % (nome_do_modelo)] = []     if nome_do_modelo == \"MetaMSS\":         modelo.fit(X=x_train, y=y_train, X_test=x_test, y_test=y_test)         yhat = modelo.predict(X=x_validation_meta, y=y_validation_meta)     else:         modelo.fit(X=x_train, y=y_train)         yhat = modelo.predict(X=x_test, y=y_test)     if nome_do_modelo == \"MetaMSS\":         result = root_relative_squared_error(             y_validation_meta[modelo.max_lag :], yhat[modelo.max_lag :]         )     if nome_do_modelo == \"FROLS-Polynomial_NARX\":         result = root_relative_squared_error(             y_test[modelo.max_lag :], yhat[modelo.max_lag :]         )     else:         result = root_relative_squared_error(y_test, yhat)     resultados[\"%s\" % (nome_do_modelo)].append(result)     print(nome_do_modelo, \"%.3f\" % np.mean(result)) <pre>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use MetaMSS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre> <pre>NARX_KNeighborsRegressor 1.165\nNARX_DecisionTreeRegressor 0.257\nNARX_RandomForestRegressor 0.191\nNARX_Catboost 0.157\nNARX_ARD 0.070\nFROLS-Polynomial_NARX 0.046\n</pre> <pre>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: You will not need to pass X_test and y_test in v0.4.0. \n You'll have to use MetaMSS(test_size=0.25) instead. \n This change will make easier to use the MetaMSS model and will follow the same structure of the other methods.\n  warnings.warn(message, FutureWarning)\nc:\\Users\\wilso\\miniconda3\\envs\\v0.3.3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\narmax_base.py:719: RuntimeWarning: overflow encountered in power\n  regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\meta_model_structure_selection.py:465: RuntimeWarning: overflow encountered in square\n  sum_of_squared_residues = np.sum(residues**2)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\meta_model_structure_selection.py:475: RuntimeWarning: invalid value encountered in sqrt\n  se_theta = np.sqrt(var_e)\nc:\\Users\\wilso\\miniconda3\\envs\\v0.3.3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\metrics\\_regression.py:216: RuntimeWarning: overflow encountered in square\n  numerator = np.sum(np.square((yhat - y)))\nc:\\Users\\wilso\\miniconda3\\envs\\v0.3.3\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2590: RuntimeWarning: divide by zero encountered in power\n  absx **= ord\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\metaheuristics\\bpsogsa.py:194: RuntimeWarning: invalid value encountered in subtract\n  agent_mass = (fitness_value - 0.99 * worst_fitness_value) / (\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\metaheuristics\\bpsogsa.py:194: RuntimeWarning: invalid value encountered in divide\n  agent_mass = (fitness_value - 0.99 * worst_fitness_value) / (\n</pre> <pre>MetaMSS 0.025\n</pre> In\u00a0[10]: Copied! <pre>for aux_results, results in sorted(\n    resultados.items(), key=lambda x: np.mean(x[1]), reverse=False\n):\n    print(aux_results, np.mean(results))\n</pre> for aux_results, results in sorted(     resultados.items(), key=lambda x: np.mean(x[1]), reverse=False ):     print(aux_results, np.mean(results)) <pre>MetaMSS 0.025270255361927386\nFROLS-Polynomial_NARX 0.04595606904272196\nNARX_ARD 0.0704838592994891\nNARX_Catboost 0.15735233093778006\nNARX_RandomForestRegressor 0.19086216946473913\nNARX_DecisionTreeRegressor 0.2570981305420654\nNARX_KNeighborsRegressor 1.1649427863250668\n</pre>"},{"location":"old_version/metamss/#meta-model-structure-selection-metamss-algorithm-for-building-polynomial-narx-models","title":"Meta-Model Structure Selection (MetaMSS) algorithm for building Polynomial NARX models\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"old_version/multiobjective_parameter_estimation/","title":"Multiobjective Parameter Estimation for NARMAX models - An Overview","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.multiobjective_parameter_estimation import AILS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.narmax_tools import set_weights\n</pre> import numpy as np import matplotlib.pyplot as plt import pandas as pd from sysidentpy.model_structure_selection import FROLS from sysidentpy.multiobjective_parameter_estimation import AILS from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_results from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.narmax_tools import set_weights In\u00a0[2]: Copied! <pre>df_train = pd.read_csv(r\"datasets/buck_id.csv\")\ndf_valid = pd.read_csv(r\"datasets/buck_valid.csv\")\n\n# Plotting the measured output (identification and validation data)\nplt.figure(1)\nplt.title(\"Output\")\nplt.plot(df_train.sampling_time, df_train.y, label=\"Identification\", linewidth=1.5)\nplt.plot(df_valid.sampling_time, df_valid.y, label=\"Validation\", linewidth=1.5)\nplt.xlabel(\"Samples\")\nplt.ylabel(\"Voltage\")\nplt.legend()\nplt.show()\n</pre> df_train = pd.read_csv(r\"datasets/buck_id.csv\") df_valid = pd.read_csv(r\"datasets/buck_valid.csv\")  # Plotting the measured output (identification and validation data) plt.figure(1) plt.title(\"Output\") plt.plot(df_train.sampling_time, df_train.y, label=\"Identification\", linewidth=1.5) plt.plot(df_valid.sampling_time, df_valid.y, label=\"Validation\", linewidth=1.5) plt.xlabel(\"Samples\") plt.ylabel(\"Voltage\") plt.legend() plt.show() In\u00a0[3]: Copied! <pre># Plotting the measured input (identification and validation data)\nplt.figure(2)\nplt.title(\"Input\")\nplt.plot(df_train.sampling_time, df_train.input, label=\"Identification\", linewidth=1.5)\nplt.plot(df_valid.sampling_time, df_valid.input, label=\"Validation\", linewidth=1.5)\nplt.ylim(2.1, 2.6)\nplt.ylabel(\"u\")\nplt.xlabel(\"Samples\")\nplt.legend()\nplt.show()\n</pre> # Plotting the measured input (identification and validation data) plt.figure(2) plt.title(\"Input\") plt.plot(df_train.sampling_time, df_train.input, label=\"Identification\", linewidth=1.5) plt.plot(df_valid.sampling_time, df_valid.input, label=\"Validation\", linewidth=1.5) plt.ylim(2.1, 2.6) plt.ylabel(\"u\") plt.xlabel(\"Samples\") plt.legend() plt.show() In\u00a0[4]: Copied! <pre># Static data\nVd = 24\nUo = np.linspace(0, 4, 50)\nYo = (4 - Uo) * Vd / 3\nUo = Uo.reshape(-1, 1)\nYo = Yo.reshape(-1, 1)\nplt.figure(3)\nplt.title(\"Buck Converter Static Curve\")\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{y}$\")\nplt.plot(Uo, Yo, linewidth=1.5, linestyle=\"-\", marker=\"o\")\nplt.show()\n</pre> # Static data Vd = 24 Uo = np.linspace(0, 4, 50) Yo = (4 - Uo) * Vd / 3 Uo = Uo.reshape(-1, 1) Yo = Yo.reshape(-1, 1) plt.figure(3) plt.title(\"Buck Converter Static Curve\") plt.xlabel(\"$\\\\bar{u}$\") plt.ylabel(\"$\\\\bar{y}$\") plt.plot(Uo, Yo, linewidth=1.5, linestyle=\"-\", marker=\"o\") plt.show() In\u00a0[5]: Copied! <pre># Defining the gain\ngain = -8 * np.ones(len(Uo)).reshape(-1, 1)\nplt.figure(3)\nplt.title(\"Buck Converter Static Gain\")\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{gain}$\")\nplt.plot(Uo, gain, linewidth=1.5, label=\"gain\", linestyle=\"-\", marker=\"o\")\nplt.legend()\nplt.show()\n</pre> # Defining the gain gain = -8 * np.ones(len(Uo)).reshape(-1, 1) plt.figure(3) plt.title(\"Buck Converter Static Gain\") plt.xlabel(\"$\\\\bar{u}$\") plt.ylabel(\"$\\\\bar{gain}$\") plt.plot(Uo, gain, linewidth=1.5, label=\"gain\", linestyle=\"-\", marker=\"o\") plt.legend() plt.show() In\u00a0[6]: Copied! <pre>x_train = df_train.input.values.reshape(-1, 1)\ny_train = df_train.y.values.reshape(-1, 1)\nx_valid = df_valid.input.values.reshape(-1, 1)\ny_valid = df_valid.y.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=2)\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=8,\n    extended_least_squares=False,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\n</pre> x_train = df_train.input.values.reshape(-1, 1) y_train = df_train.y.values.reshape(-1, 1) x_valid = df_valid.input.values.reshape(-1, 1) y_valid = df_valid.y.values.reshape(-1, 1)  basis_function = Polynomial(degree=2)  model = FROLS(     order_selection=True,     n_info_values=8,     extended_least_squares=False,     ylag=2,     xlag=2,     info_criteria=\"aic\",     estimator=\"least_squares\",     basis_function=basis_function, )  model.fit(X=x_train, y=y_train) <pre>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre> Out[6]: <pre>&lt;sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS at 0x14acaf77950&gt;</pre> In\u00a0[7]: Copied! <pre># you can use any set of model structure you want in your use case, but in this notebook we will use the one obtained above the compare with other work\nmo_estimator = AILS(final_model=model.final_model)\n\n# setting the log-spaced weights of each objective function\nw = set_weights(static_function=True, static_gain=True)\n\n# you can also use something like\n\n# w = np.array(\n#     [\n#         [0.98, 0.7, 0.5, 0.35, 0.25, 0.01, 0.15, 0.01],\n#         [0.01, 0.1, 0.3, 0.15, 0.25, 0.98, 0.35, 0.01],\n#         [0.01, 0.2, 0.2, 0.50, 0.50, 0.01, 0.50, 0.98],\n#     ]\n# )\n\n# to set the weights. Each row correspond to each objective\n</pre> # you can use any set of model structure you want in your use case, but in this notebook we will use the one obtained above the compare with other work mo_estimator = AILS(final_model=model.final_model)  # setting the log-spaced weights of each objective function w = set_weights(static_function=True, static_gain=True)  # you can also use something like  # w = np.array( #     [ #         [0.98, 0.7, 0.5, 0.35, 0.25, 0.01, 0.15, 0.01], #         [0.01, 0.1, 0.3, 0.15, 0.25, 0.98, 0.35, 0.01], #         [0.01, 0.2, 0.2, 0.50, 0.50, 0.01, 0.50, 0.98], #     ] # )  # to set the weights. Each row correspond to each objective <p>AILS has an <code>estimate</code> method that returns the cost functions (J), the Euclidean norm of the cost functions (E), the estimated parameters referring to each weight (theta), the regressor matrix of the gain and static_function affine information HR and QR, respectively.</p> In\u00a0[8]: Copied! <pre>J, E, theta, HR, QR, position = mo_estimator.estimate(\n    X=x_train, y=y_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\n    \"w1\": w[0, :],\n    \"w2\": w[2, :],\n    \"w3\": w[1, :],\n    \"J_ls\": J[0, :],\n    \"J_sg\": J[1, :],\n    \"J_sf\": J[2, :],\n    \"||J||:\": E,\n}\npd.DataFrame(result)\n</pre> J, E, theta, HR, QR, position = mo_estimator.estimate(     X=x_train, y=y_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w ) result = {     \"w1\": w[0, :],     \"w2\": w[2, :],     \"w3\": w[1, :],     \"J_ls\": J[0, :],     \"J_sg\": J[1, :],     \"J_sf\": J[2, :],     \"||J||:\": E, } pd.DataFrame(result) Out[8]: w1 w2 w3 J_ls J_sg J_sf ||J||: 0 0.006842 0.003078 0.990080 0.999970 1.095020e-05 0.000013 0.245244 1 0.007573 0.002347 0.990080 0.999938 2.294665e-05 0.000016 0.245236 2 0.008382 0.001538 0.990080 0.999885 6.504913e-05 0.000018 0.245223 3 0.009277 0.000642 0.990080 0.999717 4.505541e-04 0.000021 0.245182 4 0.006842 0.098663 0.894495 1.000000 7.393246e-08 0.000015 0.245251 ... ... ... ... ... ... ... ... 2290 0.659632 0.333527 0.006842 0.995896 3.965699e-04 1.000000 0.244489 2291 0.730119 0.263039 0.006842 0.995632 5.602981e-04 0.972842 0.244412 2292 0.808139 0.185020 0.006842 0.995364 8.321071e-04 0.868299 0.244300 2293 0.894495 0.098663 0.006842 0.995100 1.364999e-03 0.660486 0.244160 2294 0.990080 0.003078 0.006842 0.992584 9.825987e-02 0.305492 0.261455 <p>2295 rows \u00d7 7 columns</p> <p>Now we can set theta related to any weight results</p> In\u00a0[9]: Copied! <pre>model.theta = theta[-1, :].reshape(\n    -1, 1\n)  # setting the theta estimated for the last combination of the weights\n# the model structure is exactly the same, but the order of the regressors is changed in estimate method. Thats why you have to change the model.final_model\nmodel.final_model = mo_estimator.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</pre> model.theta = theta[-1, :].reshape(     -1, 1 )  # setting the theta estimated for the last combination of the weights # the model structure is exactly the same, but the order of the regressors is changed in estimate method. Thats why you have to change the model.final_model model.final_model = mo_estimator.final_model yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=3,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) r Out[9]: Regressors Parameters ERR 0 1 2.2930E+00 9.999E-01 1 y(k-1) 2.3307E-01 2.042E-05 2 y(k-2) 6.3209E-01 1.108E-06 3 x1(k-1) -5.9333E-01 4.688E-06 4 y(k-1)^2 2.7673E-01 3.922E-07 5 y(k-2)y(k-1) -5.3228E-01 8.389E-07 6 x1(k-1)y(k-1) 1.6667E-02 5.690E-07 7 y(k-2)^2 2.5766E-01 3.827E-06 In\u00a0[10]: Copied! <pre>plot_results(y=y_valid, yhat=yhat, n=1000)\n</pre> plot_results(y=y_valid, yhat=yhat, n=1000) In\u00a0[11]: Copied! <pre>plt.figure(4)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n</pre> plt.figure(4) plt.title(\"Gain\") plt.plot(     Uo,     gain,     linewidth=1.5,     linestyle=\"-\",     marker=\"o\",     label=\"Buck converter static gain\", ) plt.plot(     Uo,     HR.dot(model.theta),     linestyle=\"-\",     marker=\"^\",     linewidth=1.5,     label=\"NARX model gain\", ) plt.xlabel(\"$\\\\bar{u}$\") plt.ylabel(\"$\\\\bar{g}$\") plt.ylim(-16, 0) plt.legend() plt.show() In\u00a0[12]: Copied! <pre>plt.figure(5)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \u200b\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</pre> plt.figure(5) plt.title(\"Static Curve\") plt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\") plt.plot(     Uo,     QR.dot(model.theta),     linewidth=1.5,     label=\"NARX \u200b\u200bstatic representation\",     linestyle=\"-\",     marker=\"^\", ) plt.xlabel(\"$\\\\bar{u}$\") plt.xlabel(\"$\\\\bar{y}$\") plt.legend() plt.show() In\u00a0[13]: Copied! <pre># the variable `position` returned in `estimate` method give the position of the best weight combination\nmodel.theta = theta[position, :].reshape(\n    -1, 1\n)  # setting the theta estimated for the best combination of the weights\n# the model structure is exactly the same, but the order of the regressors is changed in estimate method. Thats why you have to change the model.final_model\nmodel.final_model = mo_estimator.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\n# The dynamic results for that chosen theta is\nplot_results(y=y_valid, yhat=yhat, n=1000)\n# The static gain result is\nplt.figure(4)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n# The static function result is\nplt.figure(5)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \u200b\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</pre> # the variable `position` returned in `estimate` method give the position of the best weight combination model.theta = theta[position, :].reshape(     -1, 1 )  # setting the theta estimated for the best combination of the weights # the model structure is exactly the same, but the order of the regressors is changed in estimate method. Thats why you have to change the model.final_model model.final_model = mo_estimator.final_model yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=3,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  # The dynamic results for that chosen theta is plot_results(y=y_valid, yhat=yhat, n=1000) # The static gain result is plt.figure(4) plt.title(\"Gain\") plt.plot(     Uo,     gain,     linewidth=1.5,     linestyle=\"-\",     marker=\"o\",     label=\"Buck converter static gain\", ) plt.plot(     Uo,     HR.dot(model.theta),     linestyle=\"-\",     marker=\"^\",     linewidth=1.5,     label=\"NARX model gain\", ) plt.xlabel(\"$\\\\bar{u}$\") plt.ylabel(\"$\\\\bar{g}$\") plt.ylim(-16, 0) plt.legend() plt.show() # The static function result is plt.figure(5) plt.title(\"Static Curve\") plt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\") plt.plot(     Uo,     QR.dot(model.theta),     linewidth=1.5,     label=\"NARX \u200b\u200bstatic representation\",     linestyle=\"-\",     marker=\"^\", ) plt.xlabel(\"$\\\\bar{u}$\") plt.xlabel(\"$\\\\bar{y}$\") plt.legend() plt.show() <pre>      Regressors   Parameters        ERR\n0              1   1.5405E+00  9.999E-01\n1         y(k-1)   2.9687E-01  2.042E-05\n2         y(k-2)   6.4693E-01  1.108E-06\n3        x1(k-1)  -4.1302E-01  4.688E-06\n4       y(k-1)^2   2.7671E-01  3.922E-07\n5   y(k-2)y(k-1)  -5.3474E-01  8.389E-07\n6  x1(k-1)y(k-1)   4.0624E-03  5.690E-07\n7       y(k-2)^2   2.5832E-01  3.827E-06\n</pre> <p>You can also plot the pareto-set solutions</p> In\u00a0[14]: Copied! <pre>plt.figure(6)\nax = plt.axes(projection=\"3d\")\nax.plot3D(J[0, :], J[1, :], J[2, :], \"o\", linewidth=0.1)\nax.set_title(\"Pareto-set solutions\", fontsize=15)\nax.set_xlabel(\"$J_{ls}$\", fontsize=10)\nax.set_ylabel(\"$J_{sg}$\", fontsize=10)\nax.set_zlabel(\"$J_{sf}$\", fontsize=10)\nplt.show()\n</pre> plt.figure(6) ax = plt.axes(projection=\"3d\") ax.plot3D(J[0, :], J[1, :], J[2, :], \"o\", linewidth=0.1) ax.set_title(\"Pareto-set solutions\", fontsize=15) ax.set_xlabel(\"$J_{ls}$\", fontsize=10) ax.set_ylabel(\"$J_{sg}$\", fontsize=10) ax.set_zlabel(\"$J_{sf}$\", fontsize=10) plt.show() <p>The polynomial NARX model built using the mono-objective approach has the following structure:</p> <p>$ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 u(k-1) y(k-1) + \\theta_4 + \\theta_5 y(k-1)^2 + \\theta_6 u(k-1) + \\theta_7 y(k-2)y(k-1) + \\theta_8 y(k-2)^2 $</p> <p>The, the goal when using the static function and static gain information in the multiobjective scenario is to estimate the vector $\\hat{\\theta}$ based on:</p> <p>$ \\theta = [w_1\\Psi^T\\Psi + w_2(HR)^T(HR) + w_3(QR)(QR)^T]^{-1} [w_1\\Psi^T y + w_2(HR)^T\\overline{g}+w_3(QR)^T\\overline{y}] $</p> <p>The $\\Psi$ matrix is built using the usual mono-objective dynamic modeling approach in SysIdentPy. However, it is still necessary to find the Q, H and R matrices. AILS have the methods to compute all of those matrices. Basically, to do that, $q_i^T$ is first estimated:</p> <p>$ q_i^T = \\begin{bmatrix} 1 &amp; \\overline{y_i} &amp; \\overline{u_1} &amp; \\overline{y_i}^2 &amp; \\cdots &amp; \\overline{y_i}^l &amp;  F_{yu} &amp; \\overline{u_i}^2 &amp; \\cdots &amp; \\overline{u_i}^l \\end{bmatrix} $</p> <p>where $F_{yu}$ stands for all non-linear monomials in the model that are related to $y(k)$ and $u(k)$, $l$ is the largest non-linearity in the model for input and output terms. For a model with a degree of nonlinearity equal to 2, we can obtain:</p> <p>$ q_i^T =  \\begin{bmatrix} 1 &amp; \\overline{y_i} &amp; \\overline{u_i} &amp; \\overline{y_i}^2 &amp; \\overline{u_i}\\:\\overline{y_i} &amp; \\overline{u_i}^2  \\end{bmatrix} $</p> <p>It is possible to encode the $q_i^T$ matrix so that it follows the model encoding defined in SysIdentPy. To do this, 0 is considered as a constant, $y_i$ equal to 1 and $u_i$ equal to 2. The number of columns indicates the degree of nonlinearity of the system and the number of rows reflects the number of terms:</p> <p>$ q_i =  \\begin{bmatrix} 0 &amp; 0\\\\ 1 &amp; 0\\\\ 2 &amp; 0\\\\ 1 &amp; 1\\\\ 2 &amp; 1\\\\ 2 &amp; 2\\\\ \\end{bmatrix} $ $ = $ $  \\begin{bmatrix} 1 \\\\ \\overline{y_i}\\\\ \\overline{u_i}\\\\ \\overline{y_i}^2\\\\ \\overline{u_i}\\:\\overline{y_i}\\\\ \\overline{u_i}^2\\\\ \\end{bmatrix} $</p> <p>Finally, the result can be easily obtained using the \u2018regressor_space\u2019 method of SysIdentPy</p> In\u00a0[15]: Copied! <pre>from sysidentpy.narmax_base import RegressorDictionary\n\nobject_qit = RegressorDictionary(xlag=1, ylag=1)\nR_example = object_qit.regressor_space(n_inputs=1) // 1000\nprint(f\"R = {R_example}\")\n</pre> from sysidentpy.narmax_base import RegressorDictionary  object_qit = RegressorDictionary(xlag=1, ylag=1) R_example = object_qit.regressor_space(n_inputs=1) // 1000 print(f\"R = {R_example}\") <pre>R = [[0 0]\n [1 0]\n [2 0]\n [1 1]\n [2 1]\n [2 2]]\n</pre> <p>such that:</p> <p>$ \\overline{y_i} = q_i^T R\\theta $</p> <p>and:</p> <p>$ \\overline{g_i} = H R\\theta $</p> <p>where $R$ is the linear mapping of the static regressors represented by $q_i^T$. In addition, the $H$ matrix holds affine information regarding $\\overline{g_i}$, which is equal to $\\overline{g_i} = \\frac{d\\overline{y}}{d\\overline{u}}{\\big |}_{(\\overline{u_i}\\:\\overline{y_i})}$.</p> <p>From now on, we will begin to apply the parameter estimation in a multiobjective manner. This will be done with the NARX polynomial model of the BUCK converter in mind. In this context, $q_i^T$ will be generic and will assume a specific format for the problem at hand. For this task, the $R_qit$ method will be used, whose objective is to return the $q_i^T$ related to the model and the matrix of the linear mapping $R$:</p> In\u00a0[16]: Copied! <pre>R, qit = mo_estimator.build_linear_mapping()\nprint(\"R matrix:\")\nprint(R)\nprint(\"qit matrix:\")\nprint(qit)\n</pre> R, qit = mo_estimator.build_linear_mapping() print(\"R matrix:\") print(R) print(\"qit matrix:\") print(qit) <pre>R matrix:\n[[1 0 0 0 0 0 0 0]\n [0 1 1 0 0 0 0 0]\n [0 0 0 1 0 0 0 0]\n [0 0 0 0 1 1 0 1]\n [0 0 0 0 0 0 1 0]]\nqit matrix:\n[[0 0]\n [1 0]\n [0 1]\n [2 0]\n [1 1]]\n</pre> <p>So</p> <p>$ q_i =  \\begin{bmatrix} 0 &amp; 0\\\\ 1 &amp; 0\\\\ 2 &amp; 0\\\\ 1 &amp; 1\\\\ 2 &amp; 1\\\\  \\end{bmatrix} $ $ =$ $ \\begin{bmatrix} 1\\\\ \\overline{y}\\\\ \\overline{u}\\\\ \\overline{y^2}\\\\ \\overline{u}\\:\\overline{y}\\\\  \\end{bmatrix} $</p> <p>You can notice that the method produces outputs consistent with what is expected:</p> <p>$ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 u(k-1) y(k-1) + \\theta_4 + \\theta_5 y(k-1)^2 + \\theta_6 u(k-1) + \\theta_7 y(k-2)y(k-1) + \\theta_8 y(k-2)^2 $</p> <p>and:</p> <p>$ R =  \\begin{bmatrix} term/\\theta &amp; \\theta_1 &amp; \\theta_2 &amp; \\theta_3 &amp; \\theta_4 &amp; \\theta_5 &amp; \\theta_6 &amp; \\theta_7 &amp; \\theta_8\\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\overline{y} &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\overline{u} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\\\ \\overline{y^2} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1\\\\ \\overline{y}\\:\\overline{u} &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\end{bmatrix} $</p> In\u00a0[17]: Copied! <pre>final_model = np.array(\n    [\n        [1001, 0],\n        [1002, 0],\n        [0, 0],\n        [2001, 0],\n        [2001, 2001],\n        [2002, 2001],\n        [2002, 0],\n        [2002, 2002],\n    ]\n)\nfinal_model\n</pre> final_model = np.array(     [         [1001, 0],         [1002, 0],         [0, 0],         [2001, 0],         [2001, 2001],         [2002, 2001],         [2002, 0],         [2002, 2002],     ] ) final_model Out[17]: <pre>array([[1001,    0],\n       [1002,    0],\n       [   0,    0],\n       [2001,    0],\n       [2001, 2001],\n       [2002, 2001],\n       [2002,    0],\n       [2002, 2002]])</pre> In\u00a0[18]: Copied! <pre>mult2 = AILS(final_model=final_model)\n</pre> mult2 = AILS(final_model=final_model) In\u00a0[19]: Copied! <pre>def psi(X, Y):\n    PSI = np.zeros((len(X), 8))\n    for k in range(2, len(Y)):\n        PSI[k, 0] = Y[k - 1]\n        PSI[k, 1] = Y[k - 2]\n        PSI[k, 2] = 1\n        PSI[k, 3] = X[k - 1]\n        PSI[k, 4] = X[k - 1] ** 2\n        PSI[k, 5] = X[k - 2] * X[k - 1]\n        PSI[k, 6] = X[k - 2]\n        PSI[k, 7] = X[k - 2] ** 2\n    return np.delete(PSI, [0, 1], axis=0)\n</pre> def psi(X, Y):     PSI = np.zeros((len(X), 8))     for k in range(2, len(Y)):         PSI[k, 0] = Y[k - 1]         PSI[k, 1] = Y[k - 2]         PSI[k, 2] = 1         PSI[k, 3] = X[k - 1]         PSI[k, 4] = X[k - 1] ** 2         PSI[k, 5] = X[k - 2] * X[k - 1]         PSI[k, 6] = X[k - 2]         PSI[k, 7] = X[k - 2] ** 2     return np.delete(PSI, [0, 1], axis=0) <p>The value of theta with the lowest mean squared error obtained with the same code implemented in Scilab was:</p> <p>$ W_{LS} = 0.3612343 $</p> <p>and:</p> <p>$ W_{SG} = 0.3548699 $</p> <p>and:</p> <p>$ W_{SF} = 0.3548699 $</p> In\u00a0[20]: Copied! <pre>PSI = psi(x_train, y_train)\nw = np.array([[0.3612343], [0.2838959], [0.3548699]])\n</pre> PSI = psi(x_train, y_train) w = np.array([[0.3612343], [0.2838959], [0.3548699]]) <pre>C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_1728\\3629615894.py:4: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 0] = Y[k - 1]\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_1728\\3629615894.py:5: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 1] = Y[k - 2]\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_1728\\3629615894.py:7: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 3] = X[k - 1]\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_1728\\3629615894.py:8: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 4] = X[k - 1] ** 2\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_1728\\3629615894.py:9: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 5] = X[k - 2] * X[k - 1]\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_1728\\3629615894.py:10: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 6] = X[k - 2]\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_1728\\3629615894.py:11: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 7] = X[k - 2] ** 2\n</pre> In\u00a0[21]: Copied! <pre>J, E, theta, HR, QR, position = mult2.estimate(\n    y=y_train, X=x_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\n    \"w1\": w[0, :],\n    \"w2\": w[2, :],\n    \"w3\": w[1, :],\n    \"J_ls\": J[0, :],\n    \"J_sg\": J[1, :],\n    \"J_sf\": J[2, :],\n    \"||J||:\": E,\n}\n# the order of the weights is different because the way we implemented in Python, but the results are very close as expected\npd.DataFrame(result)\n</pre> J, E, theta, HR, QR, position = mult2.estimate(     y=y_train, X=x_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w ) result = {     \"w1\": w[0, :],     \"w2\": w[2, :],     \"w3\": w[1, :],     \"J_ls\": J[0, :],     \"J_sg\": J[1, :],     \"J_sf\": J[2, :],     \"||J||:\": E, } # the order of the weights is different because the way we implemented in Python, but the results are very close as expected pd.DataFrame(result) Out[21]: w1 w2 w3 J_ls J_sg J_sf ||J||: 0 0.361234 0.35487 0.283896 1.0 1.0 1.0 1.0 <p>Dynamic results</p> In\u00a0[22]: Copied! <pre>model.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = mult2.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</pre> model.theta = theta[position, :].reshape(-1, 1) model.final_model = mult2.final_model yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=3,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) r Out[22]: Regressors Parameters ERR 0 1 1.4287E+00 9.999E-01 1 y(k-1) 5.5147E-01 2.042E-05 2 y(k-2) 4.0449E-01 1.108E-06 3 x1(k-1) -1.2605E+01 4.688E-06 4 x1(k-2) 1.2257E+01 3.922E-07 5 x1(k-1)^2 8.3274E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1416E+01 5.690E-07 7 x1(k-2)^2 3.0846E+00 3.827E-06 In\u00a0[23]: Copied! <pre>plot_results(y=y_valid, yhat=yhat, n=1000)\n</pre> plot_results(y=y_valid, yhat=yhat, n=1000) <p>Static gain</p> In\u00a0[24]: Copied! <pre>plt.figure(7)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n</pre> plt.figure(7) plt.title(\"Gain\") plt.plot(     Uo,     gain,     linewidth=1.5,     linestyle=\"-\",     marker=\"o\",     label=\"Buck converter static gain\", ) plt.plot(     Uo,     HR.dot(model.theta),     linestyle=\"-\",     marker=\"^\",     linewidth=1.5,     label=\"NARX model gain\", ) plt.xlabel(\"$\\\\bar{u}$\") plt.ylabel(\"$\\\\bar{g}$\") plt.ylim(-16, 0) plt.legend() plt.show() <p>Static function</p> In\u00a0[25]: Copied! <pre>plt.figure(8)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \u200b\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</pre> plt.figure(8) plt.title(\"Static Curve\") plt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\") plt.plot(     Uo,     QR.dot(model.theta),     linewidth=1.5,     label=\"NARX \u200b\u200bstatic representation\",     linestyle=\"-\",     marker=\"^\", ) plt.xlabel(\"$\\\\bar{u}$\") plt.xlabel(\"$\\\\bar{y}$\") plt.legend() plt.show() <p>Pareto-set solutions</p> In\u00a0[26]: Copied! <pre>plt.figure(9)\nax = plt.axes(projection=\"3d\")\nax.plot3D(J[0, :], J[1, :], J[2, :], \"o\", linewidth=0.1)\nax.set_title(\"Optimum pareto-curve\", fontsize=15)\nax.set_xlabel(\"$J_{ls}$\", fontsize=10)\nax.set_ylabel(\"$J_{sg}$\", fontsize=10)\nax.set_zlabel(\"$J_{sf}$\", fontsize=10)\nplt.show()\n</pre> plt.figure(9) ax = plt.axes(projection=\"3d\") ax.plot3D(J[0, :], J[1, :], J[2, :], \"o\", linewidth=0.1) ax.set_title(\"Optimum pareto-curve\", fontsize=15) ax.set_xlabel(\"$J_{ls}$\", fontsize=10) ax.set_ylabel(\"$J_{sg}$\", fontsize=10) ax.set_zlabel(\"$J_{sf}$\", fontsize=10) plt.show() In\u00a0[27]: Copied! <pre>theta[position, :]\n</pre> theta[position, :] Out[27]: <pre>array([  1.42867821,   0.55147249,   0.40449005, -12.60549001,\n        12.25730092,   8.32739876, -11.41573694,   3.08460955])</pre> <p>The following table show the results reported in \u2018IniciacaoCientifica2007\u2019 and the ones obtained with SysIdentPy implementation</p> Theta SysIdentPy IniciacaoCientifica2007 $\\theta_1$ 0.5514725 0.549144 $\\theta_2$ 0.40449005 0.408028 $\\theta_3$ 1.42867821 1.45097 $\\theta_4$ -12.60548863 -12.55788 $\\theta_5$ 8.32740057 8.1516315 $\\theta_6$ -11.41574116 -11.09728 $\\theta_7$ 12.25729955 12.215782 $\\theta_8$ 3.08461195 2.9319577 <p>where:</p> <p>$ E_{Scilab} =    17.426613 $</p> <p>and:</p> <p>$ E_{Python} = 17.474865 $</p> In\u00a0[28]: Copied! <pre>R, qit = mult2.build_linear_mapping()\nprint(\"R matrix:\")\nprint(R)\nprint(\"qit matrix:\")\nprint(qit)\n</pre> R, qit = mult2.build_linear_mapping() print(\"R matrix:\") print(R) print(\"qit matrix:\") print(qit) <pre>R matrix:\n[[1 0 0 0 0 0 0 0]\n [0 1 1 0 0 0 0 0]\n [0 0 0 1 1 0 0 0]\n [0 0 0 0 0 1 1 1]]\nqit matrix:\n[[0 0]\n [1 0]\n [0 1]\n [0 2]]\n</pre> <p>model's structure that will be utilized (\u2018IniciacaoCientifica2007\u2019):</p> <p>$ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 + \\theta_4 u(k-1) + \\theta_5 u(k-1)^2 + \\theta_6 u(k-2)u(k-1)+\\theta_7 u(k-2) + \\theta_8 u(k-2)^2 $</p> <p>$ q_i =  \\begin{bmatrix} 0 &amp; 0\\\\ 1 &amp; 0\\\\ 2 &amp; 0\\\\ 2 &amp; 2\\\\  \\end{bmatrix} $ $ = $ $ \\begin{bmatrix} 1\\\\ \\overline{y}\\\\ \\overline{u}\\\\ \\overline{u^2} \\end{bmatrix} $</p> <p>and:</p> <p>$ R =  \\begin{bmatrix} term/\\theta &amp; \\theta_1 &amp; \\theta_2 &amp; \\theta_3 &amp; \\theta_4 &amp; \\theta_5 &amp; \\theta_6 &amp; \\theta_7 &amp; \\theta_8\\\\ 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\overline{y} &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\overline{u} &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\\\ \\overline{u^2} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\end{bmatrix} $</p> <p>consistent with matrix R:</p> <p>R = [0 0 1 0 0 0 0 0;1 1 0 0 0 0 0 0;0 0 0 1 0 0 1 0;0 0 0 0 1 1 0 1]; // R</p> <p>or:</p> <p>$  R =  \\begin{bmatrix} 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\end{bmatrix} $</p> In\u00a0[29]: Copied! <pre>bi_objective = AILS(\n    static_function=True, static_gain=False, final_model=final_model, normalize=True\n)\n</pre> bi_objective = AILS(     static_function=True, static_gain=False, final_model=final_model, normalize=True ) <p>the value of theta with the lowest mean squared error obtained through the routine in Scilab was:</p> <p>$ W_{LS} = 0.9931126 $</p> <p>and:</p> <p>$ W_{SF} = 0.0068874 $</p> In\u00a0[30]: Copied! <pre>w = np.zeros((2, 2000))\nw[0, :] = np.logspace(-0.01, -6, num=2000, base=2.71)\nw[1, :] = np.ones(2000) - w[0, :]\nJ, E, theta, HR, QR, position = bi_objective.estimate(\n    y=y_train, X=x_train, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\"w1\": w[0, :], \"w2\": w[1, :], \"J_ls\": J[0, :], \"J_sg\": J[1, :], \"||J||:\": E}\npd.DataFrame(result)\n</pre> w = np.zeros((2, 2000)) w[0, :] = np.logspace(-0.01, -6, num=2000, base=2.71) w[1, :] = np.ones(2000) - w[0, :] J, E, theta, HR, QR, position = bi_objective.estimate(     y=y_train, X=x_train, y_static=Yo, X_static=Uo, weighing_matrix=w ) result = {\"w1\": w[0, :], \"w2\": w[1, :], \"J_ls\": J[0, :], \"J_sg\": J[1, :], \"||J||:\": E} pd.DataFrame(result) Out[30]: w1 w2 J_ls J_sg ||J||: 0 0.990080 0.009920 0.990863 1.000000 0.990939 1 0.987127 0.012873 0.990865 0.987032 0.990939 2 0.984182 0.015818 0.990867 0.974307 0.990939 3 0.981247 0.018753 0.990870 0.961803 0.990940 4 0.978320 0.021680 0.990873 0.949509 0.990941 ... ... ... ... ... ... 1995 0.002555 0.997445 0.999993 0.000072 0.999993 1996 0.002547 0.997453 0.999994 0.000072 0.999994 1997 0.002540 0.997460 0.999996 0.000071 0.999996 1998 0.002532 0.997468 0.999998 0.000071 0.999998 1999 0.002525 0.997475 1.000000 0.000070 1.000000 <p>2000 rows \u00d7 5 columns</p> In\u00a0[31]: Copied! <pre>model.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = bi_objective.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</pre> model.theta = theta[position, :].reshape(-1, 1) model.final_model = bi_objective.final_model yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=3,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) r Out[31]: Regressors Parameters ERR 0 1 1.3873E+00 9.999E-01 1 y(k-1) 5.4941E-01 2.042E-05 2 y(k-2) 4.0804E-01 1.108E-06 3 x1(k-1) -1.2515E+01 4.688E-06 4 x1(k-2) 1.2227E+01 3.922E-07 5 x1(k-1)^2 8.1171E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1047E+01 5.690E-07 7 x1(k-2)^2 2.9043E+00 3.827E-06 In\u00a0[32]: Copied! <pre>plot_results(y=y_valid, yhat=yhat, n=1000)\n</pre> plot_results(y=y_valid, yhat=yhat, n=1000) In\u00a0[33]: Copied! <pre>plt.figure(10)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \u200b\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</pre> plt.figure(10) plt.title(\"Static Curve\") plt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\") plt.plot(     Uo,     QR.dot(model.theta),     linewidth=1.5,     label=\"NARX \u200b\u200bstatic representation\",     linestyle=\"-\",     marker=\"^\", ) plt.xlabel(\"$\\\\bar{u}$\") plt.xlabel(\"$\\\\bar{y}$\") plt.legend() plt.show() In\u00a0[34]: Copied! <pre>plt.figure(11)\nplt.title(\"Costs Functions\")\nplt.plot(J[1, :], J[0, :], \"o\")\nplt.xlabel(\"Static Curve Information\")\nplt.ylabel(\"Prediction Error\")\nplt.show()\n</pre> plt.figure(11) plt.title(\"Costs Functions\") plt.plot(J[1, :], J[0, :], \"o\") plt.xlabel(\"Static Curve Information\") plt.ylabel(\"Prediction Error\") plt.show() <p>where the best estimated $\\theta$ is</p> Theta SysIdentPy IniciacaoCientifica2007 $\\theta_1$ 0.54940883 0.5494135 $\\theta_2$ 0.40803995 0.4080312 $\\theta_3$ 1.38725684 3.3857601 $\\theta_4$ -12.51466378 -12.513688 $\\theta_5$ 8.11712897 8.116575 $\\theta_6$ -11.04664789 -11.04592 $\\theta_7$ 12.22693907 12.227184 $\\theta_8$ 2.90425844 2.9038468 <p>where:</p> <p>$ E_{Scilab} = 17.408934 $</p> <p>and:</p> <p>$ E_{Python} = 17.408947 $</p> In\u00a0[35]: Copied! <pre>bi_objective_gain = AILS(\n    static_function=False, static_gain=True, final_model=final_model, normalize=False\n)\n</pre> bi_objective_gain = AILS(     static_function=False, static_gain=True, final_model=final_model, normalize=False ) <p>the value of theta with the lowest mean squared error obtained through the routine in Scilab was:</p> <p>$ W_{LS} = 0.9931126 $</p> <p>and:</p> <p>$ W_{SF} = 0.0068874 $</p> In\u00a0[36]: Copied! <pre>w = np.zeros((2, 2000))\nw[0, :] = np.logspace(0, -6, num=2000, base=2.71)\nw[1, :] = np.ones(2000) - w[0, :]\n# W = np.array([[0.9931126],\n# [0.0068874]])\nJ, E, theta, HR, QR, position = bi_objective_gain.estimate(\n    X=x_train, y=y_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\"w1\": w[0, :], \"w2\": w[1, :], \"J_ls\": J[0, :], \"J_sg\": J[1, :], \"||J||:\": E}\npd.DataFrame(result)\n</pre> w = np.zeros((2, 2000)) w[0, :] = np.logspace(0, -6, num=2000, base=2.71) w[1, :] = np.ones(2000) - w[0, :] # W = np.array([[0.9931126], # [0.0068874]]) J, E, theta, HR, QR, position = bi_objective_gain.estimate(     X=x_train, y=y_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w ) result = {\"w1\": w[0, :], \"w2\": w[1, :], \"J_ls\": J[0, :], \"J_sg\": J[1, :], \"||J||:\": E} pd.DataFrame(result) Out[36]: w1 w2 J_ls J_sg ||J||: 0 1.000000 0.000000 17.407256 3.579461e+01 39.802849 1 0.997012 0.002988 17.407528 2.109260e-01 17.408806 2 0.994033 0.005967 17.407540 2.082067e-01 17.408785 3 0.991063 0.008937 17.407559 2.056636e-01 17.408774 4 0.988102 0.011898 17.407585 2.031788e-01 17.408771 ... ... ... ... ... ... 1995 0.002555 0.997445 17.511596 3.340081e-07 17.511596 1996 0.002547 0.997453 17.511596 3.320125e-07 17.511596 1997 0.002540 0.997460 17.511597 3.300289e-07 17.511597 1998 0.002532 0.997468 17.511598 3.280571e-07 17.511598 1999 0.002525 0.997475 17.511599 3.260972e-07 17.511599 <p>2000 rows \u00d7 5 columns</p> In\u00a0[37]: Copied! <pre># Writing the results\nmodel.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = bi_objective_gain.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</pre> # Writing the results model.theta = theta[position, :].reshape(-1, 1) model.final_model = bi_objective_gain.final_model yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=3,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) r Out[37]: Regressors Parameters ERR 0 1 1.4853E+00 9.999E-01 1 y(k-1) 5.4940E-01 2.042E-05 2 y(k-2) 4.0806E-01 1.108E-06 3 x1(k-1) -1.2581E+01 4.688E-06 4 x1(k-2) 1.2210E+01 3.922E-07 5 x1(k-1)^2 8.1686E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1122E+01 5.690E-07 7 x1(k-2)^2 2.9455E+00 3.827E-06 In\u00a0[38]: Copied! <pre>plot_results(y=y_valid, yhat=yhat, n=1000)\n</pre> plot_results(y=y_valid, yhat=yhat, n=1000) In\u00a0[39]: Copied! <pre>plt.figure(12)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.legend()\nplt.show()\n</pre> plt.figure(12) plt.title(\"Gain\") plt.plot(     Uo,     gain,     linewidth=1.5,     linestyle=\"-\",     marker=\"o\",     label=\"Buck converter static gain\", ) plt.plot(     Uo,     HR.dot(model.theta),     linestyle=\"-\",     marker=\"^\",     linewidth=1.5,     label=\"NARX model gain\", ) plt.xlabel(\"$\\\\bar{u}$\") plt.ylabel(\"$\\\\bar{g}$\") plt.legend() plt.show() In\u00a0[40]: Copied! <pre>plt.figure(11)\nplt.title(\"Costs Functions\")\nplt.plot(J[1, :], J[0, :], \"o\")\nplt.xlabel(\"Gain Information\")\nplt.ylabel(\"Prediction Error\")\nplt.show()\n</pre> plt.figure(11) plt.title(\"Costs Functions\") plt.plot(J[1, :], J[0, :], \"o\") plt.xlabel(\"Gain Information\") plt.ylabel(\"Prediction Error\") plt.show() <p>being the selected $\\theta$:</p> Theta SysIdentPy IniciacaoCientifica2007 $\\theta_1$ 0.54939785 0.54937289 $\\theta_2$ 0.40805603 0.40810168 $\\theta_3$ 1.48525190 1.48663719 $\\theta_4$ -12.58066084 -12.58127183 $\\theta_5$ 8.16862622 8.16780294 $\\theta_6$ -11.12171897 -11.11998621 $\\theta_7$ 12.20954849 12.20927355 $\\theta_8$ 2.94548501 2.9446532 <p>where:</p> <p>$ E_{Scilab} =  17.408997 $</p> <p>and:</p> <p>$ E_{Python} = 17.408781 $</p> <p>Matrix Q:</p> In\u00a0[41]: Copied! <pre>bi_objective_gain.build_static_function_information(Uo, Yo)[1]\n</pre> bi_objective_gain.build_static_function_information(Uo, Yo)[1] Out[41]: <pre>array([[   50.        ,   800.        ,   800.        ,   100.        ,\n          100.        ,   269.3877551 ,   269.3877551 ,   269.3877551 ],\n       [  800.        , 17240.81632653, 17240.81632653,  1044.89795918,\n         1044.89795918,  2089.79591837,  2089.79591837,  2089.79591837],\n       [  800.        , 17240.81632653, 17240.81632653,  1044.89795918,\n         1044.89795918,  2089.79591837,  2089.79591837,  2089.79591837],\n       [  100.        ,  1044.89795918,  1044.89795918,   269.3877551 ,\n          269.3877551 ,   816.32653061,   816.32653061,   816.32653061],\n       [  100.        ,  1044.89795918,  1044.89795918,   269.3877551 ,\n          269.3877551 ,   816.32653061,   816.32653061,   816.32653061],\n       [  269.3877551 ,  2089.79591837,  2089.79591837,   816.32653061,\n          816.32653061,  2638.54142407,  2638.54142407,  2638.54142407],\n       [  269.3877551 ,  2089.79591837,  2089.79591837,   816.32653061,\n          816.32653061,  2638.54142407,  2638.54142407,  2638.54142407],\n       [  269.3877551 ,  2089.79591837,  2089.79591837,   816.32653061,\n          816.32653061,  2638.54142407,  2638.54142407,  2638.54142407]])</pre> <p>Matrix H+R:</p> In\u00a0[42]: Copied! <pre>bi_objective_gain.build_static_gain_information(Uo, Yo, gain)[1]\n</pre> bi_objective_gain.build_static_gain_information(Uo, Yo, gain)[1] Out[42]: <pre>array([[    0.        ,     0.        ,     0.        ,     0.        ,\n            0.        ,     0.        ,     0.        ,     0.        ],\n       [    0.        ,  3200.        ,  3200.        ,  -400.        ,\n         -400.        , -1600.        , -1600.        , -1600.        ],\n       [    0.        ,  3200.        ,  3200.        ,  -400.        ,\n         -400.        , -1600.        , -1600.        , -1600.        ],\n       [    0.        ,  -400.        ,  -400.        ,    50.        ,\n           50.        ,   200.        ,   200.        ,   200.        ],\n       [    0.        ,  -400.        ,  -400.        ,    50.        ,\n           50.        ,   200.        ,   200.        ,   200.        ],\n       [    0.        , -1600.        , -1600.        ,   200.        ,\n          200.        ,  1077.55102041,  1077.55102041,  1077.55102041],\n       [    0.        , -1600.        , -1600.        ,   200.        ,\n          200.        ,  1077.55102041,  1077.55102041,  1077.55102041],\n       [    0.        , -1600.        , -1600.        ,   200.        ,\n          200.        ,  1077.55102041,  1077.55102041,  1077.55102041]])</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"old_version/multiobjective_parameter_estimation/#multiobjective-parameter-estimation-for-narmax-models-an-overview","title":"Multiobjective Parameter Estimation for NARMAX models - An Overview\u00b6","text":"<p>Example created by Gabriel Bueno Leandro, Samir Milani Martins and Wilson Rocha Lacerda Junior</p> <p>Multiobjective parameter estimation represents a fundamental paradigm shift in the way we approach the parameter tuning problem for NARMAX models. Instead of seeking a single set of parameter values that optimally fits the model to the data, multiobjective approaches aim to identify a set of parameter solutions, known as the Pareto front, that provide a trade-off between competing objectives. These objectives often encompass a spectrum of model performance criteria, such as goodness-of-fit, model complexity, and robustness.</p>"},{"location":"old_version/multiobjective_parameter_estimation/#reference","title":"Reference\u00b6","text":"<p>For further information, check this reference: https://doi.org/10.1080/00207170601185053.</p>"},{"location":"old_version/multiobjective_parameter_estimation/#use-case-buck-converter","title":"Use case: Buck converter\u00b6","text":"A buck converter is a type of DC/DC converter that decreases the voltage (while increasing the current) from its input (power supply) to its output (load). It is similar to a boost converter (elevator) and is a type of switched-mode power supply (SMPS) that typically contains at least two semiconductors (a diode and a transistor, although modern buck converters replace the diode with a second transistor used for synchronous rectification) and at least one energy storage element, a capacitor, inductor or both combined."},{"location":"old_version/multiobjective_parameter_estimation/#dynamic-behavior","title":"Dynamic Behavior\u00b6","text":""},{"location":"old_version/multiobjective_parameter_estimation/#buck-converter-static-function","title":"Buck Converter Static Function\u00b6","text":"<p>The duty cycle, represented by the symbol $D$, is defined as the ratio of the time the system is on ($T_{on}$\u200b) to the total operation cycle time ($T$). Mathematically, this can be expressed as $D=\\frac{T_{on}}{T}$. The complement of the duty cycle, represented by $D'$, is defined as the ratio of the time the system is off ($T_{off}$) to the total operation cycle time ($T$) and can be expressed as $D'=\\frac{T_{off}}{T}$.</p> <p>The load voltage ($V_o$) is related to the source voltage ($V_d$) by the equation $V_o\u200b=D\u22c5V_d\u200b=(1\u2212D\u2019)\u22c5V_d$.  For this particular converter, it is known that $D\u2032=\\frac{\\bar{u}-1}{3}\u200b$,\u200b which means that the static function of this system can be derived from theory to be:</p> <p>$V_o = \\frac{4V_d}{3} - \\frac{V_d}{3}\\cdot \\bar{u}$</p> <p>If we assume that the source voltage $V_d$\u200b is equal to 24 V, then we can rewrite the above expression as follows:</p> <p>$V_o = (4 - \\bar{u})\\cdot 8$</p>"},{"location":"old_version/multiobjective_parameter_estimation/#buck-converter-static-gain","title":"Buck converter Static Gain\u00b6","text":"<p>The gain of a Buck converter is a measure of how its output voltage changes in response to changes in its input voltage. Mathematically, the gain can be calculated as the derivative of the converter\u2019s static function, which describes the relationship between its input and output voltages. In this case, the static function of the Buck converter is given by the equation:</p> <p>$V_o = (4 - \\bar{u})\\cdot 8$</p> <p>Taking the derivative of this equation with respect to $\\hat{u}$, we find that the gain of the Buck converter is equal to \u22128. In other words, for every unit increase in the input voltage $\\hat{u}$, the output voltage Vo\u200b will decrease by 8 units.</p> <p>so $gain=V_o'=-8$</p>"},{"location":"old_version/multiobjective_parameter_estimation/#building-a-dynamic-model-using-the-mono-objective-approach","title":"Building a dynamic model using the mono-objective approach\u00b6","text":""},{"location":"old_version/multiobjective_parameter_estimation/#affine-information-least-squares-algorithm-ails","title":"Affine Information Least Squares Algorithm (AILS)\u00b6","text":"<p>AILS is a multiobjective parameter estimation algorithm, based on a set of affine information pairs. The multiobjective approach proposed in the mentioned paper and implemented in SysIdentPy leads to a convex multiobjective optimization problem, which can be solved by AILS. AILS is a LeastSquares-type non-iterative scheme for finding the Pareto-set solutions for the multiobjective problem.</p> <p>So, with the model structure defined (we will be using the one built using the dynamic data above), one can estimate the parameters using the multiobjective approach.</p> <p>The information about static function and static gain, besides the usual dynamic input/output data, can be used to build the pair of affine information to estimate the parameters of the model. We can model the cost function as:</p> <p>$ \\gamma(\\hat\\theta) = w_1\\cdot J_{LS}(\\hat{\\theta})+w_2\\cdot J_{SF}(\\hat{\\theta})+w_3\\cdot J_{SG}(\\hat{\\theta}) $</p>"},{"location":"old_version/multiobjective_parameter_estimation/#multiobjective-parameter-estimation-considering-3-different-objectives-the-prediction-error-the-static-function-and-the-static-gain","title":"Multiobjective parameter estimation considering 3 different objectives: the prediction error, the static function and the static gain\u00b6","text":""},{"location":"old_version/multiobjective_parameter_estimation/#the-dynamic-results-for-that-chosen-theta-is","title":"The dynamic results for that chosen theta is\u00b6","text":""},{"location":"old_version/multiobjective_parameter_estimation/#the-static-gain-result-is","title":"The static gain result is\u00b6","text":""},{"location":"old_version/multiobjective_parameter_estimation/#the-static-function-result-is","title":"The static function result is\u00b6","text":""},{"location":"old_version/multiobjective_parameter_estimation/#getting-the-best-weight-combination-based-on-the-norm-of-the-cost-function","title":"Getting the best weight combination based on the norm of the cost function\u00b6","text":""},{"location":"old_version/multiobjective_parameter_estimation/#detailing-ails","title":"Detailing AILS\u00b6","text":""},{"location":"old_version/multiobjective_parameter_estimation/#validation","title":"Validation\u00b6","text":"<p>The following model structure will be used to validate the approach:</p> <p>$ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 + \\theta_4 u(k-1) + \\theta_5 u(k-1)^2 + \\theta_6 u(k-2)u(k-1)+\\theta_7 u(k-2) + \\theta_8 u(k-2)^2 $</p> <p>$\\therefore$</p> <p>$ final\\_model =  \\begin{bmatrix} 1001 &amp; 0\\\\ 1002 &amp; 0\\\\ 0 &amp; 0\\\\ 2001 &amp; 0\\\\ 2001 &amp; 2001\\\\ 2002 &amp; 2001\\\\ 2002 &amp; 0\\\\ 2002 &amp; 2002 \\end{bmatrix} $</p> <p>defining in code:</p>"},{"location":"old_version/multiobjective_parameter_estimation/#note-as-mentioned-before-the-order-of-the-regressors-in-the-model-change-but-it-is-the-same-structure-the-tables-shows-the-respective-regressor-parameter-concerning-sysidentpy-and-iniciacaocientifica2007-but-the-order-1-2-and-so-on-are-not-the-same-of-the-ones-in-modelfinal_model","title":"Note: as mentioned before, the order of the regressors in the model change, but it is the same structure. The tables shows the respective regressor parameter concerning <code>SysIdentPy</code> and <code>IniciacaoCientifica2007</code>,  but the order <code>\u03b8\u2081</code>, <code>\u03b8\u2082</code> and so on are not the same of the ones in model.final_model\u00b6","text":""},{"location":"old_version/multiobjective_parameter_estimation/#biobjective-optimization","title":"Biobjective optimization\u00b6","text":""},{"location":"old_version/multiobjective_parameter_estimation/#an-use-case-applied-to-buck-converter-cc-cc-using-as-objectives-the-static-curve-information-and-the-prediction-error-dynamic","title":"An use case applied to Buck converter CC-CC using as objectives the static curve information and the prediction error (dynamic)\u00b6","text":""},{"location":"old_version/multiobjective_parameter_estimation/#multiobjective-parameter-estimation","title":"Multiobjective parameter estimation\u00b6","text":""},{"location":"old_version/multiobjective_parameter_estimation/#use-case-considering-2-different-objectives-the-prediction-error-and-the-static-gain","title":"Use case considering 2 different objectives: the prediction error and the static gain\u00b6","text":""},{"location":"old_version/multiobjective_parameter_estimation/#additional-information","title":"Additional Information\u00b6","text":"<p>You can also access the matrix Q and H using the following methods</p>"},{"location":"old_version/multiple_inputs_example/","title":"Multiple Inputs usage","text":"In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\nfrom sysidentpy.utils.generate_data import get_miso_data, get_siso_data\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) from sysidentpy.utils.generate_data import get_miso_data, get_siso_data In\u00a0[2]: Copied! <pre>x_train, x_valid, y_train, y_valid = get_miso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n</pre> x_train, x_valid, y_train, y_valid = get_miso_data(     n=1000, colored_noise=False, sigma=0.001, train_percentage=90 ) <p>There is a specific difference for multiple input data.</p> <ul> <li>You have to pass the lags for each input in a nested list (e.g., [[1, 2], [1, 2]])</li> </ul> <p>The remainder settings remains the same.</p> In\u00a0[3]: Copied! <pre>basis_function = Polynomial(degree=2)\n\nmodel = FROLS(\n    order_selection=True,\n    n_terms=4,\n    extended_least_squares=False,\n    ylag=2,\n    xlag=[[1, 2], [1, 2]],\n    info_criteria=\"aic\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n)\n</pre> basis_function = Polynomial(degree=2)  model = FROLS(     order_selection=True,     n_terms=4,     extended_least_squares=False,     ylag=2,     xlag=[[1, 2], [1, 2]],     info_criteria=\"aic\",     estimator=\"least_squares\",     basis_function=basis_function, ) <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre> In\u00a0[4]: Copied! <pre>model.fit(X=x_train, y=y_train)\n</pre> model.fit(X=x_train, y=y_train) Out[4]: <pre>&lt;sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS at 0x21b0dea3ac0&gt;</pre> In\u00a0[5]: Copied! <pre>yhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid[:, 0])\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) plot_results(y=y_valid, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_valid, yhat, x_valid[:, 0]) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>0.0028414982487199258\n       Regressors   Parameters             ERR\n0         x2(k-1)   5.9998E-01  9.04865107E-01\n1  x2(k-2)x1(k-1)  -3.0020E-01  5.13018913E-02\n2        y(k-1)^2   4.0022E-01  4.35070199E-02\n3   x1(k-1)y(k-1)   1.0013E-01  3.18875414E-04\n</pre> In\u00a0[6]: Copied! <pre>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</pre> xaxis = np.arange(1, model.n_info_values + 1) plt.plot(xaxis, model.info_values) plt.xlabel(\"n_terms\") plt.ylabel(\"Information Criteria\") Out[6]: <pre>Text(0, 0.5, 'Information Criteria')</pre>"},{"location":"old_version/multiple_inputs_example/#multiple-inputs-usage","title":"Multiple Inputs usage\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"old_version/multiple_inputs_example/#generating-2-input-1-output-sample-data","title":"Generating 2 input 1 output sample data\u00b6","text":"<p>The data is generated by simulating the following model:</p> <p>$y_k = 0.4y_{k-1}^2 + 0.1y_{k-1}x1_{k-1} + 0.6x2_{k-1} -0.3x1_{k-1}x2_{k-2} + e_{k}$</p> <p>If colored_noise is set to True:</p> <p>$e_{k} = 0.8\\nu_{k-1} + \\nu_{k}$</p> <p>where $x$ is a uniformly distributed random variable and $\\nu$ is a gaussian distributed variable with $\\mu=0$ and $\\sigma=0.001$</p>"},{"location":"old_version/multiple_inputs_example/#build-the-model","title":"Build the model\u00b6","text":""},{"location":"old_version/multiple_inputs_example/#model-evaluation","title":"Model evaluation\u00b6","text":""},{"location":"old_version/n_steps_ahead_prediction/","title":"Example: N-steps-ahead prediction - F-16 Ground Vibration Test benchmark","text":"<p>The following text was taken from the link http://www.nonlinearbenchmark.org/#F16.</p> <p>Note: The reader is reffered to the mentioned website for a complete reference concerning the experiment. For now, this notebook is just a simple example of the performance of SysIdentPy on a real world dataset. A more detailed study of this system will be published in the future.</p> <p>The F-16 Ground Vibration Test benchmark features a high order system with clearance and friction nonlinearities at the mounting interface of the payloads.</p> <p>The experimental data made available to the Workshop participants were acquired on a full-scale F-16 aircraft on the occasion of the Siemens LMS Ground Vibration Testing Master Class, held in September 2014 at the Saffraanberg military basis, Sint-Truiden, Belgium.</p> <p>During the test campaign, two dummy payloads were mounted at the wing tips to simulate the mass and inertia properties of real devices typically equipping an F-16 in \ufb02ight. The aircraft structure was instrumented with accelerometers. One shaker was attached underneath the right wing to apply input signals. The dominant source of nonlinearity in the structural dynamics was expected to originate from the mounting interfaces of the two payloads. These interfaces consist of T-shaped connecting elements on the payload side, slid through a rail attached to the wing side. A preliminary investigation showed that the back connection of the right-wing-to-payload interface was the predominant source of nonlinear distortions in the aircraft dynamics, and is therefore the focus of this benchmark study.</p> <p>A detailed formulation of the identification problem can be found here. All the provided files and information on the F-16 aircraft benchmark system are available for download here. This zip-file contains a detailed system description, the estimation and test data sets, and some pictures of the setup. The data is available in the .csv and .mat file format.</p> <p>Please refer to the F16 benchmark as:</p> <p>J.P. No\u00ebl and M. Schoukens, F-16 aircraft benchmark based on ground vibration test data, 2017 Workshop on Nonlinear System Identification Benchmarks, pp. 19-23, Brussels, Belgium, April 24-26, 2017.</p> In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) In\u00a0[2]: Copied! <pre>f_16 = pd.read_csv(r\"examples/datasets/f-16.txt\", header=None, names=[\"x1\", \"x2\", \"y\"])\n</pre> f_16 = pd.read_csv(r\"examples/datasets/f-16.txt\", header=None, names=[\"x1\", \"x2\", \"y\"]) In\u00a0[3]: Copied! <pre>f_16.shape\n</pre> f_16.shape Out[3]: <pre>(32768, 3)</pre> In\u00a0[4]: Copied! <pre>f_16[[\"x1\", \"x2\"]][0:500].plot(figsize=(12, 8))\n</pre> f_16[[\"x1\", \"x2\"]][0:500].plot(figsize=(12, 8)) Out[4]: <pre>&lt;AxesSubplot:&gt;</pre> In\u00a0[5]: Copied! <pre>f_16[\"y\"][0:2000].plot(figsize=(12, 8))\n</pre> f_16[\"y\"][0:2000].plot(figsize=(12, 8)) Out[5]: <pre>&lt;AxesSubplot:&gt;</pre> In\u00a0[6]: Copied! <pre>x1_id, x1_val = f_16[\"x1\"][0:16384].values.reshape(-1, 1), f_16[\"x1\"][\n    16384::\n].values.reshape(-1, 1)\nx2_id, x2_val = f_16[\"x2\"][0:16384].values.reshape(-1, 1), f_16[\"x2\"][\n    16384::\n].values.reshape(-1, 1)\nx_id = np.concatenate([x1_id, x2_id], axis=1)\nx_val = np.concatenate([x1_val, x2_val], axis=1)\n\ny_id, y_val = f_16[\"y\"][0:16384].values.reshape(-1, 1), f_16[\"y\"][\n    16384::\n].values.reshape(-1, 1)\n</pre> x1_id, x1_val = f_16[\"x1\"][0:16384].values.reshape(-1, 1), f_16[\"x1\"][     16384:: ].values.reshape(-1, 1) x2_id, x2_val = f_16[\"x2\"][0:16384].values.reshape(-1, 1), f_16[\"x2\"][     16384:: ].values.reshape(-1, 1) x_id = np.concatenate([x1_id, x2_id], axis=1) x_val = np.concatenate([x1_val, x2_val], axis=1)  y_id, y_val = f_16[\"y\"][0:16384].values.reshape(-1, 1), f_16[\"y\"][     16384:: ].values.reshape(-1, 1) In\u00a0[7]: Copied! <pre>x1lag = list(range(1, 10))\nx2lag = list(range(1, 10))\nx2lag\n</pre> x1lag = list(range(1, 10)) x2lag = list(range(1, 10)) x2lag Out[7]: <pre>[1, 2, 3, 4, 5, 6, 7, 8, 9]</pre> In\u00a0[8]: Copied! <pre>basis_function = Polynomial(degree=1)\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=39,\n    extended_least_squares=False,\n    ylag=20,\n    xlag=[x1lag, x2lag],\n    info_criteria=\"bic\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_id, y=y_id)\n</pre> basis_function = Polynomial(degree=1)  model = FROLS(     order_selection=True,     n_info_values=39,     extended_least_squares=False,     ylag=20,     xlag=[x1lag, x2lag],     info_criteria=\"bic\",     estimator=\"least_squares\",     basis_function=basis_function, )  model.fit(X=x_id, y=y_id) <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre> Out[8]: <pre>&lt;sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS at 0x1886711dc10&gt;</pre> In\u00a0[9]: Copied! <pre>y_hat = model.predict(X=x_val, y=y_val, steps_ahead=1)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_val, yhat=y_hat, n=1000)\n</pre> y_hat = model.predict(X=x_val, y=y_val, steps_ahead=1) rrse = root_relative_squared_error(y_val, y_hat) print(rrse) r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  plot_results(y=y_val, yhat=y_hat, n=1000) <pre>0.09610207940697058\n   Regressors   Parameters             ERR\n0      y(k-1)   1.8387E+00  9.43378253E-01\n1      y(k-2)  -1.8938E+00  1.95167599E-02\n2      y(k-3)   1.3337E+00  1.02432261E-02\n3      y(k-6)  -1.6038E+00  8.03485985E-03\n4      y(k-9)   2.6776E-01  9.27874557E-04\n5     x2(k-7)  -2.2385E+01  3.76837313E-04\n6     x1(k-1)   8.2709E+00  6.81508210E-04\n7     x2(k-3)   1.0587E+02  1.57459800E-03\n8     x1(k-8)  -3.7975E+00  7.35086279E-04\n9     x2(k-1)   8.5725E+01  4.85358786E-04\n10     y(k-7)   1.3955E+00  2.77245281E-04\n11     y(k-5)   1.3219E+00  8.64120037E-04\n12    y(k-10)  -2.9306E-01  8.51717688E-04\n13     y(k-4)  -9.5479E-01  7.23623116E-04\n14     y(k-8)  -7.1309E-01  4.44988077E-04\n15    y(k-12)  -3.0437E-01  1.49743148E-04\n16    y(k-11)   4.8602E-01  3.34613282E-04\n17    y(k-13)  -8.2442E-02  1.43738964E-04\n18    y(k-15)  -1.6762E-01  1.25546584E-04\n19    x1(k-2)  -8.9698E+00  9.76699739E-05\n20    y(k-17)   2.2036E-02  4.55983807E-05\n21    y(k-14)   2.4900E-01  1.10314107E-04\n22    y(k-19)  -6.8239E-03  1.99734771E-05\n23    x2(k-9)  -9.6265E+01  2.98523208E-05\n24    x2(k-8)   2.2620E+02  2.34402543E-04\n25    x2(k-2)  -2.3609E+02  1.04172323E-04\n26    y(k-20)  -5.4663E-02  5.37895336E-05\n27    x2(k-6)  -2.3651E+02  2.11392628E-05\n28    x2(k-4)   1.7378E+02  2.18396315E-05\n29    x1(k-7)   4.9862E+00  2.03811842E-05\n</pre> In\u00a0[10]: Copied! <pre>y_hat = model.predict(X=x_val, y=y_val, steps_ahead=5)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nplot_results(y=y_val, yhat=y_hat, n=1000)\n</pre> y_hat = model.predict(X=x_val, y=y_val, steps_ahead=5) rrse = root_relative_squared_error(y_val, y_hat) print(rrse) plot_results(y=y_val, yhat=y_hat, n=1000) <pre>0.21684728737990655\n</pre> In\u00a0[11]: Copied! <pre>y_hat = model.predict(X=x_val, y=y_val, steps_ahead=None)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nplot_results(y=y_val, yhat=y_hat, n=1000)\n</pre> y_hat = model.predict(X=x_val, y=y_val, steps_ahead=None) rrse = root_relative_squared_error(y_val, y_hat) print(rrse) plot_results(y=y_val, yhat=y_hat, n=1000) <pre>0.2910089654605337\n</pre>"},{"location":"old_version/n_steps_ahead_prediction/#example-n-steps-ahead-prediction-f-16-ground-vibration-test-benchmark","title":"Example: N-steps-ahead prediction - F-16 Ground Vibration Test benchmark\u00b6","text":"<p>Note: The following examples do not try to replicate the results of the cited manuscripts. Even the model parameters such as ylag and xlag and size of identification and validation data are not the same of the cited papers. Moreover, sampling rate adjustment and other different data preparation are not handled here.</p>"},{"location":"old_version/n_steps_ahead_prediction/#preparing-the-data","title":"Preparing the data\u00b6","text":""},{"location":"old_version/n_steps_ahead_prediction/#building-the-model","title":"Building the model\u00b6","text":""},{"location":"old_version/n_steps_ahead_prediction/#defining-the-forecasting-horizon","title":"Defining the forecasting horizon\u00b6","text":"<p>To perform a n-steps-ahead prediction you just need to set the \"steps_ahead\" argument.</p> <p>Note The default value for steps_ahead is None and it performs a infinity-steps-ahead prediction</p>"},{"location":"old_version/narx_neural_network/","title":"Building NARX Neural Network using Sysidentpy","text":"In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>from torch import nn\nfrom sysidentpy.metrics import mean_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.neural_network import NARXNN\n\nfrom sysidentpy.basis_function._basis_function import Polynomial, Fourier\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\nfrom sysidentpy.utils.narmax_tools import regressor_code\nimport torch\n</pre> from torch import nn from sysidentpy.metrics import mean_squared_error from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.neural_network import NARXNN  from sysidentpy.basis_function._basis_function import Polynomial, Fourier from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) from sysidentpy.utils.narmax_tools import regressor_code import torch In\u00a0[2]: Copied! <pre>torch.cuda.is_available()\n</pre> torch.cuda.is_available() Out[2]: <pre>False</pre> In\u00a0[3]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" print(f\"Using {device} device\") <pre>Using cpu device\n</pre> In\u00a0[4]: Copied! <pre>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.01, train_percentage=80\n)\n</pre> x_train, x_valid, y_train, y_valid = get_siso_data(     n=1000, colored_noise=False, sigma=0.01, train_percentage=80 ) In\u00a0[5]: Copied! <pre>basis_function = Polynomial(degree=1)\n\nnarx_net = NARXNN(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    loss_func=\"mse_loss\",\n    optimizer=\"Adam\",\n    epochs=2000,\n    verbose=False,\n    device=\"cuda\",\n    optim_params={\n        \"betas\": (0.9, 0.999),\n        \"eps\": 1e-05,\n    },  # optional parameters of the optimizer\n)\n</pre> basis_function = Polynomial(degree=1)  narx_net = NARXNN(     ylag=2,     xlag=2,     basis_function=basis_function,     model_type=\"NARMAX\",     loss_func=\"mse_loss\",     optimizer=\"Adam\",     epochs=2000,     verbose=False,     device=\"cuda\",     optim_params={         \"betas\": (0.9, 0.999),         \"eps\": 1e-05,     },  # optional parameters of the optimizer ) <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\neural_network\\narx_nn.py:156: UserWarning: No CUDA available. We set the device as CPU\n  self.device = self._check_cuda(device)\n</pre> <p>Since we have defined our NARXNN using $ylag=2$, $xlag=2$ and a polynomial basis function with $degree=1$, we have a regressor matrix with 4 features. We need the size of the regressor matrix to build the layers of our network. Our input data(x_train) have only one feature, but since we are creating an NARX network, a regressor matrix is built behind the scenes with new features based on the xlag and ylag.</p> <p>If you need help finding how many regressors are created behind the scenes you can use the narmax_tools function regressor_code and take the size of the regressor code generated:</p> In\u00a0[6]: Copied! <pre>basis_function = Polynomial(degree=1)\n\nregressors = regressor_code(\n    X=x_train,\n    xlag=2,\n    ylag=2,\n    model_type=\"NARMAX\",\n    model_representation=\"neural_network\",\n    basis_function=basis_function,\n)\n</pre> basis_function = Polynomial(degree=1)  regressors = regressor_code(     X=x_train,     xlag=2,     ylag=2,     model_type=\"NARMAX\",     model_representation=\"neural_network\",     basis_function=basis_function, ) In\u00a0[7]: Copied! <pre>n_features = regressors.shape[0]  # the number of features of the NARX net\nn_features\n</pre> n_features = regressors.shape[0]  # the number of features of the NARX net n_features Out[7]: <pre>4</pre> In\u00a0[8]: Copied! <pre>regressors\n</pre> regressors Out[8]: <pre>array([[1001],\n       [1002],\n       [2001],\n       [2002]])</pre> In\u00a0[9]: Copied! <pre>class NARX(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(n_features, 30)\n        self.lin2 = nn.Linear(30, 30)\n        self.lin3 = nn.Linear(30, 1)\n        self.tanh = nn.Tanh()\n\n    def forward(self, xb):\n        z = self.lin(xb)\n        z = self.tanh(z)\n        z = self.lin2(z)\n        z = self.tanh(z)\n        z = self.lin3(z)\n        return z\n</pre> class NARX(nn.Module):     def __init__(self):         super().__init__()         self.lin = nn.Linear(n_features, 30)         self.lin2 = nn.Linear(30, 30)         self.lin3 = nn.Linear(30, 1)         self.tanh = nn.Tanh()      def forward(self, xb):         z = self.lin(xb)         z = self.tanh(z)         z = self.lin2(z)         z = self.tanh(z)         z = self.lin3(z)         return z <p>We have to pass the defined network to our NARXNN estimator.</p> In\u00a0[10]: Copied! <pre>narx_net.net = NARX()\n</pre> narx_net.net = NARX() In\u00a0[11]: Copied! <pre>if device == \"cuda\":\n    narx_net.net.to(torch.device(\"cuda\"))\n</pre> if device == \"cuda\":     narx_net.net.to(torch.device(\"cuda\")) In\u00a0[12]: outputPrepend Copied! <pre>narx_net.fit(X=x_train, y=y_train, X_test=x_valid, y_test=y_valid)\n</pre> narx_net.fit(X=x_train, y=y_train, X_test=x_valid, y_test=y_valid) Out[12]: <pre>&lt;sysidentpy.neural_network.narx_nn.NARXNN at 0x2971a008fa0&gt;</pre> In\u00a0[13]: Copied! <pre>yhat = narx_net.predict(X=x_valid, y=y_valid)\n</pre> yhat = narx_net.predict(X=x_valid, y=y_valid) In\u00a0[14]: Copied! <pre>print(\"MSE: \", mean_squared_error(y_valid, yhat))\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> print(\"MSE: \", mean_squared_error(y_valid, yhat)) plot_results(y=y_valid, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>MSE:  0.00014936315019433723\n</pre> In\u00a0[15]: outputPrepend Copied! <pre>class NARX(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(n_features, 30)\n        self.lin2 = nn.Linear(30, 30)\n        self.lin3 = nn.Linear(30, 1)\n        self.tanh = nn.Tanh()\n\n    def forward(self, xb):\n        z = self.lin(xb)\n        z = self.tanh(z)\n        z = self.lin2(z)\n        z = self.tanh(z)\n        z = self.lin3(z)\n        return z\n\n\nnarx_net2 = NARXNN(\n    net=NARX(),\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    loss_func=\"mse_loss\",\n    optimizer=\"Adam\",\n    epochs=2000,\n    verbose=False,\n    optim_params={\n        \"betas\": (0.9, 0.999),\n        \"eps\": 1e-05,\n    },  # optional parameters of the optimizer\n)\n\nnarx_net2.fit(X=x_train, y=y_train)\nyhat = narx_net2.predict(X=x_valid, y=y_valid)\nprint(\"MSE: \", mean_squared_error(y_valid, yhat))\n\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> class NARX(nn.Module):     def __init__(self):         super().__init__()         self.lin = nn.Linear(n_features, 30)         self.lin2 = nn.Linear(30, 30)         self.lin3 = nn.Linear(30, 1)         self.tanh = nn.Tanh()      def forward(self, xb):         z = self.lin(xb)         z = self.tanh(z)         z = self.lin2(z)         z = self.tanh(z)         z = self.lin3(z)         return z   narx_net2 = NARXNN(     net=NARX(),     ylag=2,     xlag=2,     basis_function=basis_function,     model_type=\"NARMAX\",     loss_func=\"mse_loss\",     optimizer=\"Adam\",     epochs=2000,     verbose=False,     optim_params={         \"betas\": (0.9, 0.999),         \"eps\": 1e-05,     },  # optional parameters of the optimizer )  narx_net2.fit(X=x_train, y=y_train) yhat = narx_net2.predict(X=x_valid, y=y_valid) print(\"MSE: \", mean_squared_error(y_valid, yhat))  plot_results(y=y_valid, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_valid, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_valid, yhat, x_valid) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>MSE:  0.00017153869801798193\n</pre>"},{"location":"old_version/narx_neural_network/#building-narx-neural-network-using-sysidentpy","title":"Building NARX Neural Network using Sysidentpy\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"old_version/narx_neural_network/#series-parallel-training-and-parallel-prediction","title":"Series-Parallel Training and Parallel prediction\u00b6","text":"<p>Currently SysIdentPy support a Series-Parallel (open-loop) Feedforward Network training process, which make the training process easier. We convert the NARX network from Series-Parallel to the Parallel (closed-loop) configuration for prediction.</p> <p>Series-Parallel allows us to use Pytorch directly for training, so we can use all the power of the Pytorch library to build our NARX Neural Network model!</p> <p></p> <p>The reader is referred to the following paper for a more in depth discussion about Series-Parallel and Parallel configurations regarding NARX neural network:</p> <p>Parallel Training Considered Harmful?: Comparing series-parallel and parallel feedforward network training</p>"},{"location":"old_version/narx_neural_network/#building-a-narx-neural-network","title":"Building a NARX Neural Network\u00b6","text":"<p>First, just import the necessary packages</p>"},{"location":"old_version/narx_neural_network/#getting-the-data","title":"Getting the data\u00b6","text":"<p>The data is generated by simulating the following model:</p> <p>$y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_{k}$.</p> <p>If colored_noise is set to True:</p> <p>$e_{k} = 0.8\\nu_{k-1} + \\nu_{k}$,</p> <p>where $x$ is a uniformly distributed random variable and $\\nu$ is a gaussian distributed variable with $\\mu=0$ and $\\sigma=0.1$</p>"},{"location":"old_version/narx_neural_network/#choosing-the-narx-parameters-loss-function-and-optimizer","title":"Choosing the NARX parameters, loss function and optimizer\u00b6","text":"<p>One can create a NARXNN object and choose the maximum lag of both input and output for building the regressor matrix to serve as input of the network.</p> <p>In addition, you can choose the loss function, the optimizer, the optional parameters of the optimizer, the number of epochs.</p> <p>Because we built this feature on top of Pytorch, you can choose any of the loss function of the torch.nn.functional. Click here for a list of the loss functions you can use. You just need to pass the name of the loss function you want.</p> <p>Similarly, you can choose any of the optimizers of the torch.optim. Click here for a list of optimizers available.</p>"},{"location":"old_version/narx_neural_network/#building-the-narx-neural-network","title":"Building the NARX Neural Network\u00b6","text":"<p>The configuration of your network follows exactly the same pattern of a network defined in Pytorch. The following representing our NARX neural network.</p>"},{"location":"old_version/narx_neural_network/#fit-and-predict","title":"Fit and Predict\u00b6","text":"<p>Because we have a fit (for training) and predict function for Polynomial NARMAX, we create the same pattern for the NARX net. So, you only have to fit and predict using the following:</p>"},{"location":"old_version/narx_neural_network/#results","title":"Results\u00b6","text":"<p>Now we show the results</p>"},{"location":"old_version/narx_neural_network/#note","title":"Note\u00b6","text":"<p>If you built the net configuration before calling the NARXNN, you can just pass the model to the NARXNN as follows:</p>"},{"location":"old_version/narx_neural_network/#note","title":"Note\u00b6","text":"<p>Remember you can use n-steps-ahead prediction and NAR and NFIR models now. Check how to use it in their respective examples.</p>"},{"location":"old_version/parameter_estimation/","title":"Parameter Estimation","text":"<p>Here we import the NARMAX model, the metric for model evaluation and the methods to generate sample data for tests. Also, we import pandas for specific usage.</p> In\u00a0[1]: Copied! <pre>import pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\n</pre> import pandas as pd from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.utils.display_results import results In\u00a0[2]: Copied! <pre>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n</pre> x_train, x_valid, y_train, y_valid = get_siso_data(     n=1000, colored_noise=False, sigma=0.001, train_percentage=90 ) In\u00a0[3]: Copied! <pre>basis_function = Polynomial(degree=2)\n\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    extended_least_squares=False,\n    ylag=2,\n    xlag=2,\n    estimator=\"total_least_squares\",\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</pre> basis_function = Polynomial(degree=2)  model = FROLS(     order_selection=False,     n_terms=3,     extended_least_squares=False,     ylag=2,     xlag=2,     estimator=\"total_least_squares\",     basis_function=basis_function, ) model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre> <pre>0.002052875265903371\n      Regressors  Parameters             ERR\n0        x1(k-2)  8.9997E-01  9.56561336E-01\n1         y(k-1)  1.9994E-01  4.00308024E-02\n2  x1(k-1)y(k-1)  1.0009E-01  3.40419218E-03\n</pre> In\u00a0[4]: Copied! <pre># recursive least squares\nbasis_function = Polynomial(degree=2)\n\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    extended_least_squares=False,\n    ylag=2,\n    xlag=2,\n    estimator=\"recursive_least_squares\",\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</pre> # recursive least squares basis_function = Polynomial(degree=2)  model = FROLS(     order_selection=False,     n_terms=3,     extended_least_squares=False,     ylag=2,     xlag=2,     estimator=\"recursive_least_squares\",     basis_function=basis_function, ) model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) <pre>0.0020701726710049364\n      Regressors  Parameters             ERR\n0        x1(k-2)  9.0015E-01  9.56561336E-01\n1         y(k-1)  1.9989E-01  4.00308024E-02\n2  x1(k-1)y(k-1)  9.9799E-02  3.40419218E-03\n</pre> <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre> In\u00a0[5]: Copied! <pre># recursive least squares\nbasis_function = Polynomial(degree=2)\n\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    extended_least_squares=False,\n    ylag=2,\n    xlag=2,\n    estimator=\"least_mean_squares\",\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</pre> # recursive least squares basis_function = Polynomial(degree=2)  model = FROLS(     order_selection=False,     n_terms=3,     extended_least_squares=False,     ylag=2,     xlag=2,     estimator=\"least_mean_squares\",     basis_function=basis_function, ) model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) <pre>0.0076840766645651214\n      Regressors  Parameters             ERR\n0        x1(k-2)  8.9806E-01  9.56561336E-01\n1         y(k-1)  1.9729E-01  4.00308024E-02\n2  x1(k-1)y(k-1)  9.1871E-02  3.40419218E-03\n</pre> <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre> In\u00a0[6]: Copied! <pre># recursive least squares\nbasis_function = Polynomial(degree=2)\n\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    extended_least_squares=False,\n    ylag=2,\n    xlag=2,\n    estimator=\"affine_least_mean_squares\",\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</pre> # recursive least squares basis_function = Polynomial(degree=2)  model = FROLS(     order_selection=False,     n_terms=3,     extended_least_squares=False,     ylag=2,     xlag=2,     estimator=\"affine_least_mean_squares\",     basis_function=basis_function, ) model.fit(X=x_train, y=y_train) yhat = model.predict(X=x_valid, y=y_valid) rrse = root_relative_squared_error(y_valid, yhat) print(rrse)  r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r) <pre>0.0020846666245813036\n      Regressors  Parameters             ERR\n0        x1(k-2)  8.9986E-01  9.56561336E-01\n1         y(k-1)  1.9991E-01  4.00308024E-02\n2  x1(k-1)y(k-1)  1.0007E-01  3.40419218E-03\n</pre> <pre>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:37: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use FROLS(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning)\n</pre>"},{"location":"old_version/parameter_estimation/#parameter-estimation","title":"Parameter Estimation\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"old_version/parameter_estimation/#generating-1-input-1-output-sample-data","title":"Generating 1 input 1 output sample data\u00b6","text":"<p>The data is generated by simulating the following model:</p> <p>$y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_{k}$</p> <p>If colored_noise is set to True:</p> <p>$e_{k} = 0.8\\nu_{k-1} + \\nu_{k}$</p> <p>where $x$ is a uniformly distributed random variable and $\\nu$ is a gaussian distributed variable with $\\mu=0$ and $\\sigma=0.1$</p> <p>In the next example we will generate a data with 1000 samples with white noise and selecting 90% of the data to train the model.</p>"},{"location":"old_version/parameter_estimation/#there-are-several-method-to-be-used-for-parameter-estimation","title":"There are several method to be used for parameter estimation.\u00b6","text":"<ul> <li>least_squares;</li> <li>total_least_squares;</li> <li>recursive_least_squares;</li> <li>least_mean_squares</li> <li>affine_least_mean_squares</li> <li>least_mean_squares_sign_error</li> <li>normalized_least_mean_squares</li> <li>least_mean_squares_normalized_sign_error</li> <li>least_mean_squares_sign_regressor</li> <li>least_mean_squares_normalized_sign_regressor</li> <li>least_mean_squares_sign_sign</li> <li>least_mean_squares_normalized_sign_sign</li> <li>least_mean_squares_normalized_leaky</li> <li>least_mean_squares_leaky</li> <li>least_mean_squares_fourth</li> <li>least_mean_squares_mixed_norm</li> </ul> <p>Polynomial NARMAX models are linear-in-the-parameter, so Least Squares based methods works well for most cases (using with extended least squares algorithm when dealing with colered noise).</p> <p>However, the user can choose some recursive and stochastic gradient descent methods (in this case, the least mean squares algorithm and its variants) to that task too.</p> <p>Choosing the method is straightforward: pass any of the methods mentioned above on estimator parameters.</p> <ul> <li>Note: The adaptative filters have specifc parameter that need to be tunned. In the following examples we will use the default ones. More examples regarding tunned parameter will be available soon. For now, the user can read the method documentation for more information.</li> </ul>"},{"location":"old_version/parameter_estimation/#total-least-squares","title":"Total Least Squares\u00b6","text":""},{"location":"old_version/parameter_estimation/#recursive-least-squares","title":"Recursive Least Squares\u00b6","text":""},{"location":"old_version/parameter_estimation/#least-mean-squares","title":"Least Mean Squares\u00b6","text":""},{"location":"old_version/parameter_estimation/#affine-least-mean-squares","title":"Affine Least Mean Squares\u00b6","text":""},{"location":"old_version/save_and_load_models/","title":"Saving and Loading models using .syspy extension","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.utils.save_load import save_model, load_model\n\n\n# Generating 1 input 1 output sample data from a benchmark system\nx_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.0001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=3,\n    extended_least_squares=False,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\n\nyhat = model.predict(X=x_valid, y=y_valid)\n\n# Gathering results\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n</pre> import pandas as pd from sysidentpy.model_structure_selection import FROLS from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_results from sysidentpy.utils.save_load import save_model, load_model   # Generating 1 input 1 output sample data from a benchmark system x_train, x_valid, y_train, y_valid = get_siso_data(     n=1000, colored_noise=False, sigma=0.0001, train_percentage=90 )  basis_function = Polynomial(degree=2)  model = FROLS(     order_selection=True,     n_info_values=3,     extended_least_squares=False,     ylag=2,     xlag=2,     info_criteria=\"aic\",     estimator=\"least_squares\",     basis_function=basis_function, )  model.fit(X=x_train, y=y_train)  yhat = model.predict(X=x_valid, y=y_valid)  # Gathering results r = pd.DataFrame(     results(         model.final_model,         model.theta,         model.err,         model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) In\u00a0[2]: Copied! <pre># save_model(model_variable, file_name.syspy, path (optional))\nsave_model(model=model, file_name=\"model_name.syspy\")\n</pre> # save_model(model_variable, file_name.syspy, path (optional)) save_model(model=model, file_name=\"model_name.syspy\") In\u00a0[3]: Copied! <pre># load_model(file_name.syspy, path (optional))\nloaded_model = load_model(file_name=\"model_name.syspy\")\n\n# Predicting output with loaded_model\nyhat_loaded = loaded_model.predict(X=x_valid, y=y_valid)\n\nr_loaded = pd.DataFrame(\n    results(\n        loaded_model.final_model,\n        loaded_model.theta,\n        loaded_model.err,\n        loaded_model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\n# Printing both: original model and model loaded from file\nprint(\"\\n Original model \\n\", r)\nprint(\"\\n Model Loaded from file \\n\", r_loaded)\n\n# Checking predictions from both: original model and model loaded from file\nif (yhat == yhat_loaded).all():\n    print(\"\\n Predictions are the same!\")\n\n# Ploting results\nplot_results(y=y_valid, yhat=yhat_loaded, n=1000)\n</pre> # load_model(file_name.syspy, path (optional)) loaded_model = load_model(file_name=\"model_name.syspy\")  # Predicting output with loaded_model yhat_loaded = loaded_model.predict(X=x_valid, y=y_valid)  r_loaded = pd.DataFrame(     results(         loaded_model.final_model,         loaded_model.theta,         loaded_model.err,         loaded_model.n_terms,         err_precision=8,         dtype=\"sci\",     ),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], )  # Printing both: original model and model loaded from file print(\"\\n Original model \\n\", r) print(\"\\n Model Loaded from file \\n\", r_loaded)  # Checking predictions from both: original model and model loaded from file if (yhat == yhat_loaded).all():     print(\"\\n Predictions are the same!\")  # Ploting results plot_results(y=y_valid, yhat=yhat_loaded, n=1000) <pre>\n Original model \n       Regressors  Parameters             ERR\n0        x1(k-2)  9.0000E-01  9.57022506E-01\n1         y(k-1)  1.9999E-01  3.96079210E-02\n2  x1(k-1)y(k-1)  1.0001E-01  3.36953837E-03\n\n Model Loaded from file \n       Regressors  Parameters             ERR\n0        x1(k-2)  9.0000E-01  9.57022506E-01\n1         y(k-1)  1.9999E-01  3.96079210E-02\n2  x1(k-1)y(k-1)  1.0001E-01  3.36953837E-03\n\n Predictions are the same!\n</pre>"},{"location":"old_version/save_and_load_models/#saving-and-loading-models-using-syspy-extension","title":"Saving and Loading models using .syspy extension\u00b6","text":"<p>Example created by Samir Angelo Milani Martins</p>"},{"location":"old_version/save_and_load_models/#obtaining-the-model-using-frols","title":"Obtaining the model using FROLS.\u00b6","text":""},{"location":"old_version/save_and_load_models/#saving-obtained-model-in-file-model_namesyspy","title":"Saving obtained model in file \"model_name.syspy\"\u00b6","text":""},{"location":"old_version/save_and_load_models/#loading-model-and-checking-if-everything-went-smoothly","title":"Loading model and checking if everything went smoothly\u00b6","text":""},{"location":"old_version/simulating_a_predefined_model/","title":"Simulate a Predefined Model","text":"In\u00a0[\u00a0]: Copied! <pre>pip install sysidentpy\n</pre> pip install sysidentpy In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom sysidentpy.simulation import SimulateNARMAX\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.basis_function._basis_function import Polynomial\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</pre> import numpy as np import pandas as pd from sysidentpy.simulation import SimulateNARMAX from sysidentpy.metrics import root_relative_squared_error from sysidentpy.utils.generate_data import get_siso_data from sysidentpy.basis_function._basis_function import Polynomial from sysidentpy.utils.display_results import results from sysidentpy.utils.plotting import plot_residues_correlation, plot_results from sysidentpy.residues.residues_correlation import (     compute_residues_autocorrelation,     compute_cross_correlation, ) In\u00a0[2]: Copied! <pre>x_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n</pre> x_train, x_test, y_train, y_test = get_siso_data(     n=1000, colored_noise=False, sigma=0.001, train_percentage=90 ) In\u00a0[3]: Copied! <pre>s = SimulateNARMAX(\n    basis_function=Polynomial(),\n    calculate_err=True,\n    estimate_parameter=False,\n    extended_least_squares=True,\n)\n\n# the model must be a numpy array\nmodel = np.array(\n    [\n        [1001, 0],  # y(k-1)\n        [2001, 1001],  # x1(k-1)y(k-1)\n        [2002, 0],  # x1(k-2)\n    ]\n)\n# theta must be a numpy array of shape (n, 1) where n is the number of regressors\ntheta = np.array([[0.2, 0.9, 0.1]]).T\n</pre> s = SimulateNARMAX(     basis_function=Polynomial(),     calculate_err=True,     estimate_parameter=False,     extended_least_squares=True, )  # the model must be a numpy array model = np.array(     [         [1001, 0],  # y(k-1)         [2001, 1001],  # x1(k-1)y(k-1)         [2002, 0],  # x1(k-2)     ] ) # theta must be a numpy array of shape (n, 1) where n is the number of regressors theta = np.array([[0.2, 0.9, 0.1]]).T In\u00a0[4]: Copied! <pre>yhat = s.simulate(\n    X_test=x_test,\n    y_test=y_test,\n    model_code=model,\n    theta=theta,\n)\n\nr = pd.DataFrame(\n    results(s.final_model, s.theta, s.err, s.n_terms, err_precision=8, dtype=\"sci\"),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_test, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_test, yhat, x_test)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> yhat = s.simulate(     X_test=x_test,     y_test=y_test,     model_code=model,     theta=theta, )  r = pd.DataFrame(     results(s.final_model, s.theta, s.err, s.n_terms, err_precision=8, dtype=\"sci\"),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  plot_results(y=y_test, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_test, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_test, yhat, x_test) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>      Regressors  Parameters             ERR\n0         y(k-1)  2.0000E-01  0.00000000E+00\n1        x1(k-2)  9.0000E-01  0.00000000E+00\n2  x1(k-1)y(k-1)  1.0000E-01  0.00000000E+00\n</pre> In\u00a0[5]: Copied! <pre>yhat = s.simulate(\n    X_test=x_test,\n    y_test=y_test,\n    model_code=model,\n    theta=theta,\n    steps_ahead=1,\n)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n</pre> yhat = s.simulate(     X_test=x_test,     y_test=y_test,     model_code=model,     theta=theta,     steps_ahead=1, ) rrse = root_relative_squared_error(y_test, yhat) print(rrse) <pre>0.0018510653044857056\n</pre> In\u00a0[6]: Copied! <pre>yhat = s.simulate(\n    X_test=x_test,\n    y_test=y_test,\n    model_code=model,\n    theta=theta,\n    steps_ahead=21,\n)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n</pre> yhat = s.simulate(     X_test=x_test,     y_test=y_test,     model_code=model,     theta=theta,     steps_ahead=21, ) rrse = root_relative_squared_error(y_test, yhat) print(rrse) <pre>0.001899504046505141\n</pre> In\u00a0[7]: Copied! <pre>s = SimulateNARMAX(\n    basis_function=Polynomial(),\n    estimate_parameter=True,\n    estimator=\"least_squares\",\n    calculate_err=True,\n)\n\nyhat = s.simulate(\n    X_train=x_train,\n    y_train=y_train,\n    X_test=x_test,\n    y_test=y_test,\n    model_code=model,\n    # theta will be estimated using the defined estimator\n)\n\nr = pd.DataFrame(\n    results(s.final_model, s.theta, s.err, s.n_terms, err_precision=8, dtype=\"sci\"),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_test, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_test, yhat, x_test)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</pre> s = SimulateNARMAX(     basis_function=Polynomial(),     estimate_parameter=True,     estimator=\"least_squares\",     calculate_err=True, )  yhat = s.simulate(     X_train=x_train,     y_train=y_train,     X_test=x_test,     y_test=y_test,     model_code=model,     # theta will be estimated using the defined estimator )  r = pd.DataFrame(     results(s.final_model, s.theta, s.err, s.n_terms, err_precision=8, dtype=\"sci\"),     columns=[\"Regressors\", \"Parameters\", \"ERR\"], ) print(r)  plot_results(y=y_test, yhat=yhat, n=1000) ee = compute_residues_autocorrelation(y_test, yhat) plot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\") x1e = compute_cross_correlation(y_test, yhat, x_test) plot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\") <pre>      Regressors  Parameters             ERR\n0         y(k-1)  2.0003E-01  9.61188901E-01\n1        x1(k-2)  9.0000E-01  3.55184583E-02\n2  x1(k-1)y(k-1)  1.0001E-01  3.28896573E-03\n</pre>"},{"location":"old_version/simulating_a_predefined_model/#simulate-a-predefined-model","title":"Simulate a Predefined Model\u00b6","text":"<p>Example created by Wilson Rocha Lacerda Junior</p>"},{"location":"old_version/simulating_a_predefined_model/#generating-1-input-1-output-sample-data","title":"Generating 1 input 1 output sample data\u00b6","text":""},{"location":"old_version/simulating_a_predefined_model/#the-data-is-generated-by-simulating-the-following-model","title":"The data is generated by simulating the following model:\u00b6","text":"<p>$y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-2} + e_{k}$</p> <p>If colored_noise is set to True:</p> <p>$e_{k} = 0.8\\nu_{k-1} + \\nu_{k}$</p> <p>where $x$ is a uniformly distributed random variable and $\\nu$ is a gaussian distributed variable with $\\mu=0$ and $\\sigma=0.1$</p> <p>In the next example we will generate a data with 1000 samples with white noise and selecting 90% of the data to train the model.</p>"},{"location":"old_version/simulating_a_predefined_model/#defining-the-model","title":"Defining the model\u00b6","text":"<p>We already know that the generated data is a result of the model  $\ud835\udc66_\ud835\udc58=0.2\ud835\udc66_{\ud835\udc58\u22121}+0.1\ud835\udc66_{\ud835\udc58\u22121}\ud835\udc65_{\ud835\udc58\u22121}+0.9\ud835\udc65_{\ud835\udc58\u22122}+\ud835\udc52_\ud835\udc58$ . Thus, we can create a model with those regressors follwing a codification pattern:</p> <ul> <li>$0$ is the constant term,</li> <li>$[1001] = y_{k-1}$</li> <li>$[100n] = y_{k-n}$</li> <li>$[200n] = x1_{k-n}$</li> <li>$[300n] = x2_{k-n}$</li> <li>$[1011, 1001] = y_{k-11} \\times y_{k-1}$</li> <li>$[100n, 100m] = y_{k-n} \\times y_{k-m}$</li> <li>$[12001, 1003, 1001] = x11_{k-1} \\times y_{k-3} \\times y_{k-1}$</li> <li>and so on</li> </ul>"},{"location":"old_version/simulating_a_predefined_model/#important-note","title":"Important Note\u00b6","text":"<p>The order of the arrays matter.</p> <p>If you use [2001, 1001], it will work, but [1001, 2001] will not (the regressor will be ignored). Always put the highest value first:</p> <ul> <li>$[2003, 2001]$ works</li> <li>$[2001, 2003]$ do not work</li> </ul> <p>We will handle this limitation in upcoming update.</p>"},{"location":"old_version/simulating_a_predefined_model/#simulating-the-model","title":"Simulating the model\u00b6","text":"<p>After defining the model and theta we just need to use the simulate method.</p> <p>The simulate method returns the predicted values and the results where we can look at regressors, parameters and ERR values.</p>"},{"location":"old_version/simulating_a_predefined_model/#options","title":"Options\u00b6","text":"<p>You can set the <code>steps_ahead</code> to run the prediction/simulation:</p>"},{"location":"old_version/simulating_a_predefined_model/#estimating-the-parameters","title":"Estimating the parameters\u00b6","text":"<p>If you have only the model strucuture, you can create an object with <code>estimate_parameter=True</code> and choose the methed for estimation using <code>estimator</code>. In this case, you have to pass the training data for parameters estimation.</p> <p>When <code>estimate_parameter=True</code>, we also computate the ERR considering only the regressors defined by the user.</p>"}]}